keywords:
  positive:
    - affordance
    - imagin
    - encode
    - robot
    - embodied
    - agent
    - task
    - egocentric
    - emergent
  negative:
    - large language model
    - llm
    - supervised
    - label
    - discretize
  neutral:
    - visual
    - predict
    - centric
    - learn
    - transformer
    - unseen
    - embedding
    - latent
references:
  Visual affordance prediction for guiding robot exploration: &ref_0
    title: Visual affordance prediction for guiding robot exploration
    resumeStatus: 'relevent:abstract'
    reasonsNotRelevant: []
    relevanceStages:
      - title
      - abstract
    possibleYear: '2023'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - H Bharadhwaj
      - A Gupta
    pdfLink: 'https://ieeexplore.ieee.org/iel7/10160211/10160212/10161288.pdf'
    link: 'https://ieeexplore.ieee.org/abstract/document/10161288/'
    citationId: '15979805052095820058'
    multiArticleId: '15979805052095820058'
    citedByLink: 'https://scholar.google.com//scholar?cites=15979805052095820058&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: '2023 IEEE International, 2023'
    events:
      saw title: '2024-12-12T16:30:43.287Z'
    abstract: 'Motivated by the intuitive understanding humans have about the space of possible interactions, and the ease with which they can generalize this understanding to previously unseen scenes, we develop an approach for learning ‘visual affordances’. Given an input image of a scene, we infer a distribution over plausible future states that can be achieved via interactions with it. To allow predicting diverse plausible futures, we discretize the space of continuous images with a VQ-VAE and use a Transformer-based model to learn a conditional distribution in the latent embedding space. We show that these models can be trained using large-scale and diverse passive data, and that the learned models exhibit compositional generalization to diverse objects beyond the training distribution. We evaluate the quality and diversity of the generations, and demonstrate how the trained affordance model can be used for guiding exploration during visual goal-conditioned policy learning in robotic manipulation.'
    doi: 10.1109/icra48891.2023.10161288
  'RAIL: Robot Affordance Imagination with Large Language Models': &ref_1
    title: 'RAIL: Robot Affordance Imagination with Large Language Models'
    resumeStatus: 'unclear:title'
    reasonsNotRelevant: []
    relevanceStages: []
    possibleYear: '1936'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - C Zhang
      - X Meng
      - D Qi
      - GS Chirikjian�
    pdfLink: 'https://arxiv.org/pdf/2403.19369'
    link: 'https://arxiv.org/abs/2403.19369'
    citationId: '8172269612940938567'
    multiArticleId: '8172269612940938567'
    citedByLink: 'https://scholar.google.com//scholar?cites=8172269612940938567&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: 'arXiv preprint arXiv:2403.19369, 2024'
    events:
      saw title: '2024-12-12T16:30:58.343Z'
  Affordance-Based Goal Imagination for Embodied AI Agents: &ref_2
    title: Affordance-Based Goal Imagination for Embodied AI Agents
    resumeStatus: 'irrelevent:abstract'
    reasonsNotRelevant:
      - abstract
    relevanceStages:
      - title
    possibleYear: '2024'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - V Aregbede
      - SS Abraham
      - A Persson
    pdfLink: 'https://ieeexplore.ieee.org/iel8/10644131/10644157/10644764.pdf'
    link: 'https://ieeexplore.ieee.org/abstract/document/10644764/'
    citationId: null
    multiArticleId: null
    citedByLink: null
    publisherInfo: 'on Development and, 2024'
    events:
      saw title: '2024-12-12T16:31:01.901Z'
    abstract: 'Goal imagination in robotics is an emerging concept and involves the capability to automatically generate realistic goals, which, in turn, requires the assessment of the feasibility of transitioning from the current conditions of an initial scene to the desired goal state. Existing research has explored the utilization of diverse image-generative models to create images depicting potential goal states based on the current state and instructions. In this paper, we illustrate the limitations of current state-of-the-art image generative models in accurately assessing the feasibility of specific actions in particular situations. Consequently, we present how integrating large language models, which possess profound knowledge of real-world objects and affordances, can enhance the performance of image-generative models in discerning plausible from implausible actions and simulating the outcomes of actions in a given context. This will be a step towards achieving the pragmatic goal of imagination in robotics.'
    doi: 10.1109/icdl61372.2024.10644764
  Learning to anticipate egocentric actions by imagination: &ref_3
    title: Learning to anticipate egocentric actions by imagination
    resumeStatus: 'unclear:title'
    reasonsNotRelevant: []
    relevanceStages: []
    possibleYear: '2020'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - Y Wu
      - L Zhu
      - X Wang
      - Y Yang
    pdfLink: 'https://ieeexplore.ieee.org/iel7/83/9263394/09280353.pdf'
    link: 'https://ieeexplore.ieee.org/abstract/document/9280353/'
    citationId: '6012752103031775791'
    multiArticleId: '6012752103031775791'
    citedByLink: 'https://scholar.google.com//scholar?cites=6012752103031775791&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: 'IEEE Transactions on, 2020'
    events:
      saw title: '2024-12-12T21:30:37.470Z'
    doi: 10.1109/tip.2020.3040521
  Imagine that! Leveraging emergent affordances for 3d tool synthesis: &ref_4
    title: Imagine that! Leveraging emergent affordances for 3d tool synthesis
    resumeStatus: 'relevent:abstract'
    reasonsNotRelevant: []
    relevanceStages:
      - title
      - abstract
    possibleYear: '2019'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - Y Wu
      - S Kasewa
      - O Groth
      - S Salter
      - L Sun
    pdfLink: 'https://arxiv.org/pdf/1909.13561'
    link: 'https://arxiv.org/abs/1909.13561'
    citationId: '1893778644382906900'
    multiArticleId: '1893778644382906900'
    citedByLink: 'https://scholar.google.com//scholar?cites=1893778644382906900&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: 'arXiv preprint arXiv, 2019'
    events:
      saw title: '2024-12-12T21:30:43.250Z'
    abstract: '     In this paper we explore the richness of information captured by the latent space of a vision-based generative model. The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool. While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand. Our results indicate that affordances-like the utility for reaching-are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way. '
  What can i do here? learning new skills by imagining visual affordances: &ref_5
    title: What can i do here? learning new skills by imagining visual affordances
    resumeStatus: 'relevent:title'
    reasonsNotRelevant: []
    relevanceStages:
      - title
    possibleYear: '2021'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - A Khazatsky
      - A Nair
      - D Jing
    pdfLink: 'https://ieeexplore.ieee.org/iel7/9560720/9560666/09561692.pdf'
    link: 'https://ieeexplore.ieee.org/abstract/document/9561692/'
    citationId: '15430638141641676250'
    multiArticleId: '15430638141641676250'
    citedByLink: 'https://scholar.google.com//scholar?cites=15430638141641676250&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: '2021 IEEE International, 2021'
    events:
      saw title: '2024-12-12T21:40:37.271Z'
    doi: 10.1109/icra48506.2021.9561692
  'Learning to act properly: Predicting and explaining affordances from images': &ref_6
    title: 'Learning to act properly: Predicting and explaining affordances from images'
    resumeStatus: 'relevent:title'
    reasonsNotRelevant: []
    relevanceStages:
      - title
    possibleYear: '2018'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - CY Chuang
      - J Li
      - A Torralba
    pdfLink: 'https://openaccess.thecvf.com/content_cvpr_2018/papers/Chuang_Learning_to_Act_CVPR_2018_paper.pdf'
    link: 'http://openaccess.thecvf.com/content_cvpr_2018/html/Chuang_Learning_to_Act_CVPR_2018_paper.html'
    citationId: '9413040909214203914'
    multiArticleId: '9413040909214203914'
    citedByLink: 'https://scholar.google.com//scholar?cites=9413040909214203914&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: 'Proceedings of the IEEE, 2018'
    events:
      saw title: '2024-12-12T21:41:04.424Z'
    doi: 10.1109/cvpr.2018.00108
  Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks: &ref_7
    title: Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks
    resumeStatus: 'relevent:title'
    reasonsNotRelevant: []
    relevanceStages:
      - title
    possibleYear: '2019'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - Y Wu
      - S Kasewa
      - O Groth
      - S Salter
      - L Sun
      - OP Jones…
    pdfLink: 'https://openreview.net/pdf?id=BkeyOxrYwH'
    link: 'https://openreview.net/forum?id=BkeyOxrYwH'
    citationId: '2961549995427504858'
    multiArticleId: null
    citedByLink: 'https://scholar.google.com//scholar?cites=2961549995427504858&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: '2019'
    events:
      saw title: '2024-12-12T21:41:38.912Z'
  'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text': &ref_8
    title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
    resumeStatus: 'relevent:title'
    reasonsNotRelevant: []
    relevanceStages:
      - title
    possibleYear: '2024'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - S Adak
      - D Agrawal
      - A Mukherjee
    pdfLink: 'https://aclanthology.org/2024.conll-1.27.pdf'
    link: 'https://aclanthology.org/2024.conll-1.27/'
    citationId: null
    multiArticleId: null
    citedByLink: null
    publisherInfo: 'Proceedings of the 28th, 2024'
    events:
      saw title: '2024-12-12T21:43:23.737Z'
    doi: 10.18653/v1/2024.conll-1.27
  Learning affordances in object-centric generative models: &ref_9
    title: Learning affordances in object-centric generative models
    resumeStatus: 'relevent:title'
    reasonsNotRelevant: []
    relevanceStages:
      - title
    possibleYear: '2020'
    notesConsideredRelevent: null
    notesCustomKeywords: []
    notesComment: null
    notesWasRelatedTo: []
    notesIsCitedByTitles: []
    notesCites: []
    discoveryMethod: null
    authorNames:
      - Y Wu
      - S Kasewa
      - O Groth
      - S Salter
      - L Sun…
    pdfLink: 'https://ora.ox.ac.uk/objects/uuid:003cbbd9-a3aa-42e7-8e2d-bcc6b22db89a/download_file?safe_filename=OOL_7.pdf&file_format=pdf&type_of_work=Conference+item'
    link: 'https://ora.ox.ac.uk/objects/uuid:003cbbd9-a3aa-42e7-8e2d-bcc6b22db89a'
    citationId: '7478229689628350853'
    multiArticleId: '7478229689628350853'
    citedByLink: 'https://scholar.google.com//scholar?cites=7478229689628350853&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
    publisherInfo: '2020'
    events:
      saw title: '2024-12-12T21:43:33.262Z'
discoveryAttempts:
  - query: affordance based encoder imagination prediction
    date: 12/12/2024
    dateTime: 1734020081395
    searchEngine: scholar
    referenceLinks:
      - hadBeenSeenBefore: false
        title: Visual affordance prediction for guiding robot exploration
        link: *ref_0
      - hadBeenSeenBefore: false
        title: 'RAIL: Robot Affordance Imagination with Large Language Models'
        link: *ref_1
      - hadBeenSeenBefore: false
        title: Affordance-Based Goal Imagination for Embodied AI Agents
        link: *ref_2
      - hadBeenSeenBefore: false
        title: Learning to anticipate egocentric actions by imagination
        link: *ref_3
      - hadBeenSeenBefore: false
        title: Imagine that! Leveraging emergent affordances for 3d tool synthesis
        link: *ref_4
      - hadBeenSeenBefore: false
        title: What can i do here? learning new skills by imagining visual affordances
        link: *ref_5
      - hadBeenSeenBefore: false
        title: 'Learning to act properly: Predicting and explaining affordances from images'
        link: *ref_6
      - hadBeenSeenBefore: false
        title: Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks
        link: *ref_7
      - hadBeenSeenBefore: false
        title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
        link: *ref_8
      - hadBeenSeenBefore: false
        title: Learning affordances in object-centric generative models
        link: *ref_9
