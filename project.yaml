keywords:
    positive:
        - affordance
        - imagin
        - encode
        - robot
        - embodied
        - agent
        - task
        - egocentric
        - emergent
        - ecological
        - ecology
        - self-supervised
        - percept
        - unsupervised
        - visual cortex
    negative:
        - large language model
        - language model
        - llm
        - supervised
        - label
        - discretize
    neutral:
        - visual
        - predict
        - centric
        - learn
        - transformer
        - unseen
        - embedding
        - latent
        - multimodal
        - behavior
        - mice
        - mouse
        - animal
        - biolog
        - primate
references:
    Visual Affordance Prediction for Guiding Robot Exploration: &ref_10
        notes:
            resumeStatus: relevent|title
            comment: ''
            reasonsRelevant:
                - title
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:30:42.560Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: Visual Affordance Prediction for Guiding Robot Exploration
                subtitle: null
                abstract: null
                authorNames:
                    - Homanga Bharadhwaj
                    - Abhinav Gupta
                    - Shubham Tulsiani
                year: 2023
                doi: 10.1109/icra48891.2023.10161288
                url: 'https://doi.org/10.1109/icra48891.2023.10161288'
                pdfLink: null
                citationCount: 7
                citedDois:
                    - 10.1109/CVPRW.2017.70
                    - 10.1007/978-3-642-33715-4_53
                    - 10.1109/ICCV.2017.622
                    - 10.1109/ICRA.2016.7487517
                    - 10.1109/ICCV.2019.00878
                    - 10.1109/CVPR.2019.00891
                    - 10.1109/ICCV.2017.438
                    - 10.1109/CVPR.2011.5995448
                    - 10.1109/IROS40897.2019.8967784
                    - 10.1109/ICRA48506.2021.9561692
                    - 10.1109/CVPR.2018.00068
                    - 10.1109/ICRA40945.2020.9197415
                    - 10.1109/IROS.2018.8593986
                    - 10.1109/ICCV.2017.244
                    - 10.1109/CVPR.2017.632
                    - 10.1109/CVPR.2017.359
                    - 10.1109/CVPR42600.2020.00813
                    - 10.7717/peerj.1058
                    - 10.1109/CVPR.2019.00453
                    - 10.1109/ICRA48506.2021.9561802
                    - 10.1109/ICCV48922.2021.00674
                    - 10.1109/CVPR46437.2021.01268
            googleScholar:
                title: Visual affordance prediction for guiding robot exploration
                authorNames:
                    - H Bharadhwaj
                    - A Gupta
                pdfLink: 'https://ieeexplore.ieee.org/iel7/10160211/10160212/10161288.pdf'
                link: 'https://ieeexplore.ieee.org/abstract/document/10161288/'
                citationId: '15979805052095820058'
                multiArticleId: '15979805052095820058'
                citedByLink: 'https://scholar.google.com//scholar?cites=15979805052095820058&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: '2023 IEEE International, 2023'
                year: '2023'
        publisherInfo: ''
    'RAIL: Robot Affordance Imagination with Large Language Models': &ref_11
        notes:
            resumeStatus: unclear|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:39:28.970Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: null
            googleScholar:
                title: 'RAIL: Robot Affordance Imagination with Large Language Models'
                authorNames:
                    - C Zhang
                    - X Meng
                    - D Qi
                    - GS Chirikjian
                pdfLink: 'https://arxiv.org/pdf/2403.19369'
                link: 'https://arxiv.org/abs/2403.19369'
                citationId: '8172269612940938567'
                multiArticleId: '8172269612940938567'
                citedByLink: 'https://scholar.google.com//scholar?cites=8172269612940938567&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'arXiv preprint arXiv:2403.19369, 2024'
                year: '1936'
        publisherInfo: ''
    Affordance-Based Goal Imagination for Embodied AI Agents: &ref_12
        notes:
            resumeStatus: relevent|title
            comment: ''
            reasonsRelevant:
                - title
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:30:36.897Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: Affordance-Based Goal Imagination for Embodied AI Agents
                subtitle: null
                abstract: null
                authorNames:
                    - Victor Aregbede
                    - Savitha Sam Abraham
                    - Andreas Persson
                    - Martin Längkvist
                    - Amy Loutfi
                year: 2024
                doi: 10.1109/icdl61372.2024.10644764
                url: 'https://doi.org/10.1109/icdl61372.2024.10644764'
                pdfLink: null
                citationCount: 0
                citedDois:
                    - 10.1016/j.cobeha.2018.12.011
                    - 10.1109/DEVLRN.2019.8850723
                    - 10.1109/LRA.2023.3272516
                    - 10.1006/nimg.2001.0832
                    - 10.1109/DEVLRN.2007.4354054
                    - 10.1109/ICRA48506.2021.9561802
                    - 10.1177/0278364914549607
                    - 10.1007/978-3-031-24628-9_16
                    - 10.1016/j.robot.2020.103630
                    - 10.1109/IJCNN.2019.8852254
                    - 10.1109/CVPR52729.2023.00582
                    - 10.15607/RSS.2023.XIX.010
                    - 10.1109/TCDS.2019.2915763
                    - 10.1109/CVPR52729.2023.01436
                    - 10.1109/ICCV48922.2021.01091
                    - 10.1609/aaai.v36i3.20215
                    - 10.18653/v1/2022.emnlp-main.812
                    - 10.1109/ICCV51070.2023.00371
                    - 10.1109/ICRA48891.2023.10161288
            googleScholar:
                title: Affordance-Based Goal Imagination for Embodied AI Agents
                authorNames:
                    - V Aregbede
                    - SS Abraham
                    - A Persson
                pdfLink: 'https://ieeexplore.ieee.org/iel8/10644131/10644157/10644764.pdf'
                link: 'https://ieeexplore.ieee.org/abstract/document/10644764/'
                citationId: null
                multiArticleId: null
                citedByLink: null
                publisherInfo: 'on Development and, 2024'
                year: '2024'
        publisherInfo: ''
    Learning to Anticipate Egocentric Actions by Imagination: &ref_13
        notes:
            resumeStatus: unclear|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:34:23.116Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: Learning to Anticipate Egocentric Actions by Imagination
                subtitle: null
                abstract: null
                authorNames:
                    - Yu Wu
                    - Linchao Zhu
                    - Xiaohan Wang
                    - Yi Yang
                    - Fei Wu
                year: 2021
                doi: 10.1109/tip.2020.3040521
                url: 'https://doi.org/10.1109/tip.2020.3040521'
                pdfLink: null
                citationCount: 43
                citedDois:
                    - 10.1109/ICCV.2017.399
                    - 10.1109/ICCV.2017.326
                    - 10.1109/TIP.2019.2937757
                    - 10.5244/C.31.92
                    - 10.1109/TIP.2020.3021497
                    - 10.1109/ICCV.2017.616
                    - 10.1016/j.jvcir.2017.10.004
                    - 10.1109/ICCV.2017.362
                    - 10.1109/CVPR.2018.00560
                    - 10.1109/CVPR.2019.00587
                    - 10.1109/TMM.2018.2834873
                    - 10.1109/CVPR.2019.00038
                    - 10.1007/978-3-030-01219-9_26
                    - 10.1109/ICCV.2017.39
                    - 10.1109/TPAMI.2020.3015894
                    - 10.1007/978-3-319-46454-1_17
                    - 10.1109/CVPR.2016.214
                    - 10.1109/WACV.2018.00173
                    - 10.1162/neco.1997.9.8.1735
                    - 10.1109/ICRA.2016.7487478
                    - 10.1109/CVPR.2016.18
                    - 10.1109/TPAMI.2015.2430335
                    - 10.1109/TIP.2019.2891895
                    - 10.1109/CVPR.2018.00393
                    - 10.1109/ICCV.2019.00566
                    - 10.1109/TIP.2016.2613686
                    - 10.1007/978-3-030-01228-1_38
                    - 10.1007/978-3-319-46484-8_2
                    - 10.1109/ICCV.2015.510
                    - 10.1109/CVPR.2017.147
                    - 10.1109/ICCV.2019.00559
                    - 10.1109/CVPR.2019.01019
                    - 10.1109/ICCV.2019.00635
                    - 10.1109/CVPRW.2019.00351
                    - 10.1109/TCSVT.2018.2875441
                    - 10.1007/978-3-030-01240-3_36
                    - 10.1109/CVPR.2018.00684
                    - 10.1109/CVPR42600.2020.00975
                    - 10.1007/978-3-030-01249-6_19
                    - 10.1109/ICCVW.2019.00151
                    - 10.1007/978-3-030-58517-4_10
                    - 10.1016/j.neucom.2020.07.135
            googleScholar:
                title: Learning to anticipate egocentric actions by imagination
                authorNames:
                    - Y Wu
                    - L Zhu
                    - X Wang
                    - Y Yang
                pdfLink: 'https://ieeexplore.ieee.org/iel7/83/9263394/09280353.pdf'
                link: 'https://ieeexplore.ieee.org/abstract/document/9280353/'
                citationId: '6012752103031775791'
                multiArticleId: '6012752103031775791'
                citedByLink: 'https://scholar.google.com//scholar?cites=6012752103031775791&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'IEEE Transactions on, 2020'
                year: '2020'
        publisherInfo: ''
    Imagine that! Leveraging emergent affordances for 3d tool synthesis: &ref_14
        notes:
            resumeStatus: relevent|title
            comment: ''
            reasonsRelevant:
                - title
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:38:14.464Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: null
            googleScholar:
                title: Imagine that! Leveraging emergent affordances for 3d tool synthesis
                authorNames:
                    - Y Wu
                    - S Kasewa
                    - O Groth
                    - S Salter
                    - L Sun
                pdfLink: 'https://arxiv.org/pdf/1909.13561'
                link: 'https://arxiv.org/abs/1909.13561'
                citationId: '1893778644382906900'
                multiArticleId: '1893778644382906900'
                citedByLink: 'https://scholar.google.com//scholar?cites=1893778644382906900&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'arXiv preprint arXiv, 2019'
                year: '2019'
        publisherInfo: ''
    What Can I Do Here? Learning New Skills by Imagining Visual Affordances: &ref_15
        notes:
            resumeStatus: relevent|title
            comment: ''
            reasonsRelevant:
                - title
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:34:31.778Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: What Can I Do Here? Learning New Skills by Imagining Visual Affordances
                subtitle: null
                abstract: null
                authorNames:
                    - Alexander Khazatsky
                    - Ashvin Nair
                    - Daniel Jing
                    - Sergey Levine
                year: 2021
                doi: 10.1109/icra48506.2021.9561692
                url: 'https://doi.org/10.1109/icra48506.2021.9561692'
                pdfLink: null
                citationCount: 12
                citedDois:
                    - 10.1109/ICRA.2017.7989202
                    - 10.1038/nature16961
                    - 10.1016/S0921-8890(97)00043-2
                    - 10.1609/aaai.v24i1.7727
                    - 10.1016/j.robot.2012.05.008
                    - 10.1109/CVPRW.2017.70
            googleScholar:
                title: What can i do here? learning new skills by imagining visual affordances
                authorNames:
                    - A Khazatsky
                    - A Nair
                    - D Jing
                pdfLink: 'https://ieeexplore.ieee.org/iel7/9560720/9560666/09561692.pdf'
                link: 'https://ieeexplore.ieee.org/abstract/document/9561692/'
                citationId: '15430638141641676250'
                multiArticleId: '15430638141641676250'
                citedByLink: 'https://scholar.google.com//scholar?cites=15430638141641676250&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: '2021 IEEE International, 2021'
                year: '2021'
        publisherInfo: ''
    'Learning to Act Properly: Predicting and Explaining Affordances from Images': &ref_16
        notes:
            resumeStatus: unclear|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:39:12.076Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: 'Learning to Act Properly: Predicting and Explaining Affordances from Images'
                subtitle: null
                abstract: null
                authorNames:
                    - Ching-Yao Chuang
                    - Jiaman Li
                    - Antonio Torralba
                    - Sanja Fidler
                year: 2018
                doi: 10.1109/cvpr.2018.00108
                url: 'https://doi.org/10.1109/cvpr.2018.00108'
                pdfLink: null
                citationCount: 47
                citedDois:
                    - 10.1109/CVPR.2017.544
                    - 10.1109/ICCV.2013.312
                    - 10.1109/CVPR.2011.5995327
                    - 10.1109/CVPR.2016.90
                    - 10.1109/CVPR.2013.385
                    - 10.1109/CVPR.2016.494
                    - 10.1109/ICCV.2017.448
                    - 10.1109/CVPR.2015.7298935
                    - 10.1109/ICCV.2017.64
                    - 10.1109/CVPR.2015.7299054
                    - 10.1109/ICCV.2017.323
                    - 10.1109/CVPR.2015.7298932
                    - 10.1109/ICCV.2017.448
                    - 10.1109/ICCV.2017.556
                    - 10.1007/s11263-015-0816-y
            googleScholar:
                title: 'Learning to act properly: Predicting and explaining affordances from images'
                authorNames:
                    - CY Chuang
                    - J Li
                    - A Torralba
                pdfLink: 'https://openaccess.thecvf.com/content_cvpr_2018/papers/Chuang_Learning_to_Act_CVPR_2018_paper.pdf'
                link: 'http://openaccess.thecvf.com/content_cvpr_2018/html/Chuang_Learning_to_Act_CVPR_2018_paper.html'
                citationId: '9413040909214203914'
                multiArticleId: '9413040909214203914'
                citedByLink: 'https://scholar.google.com//scholar?cites=9413040909214203914&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'Proceedings of the IEEE, 2018'
                year: '2018'
        publisherInfo: ''
    Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks: &ref_17
        notes:
            resumeStatus: relevent|title
            comment: ''
            reasonsRelevant:
                - title
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:38:01.850Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: null
            googleScholar:
                title: Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks
                authorNames:
                    - Y Wu
                    - S Kasewa
                    - O Groth
                    - S Salter
                    - L Sun
                    - OP Jones
                pdfLink: 'https://openreview.net/pdf?id=BkeyOxrYwH'
                link: 'https://openreview.net/forum?id=BkeyOxrYwH'
                citationId: '2961549995427504858'
                multiArticleId: null
                citedByLink: 'https://scholar.google.com//scholar?cites=2961549995427504858&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: null
                year: '2019'
        publisherInfo: ''
    'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text': &ref_18
        notes:
            resumeStatus: relevent|title
            comment: ''
            reasonsRelevant:
                - title
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:34:08.243Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
                subtitle: null
                abstract: null
                authorNames:
                    - Sayantan Adak
                    - Daivik Agrawal
                    - Animesh Mukherjee
                    - Somak Aditya
                year: 2024
                doi: 10.18653/v1/2024.conll-1.27
                url: 'https://doi.org/10.18653/v1/2024.conll-1.27'
                pdfLink: null
                citationCount: 0
                citedDois: []
            googleScholar:
                title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
                authorNames:
                    - S Adak
                    - D Agrawal
                    - A Mukherjee
                pdfLink: 'https://aclanthology.org/2024.conll-1.27.pdf'
                link: 'https://aclanthology.org/2024.conll-1.27/'
                citationId: null
                multiArticleId: null
                citedByLink: null
                publisherInfo: 'Proceedings of the 28th, 2024'
                year: '2024'
        publisherInfo: ''
    Learning affordances in object-centric generative models: &ref_19
        notes:
            resumeStatus: unclear|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T18:27:21.952Z'
                query: affordance based encoder imagination prediction
                searchEngine: googleScholar
            events:
                added: '2024-12-13T18:27:32.072Z'
                sawTitle: '2024-12-13T18:38:33.224Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: null
            googleScholar:
                title: Learning affordances in object-centric generative models
                authorNames:
                    - Y Wu
                    - S Kasewa
                    - O Groth
                    - S Salter
                    - L Sun
                pdfLink: 'https://ora.ox.ac.uk/objects/uuid:003cbbd9-a3aa-42e7-8e2d-bcc6b22db89a/download_file?safe_filename=OOL_7.pdf&file_format=pdf&type_of_work=Conference+item'
                link: 'https://ora.ox.ac.uk/objects/uuid:003cbbd9-a3aa-42e7-8e2d-bcc6b22db89a'
                citationId: '7478229689628350853'
                multiArticleId: '7478229689628350853'
                citedByLink: 'https://scholar.google.com//scholar?cites=7478229689628350853&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: null
                year: '2020'
        publisherInfo: ''
    'CoSense3D: an Agent-based Efficient Learning Framework for Collective Perception': &ref_0
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.467Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: 10.1109/iv55156.2024.10588865
            googleScholar:
                title: 'CoSense3D: an Agent-based Efficient Learning Framework for Collective Perception'
                authorNames:
                    - Y Yuan
                    - M Sester
                pdfLink: 'https://arxiv.org/pdf/2404.18617'
                link: 'https://arxiv.org/abs/2404.18617'
                citationId: null
                multiArticleId: '14674823755777045882'
                citedByLink: null
                publisherInfo: 'arXiv preprint arXiv:2404.18617, 2024'
                year: '2024'
    Multi-Agent Active Perception Based on Reinforcement Learning and POMDP: &ref_1
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: 10.1109/access.2024.3383544
            googleScholar:
                title: Multi-Agent Active Perception Based on Reinforcement Learning and POMDP
                authorNames:
                    - T Selimović
                    - M Peti
                    - S Bogdan
                pdfLink: 'https://ieeexplore.ieee.org/iel7/6287639/6514899/10486889.pdf'
                link: 'https://ieeexplore.ieee.org/abstract/document/10486889/'
                citationId: null
                multiArticleId: '13114516728628612025'
                citedByLink: null
                publisherInfo: 'IEEE Access, 2024'
                year: '2024'
    Mutual influence between language and perception in multi-agent communication games: &ref_2
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: Mutual influence between language and perception in multi-agent communication games
                subtitle: null
                abstract: 'Language interfaces with many other cognitive domains. This paper explores how interactions at these interfaces can be studied with deep learning methods, focusing on the relation between language emergence and visual perception. To model the emergence of language, a sender and a receiver agent are trained on a reference game. The agents are implemented as deep neural networks, with dedicated vision and language modules. Motivated by the mutual influence between language and perception in cognition, we apply systematic manipulations to the agents’ (i) visual representations, to analyze the effects on emergent communication, and (ii) communication protocols, to analyze the effects on visual representations. Our analyses show that perceptual biases shape semantic categorization and communicative content. Conversely, if the communication protocol partitions object space along certain attributes, agents learn to represent visual information about these attributes more accurately, and the representations of communication partners align. Finally, an evolutionary analysis suggests that visual representations may be shaped in part to facilitate the communication of environmentally relevant distinctions. Aside from accounting for co-adaptation effects between language and perception, our results point out ways to modulate and improve visual representation learning and emergent communication in artificial agents.'
                authorNames:
                    - Xenia Ohmer
                    - Michael Marino
                    - Michael Franke
                    - Peter König
                year: 2022
                doi: 10.1371/journal.pcbi.1010658
                url: 'https://doi.org/10.1371/journal.pcbi.1010658'
                pdfLink: null
                citationCount: 0
                citedDois:
                    - 10.18653/v1/2020.emnlp-main.703
                    - 10.1073/pnas.2016569118
                    - 10.1371/journal.pone.0234894
                    - 10.18653/v1/D19-1384
                    - 10.18653/v1/2021.conll-1.48
                    - 10.1146/annurev-vision-082114-035447
                    - 10.1038/srep27755
                    - 10.3389/fpsyg.2017.01726
                    - 10.1111/cogs.12670
                    - 10.1073/pnas.0610341104
                    - 10.1016/j.tics.2020.08.005
                    - 10.1073/pnas.0701644104
                    - 10.1037/xge0000560
                    - 10.1016/S1364-6613(99)01333-9
                    - 10.18653/v1/2020.findings-emnlp.397
                    - 10.18653/v1/2020.acl-main.685
                    - 10.1016/S1364-6613(03)00109-8
                    - 10.1016/0022-5193(74)90110-6
                    - 10.2307/1913390
                    - 10.1006/jeth.1997.2359
                    - 10.1162/106454602320184248
                    - 10.1017/S0140525X05000087
                    - 10.18653/v1/P19-1380
                    - 10.18653/v1/2020.blackboxnlp-1.2
                    - 10.18653/v1/2020.acl-main.407
                    - 10.18653/v1/D18-1119
                    - 10.3115/v1/D14-1179
                    - 10.1080/23273798.2017.1391398
                    - 10.1007/BF00992696
                    - 10.1016/0025-5564(78)90077-9
                    - 10.1017/CBO9781139173179
                    - 10.1093/bjps/axx002
                    - 10.1006/jeth.1997.2319
                    - 10.7551/mitpress/2884.001.0001
                    - 10.1016/0010-0285(76)90013-X
                    - 10.1037/0096-3445.131.4.477
                    - 10.1016/j.cognition.2008.03.009
                    - 10.1075/eoc.4.1.07can
                    - 10.1037/0096-3445.127.4.331
                    - 10.1111/cogs.12796
                    - 10.1162/NECO_a_00475
                    - 10.1016/j.tics.2004.05.009
                    - 10.1007/s00429-021-02437-y
                    - 10.1017/S0140525X08004998
                    - 10.1016/0022-5193(83)90445-9
                    - '10.1093/acprof:oso/9780199580828.001.0001'
                    - 10.1167/15.8.22
                    - 10.1162/jocn_a_01544
                    - 10.1093/acrefore/9780190264086.013.46
                    - 10.1038/s41593-019-0520-2
                    - 10.1073/pnas.1905544116
                    - 10.1016/j.neunet.2020.11.004
                    - 10.1515/lity.1997.1.1.33
                    - 10.1145/3465055
                    - 10.1038/nature20101
                    - 10.1002/wcs.104
                    - 10.1098/rstb.2012.0117
                    - 10.1016/j.neunet.2009.03.007
            googleScholar:
                title: Mutual influence between language and perception in multi-agent communication games
                authorNames:
                    - X Ohmer
                    - M Marino
                    - M Franke
                pdfLink: null
                link: 'https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010658'
                citationId: '5350802694831905244'
                multiArticleId: '5350802694831905244'
                citedByLink: 'https://scholar.google.com//scholar?cites=5350802694831905244&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'PLOS Computational, 2022'
                year: '2022'
    'Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness': &ref_3
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: 10.1038/s42256-023-00720-7
            googleScholar:
                title: 'Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness'
                authorNames:
                    - P Pataranutaporn
                    - R Liu
                    - E Finn
                    - P Maes
                pdfLink: 'https://dspace.mit.edu/bitstream/handle/1721.1/152316/NMI_AI_beholder_Final-Unformatted%5B85%5D.pdf?sequence=1&isAllowed=y&utm_source=nationaltribune&utm_medium=nationaltribune&utm_campaign=news'
                link: 'https://www.nature.com/articles/s42256-023-00720-7'
                citationId: '14041336950901741464'
                multiArticleId: '14041336950901741464'
                citedByLink: 'https://scholar.google.com//scholar?cites=14041336950901741464&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'Nature Machine Intelligence, 2023'
                year: '2023'
    Why and how to study the impact of perception on language emergence in artificial agents: &ref_4
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: null
            googleScholar:
                title: Why and how to study the impact of perception on language emergence in artificial agents
                authorNames:
                    - X Ohmer
                    - M Marino
                    - M Franke
                pdfLink: 'https://escholarship.org/content/qt6p82v6st/qt6p82v6st.pdf'
                link: 'https://escholarship.org/uc/item/6p82v6st'
                citationId: '2179981516739486040'
                multiArticleId: '2179981516739486040'
                citedByLink: 'https://scholar.google.com//scholar?cites=2179981516739486040&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'Proceedings of the Annual, 2021'
                year: '2021'
    Interaction Algorithm Effect on Human Experience with Reinforcement Learning: &ref_5
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                title: Interaction Algorithm Effect on Human Experience with Reinforcement Learning
                subtitle: null
                abstract: 'A goal of interactive machine learning (IML) is to enable people with no specialized training to intuitively teach intelligent agents how to perform tasks. Toward achieving that goal, we are studying how the design of the interaction method for a Bayesian Q-Learning algorithm impacts aspects of the human’s experience of teaching the agent using human-centric metrics such as frustration in addition to traditional ML performance metrics. This study investigated two methods of natural language instruction: critique and action advice. We conducted a human-in-the-loop experiment in which people trained two agents with different teaching methods but, unknown to each participant, the same underlying reinforcement learning algorithm. The results show an agent that learns from action advice creates a better user experience compared to an agent that learns from binary critique in terms of frustration, perceived performance, transparency, immediacy, and perceived intelligence. We identified nine main characteristics of an IML algorithm’s design that impact the human’s experience with the agent, including using human instructions about the future, compliance with input, empowerment, transparency, immediacy, a deterministic interaction, the complexity of the instructions, accuracy of the speech recognition software, and the robust and flexible nature of the interaction algorithm.'
                authorNames:
                    - Samantha Krening
                    - Karen M. Feigh
                year: 2018
                doi: 10.1145/3277904
                url: 'https://doi.org/10.1145/3277904'
                pdfLink: null
                citationCount: 22
                citedDois:
                    - 10.1109/IROS.2008.4651020
                    - 10.1007/s12369-009-0013-7
                    - 10.1145/778712.778756
                    - 10.1016/S0921-8890(02)00374-3
                    - 10.1016/S0921-8890(02)00372-X
                    - 10.1177/154193120605000909
                    - 10.1145/375735.376334
                    - 10.1007/s10458-006-0005-z
                    - 10.1109/ICTAI.2012.152
                    - 10.1109/TCDS.2016.2628365
                    - 10.15607/RSS.2015.XI.018
                    - 10.3115/v1/P14-5010
                    - 10.3115/1219840.1219855
                    - 10.1561/1500000011
                    - 10.1518/001872097778543886
                    - 10.1037/a0021522
                    - 10.1126/science.7244649
                    - 10.1609/aaai.v25i1.7979
                    - 10.1016/j.artint.2007.09.009
            googleScholar:
                title: Interaction algorithm effect on human experience with reinforcement learning
                authorNames:
                    - S Krening
                    - KM Feigh- ACM Transactions on Human
                pdfLink: 'https://dl.acm.org/doi/pdf/10.1145/3277904'
                link: 'https://dl.acm.org/doi/abs/10.1145/3277904'
                citationId: '15452678659896386539'
                multiArticleId: '15452678659896386539'
                citedByLink: 'https://scholar.google.com//scholar?cites=15452678659896386539&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'Robot Interaction, 2018'
                year: '2018'
    'Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues': &ref_6
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: 10.1016/j.array.2021.100057
            googleScholar:
                title: 'Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues'
                authorNames:
                    - A Gupta
                    - A Anpalagan
                    - L Guan
                    - AS Khwaja
                pdfLink: null
                link: 'https://www.sciencedirect.com/science/article/pii/S2590005621000059'
                citationId: '2507582518469208253'
                multiArticleId: '2507582518469208253'
                citedByLink: 'https://scholar.google.com//scholar?cites=2507582518469208253&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'Array, 2021'
                year: '2021'
    Research on autonomous driving decision-making strategies based deep reinforcement learning: &ref_7
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: 10.1145/3697467.3697643
            googleScholar:
                title: Research on autonomous driving decision-making strategies based deep reinforcement learning
                authorNames:
                    - Z Wang
                    - H Yan
                    - C Wei
                    - J Wang
                    - S Bo
                pdfLink: 'https://dl.acm.org/doi/pdf/10.1145/3697467.3697643'
                link: 'https://dl.acm.org/doi/abs/10.1145/3697467.3697643'
                citationId: '1006030502322123597'
                multiArticleId: '1006030502322123597'
                citedByLink: 'https://scholar.google.com//scholar?cites=1006030502322123597&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'and Machine Learning, 2024'
                year: '2024'
    Finding and visualizing weaknesses of deep reinforcement learning agents: &ref_8
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: null
            googleScholar:
                title: Finding and visualizing weaknesses of deep reinforcement learning agents
                authorNames:
                    - C Rupprecht
                    - C Ibrahim
                    - CJ Pal
                pdfLink: 'https://arxiv.org/pdf/1904.01318'
                link: 'https://arxiv.org/abs/1904.01318'
                citationId: '11518984940352760452'
                multiArticleId: '11518984940352760452'
                citedByLink: 'https://scholar.google.com//scholar?cites=11518984940352760452&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'arXiv preprint arXiv:1904.01318, 2019'
                year: '1904'
    Simulation-based reinforcement learning for real-world autonomous driving: &ref_9
        notes:
            resumeStatus: unseen|title
            comment: ''
            reasonsRelevant: []
            reasonsNotRelevant: []
            customKeywords: []
            discoveryMethod:
                dateTime: '2024-12-13T20:06:55.468Z'
                query: ' agent driven affect based perception machine learning -emotion'
                searchEngine: googleScholar
            events:
                added: '2024-12-13T20:07:00.834Z'
        accordingTo:
            $manuallyEntered:
                title: null
                doi: null
                year: null
                publisherFlags: null
                authorNames: null
                link: null
                pdfLink: null
                cites: null
                citedBy: null
            crossref:
                doi: 10.1109/icra40945.2020.9196730
            googleScholar:
                title: Simulation-based reinforcement learning for real-world autonomous driving
                authorNames:
                    - B Osiński
                    - A Jakubowski
                    - P Zięcina
                pdfLink: 'https://arxiv.org/pdf/1911.12905'
                link: 'https://ieeexplore.ieee.org/abstract/document/9196730/'
                citationId: '18072154790906392218'
                multiArticleId: '18072154790906392218'
                citedByLink: 'https://scholar.google.com//scholar?cites=18072154790906392218&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
                publisherInfo: 'on robotics and, 2020'
                year: '2020'
discoveryAttempts:
    -
        query: ' agent driven affect based perception machine learning -emotion'
        dateTime: '2024-12-13T20:06:54.724Z'
        searchEngine: googleScholar
        referenceLinks:
            -
                hadBeenSeenBefore: false
                title: 'CoSense3D: an Agent-based Efficient Learning Framework for Collective Perception'
                link: *ref_0
            -
                hadBeenSeenBefore: false
                title: Multi-Agent Active Perception Based on Reinforcement Learning and POMDP
                link: *ref_1
            -
                hadBeenSeenBefore: false
                title: Mutual influence between language and perception in multi-agent communication games
                link: *ref_2
            -
                hadBeenSeenBefore: false
                title: 'Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness'
                link: *ref_3
            -
                hadBeenSeenBefore: false
                title: Why and how to study the impact of perception on language emergence in artificial agents
                link: *ref_4
            -
                hadBeenSeenBefore: false
                title: Interaction Algorithm Effect on Human Experience with Reinforcement Learning
                link: *ref_5
            -
                hadBeenSeenBefore: false
                title: 'Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues'
                link: *ref_6
            -
                hadBeenSeenBefore: false
                title: Research on autonomous driving decision-making strategies based deep reinforcement learning
                link: *ref_7
            -
                hadBeenSeenBefore: false
                title: Finding and visualizing weaknesses of deep reinforcement learning agents
                link: *ref_8
            -
                hadBeenSeenBefore: false
                title: Simulation-based reinforcement learning for real-world autonomous driving
                link: *ref_9
    -
        query: affordance based encoder imagination prediction
        date: null
        dateTime: '2024-12-13T18:27:21.644Z'
        searchEngine: googleScholar
        referenceLinks:
            -
                hadBeenSeenBefore: false
                title: Visual Affordance Prediction for Guiding Robot Exploration
                link: *ref_10
            -
                hadBeenSeenBefore: false
                title: 'RAIL: Robot Affordance Imagination with Large Language Models'
                link: *ref_11
            -
                hadBeenSeenBefore: false
                title: Affordance-Based Goal Imagination for Embodied AI Agents
                link: *ref_12
            -
                hadBeenSeenBefore: false
                title: Learning to Anticipate Egocentric Actions by Imagination
                link: *ref_13
            -
                hadBeenSeenBefore: false
                title: Imagine that! Leveraging emergent affordances for 3d tool synthesis
                link: *ref_14
            -
                hadBeenSeenBefore: false
                title: What Can I Do Here? Learning New Skills by Imagining Visual Affordances
                link: *ref_15
            -
                hadBeenSeenBefore: false
                title: 'Learning to Act Properly: Predicting and Explaining Affordances from Images'
                link: *ref_16
            -
                hadBeenSeenBefore: false
                title: Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks
                link: *ref_17
            -
                hadBeenSeenBefore: false
                title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
                link: *ref_18
            -
                hadBeenSeenBefore: false
                title: Learning affordances in object-centric generative models
                link: *ref_19
