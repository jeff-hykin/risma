{
    "VLAI: Exploration and Exploitation based on Visual-Language Aligned Information for Robotic Object Goal Navigation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "VLAI: Exploration and Exploitation based on Visual-Language Aligned Information for Robotic Object Goal Navigation",
                "abstract": "Object Goal Navigation(ObjectNav) is the task that an agent need navigate to an instance of a specific category in an unseen environment through visual observations within limited time steps. This work plays a significant role in enhancing the efficiency of locating specific items in indoor spaces and assisting individuals in completing various tasks, as well as providing support for people with disabilities. To achieve efficient ObjectNav in unfamiliar environments, global perception capabilities, understanding the regularities of space and semantics in the environment layout are significant. In this work, we propose an explicit-prediction method called VLAI that utilizes visual-language alignment information to guide the agent's exploration, unlike previous navigation methods based on frontier potential prediction or egocentric map completion, which only leverage visual observations to construct semantic maps, thus failing to help the agent develop a better global perception. Specifically, when predicting long-term goals, we retrieve previously saved visual observations to obtain visual information around the frontiers based on their position on the incrementally built incomplete semantic map. Then, we apply our designed Chat Describer to this visual information to obtain detailed frontier object descriptions. The Chat Describer, a novel automatic-questioning approach deployed in Visual-to-Language, is composed of Large Language Model(LLM) and the visual-to-language model(VLM), which has visual question-answering functionality. In addition, we also obtain the semantic similarity of target object and frontier object categories. Ultimately, by combining the semantic similarity and the boundary descriptions, the agent can predict the long-term goals more accurately. Our experiments on the Gibson and HM3D datasets reveal that our VLAI approach yields significantly better results compared to earlier methods. The code is released at\n\nhttps://github.com/31539lab/VLAI.",
                "year": 2024,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Exploiting multimodal synthetic data for egocentric human-object interaction detection in an industrial scenario": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314224000651/pdfft?md5=d21e431595cc95a702b31a95d96ba1c0&pid=1-s2.0-S1077314224000651-main.pdf",
                "title": "Exploiting multimodal synthetic data for egocentric human-object interaction detection in an industrial scenario",
                "abstract": "In this paper, we tackle the problem of Egocentric Human-Object Interaction (EHOI) detection in an industrial setting. To overcome the lack of public datasets in this context, we propose a pipeline and a tool for generating synthetic images of EHOIs paired with several annotations and data signals (e.g., depth maps or segmentation masks). Using the proposed pipeline, we present EgoISM-HOI a new multimodal dataset composed of synthetic EHOI images in an industrial environment with rich annotations of hands and objects. To demonstrate the utility and effectiveness of synthetic EHOI data produced by the proposed tool, we designed a new method that predicts and combines different multimodal signals to detect EHOIs in RGB images. Our study shows that exploiting synthetic data to pre-train the proposed method significantly improves performance when tested on real-world data. Moreover, to fully understand the usefulness of our method, we conducted an in-depth analysis in which we compared and highlighted the superiority of the proposed approach over different state-of-the-art class-agnostic methods. To support research in this field, we publicly release the datasets, source code, and pre-trained models at https://iplab.dmi.unict.it/egoism-hoi.",
                "year": 2024,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Egocentric zone-aware action recognition across environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003635/pdfft?md5=5b694a991b51cc80bb8e6929c7a79a45&pid=1-s2.0-S0167865524003635-main.pdf",
                "title": "Egocentric zone-aware action recognition across environments",
                "abstract": "Human activities exhibit a strong correlation between actions and the places where these are performed, such as washing something at a sink. More specifically, in daily living environments we may identify particular locations, hereinafter named activity-centric zones, which may afford a set of homogeneous actions. Their knowledge can serve as a prior to favor vision models to recognize human activities. However, the appearance of these zones is scene-specific, limiting the transferability of this prior information to unfamiliar areas and domains. This problem is particularly relevant in egocentric vision, where the environment takes up most of the image, making it even more difficult to separate the action from the context. In this paper, we discuss the importance of decoupling the domain-specific appearance of activity-centric zones from their universal, domain-agnostic representations, and show how the latter can improve the cross-domain transferability of Egocentric Action Recognition (EAR) models. We validate our solution on the EPIC-Kitchens-100 and Argo1M datasets.",
                "year": 2025,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "A survey on human-aware robot navigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889021001226/pdfft?md5=326da45b28ade545da67ae8e0b7a24c0&pid=1-s2.0-S0921889021001226-main.pdf",
                "title": "A survey on human-aware robot navigation",
                "abstract": "Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human–robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.",
                "year": 2021,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Neuromorphic sensorimotor adaptation for robotic mobile manipulation: From sensing to behaviour": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Neuromorphic sensorimotor adaptation for robotic mobile manipulation: From sensing to behaviour",
                "abstract": "We propose a neuromorphic approach to perception, reasoning and motor control using Spiking Neural Networks in mobile robotics. We demonstrate this by using a mobile robotic manipulator solving a pick-and-place task. All sensory data is provided by spike-based silicon retina cameras - eDVS (embedded Dynamic Vision Sensor) - and all reasoning and motor control is implemented in Spiking Neural Networks. For the given scenario, the robot is capable of detecting a sequence of objects blinking at different frequencies, finding one object that is not in the right place of the sequence, picking up this object and moving it to its correct position. Such a scenario demonstrates how to build large-scale networks solving a high-level cognitive task by combining several smaller networks responsible for low-level tasks. Importantly, here we focus only on generating a neural network that is capable of performing the task. This will be the basis of future work using neural network learning algorithms to improve task performance. The long-term goal is to learn sophisticated behaviours by experience while at the same time being able to introduce expert knowledge for intermediate tasks that can be used to initialize the network or to speed up the learning process.",
                "year": 2018,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Symbol grounding through robotic manipulation in cognitive systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889007001236/pdfft?md5=e5919b45a480d9e16f82ff58f733a79c&pid=1-s2.0-S0921889007001236-main.pdf",
                "title": "Symbol grounding through robotic manipulation in cognitive systems",
                "abstract": "Though proposals have been put forth to solve the classical symbol grounding problem through robotic sensorimotor interactions, only little progress has been made in this direction with actual working systems, and symbol grounding through physical interaction has been rarely dealt with. We address this problem in the context of robotic manipulation for cognitive systems, and claim that there are symbols which do not refer simply to physical objects but rather to the embodied interactions between the robot and the objects in its environment. Through the description of two manipulation experiments we offer a proposal on which to build a theory of symbolic representations for physical interactions. Some important neuroscience studies that support our view are also briefly described.",
                "year": 2007,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "SemNav-HRO: A target-driven semantic navigation strategy with human–robot–object ternary fusion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197623015543/pdfft?md5=976349cc5a4e230f69100c2556230496&pid=1-s2.0-S0952197623015543-main.pdf",
                "title": "SemNav-HRO: A target-driven semantic navigation strategy with human–robot–object ternary fusion",
                "abstract": "Target-Driven Semantic Navigation (TDSN) shows great potential to be applied in intelligent domestic assistants supporting humans with daily activities. Although numerous methods have been explored to achieve efficient static TDSN, socially aware TDSN in dynamic and crowded scenarios remains challenging and has not been adequately investigated. The main challenges come from the complex human–robot interaction mechanisms and the semantic relation exploitation, which requires the robot to understand the surroundings and perform foresighted behaviors. In this paper, a TDSN strategy named SemNav-HRO is proposed by considering Human–Robot–Object (HRO) ternary feature fusion. Specifically, a Deep Reinforcement Learning (DRL) based Dual-Channel Value Estimation Network (DCVEN) is first proposed by integrating multi-granularity map features and social awareness to learn crowded TDSN strategies. Meanwhile, the tricky and socially aware TDSN problem is slackened by eliminating the dependence on costly features (e.g., pedestrian speed) and introducing a pedestrian trajectory predictor. For the learning and evaluation of crowded TDSN strategies, a novel and semantic-rich simulator with complex layouts is constructed based on realistic domestic scenes, instead of employing the previous naive simulation settings. Experimental results show that our method relatively improves the navigation success rates by 12.8%∼25.5% and 14.6%∼19.2% compared to the baselines on the MP3D and Gibson datasets, respectively. Furthermore, we experimentally verify the promising generalization and interpretability of our method.",
                "year": 2024,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Egocentric interaction as a tool for designing ambient ecologies—The case of the easy ADL ecology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119211001556/pdfft?md5=6d9417c683735d6377adb3d28eaafb60&pid=1-s2.0-S1574119211001556-main.pdf",
                "title": "Egocentric interaction as a tool for designing ambient ecologies—The case of the easy ADL ecology",
                "abstract": "The visions of ambient intelligence demand novel interaction paradigms that enable designers and system developers to frame and manage the dynamic and complex interaction between humans and environments populated with physical (real) and virtual (digital) objects of interest. So far, many proposed approaches have adhered to a device-centric stance when including virtual objects into the ambient ecology; a stance inherited from existing interaction paradigms for mobile and stationary interactive devices. In this article, we introduce egocentric interaction as an alternative approach, taking the human agent’s body and mind as the center of reference. We show how this interaction paradigm has influenced both the conception and implementation of the easy ADL ecology, comprising of smart objects, a personal activity-centric middleware attempting to simplify interaction given available resources, ambient intelligence applications aimed at everyday activity support, and a human agent literally in the middle of it all.",
                "year": 2012,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "Object affordance based multimodal fusion for natural Human-Robot interaction": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Object affordance based multimodal fusion for natural Human-Robot interaction",
                "abstract": "Spoken language based natural Human-Robot Interaction (HRI) requires robots to have the ability to understand spoken language, and extract the intention-related information from the working scenario. For grounding the intention-related object in the working environment, object affordance recognition could be a feasible way. To this end, we propose a dataset and a deep CNN based architecture to learn the human-centered object affordance. Furthermore, we present an affordance based multimodal fusion framework to realize intended object grasping according to the spoken instructions of human users. The proposed framework contains an intention semantics extraction module which is employed to extract the intention from spoken language, a deep Convolutional Neural Network (CNN) based object affordance recognition module which is applied to recognize human-centered object affordance, and a multimodal fusion module which is adopted to bridge the extracted intentions and the recognized object affordances. We also complete multiple intended object grasping experiments on a PR2 platform to validate the feasibility and practicability of the presented HRI framework.",
                "year": 2019,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "A reference architecture for social robots": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A reference architecture for social robots",
                "abstract": "Social robotics poses tough challenges to software designers who are required to take care of difficult architectural drivers like acceptability, trust of robots as well as to guarantee that robots establish a personalized interaction with their users. Moreover, in this context recurrent software design issues such as ensuring interoperability, improving reusability and customizability of software components also arise. Designing and implementing social robotic software architectures is a time-intensive activity requiring multi-disciplinary expertise: this makes it difficult to rapidly develop, customize, and personalize robotic solutions. These challenges may be mitigated at design time by choosing certain architectural styles, implementing specific architectural patterns and using particular technologies. Leveraging on our experience in the MARIO project, in this paper we propose a series of principles that social robots may benefit from. These principles lay also the foundations for the design of a reference software architecture for social robots. The goal of this work is twofold: (i) Establishing a reference architecture whose components are unambiguously characterized by an ontology thus allowing to easily reuse them in order to implement and personalize social robots; (ii) Introducing a series of standardized software components for social robots architecture (mostly relying on ontologies and semantic technologies) to enhance interoperability, to improve explainability, and to favor rapid prototyping.",
                "year": 2022,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "A unified model for egocentric video summarization: an instance-based approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045790621001634/pdfft?md5=7ed9729680c907e96e0859d6fb11dd51&pid=1-s2.0-S0045790621001634-main.pdf",
                "title": "A unified model for egocentric video summarization: an instance-based approach",
                "abstract": "Video summarization generates compact representations of videos in the form of summaries. The proposed framework is a unified model for instance-driven egocentric video summarization addressing generic and query-based summarization along with multi-video summarization. The model employs deep learning for object detection and semantic web technologies in the form of ontologies for query inferences. Combining user preferences in the form of object queries has aided in producing summaries that are subjective in nature. Quantitative evaluations performed on two novel datasets namely, ‘vehicle expo’ and ‘academic inspection’ prove that the proposed framework produces remarkable results with the employment of instance-driven modules for summarization. Additional experimental analysis for shot boundary detection have been conducted based on proposed method and conventional methods establishing the significance of the instance-based model. Moreover, qualitative evaluations further ensure that the summaries are concise, representative, diverse and semantically relevant further substantiating the need for instance-driven models in video summarization.",
                "year": 2021,
                "publisher": "Computers & Electrical Engineering"
            }
        }
    },
    "Egocentric distance perception and performance of direct pointing in stereoscopic displays": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687017301187/pdfft?md5=3f091fcd9b320489028d4f248a331425&pid=1-s2.0-S0003687017301187-main.pdf",
                "title": "Egocentric distance perception and performance of direct pointing in stereoscopic displays",
                "abstract": "In this study, the interaction performances and spatial perceptions in stereoscopic environments were investigated. The experiment compared direct user interactions during pointing at a target, which was continuously visible or presented briefly and disappeared, in both stereoscopic and real environments, at three parallax/depth levels. The position data, collected by a motion system, were used to compute accuracy, signed error, movement time, and throughput. The results showed inaccurate egocentric distance judgment in stereoscopic displays and accurate perceptions in the real world. The overall inaccuracy, which was overestimation of about 10 cm, was relatively better than that achieved in previous studies. The overestimation decreased as the egocentric distance increased. However, providing visual objects did not improve the accuracy. The study concluded that direct pointing could minimize the underestimation problems commonly reported in stereoscopic viewing studies and showed practical significance for developers to focus on incorporating more direct and natural human-virtual reality interactions for improved performance. The findings of this study provide insight towards the development of less expensive displays and their applications. Implications of this work and engineering solutions are also discussed.",
                "year": 2017,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Incorporating simulated spatial context information improves the effectiveness of contrastive learning models": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2666389924000710/pdfft?md5=1610f367696157a33967e65d4a5ca101&pid=1-s2.0-S2666389924000710-main.pdf",
                "title": "Incorporating simulated spatial context information improves the effectiveness of contrastive learning models",
                "abstract": "Visual learning often occurs in a specific context, where an agent acquires skills through exploration and tracking of its location in a consistent environment. The historical spatial context of the agent provides a similarity signal for self-supervised contrastive learning. We present a unique approach, termed environmental spatial similarity (ESS), that complements existing contrastive learning methods. Using images from simulated, photorealistic environments as an experimental setting, we demonstrate that ESS outperforms traditional instance discrimination approaches. Moreover, sampling additional data from the same environment substantially improves accuracy and provides new augmentations. ESS allows remarkable proficiency in room classification and spatial prediction tasks, especially in unfamiliar environments. This learning paradigm has the potential to enable rapid visual learning in agents operating in new environments with unique visual characteristics. Potentially transformative applications span from robotics to space exploration. Our proof of concept demonstrates improved efficiency over methods that rely on extensive, disconnected datasets.",
                "year": 2024,
                "publisher": "Patterns"
            }
        }
    },
    "A computational model of the allocentric and egocentric spatial memory by means of virtual agents, or how simple virtual agents can help to build complex computational models": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041711000404/pdfft?md5=52190fa11e32592effa1a9b5ea4a805b&pid=1-s2.0-S1389041711000404-main.pdf",
                "title": "A computational model of the allocentric and egocentric spatial memory by means of virtual agents, or how simple virtual agents can help to build complex computational models",
                "abstract": "The ability to acquire, remember and use information about locations of objects in one’s proximal surrounding is a fundamental aspect of human spatial cognition. In this paper, we present a computational model of this ability. The model provides a possible explanation of contradictory results from experimental psychology related to this ability, namely explanation of why some experiments have reproduced the so-called “disorientation effect” while others have failed to do so. Additionally, in contrast to other computational models of various aspects of spatial cognition, our model is integrated within an intelligent virtual agent. Thus, on a more general level, this paper also demonstrates that it is possible to use intelligent virtual agents as a powerful research tool in computational cognitive sciences.",
                "year": 2012,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Research on the promotion of intelligent entertainment voice robots in personalized English learning based on data mining and gamified teaching experience": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Research on the promotion of intelligent entertainment voice robots in personalized English learning based on data mining and gamified teaching experience",
                "abstract": "With the development of educational technology and the arrival of the intelligent era, intelligent entertainment voice robots have gradually become a research hotspot in the field of education. This study aims to explore the application of intelligent entertainment voice robots based on data mining in the promotion of personalized English learning, and combine with gamified virtual teaching to improve learners’ learning interest and learning effect. The study uses data mining technology to analyze learners’ learning data, interest preferences and learning performance, and provides learners with targeted learning resources and gamified virtual teaching environment based on personalized recommendation. The study creates a virtual teaching environment presented in the form of games, which contains various interesting tasks, challenges and reward mechanisms. Learners can participate in the virtual teaching environment at the same time, experience the fun and stimulation of games, so as to stimulate the interest and motivation of learning. Learners can engage in interactive speech learning through conversations with intelligent entertainment speech robots. The robot can provide real-time answers to learners’ questions, provide personalized learning suggestions, and provide encouragement and feedback. By interacting with robots, learners can get more flexible and personalized learning support, improving learning effectiveness and satisfaction. Through experiments and data analysis, it is found that intelligent entertainment voice robot based on data mining and gamification teaching experience can effectively improve learners’ learning interest and learning effect. Learners show higher engagement and motivation in personalized recommended learning resources and gamified virtual teaching environments.",
                "year": 2025,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "In robot we trust? The effect of emotional expressions and contextual cues on anthropomorphic trustworthiness": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687023000054/pdfft?md5=a702fea01e42f0c1a55135ed9696f62c&pid=1-s2.0-S0003687023000054-main.pdf",
                "title": "In robot we trust? The effect of emotional expressions and contextual cues on anthropomorphic trustworthiness",
                "abstract": "Following the evolution of technology and its application in various daily contexts, social robots work as an advanced artificial intelligence (AI) system to interact with humans. However, limited research has been done to discuss the role of emotional expressions and contextual cues in influencing anthropomorphic trustworthiness, especially from the design perspective. To address this research gap, the current study designed a specific robot prototype and conducted two lab experiments to explore the effect of emotional expressions and contextual cues on trustworthiness via a combination of subjective ratings and physiological measures. Results showed that: 1) positive (vs. negative) emotional expressions enjoyed a higher level of anthropomorphic trustworthiness and visual attention; 2) regulatory fit was expanded in parasocial interaction and worked as a prime to activate anthropomorphic trustworthiness for social robots. Theoretical contributions and design implications were also discussed in this study.",
                "year": 2023,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Design of FDM-printable tendon-driven continuum robots using a serial S-shaped backbone structure": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2667379724000469/pdfft?md5=b3db287ec40c71dd2bd6b9584c60f800&pid=1-s2.0-S2667379724000469-main.pdf",
                "title": "Design of FDM-printable tendon-driven continuum robots using a serial S-shaped backbone structure",
                "abstract": "Tendon-driven continuum robots (TDCR) are widely used in various engineering disciplines due to their exceptional flexibility and dexterity. However, their complex structure often leads to significant manufacturing costs and lengthy prototyping cycles. To cope with this problem, we propose a fused-deposition-modeling-printable (FDM-printable) TDCR structure design using a serial S-shaped backbone, which enables planar bending motion with minimized plastic deformation. A kinematic model for the proposed TDCR structure based on the pseudo-rigid-body model (PRBM) approach is developed. Experimental results have revealed that the proposed kinematic model can effectively predict the bending motion under certain tendon forces. In addition, analyses of mechanical hysteresis and factors influencing bending stiffness are conducted. Finally, A three-finger gripper is fabricated to demonstrate a possible application of the proposed TDCR structure.",
                "year": 2025,
                "publisher": "Biomimetic Intelligence and Robotics"
            }
        }
    },
    "Identification of physically consistent dynamics parameter of the ABB IRB 360-6/1600 delta robot and its use for time-optimal motion planning under consideration of constraint forces": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889024002070/pdfft?md5=a528a2538b907e34d5c842889a6d16e5&pid=1-s2.0-S0921889024002070-main.pdf",
                "title": "Identification of physically consistent dynamics parameter of the ABB IRB 360-6/1600 delta robot and its use for time-optimal motion planning under consideration of constraint forces",
                "abstract": "Model-based control schemes, forward dynamics simulations, constraint force computation and time-optimal motion planning have one major thing in common, they all depend on the dynamics parameters of the system. Physical consistency of the dynamics parameters ensures a positive definite mass matrix and correct constraint forces. The most common inverse dynamics identification method – the base-parameters – lack physical consistency. This paper proposes an identification method to identify physically consistent dynamics parameters for Delta-like robots while further showing the effects of friction in passive joints. A tailored model to compute the crucial constraint forces appearing in the mechanism based on the identified dynamics parameters is derived. This model is used to additionally consider constraint forces besides actuation torques for time-optimal motion planning of a typical pick and place task. This is done without any prior CAD data of the robot from the manufacturer.",
                "year": 2024,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Exploring the role of aliveness in children's responses to a dog, biomimetic robot, and toy dog": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563223000110/pdfft?md5=c7452c64110e2ccc5998d55353cf4e73&pid=1-s2.0-S0747563223000110-main.pdf",
                "title": "Exploring the role of aliveness in children's responses to a dog, biomimetic robot, and toy dog",
                "abstract": "The Core Knowledge System of Agency states that children, from an early age, can discriminate between living agents and non-living artifacts. Building on this, the ‘Biophilia hypothesis’ suggests that children and adults have a natural affinity for living organisms and benefit from interacting with them. These theories may underpin the use of dogs for children's general wellbeing and for therapeutic purposes, yet it is presently unclear whether a comparable non-living artefact, such as social robot, could capitalise on similar mechanisms. In the current study, child members of the public aged 14-months to 14-years old (N = 115), engaged in free interactions with a dog, a MiRo-E biomimetic robot, and a basic moving toy dog, and then completed an age-appropriate questionnaire evaluating their attitudes towards the three animal/robots (N = 99). As was predicted, most participants preferred the dog, and behavioural observations indicated that participants approached the dog first most frequently and spent the longest duration engaged in positive behaviours with the dog. Participants also attributed the dog with higher mental state abilities than the robot, with several participants referring to the “aliveness” of the dog when explaining their preference. However, similar emotions were reported for all conditions and participants spent a comparable amount of time overall with the dog and robot, and participants engaged in more exploratory behaviours with the toy. This suggested that, whilst the children recognised the categorical distinction between the living status of the three entities, the robot provided an enjoyable experience for the children and sustained their attention. Therefore, a biomimetic robot has the potential to provide a valid alternative to a live dog in certain contexts.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Exploring the effect of VR-enhanced teaching aids in STEAM education: An embodied cognition perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2949678024000175/pdfft?md5=ccb3a32f79070032d7a04956a0af1b11&pid=1-s2.0-S2949678024000175-main.pdf",
                "title": "Exploring the effect of VR-enhanced teaching aids in STEAM education: An embodied cognition perspective",
                "abstract": "The purpose of this study was to examine the effect of Physical Teaching Aids (PTAs) and VR-Enhanced Teaching Aids (VETAs) on students’ learning performance, attitudes, and classroom interactions in a STEAM course. A quasi-experimental study was conducted with a total of 85 fifth-grade students, 42 in the treatment group and 43 in the control group. The students in the treatment group reported a high level of acceptance of the VETAs in the post-course interview. The Lag Sequential Analysis results indicated that the use of VETAs improved students' learning performance and made the classroom more learner-centered than those who were facilitated with PTAs. In the treatment group, students were able to investigate problems more actively, practice and innovate more interactively, and achieve deeper learning. The current study further elaborates on the reasons and summarizes the mechanism of VETAs supporting the perceptual field in STEAM education.",
                "year": 2024,
                "publisher": "Computers & Education: X Reality"
            }
        }
    },
    "Context-based affordance segmentation from 2D images for robot actions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889018309990/pdfft?md5=30bd538bfd802cf17f7fbe5bdc914da1&pid=1-s2.0-S0921889018309990-main.pdf",
                "title": "Context-based affordance segmentation from 2D images for robot actions",
                "abstract": "Affordances play a crucial role in robotics since they allow developing truly autonomous robots, which can freely explore and interact with the environment. Most of the existing approaches for analyzing affordances in a scene consider only one or few types of affordance, e.g., grasping points, object manipulation or locomotion. In many cases only whole objects are considered. In our study we include in total 12 affordances of object-related, manipulation and locomotion affordances, considering affordances of both objects and/or their parts. We design a system that can densely predict affordances given only a single 2D RGB image. For this, we propose a method that transfers object class labels to affordances. This enables us to train convolutional neural networks, a PSPNet-based network and a U-Net-style network, to directly predict affordances from an image using a selective binary cross entropy loss function. The method is able to handle (potentially multiple) affordances of objects and their parts in a pixel-wise manner even in the case of incomplete data. We perform qualitative as well as quantitative evaluations with simulated and real data including robot experiments. In general, we find that frequent affordances are recognized with a substantial fraction of correctly assigned pixels, while this is harder for infrequent affordances and small objects. In addition, we demonstrate that our method performs better than a recent competitive approach. As the proposed method operates on 2D images, it is easier to implement than competing 3D methods and it could therefore more easily provide useful affordance estimates for robotic actions as demonstrated experimentally.",
                "year": 2019,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Global path guided model predictive path integral control: Applications to GPU-parallelizable robot simulation systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S004579062400572X/pdfft?md5=6309b4f5a243d89dd1564386478ab3e4&pid=1-s2.0-S004579062400572X-main.pdf",
                "title": "Global path guided model predictive path integral control: Applications to GPU-parallelizable robot simulation systems",
                "abstract": "In this paper, a global path guided model predictive path integral (Global-MPPI) control algorithmic framework is proposed, which integrates the model predictive path integral (MPPI) control with global path planning algorithms. The method is designed to address the challenges of robot path planning and execution in large-scale spaces with obstacles. It is applicable in both 2D and 3D spaces and can be implemented for both mobile robots and manipulators. To enhance practicality, we present an obstacle size expansion method aimed at minimizing collision risks. Validation of the algorithm is conducted using the advanced parallel physics simulator, IsaacGym, highlighting the effectiveness of the method in a randomly populated obstacle space. Multiple global path planning algorithms have been tested using Global-MPPI framework and the key factors affecting the effectiveness of the algorithms are analyzed. Furthermore, Global-MPPI is compared with recent algorithms solving similar problems. It can substantially increase the success rate when the right parameters are set.",
                "year": 2024,
                "publisher": "Computers and Electrical Engineering"
            }
        }
    },
    "A computational cognitive framework of spatial memory in brains and robots": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A computational cognitive framework of spatial memory in brains and robots",
                "abstract": "Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible.\n\nWe motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models.\n\nWe briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.",
                "year": 2018,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Enabling robotic social intelligence by engineering human social-cognitive mechanisms": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Enabling robotic social intelligence by engineering human social-cognitive mechanisms",
                "abstract": "For effective human-robot interaction, we argue that robots must gain social-cognitive mechanisms that allow them to function naturally and intuitively during social interactions with humans. However, a lack of consensus on social cognitive processes poses a challenge for how to design such mechanisms for artificial cognitive systems. We discuss a recent integrative perspective of social cognition to provide a systematic theoretical underpinning for computational instantiations of these mechanisms. We highlight several commitments of our approach that we refer to as Engineering Human Social Cognition. We then provide a series of recommendations to facilitate the development of the perceptual, motor, and cognitive architecture for this proposed artificial cognitive system in future work. For each recommendation, we highlight their relation to the discussed social-cognitive mechanisms, provide the rationale for these recommendations and potential benefits, and detail examples of associated computational formalisms that could be leveraged to instantiate our recommendations. Overall, the goal of this paper is to outline an interdisciplinary and multi-theoretic approach to facilitate the design of robots that will one day function, and be perceived, as socially interactive and effective teammates.",
                "year": 2017,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Chemspyd: an open-source python interface for Chemspeed robotic chemistry and materials platforms": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Chemspyd: an open-source python interface for Chemspeed robotic chemistry and materials platforms",
                "abstract": "We introduce Chemspyd, a lightweight, open-source Python package for operating the popular laboratory robotic platforms from Chemspeed Technologies. As an add-on to the existing proprietary software suite, Chemspyd enables dynamic communication with the automated platform, laying the foundation for its modular integration into customizable, higher-level laboratory workflows. We show the applicability of Chemspyd in a set of case studies from chemistry and materials science. We demonstrate how the package can be used with large language models to provide a natural language interface. By providing an open-source software interface for a commercial robotic platform, we hope to inspire the development of open interfaces that facilitate the flexible, adaptive integration of existing laboratory equipment into automated laboratories.",
                "year": 2024,
                "publisher": "Digital Discovery"
            }
        }
    },
    "Grounding semantic categories in behavioral interactions: Experiments with 100 objects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S092188901200190X/pdfft?md5=b19c4e38d3432b196f89685a472cd02f&pid=1-s2.0-S092188901200190X-main.pdf",
                "title": "Grounding semantic categories in behavioral interactions: Experiments with 100 objects",
                "abstract": "From an early stage in their development, human infants show a profound drive to explore the objects around them. Research in psychology has shown that this exploration is fundamental for learning the names of objects and object categories. To address this problem in robotics, this paper presents a behavior-grounded approach that enables a robot to recognize the semantic labels of objects using its own behavioral interaction with them. To test this method, our robot interacted with 100 different objects grouped according to 20 different object categories. The robot performed 10 different behaviors on them, while using three sensory modalities (vision, proprioception and audio) to detect any perceptual changes. The results show that the robot was able to use multiple sensorimotor contexts in order to recognize a large number of object categories. Furthermore, the category recognition model presented in this paper was able to identify sensorimotor contexts that can be used to detect specific categories. Most importantly, the robot’s model was able to reduce exploration time by half by dynamically selecting which exploratory behavior should be applied next when classifying a novel object.",
                "year": 2014,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Autonomous agriculture in public perception - German consumer segments’ view of crop robots": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169922006937/pdfft?md5=5fc7e3a518b037033ba626f848cd874e&pid=1-s2.0-S0168169922006937-main.pdf",
                "title": "Autonomous agriculture in public perception - German consumer segments’ view of crop robots",
                "abstract": "Public acceptance of agricultural technologies is an important determinant of their success. In the case of autonomous crop robots, recent research from Germany suggests that societal acceptance of the technology plays an important role for farmer acceptance of crop robots. Yet little research has been conducted so far into how the public perceives autonomous agricultural technologies like crop robots. Investigating the public’s opinion on crop robots will provide answers to one of farmers’ questions in the decision to invest in crop robots. Through consumer segmentation and hypotheses-based characterization, specific consumer groups with differing attitudes towards crop robots are identified. Two discrete choice experiments on digital and autonomous methods of weed management (n = 675) and tractor size and degree of autonomy (n = 645), conducted as part of a larger representative consumer survey in Germany (n = 2,012), are submitted to a Hierarchical Bayes estimation and subsequent latent class analysis. The identified consumer segments are characterized in a hypotheses-based approach with hypotheses centering on consumer attitudes measured as 5-point Likert-type items and as spontaneous image associations. Both subsamples can be segmented into three groups, which are comparable between the experiments in their socio-economic composition. Results suggest that the German public is largely positively inclined towards autonomous agricultural technologies. The method of weed control is considered more important than the vehicle type (i.e., conventional tractor or crop robot) and vehicle size is considered more important than degree of autonomy. Only the respective smallest consumer segments in the two experiments indicate indifference or a more conservative perspective. Participants’ attitude towards environmental preservation appears to have a positive influence on their evaluation of autonomous agricultural technologies. To the authors’ best knowledge, this is the first investigation into the public opinion of crop robots, based on a large sample representative of the German population in four socio-demographic variables. It indicates that the German population is most interested in the reduction of agrochemicals in plant production and will also accept autonomous agricultural technologies to achieve this goal. Policymakers should make use of these insights when communicating about novel technologies in agriculture and extension agents should relay this information to farmers, particularly those already interested in investing in crop robots.",
                "year": 2022,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Mirror neuron framework yields representations for robot interaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231208004797/pdfft?md5=9e663ae414d030e6d1c2c9e27687e15d&pid=1-s2.0-S0925231208004797-main.pdf",
                "title": "Mirror neuron framework yields representations for robot interaction",
                "abstract": "Common coding is a functional principle that underlies the mirror neuron paradigm. It insures actual parity between perception and action, since the perceived and performed actions are equivalently and simultaneously represented within the mirror neuron system. Based on the parity of this representation we show how the mirror neuron system may facilitate the interaction between two robots. Synchronization between neuron groups in different structures of the mirror neuron system are on the basis of the interaction behavior. The robotic simulation is used to illustrate several interactions. The resulting synchronization and turn taking behaviors show the potential of the mirror neuron paradigm for designing of socially meaningful behaviors.",
                "year": 2009,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Sampling-based time-optimal path parameterization with jerk constraints for robotic manipulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889023001690/pdfft?md5=d7d2e2a0784c43b2444d595e3a97e29f&pid=1-s2.0-S0921889023001690-main.pdf",
                "title": "Sampling-based time-optimal path parameterization with jerk constraints for robotic manipulation",
                "abstract": "In this paper, a sampling-based time-optimal path parameterization (S-TOPP) method is proposed to address time-optimal trajectory planning problems with bounded jerks. The key insight of S-TOPP is that a tree of feasible nodes connected by edges is established to find a time-optimal trajectory on the temporal dimension. The tree establishment process includes two major phases at each stage, namely sampling and one-step backtracking. In the sampling phase, a new sampling strategy integrating historical information is proposed to obtain superior samples whereby fewer samples can be controlled automatically, reducing the calculation loss. In one-step backtracking phase, a “lazy” strategy is used to lazily skip constraint-checking when evaluating local connections, enabling S-TOPP to avoid checking the vast majority of nodes that have no chance of being in an optimal trajectory. Simulations and real-world experiments validate the feasibility and practicability of S-TOPP. Results show that S-TOPP is an effective solution to jerk-bounded time-optimal trajectory planning, with features that are more in line with the needs of the practical tasks compared with other methods.",
                "year": 2023,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Accumulation of object representations utilising interaction of robot action and perception": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705101001277/pdfft?md5=ba7774f946ba8024e3187b9f03801bd7&pid=1-s2.0-S0950705101001277-main.pdf",
                "title": "Accumulation of object representations utilising interaction of robot action and perception",
                "abstract": "We introduce a robotic-vision system which is able to extract object representations autonomously utilising a tight interaction of visual perception and robotic action within a perception action cycle [Ecological Psychology 4 (1992) 121; Algebraic Frames for the Perception and Action Cycle, 1997, 1]. Controlled movement of the object grasped by the robot enables us to compute the transformations of entities which are used to represent aspects of objects and to find correspondences of entities within an image sequence.\n\nA general accumulation scheme allows to acquire robust information from partly missing information extracted from single frames of an image sequence. Here we use this scheme with a preprocessing stage in which 3D-line segments are extracted from stereo images. However, the accumulation scheme can be used with any kind of preprocessing as long as the entities used to represent objects can be brought to correspondence by certain equivalence relations such as ‘rigid body motion’.\n\nWe show that an accumulated representation can be applied within a tracking algorithm. The accumulation scheme is an important module of a vision based robot system on which we are currently working. In this system, objects are planned to be represented by different visual and tactile entities. The object representations are going to be learned autonomously. We discuss the accumulation scheme in the context of this project.",
                "year": 2002,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Model-based strategy for grasping 3D deformable objects using a multi-fingered robotic hand": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889016308089/pdfft?md5=e0ebc79675d440bbf899404b3c193527&pid=1-s2.0-S0921889016308089-main.pdf",
                "title": "Model-based strategy for grasping 3D deformable objects using a multi-fingered robotic hand",
                "abstract": "This paper presents a model-based strategy for 3D deformable object grasping using a multi-fingered robotic hand. The developed contact model is based on two force components (normal force and tangential friction force, including slipping and sticking effects) and uses a non-linear mass–spring system to describe the object deformations due the mechanical load applied by the fingers of the robotic hand. The object–finger interaction is simulated in order to compute the required contact forces and deformations to robustly grasp objects with large deformations. Our approach is able to achieve this by using a non-linear model that outperforms current techniques that are limited to using linear models. After the contact forces computed by the simulation of the contact model guarantee the equilibrium of the grasp, they will be used as set-points for force-controlling the closing of the real fingers, and thus, the proposed grasping strategy is implemented. Two different objects (cube and sphere) made from two soft materials (foam and rubber) are tested in order to verify that the proposed model can represent their non-linear deformations and that the proposed grasp strategy can implement a robust grasp of them with a multi-fingered robotic hand equipped with tactile sensors. Thereby, both the grasping strategy and the proposed contact model are validated experimentally.",
                "year": 2017,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Three-dimensional spatial energy-quality map construction for optimal robot placement in multi-robot additive manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584524000218/pdfft?md5=07815bddaa4d71314d48e7bfa3f63e19&pid=1-s2.0-S0736584524000218-main.pdf",
                "title": "Three-dimensional spatial energy-quality map construction for optimal robot placement in multi-robot additive manufacturing",
                "abstract": "The adoption of multiple robots for collaborative additive manufacturing is rapidly gaining attention in the industry and research community due to their numerous advantages, such as fast and efficient printing of large-scale parts and their suitability for hazardous or extraterrestrial environments. However, to fully harness the potential of multi-robot additive manufacturing systems, several challenges must be addressed from a process planning perspective. These include part decomposition, part/robot placement, trajectory planning, and print scheduling considering various quality, energy efficiency, time, and reachability constraints as well as different robotic team compositions including mobile/stationary robots, aerial mobility/ground mobility, and heterogenous/homogenous teams. This work explores the optimal positioning of part with respect to the 3D printer robots and inversely the 3D printer robots with respect to the part in case of large structures (given that the structure is assumed to be grounded/fixed and not movable) in multi-robot additive manufacturing scenarios. A novel decision-making methodology for the robot placement problem, i.e., optimal positioning of multiple robots around a large-scale structure, based on the energy consumption of the robots during the additive manufacturing process as well as the final dimensional accuracy of the printed structure, is proposed. The decision making is guided by the construction of a 3D spatial energy-quality map around each of the robot's bases based on their kinematics as well as the geometry of the assigned part for additive manufacturing using the proposed energy and quality modules. Additionally, the simulated annealing algorithm is adopted to quickly identify the optimal robot positionings for the collaborative additive manufacturing task. Different case studies demonstrating the effectiveness of the proposed methodology in reducing energy consumption while maintaining the required print quality are presented. Finally, sensitivity analyses are performed to evaluate the impact of various parameters including the robot velocity and acceleration, number of robots, decomposition scenarios, and ratio of the printed geometry with respect to the robot's reach on the energy and quality metrics.",
                "year": 2024,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Home service robot task planning using semantic knowledge and probabilistic inference": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705120304093/pdfft?md5=84898c40a6e76ed42bfc0416626b8efc&pid=1-s2.0-S0950705120304093-main.pdf",
                "title": "Home service robot task planning using semantic knowledge and probabilistic inference",
                "abstract": "In the face of unstructured home environment, home service robots are inevitably confronted with uncertainty and incompleteness of environment information. How to make the home service robot obtain enough environment information and plan a discrete sequence of actions through task planning is the key problem of robot intelligence. In this paper, a hierarchical task network based on semantic knowledge and probabilistic inference method is proposed. We use the object location ontology, the location relation between dynamic and static objects to build semantic knowledge of home environment, and build the probability model between dynamic and static objects, as well as between static objects and home scenes. The location of the object is determined by the semantic knowledge and the probability model. Hierarchical task network is selected as an engine of task planner, which can be provided with the location information to improve the autonomy and effectiveness of robot task planning. In order to prevent task execution failure and enhance the adaptability of robot to unstructured home environment, a mechanism of task execution diagnosis and replanning is designed. Experimental results in simulation and real home environment demonstrate that our method can effectively improve the performance of service robot task planning and generate better task execution sequence.",
                "year": 2020,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Piagetian experiments to DevRobotics": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Piagetian experiments to DevRobotics",
                "abstract": "Integrating robots into our daily lives, once a distant dream, is gradually becoming a reality, surpassing our initial expectations. Today, we aspire for these robots to not only perform rudimentary tasks but to emulate human behavior, and in some aspects, even exceed it. The realm of research dedicated to achieving human-like competencies in robots has given rise to the fields of Developmental and Cognitive Robotics. These domains find their foundation in cognitive architectures and insights from human development. Despite the substantial progress in these fields, a conspicuous gap exists in the literature related to the evaluation of cognitive architectures and the advanced capabilities exhibited by robots. Recognizing this void, we aim at establishing a bridge between the insights gleaned from human developmental theories and the potential applications in robotics. Central to our investigation is the notion that learning follows a cumulative trajectory of escalating complexity. Consequently, our focus centers on the early stages of human development, particularly within the realm of children aged 0 to 2 years. Drawing inspiration from Piaget’s constructivist theory aligned with empirical studies in the Developmental Robotics domain, we unveil a framework that facilitates the classification of these studies. In light of this, we curate a series of progressive experiments, mirroring the motor and cognitive growth exhibited by children from birth to two years of age, to be conducted with robots. We also described a methodology for designing these experiments considering the robotics aspects.",
                "year": 2024,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Robot semantic mapping through human activity recognition: A wearable sensing and computing approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889015000202/pdfft?md5=6b9e4d579c2716611cf5639957936e4d&pid=1-s2.0-S0921889015000202-main.pdf",
                "title": "Robot semantic mapping through human activity recognition: A wearable sensing and computing approach",
                "abstract": "Semantic information can help robots understand unknown environments better. In order to obtain semantic information efficiently and link it to a metric map, we present a new robot semantic mapping approach through human activity recognition in a human–robot coexisting environment. An intelligent mobile robot platform called ASCCbot creates a metric map while wearable motion sensors attached to the human body are used to recognize human activities. Combining pre-learned models of activity–furniture correlation and location–furniture correlation, the robot determines the probability distribution of the furniture types through a Bayesian framework and labels them on the metric map. Computer simulations and real experiments demonstrate that the proposed approach is able to create a semantic map of an indoor environment effectively.",
                "year": 2015,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Structure-based object representation and classification in mobile robotics through a Microsoft Kinect": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889013001139/pdfft?md5=65923765aff636aeacb2eeb99976af3c&pid=1-s2.0-S0921889013001139-main.pdf",
                "title": "Structure-based object representation and classification in mobile robotics through a Microsoft Kinect",
                "abstract": "A new approach enabling a mobile robot to recognize and classify furniture-like objects composed of assembled parts using a Microsoft Kinect is presented. Starting from considerations about the structure of furniture-like objects, i.e., objects which can play a role in the course of a mobile robot mission, the 3D point cloud returned by the Kinect is first segmented into a set of “almost convex” clusters. Objects are then represented by means of a graph expressing mutual relationships between such clusters. Off-line, snapshots of the same object taken from different positions are processed and merged, in order to produce multiple-view models that are used to populate a database. On-line, as soon as a new object is observed, a run-time window of subsequent snapshots is used to search for a correspondence in the database.\n\nExperiments validating the approach with a set of objects (i.e., chairs, tables, but also other robots) are reported and discussed in detail.",
                "year": 2013,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Multisensor fusion-based digital twin for localized quality prediction in robotic laser-directed energy deposition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584523000571/pdfft?md5=4cde933c3d85e525a5eac2d27ac0140f&pid=1-s2.0-S0736584523000571-main.pdf",
                "title": "Multisensor fusion-based digital twin for localized quality prediction in robotic laser-directed energy deposition",
                "abstract": "Early detection of defects, such as keyhole pores and cracks is crucial in laser-directed energy deposition (L-DED) additive manufacturing (AM) to prevent build failures. However, the complex melt pool behaviour cannot be adequately captured by conventional single-modal process monitoring approaches. This study introduces a multisensor fusion-based digital twin (MFDT) for localized quality prediction in the robotic L-DED process. The data used in multisensor fusion includes features extracted from a coaxial melt pool vision camera, a microphone, and an off-axis short wavelength infrared thermal camera. The key novelty of this work is a spatiotemporal data fusion method that synchronizes multisensor features with the real-time robot motion data to achieve localized quality prediction. Optical microscope (OM) images of the printed part are used to locate defect-free and defective regions (i.e., cracks and keyhole pores), which serve as ground truth labels for training supervised machine learning (ML) models for quality prediction. The trained ML model is then used to generate a virtual quality map that registers quality prediction outcomes within the 3D volume of the printed part, thus eliminating the need of physical inspections by destructive methods. Experiments show that the virtual quality map closely matches the actual quality observed by OM. Compared to traditional single-sensor-based quality prediction, the MFDT has achieved a significantly higher quality prediction accuracy (96%), a higher ROC-AUC score (99%), and a lower false alarm rate (4.4%). As a result, the MFDT is a more reliable method for defect prediction. The proposed MFDT also lays the groundwork for our future development of a self-adaptive hybrid processing strategy that combines machining with AM for defect removal and quality improvement.",
                "year": 2023,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "MRoCS: A new multi-robot communication system based on passive action recognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889015301482/pdfft?md5=c44deb464e7a8e4df43b65c7da36a00d&pid=1-s2.0-S0921889015301482-main.pdf",
                "title": "MRoCS: A new multi-robot communication system based on passive action recognition",
                "abstract": "Multi-robot search-and-rescue missions often face major challenges in adverse environments due to the limitations of traditional implicit and explicit communication. This paper proposes a novel multi-robot communication system (MRoCS), which uses a passive action recognition technique that overcomes the shortcomings of traditional models. The proposed MRoCS relies on individual motion, by mimicking the waggle dance of honey bees and thus forming and recognising different patterns accordingly. The system was successfully designed and implemented in simulation and with real robots. Experimental results show that, the pattern recognition process successfully reported high sensitivity with good precision in all cases for three different patterns thus corroborating our hypothesis.",
                "year": 2016,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "An innovative robotic training system imitating the cervical spine behaviors during rotation–traction manipulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889017304554/pdfft?md5=0a9ecaa47381fae7c5429bcacab80d8c&pid=1-s2.0-S0921889017304554-main.pdf",
                "title": "An innovative robotic training system imitating the cervical spine behaviors during rotation–traction manipulation",
                "abstract": "The present study demonstrated an innovative humanoid robot applied during Rotation–Traction (RT) manipulation practice and evaluation process. A mass–damper–spring mechanical system with an electromagnetic clutch was designed to emulate the cervical spine and a 3-DOF non-planar model was built to replace the neck part. With the help of an excellent electromechanical system and appropriate control strategy, the robot could imitate the entire dynamic responses of the human cervical spine during the RT manipulation process. Moreover, a novel adaptive force tracking impedance control was adopted to ensure a variable contact force in the unknown environment to imitate the real biomechanics of the human neck. In comparison to existing impedance control methods, the proposed control scheme is not only utility but also robust against external disturbances such as varying stiffness or uncertainties of the robot. The stability of the proposed impedance control was theoretically examined. Test results revealed that the cervical spine robot could faithfully replicate the biomechanical properties of the human cervical spine during RT manipulation and it is helpful in training and evaluating interns.",
                "year": 2018,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Sensorimotor transformations in the worlds of frogs and robots": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370294000556/pdfft?md5=555cd8d646c14cdb4fb110a88feb48f4&pid=1-s2.0-0004370294000556-main.pdf",
                "title": "Sensorimotor transformations in the worlds of frogs and robots",
                "abstract": "The paper develops a multilevel approach to the design and analysis of systems with “action-oriented perception”, situating various robot and animal “designs” in an evolutionary perspective. We present a set of biological design principles within a broader perspective that shows their relevance for robot design. We introduce schemas to provide a coarse-grain analysis of “cooperative computation” in the brains of animals and the “brains” of robots, starting with an analysis of approach, avoidance, detour behavior, and path planning in frogs. An explicit account of neural mechanism of avoidance behavior in the frog illustrates how schemas may be implemented in neural networks. The focus of the rest of the article is on the relation of instinctive to reflective behavior. We generalize an analysis of the interaction of perceptual schemas in the VISIONS system for computer vision to a view of the interaction of perceptual and motor schemas in distributed planning which, we argue, has great promise for integrating mechanisms for action and perception in both animal and robot. We conclude with general observations on the lessons on relating structure and function which can be carried from biology to technology.",
                "year": 1995,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "How the brain can discover the existence of external egocentric space": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0925231295001158/pdfft?md5=d6c5ae09ac06c535ae539dd6d6c93dec&pid=1-s2.0-0925231295001158-main.pdf",
                "title": "How the brain can discover the existence of external egocentric space",
                "abstract": "The neurobiological problem is addressed of how the brain might build an internal representation of external egocentric space by fusing the information from different sensory modalities. The proposed model is based on self-organizing topology representing networks, activated by multi-modal sensory vectors. The learnt representation is invariant to coordinate transformations and can support an active planning function by exploiting the lateral connectivity of the network.",
                "year": 1996,
                "publisher": "Neurocomputing"
            }
        }
    },
    "A 3D shape segmentation approach for robot grasping by parts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889011001552/pdfft?md5=1b7196c6703e3f0fcd49fc0518fcf811&pid=1-s2.0-S0921889011001552-main.pdf",
                "title": "A 3D shape segmentation approach for robot grasping by parts",
                "abstract": "Neuro-psychological findings have shown that human perception of objects is based on part decomposition. Most objects are made of multiple parts which are likely to be the entities actually involved in grasp affordances. Therefore, automatic object recognition and robot grasping should take advantage from 3D shape segmentation. This paper presents an approach toward planning robot grasps across similar objects by part correspondence. The novelty of the method lies in the topological decomposition of objects that enables high-level semantic grasp planning.\n\nIn particular, given a 3D model of an object, the representation is initially segmented by computing its Reeb graph. Then, automatic object recognition and part annotation are performed by applying a shape retrieval algorithm. After the recognition phase, queries are accepted for planning grasps on individual parts of the object. Finally, a robot grasp planner is invoked for finding stable grasps on the selected part of the object. Grasps are evaluated according to a widely used quality measure. Experiments performed in a simulated environment on a reasonably large dataset show the potential of topological segmentation to highlight candidate parts suitable for grasping.",
                "year": 2012,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "A self-trainable depth perception method from eye pursuit and motion parallax": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889017308539/pdfft?md5=921fa1b3ef43b867f47d98732065b2f2&pid=1-s2.0-S0921889017308539-main.pdf",
                "title": "A self-trainable depth perception method from eye pursuit and motion parallax",
                "abstract": "When humans move in a lateral direction (frontal plane), they intuitively understand the motion parallax phenomenon while jointly developing sensory neurons and pursuit eye movements with the help of their life-long learning experiences. At that time, various ranges of motion parallax effects are used to extract meaningful pieces of information such as relative depth of variously positioned objects and the spatial separation between the robot and the fixating object (absolute distance). By mimicking the visual learning in mammals to realize an autonomous robot system, a visual learning framework (Prucksakorn, 2016) was proposed to concurrently develop both visual sensory coding and pursuit eye movement with an addition of depth perception. Within the proposed framework, an artificial neural network was used to learn the relationship between the eye movements and the absolute distance. Nonetheless, the limitation of the proposed framework is that the predefined single lateral body movement cannot fully evoke the motion parallax effect for depth perception. Here, we extend the presented visual learning framework to accurately and autonomously represent the various ranges of absolute distance by using pursuit eye movements from multiple lateral body movements. We show that the proposed model, which is implemented in a HOAP3 humanoid robot simulator, can successfully enhance the smooth pursuit eye movement control with the self-calibrating ability and the distance estimation comparing to the single lateral movement based approach.",
                "year": 2018,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Autonomous construction and exploitation of a spatial memory by a self-motivated agent": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Autonomous construction and exploitation of a spatial memory by a self-motivated agent",
                "abstract": "We propose an architecture for self-motivated agents allowing them to construct their own knowledge of objects and of geometrical properties of space through interaction with their environment. Self-motivation is defined here as a tendency to experiment and to respond to behavioral opportunities afforded by the environment. Interactions have predefined valences that specify inborn behavioral preferences. The long-term goal is to design agents that construct their own knowledge of their environment through experience, rather than exploiting pre-coded knowledge. Over time, the agent learns relations between elements of the environment that afford its interactions, and its perception of these elements, in the form of data structures called signatures of interactions. These signatures allow the agent to attribute a low level semantics to elements that constitute its environment based on valences of interactions, without predefined knowledge about these elements and regardless of the number of element types. Signatures of interaction are then used to localize elements in space and to construct data structures that characterize spatial properties of space, called signatures of places and signatures of presence. Signatures of place and of presence characterize space using interactions rather than geometrical or topological properties. The agent uses these structures to maintain an egocentric representation of affordances of the surrounding environment, without any preconception about the elements that compose the environment, and without using notions of geometrical space. Experiments with simulated agents show that they learn to behave in their environment, taking into account multiple surrounding objects, reaching or avoiding objects according to the valence of the interactions that they afford.",
                "year": 2017,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Application of robotics in offshore oil and gas industry— A review Part II": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889015002018/pdfft?md5=fbac4483bb54666bf3084e84d0ff7717&pid=1-s2.0-S0921889015002018-main.pdf",
                "title": "Application of robotics in offshore oil and gas industry— A review Part II",
                "abstract": "Demands for oil and gas are increasing with urbanization and industrialization of the world’s increasing population. Giant oil fields are declining in their production worldwide and this situation is creating need for search of new conventional and non-conventional fossil reserves. With steep depletion of major onshore and shallow-water-offshore oil fields new search of fossil fuel is moving towards deep-water and ultra-deep water offshore fields. Obviously new reserves are located in extreme, hostile and hard-to-reach environmental conditions. Exploration, development and production of oil from such difficult offshore fields have many serious challenges to health, safety and environment (HSE) therefore, require sophisticated technological innovations to support increasing energy demand. Biggest oil spill accidents in explosion of Deepwater Horizon offshore oil platform are burning example of such challenges which human society cannot risk to repeat. Therefore, development of advance drilling system, more accurate and intelligent inspection mechanism, faster responsive system in cases of unfortunate incidence and efficient damage control system is need of the safer future. Successful implementation of robotics, in space and manufacturing industry, is an critical example of how robotic assistance and automation is the only option for safe and cost-effective production of oil in foreseeable future. Teleoperation of unmanned drilling and production platforms, remote operated vehicles (ROVs), autonomous underwater vehicles (AUVs), under-water welding, welding robots for double hulled ships and under-water manipulator are such key robotic technologies which have facilitated smooth transition of offshore rigs from shallow waters to ultra-deep waters in modern time. Considering the sensitivity of product and difficulty of environment, most of these technologies fall under semi-autonomous category, where human operator is in loop for providing cognitive assistance to the overall operation for safe execution. This paper summarizes the key robotic technologies currently used in offshore oil and gas facilities.",
                "year": 2016,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Cognitive vision: The case for embodied perception": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885606000631/pdfft?md5=7766825aaedb83ddb59ef440f2313f3b&pid=1-s2.0-S0262885606000631-main.pdf",
                "title": "Cognitive vision: The case for embodied perception",
                "abstract": "This paper considers arguments for the necessity of embodiment in cognitive vision systems. We begin by delineating the scope of cognitive vision, and follow this by a survey of the various approaches that can be taken to the realization of artificial cognitive vision systems, focussing on cognitive aspects. These range from the cognitivist symbolic representational paradigm, through connectionist systems and self-organizing dynamical systems, to the enactive cognition paradigm. We then consider various arguments for embodiment, beginning with paradigm-specific cases, and concluding with a paradigm-independent argument for embodied perception and cognition. We explore briefly different forms of embodiment and their relevance to the foregoing viewpoints. We highlight some of the key problems associated with embodied cognitive vision, including the phylogeny/ontogeny trade-off in artificial systems and the developmental limitations imposed by real-time environmental coupling. Finally, we conclude by considering some aspects of natural cognitive systems to see how they can provide insights to help in addressing these problems.",
                "year": 2008,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Robotic disassembly re-planning using a two-pointer detection strategy and a super-fast bees algorithm": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584518306483/pdfft?md5=079cf889751b39d938bc8399707699c4&pid=1-s2.0-S0736584518306483-main.pdf",
                "title": "Robotic disassembly re-planning using a two-pointer detection strategy and a super-fast bees algorithm",
                "abstract": "Automated disassembly of End-of-Life (EoL) products can be difficult to implement due to uncertainties in their conditions. An automatic re-planning function is required to enable flexible adjustments of disassembly plans and thus increase disassembly efficiency. The re-planning function is able to detect subassemblies and separable components, and adjust disassembly sequences and directions when components interlock and are irremovable. This paper presents a two-pointer detection strategy to find detachable subassemblies very quickly. A summation operator and a list with two pointers are used to check the interferences between components in a minimum number of steps. Then, a ternary bees algorithm is proposed to identify new disassembly sequences and directions. The algorithm combines the merits of a greedy search and meta-heuristic techniques by using only three collaborative potential solutions and three concurrent operations. Experimental results show that the proposed approach is able to perform a rapid subassembly detection and sequence optimisation for a robotic disassembly task, thus allowing real-time re-planning.",
                "year": 2019,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Deep sensorimotor learning for RGB-D object recognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314219301432/pdfft?md5=fa326c47ef3d79031e84c4c601380e9b&pid=1-s2.0-S1077314219301432-main.pdf",
                "title": "Deep sensorimotor learning for RGB-D object recognition",
                "abstract": "Research findings in cognitive neuroscience establish that humans, early on, develop their understanding of real-world objects by observing others interact with them or by performing active exploration and physical interactions with them. This fact has motivated the so-called “sensorimotor” learning approach, where the object appearance information (sensory) is combined with the object affordances (motor), i.e. the types of actions a human can perform with the object. In this work, the aforementioned paradigm is adopted, and a neuro-biologically inspired two-stream model for RGB-D object recognition is investigated. Both streams are realized as state-of-the-art deep neural networks that process and fuse appearance and affordance information in multiple ways. In particular, three model variants are developed to efficiently encode the spatio-temporal nature of the hand–object interaction, while an attention mechanism that relies on the appearance stream confidence is also investigated. Additionally, a suitable auxiliary loss is proposed for model training, utilized to further optimize both information streams. Experiments on the challenging SOR3D dataset, which consists of 14 object types and 13 object affordances, demonstrate the efficacy of the proposed model in RGB-D object recognition. Overall, the best performing developed model achieves 90.70% classification accuracy, which is further increased to 91.98% when trained using the auxiliary loss. The latter corresponds to 46% relative error reduction compared to the appearance-only classifier performance. Finally, a cross-view analysis on the SOR3D dataset provides valuable feedback for the viewpoint impact on the affordance information.",
                "year": 2020,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Assessment of ISO Standardisation to Identify an Industrial Robot's Base Frame": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584521001551/pdfft?md5=43fa95126a84a4ff1c3abbc451b6d0ae&pid=1-s2.0-S0736584521001551-main.pdf",
                "title": "Assessment of ISO Standardisation to Identify an Industrial Robot's Base Frame",
                "abstract": "There is a growing need for industrial robots to undertake high tolerance operations in line with Industry 4.0 (I4.0) demands and requirements. This requires robotic accuracy to be understood and quantified within appropriate bounds, relative to their application area. One such area is complex aerospace assembly where robotic accuracy is of particular importance if the full potential and benefits of I4.0 are to be realised in the sector. The standards BS EN ISO 9283:1998 and ISO/TR 13,309:1995 outline the requirements for calculating an industrial robot's accuracy. The standards highlight equipment capable of achieving this and they note that the robot base frame (RBF) must be determined as part of the process. The RBF is an exact position within the robot, and therefore it should be established with both accuracy and precision. However, a specific process or approach is not provided in either standard for the determination of a RBF. This has resulted in the use of bespoke methods by various researchers for the determination of their RBF using metrological equipment and associated software. These ad-hoc methods are not globally applied and the rationale and justification for their use remains unpublished. Previous research that presented a process used to construct the RBF, resulted in a varying RBF origin position when repeated. The work presented in this paper provides the basis for a common approach to the determination of a RBF which integrates metrology hardware with a Design of Experiments (DOE) approach to select an appropriate measurement routine. The DOE approach investigates how different factors (e.g. robot axis used, number of point positions used, their positions, and repetitions of their occurrence) influence the repeatability in establishing the RBF, by measuring the positions of points that the robot attains using different levels for each factor. This study used a Universal Robot to develop and demonstrate the proposed method for RBF determination using the factors that were found to affect point repeatability. This new method was validated by comparing the outcome of the applied process to four methods that used random combinations of factors. The approach was found to increase the repeatability in establishing the RBF origin point by 93.4%, compared to a previous method that used arbitrarily chosen factors.",
                "year": 2022,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Exploring retention and behavioral intentions when using social robotics to communicate a weather risk": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563218304047/pdfft?md5=0032c23754b7b5a99ad732f9863cdd01&pid=1-s2.0-S0747563218304047-main.pdf",
                "title": "Exploring retention and behavioral intentions when using social robotics to communicate a weather risk",
                "abstract": "During weather risks and crises, emergency managers and practitioners are often tasked with disseminating messages to individuals or groups of people who are located in a variety of hard-to-reach areas. Social robotics may serve as a useful delivery medium for disseminating some of these messages, if a group is located in an area that is hard to reach. This exploratory study investigated the information retention and behavioral intentions after being disseminated a mock weather risk message. The results provide support for delivery platform being important to consider when disseminating a risk message, such that novel technologies may actually lead to lower retention of message content. Discussion is centered around the implications of using novel technology to communicate risk messages.",
                "year": 2019,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A mixed truck and robot delivery approach for the daily supply of customers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221722001333/pdfft?md5=e064450e88a9f88dbe6607bf6e11d3de&pid=1-s2.0-S0377221722001333-main.pdf",
                "title": "A mixed truck and robot delivery approach for the daily supply of customers",
                "abstract": "Innovative last-mile logistics solutions are needed to reduce delivery costs, traffic congestion, and pollution in cities. A promising concept in this context are truck-and-robot systems with robot depots, as they enable significant cost and traffic reduction compared to classic truck deliveries. The system relies on small autonomous delivery robots to cover the last meters to a customer. Existing concepts consider home deliveries by robots, while trucks are only used to transport robots and not for deliveries. This assumption disregards the fact that regular truck deliveries are still needed for some delivery requests, such as for the delivery of bulky items, or for customers who do not accept robots.\n\nOur research addresses this issue and proposes the first mixed truck and robot delivery concept with robot depots in which both robots and the delivery truck can visit customers. Our tailored solution approach is based on a General Variable Neighborhood Search that efficiently solves the routing problem and outperforms existing truck-and-robot routing algorithms (reducing the runtime by 37 to 94% in experiments). The numerical experiments show that this approach enables cost reductions of up to 43% compared to classical truck deliveries and up to 22% compared to a truck-and-robot system that does not allow deliveries by both truck and robots on the same tour. Further analyses reveal additional benefits of such mixed tours and the robustness of our approach for different problem settings.",
                "year": 2022,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Robust optimization with applications to design of context specific robot solutions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584517301710/pdfft?md5=846512443958d30246997fd85f3f8d1b&pid=1-s2.0-S0736584517301710-main.pdf",
                "title": "Robust optimization with applications to design of context specific robot solutions",
                "abstract": "This paper presents an investigation of five optimization algorithms for simulation-based optimization for robotic tasks, where robust solutions are required. We evaluate the optimization methods on three use cases. The use cases involve using a robot for handling meat, optimizing gripper design for aligning objects and optimizing gripper design for table picking in cluttered scenes. We use dynamic simulations to model the use cases, where the most important physical aspects are captured. We have a focus on the robustness with respect to crucial system uncertainties, which is important in an industrial setting. The choice of parameterization and objective scores is also discussed since this choice has some impact on the performance of the optimization algorithms. For all problems, we find feasible solutions ready for real world testing, and overall the optimization method RBFopt has the best performance in terms of finding robust solutions within the fewest amount of simulations.",
                "year": 2018,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "A constant plunge depth control strategy for robotic FSW based on online trajectory generation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584522001612/pdfft?md5=ca80ae1d8ae3c413475f51a892c36a75&pid=1-s2.0-S0736584522001612-main.pdf",
                "title": "A constant plunge depth control strategy for robotic FSW based on online trajectory generation",
                "abstract": "Robotic friction stir welding (RFSW) usually comes with a huge upsetting force, and the stiffness of the welding system distributes unevenly over the position, which leads to a large deviation of the plunge depth of the tool at the end of the robot. The conventional constant distance tracking control suffers from the problem of unsmooth compensation leading to the vibration of the robot and thus degrading the weld quality. For this problem, a constant plunge depth control based on online trajectory generation for RFSW is studied, which can generate an accurate welding trajectory according to the rough initial reference path and smoothly compensate for the plunge deviation. Initially, three laser-ranging sensors are utilized to measure the pose deviation of the tool in real-time and generate the ideal welding trajectory according to the projection vector method. Then, a deformation compensation model is established to realize the real-time prediction of the correct value. To ensure the smoothness and rapidity of the dynamic tracking process of displacement deviation, we adopt an online trajectory generator as the core of optimization control to meet the process constraints such as speed, acceleration, and jerk during the compensation process. Finally, simulation and experiment are carried out. The results show that the proposed method can effectively reduce the vibration caused by compensation during the welding process and reduce flash, which can improve the welding quality.",
                "year": 2023,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "The physical symbol grounding problem": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041702000517/pdfft?md5=543d39576833df7d63fff145e9aee124&pid=1-s2.0-S1389041702000517-main.pdf",
                "title": "The physical symbol grounding problem",
                "abstract": "This paper presents an approach to solve the symbol grounding problem within the framework of embodied cognitive science. It will be argued that symbolic structures can be used within the paradigm of embodied cognitive science by adopting an alternative definition of a symbol. In this alternative definition, the symbol may be viewed as a structural coupling between an agent’s sensorimotor activations and its environment. A robotic experiment is presented in which mobile robots develop a symbolic structure from scratch by engaging in a series of language games. In this experiment it is shown that robots can develop a symbolic structure with which they can communicate the names of a few objects with a remarkable degree of success. It is further shown that, although the referents may be interpreted differently on different occasions, the objects are usually named with only one form.",
                "year": 2002,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Safety assurance of an industrial robotic control system using hardware/software co-verification": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642321001593/pdfft?md5=b9282e05812255b3831018edaf21968e&pid=1-s2.0-S0167642321001593-main.pdf",
                "title": "Safety assurance of an industrial robotic control system using hardware/software co-verification",
                "abstract": "As a general trend in industrial robotics, an increasing number of safety functions are being developed or re-engineered to be handled in software rather than by physical hardware such as safety relays or interlock circuits. This trend reinforces the importance of supplementing traditional, input-based testing and quality procedures which are widely used in industry today, with formal verification and model-checking methods. To this end, this paper focuses on a representative safety-critical system in an ABB industrial paint robot, namely the High-Voltage electrostatic Control system (HVC). The practical convergence of the high-voltage produced by the HVC, essential for safe operation, is formally verified using a novel and general co-verification framework where hardware and software models are related via platform mappings. This approach enables the pragmatic combination of highly diverse and specialised tools. The paper's main contribution includes details on how hardware abstraction and verification results can be transferred between tools in order to verify system-level safety properties. It is noteworthy that the HVC application considered in this paper has a rather generic form of a feedback controller. Hence, the co-verification framework and experiences reported here are also highly relevant for any cyber-physical system tracking a setpoint reference.",
                "year": 2022,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Behavioral models of the praying mantis as a basis for robotic behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889099001219/pdfft?md5=cc6b2738002eab1c7e77e829ce078f65&pid=1-s2.0-S0921889099001219-main.pdf",
                "title": "Behavioral models of the praying mantis as a basis for robotic behavior",
                "abstract": "Formal models of animal sensorimotor behavior can provide effective methods for generating robotic intelligence. In this article we describe how schema-theoretic models of the praying mantis derived from behavioral and neuroscientific data can be implemented on a hexapod robot equipped with a real time color vision system. This implementation incorporates a wide range of behaviors, including obstacle avoidance, prey acquisition, predator avoidance, mating, and chantlitaxia behaviors that can provide guidance to neuroscientists, ethologists, and roboticists alike. The goals of this study are threefold: to provide an understanding and means by which fielded robotic systems are not competing with other agents that are more effective at their designated task; to permit them to be successful competitors within the ecological system and capable of displacing less efficient agents; and that they are ecologically sensitive so that agent–environment dynamics are well-modeled and as predictable as possible whenever new robotic technology is introduced.",
                "year": 2000,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Goal emulation and planning in perceptual space using learned affordances": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889011000741/pdfft?md5=bb7095f9bf1d81a6d67757ff5146e5bb&pid=1-s2.0-S0921889011000741-main.pdf",
                "title": "Goal emulation and planning in perceptual space using learned affordances",
                "abstract": "In this paper, we show that through self-interaction and self-observation, an anthropomorphic robot equipped with a range camera can learn object affordances and use this knowledge for planning. In the first step of learning, the robot discovers commonalities in its action-effect experiences by discovering effect categories. Once the effect categories are discovered, in the second step, affordance predictors for each behavior are obtained by learning the mapping from the object features to the effect categories. After learning, the robot can make plans to achieve desired goals, emulate end states of demonstrated actions, monitor the plan execution and take corrective actions using the perceptual structures employed or discovered during learning. We argue that the learning system proposed shares crucial elements with the development of infants of 7–10 months age, who explore the environment and learn the dynamics of the objects through goal-free exploration. In addition, we discuss goal emulation and planning in relation to older infants with no symbolic inference capability and non-linguistic animals which utilize object affordances to make action plans.",
                "year": 2011,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "A jerk-limited heuristic feedrate scheduling method based on particle swarm optimization for a 5-DOF hybrid robot": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584522000837/pdfft?md5=3a17d73214a3e88966ac62c035d8da45&pid=1-s2.0-S0736584522000837-main.pdf",
                "title": "A jerk-limited heuristic feedrate scheduling method based on particle swarm optimization for a 5-DOF hybrid robot",
                "abstract": "Compared with machine tools, five degrees-of-freedom (DOFs) hybrid robots have been widely concerned in the manufacturing of large complex surface parts due to their characteristics of high flexibility and large workspace. It is of great significance to schedule the time-optimal feedrate that satisfies the high-order constraints (e.g., jerk or jounce) in the toolpath and joint systems to achieve the high-precision and high-efficiency machining of the robot. To overcome the complexity of five-axis feedrate scheduling and improve the optimality of machining time, this paper proposes a jerk-limited heuristic feedrate scheduling (HFS) method with near-optimal time. Firstly, the analytical equations between all constraints and the parametric feedrate are derived, and the mathematical model for control points optimization of the parametric feedrate profile expressed by a B-spline curve is established. Then, combined with the global search particle swarm optimization (GSPSO) algorithm, a proposed moving window planning method optimizes a small number of control points so that the feedrate curve has enough rising space. Subsequently, to obtain the near time-optimal feedrate, the local search PSO (LSPSO) algorithm within the moving window is developed to locally adjust the non-uniformly inserted control points. Compared with the existing methods, the HFS method is beneficial to further optimize the machining time with a faster computation speed, and ensure the stability of the robot machining. Finally, simulations and experiments on the developed TriMule-800 hybrid robot verify the effectiveness of this method.",
                "year": 2022,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Cognitive agents — a procedural perspective relying on the predictability of Object-Action-Complexes (OACs)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889008000973/pdfft?md5=1fae162d3de23900d740bc1e0d028253&pid=1-s2.0-S0921889008000973-main.pdf",
                "title": "Cognitive agents — a procedural perspective relying on the predictability of Object-Action-Complexes (OACs)",
                "abstract": "Embodied cognition suggests that complex cognitive traits can only arise when agents have a body situated in the world. The aspects of embodiment and situatedness are being discussed here from the perspective of linear systems theory. This perspective treats bodies as dynamic, temporally variable entities, which can be extended (or curtailed) at their boundaries. We show how acting agents can, for example, actively extend their body for some time by incorporating predictably behaving parts of the world and how this affects the transfer functions. We suggest that primates have mastered this to a large degree increasingly splitting their world into predictable and unpredictable entities. We argue that temporary body extension may have been instrumental in paving the way for the development of higher cognitive complexity as it is reliably widening the cause-effect horizon about the actions of the agent. A first robot experiment is sketched to support these ideas.\n\nWe continue discussing the concept of Object-Action Complexes (OACs) introduced by the European PACO-PLUS consortium to emphasize the notion that, for a cognitive agent, objects and actions are inseparably intertwined. In another robot experiment we devise a semi-supervised procedure using the OAC-concept to demonstrate how an agent can acquire knowledge about its world. Here the notion of predicting changes fundamentally underlies the implemented procedure and we try to show how this concept can be used to improve the robot’s inner model and behaviour. Hence, in this article we have tried to show how predictability can be used to augment the agent’s body and to acquire knowledge about the external world, possibly leading to more advanced cognitive traits.",
                "year": 2009,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Object–object interaction affordance learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889013002339/pdfft?md5=3dc4c69895dc641fafbec7c086d44197&pid=1-s2.0-S0921889013002339-main.pdf",
                "title": "Object–object interaction affordance learning",
                "abstract": "This paper presents a novel object–object affordance learning approach that enables intelligent robots to learn the interactive functionalities of objects from human demonstrations in everyday environments. Instead of considering a single object, we model the interactive motions between paired objects in a human–object–object way. The innate interaction-affordance knowledge of the paired objects are learned from a labeled training dataset that contains a set of relative motions of the paired objects, human actions, and object labels. The learned knowledge is represented with a Bayesian Network, and the network can be used to improve the recognition reliability of both objects and human actions and to generate proper manipulation motion for a robot if a pair of objects is recognized. This paper also presents an image-based visual servoing approach that uses the learned motion features of the affordance in interaction as the control goals to control a robot to perform manipulation tasks.",
                "year": 2014,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Perceptually-guided deep neural networks for ego-action prediction: Object grasping": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320318304011/pdfft?md5=8f00864632e2ad90eab3756201084054&pid=1-s2.0-S0031320318304011-main.pdf",
                "title": "Perceptually-guided deep neural networks for ego-action prediction: Object grasping",
                "abstract": "We tackle the problem of predicting a grasping action in ego-centric video for the assistance to upper-limb amputees. Our work is based on paradigms of neuroscience that state that human gaze expresses intention and anticipates actions. In our scenario, human gaze fixations are recorded by a glass-worn eye-tracker and then used to predict the grasping actions. We have studied two aspects of the problem: which object from a given taxonomy will be grasped, and when is the moment to trigger the grasping action. To recognize objects, we using gaze to guide Convolutional Neural Networks (CNN) to focus on an object-to-grasp area. However, the acquired sequence of fixations is noisy due to saccades toward distractors and visual fatigue, and gaze is not always reliably directed toward the object-of-interest. To deal with this challenge, we use video-level annotations indicating the object to be grasped and a weak loss in Deep CNNs. To detect a moment when a person will take an object we take advantage of the predictive power of Long-Short Term Memory networks to analyze gaze and visual dynamics. Results show that our method achieves better performance than other approaches on a real-life dataset.",
                "year": 2019,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Children’s embodied intuitive interaction — Design aspects of embodiment": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Children’s embodied intuitive interaction — Design aspects of embodiment",
                "abstract": "Intuitive features could make complex products and interfaces easier to use for children, and designing for embodied interactions is considered as one of the ways to make products and interfaces intuitive to use. However, there is lack of empirical study to validate this relationship and to determine how embodiment could be integrated in the design of products and interfaces.\n\nThis study has explored embodiment for intuitive interaction in children. The research question for the study was: what is the role of design aspects of embodiment in facilitation of intuitive interaction in children in the context of tactile interactions. The study identified the extent to which design aspects of embodiment facilitate intuitive interaction in children. An observational study with 108 children (55 girls and 53 boys) was carried out. Half of them played with physical Jenga and the other half played with a virtual Jenga.\n\nThe physical Jenga demonstrated more intuitive interactions than the equivalent virtual interface. Physical affordance is the prime contributor to children’s intuitive interaction with physical products while perceived affordance is the prime contributor to children’s intuitive interaction with virtual interfaces. Embodied interactions can be achieved through the following design aspects of embodiment — physical affordances, perceived affordances, scaffolding, emergence and cooperative activity. The study has further provided recommendations to make interfaces embodied and intuitive through the Enhanced Framework for Intuitive Interaction. These findings are significant as they provide insights into children’s embodied and intuitive interactions, which contribute to the broader context of children’s interaction with physical products and virtual interfaces.",
                "year": 2019,
                "publisher": "International Journal of Child-Computer Interaction"
            }
        }
    },
    "Collaborative robotics in wire harnesses spot taping process": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Collaborative robotics in wire harnesses spot taping process",
                "abstract": "Wire harnesses are used in several industrial sectors such as automotive, white goods, toys, or electrical and mechanical engineering. One of the key tasks of the process is to assemble and secure harnesses with a taping pistol in several spots to group single wires and make the final harness with all the required ramifications. The proposed method tries to advance the state of the art and enhance the current process, which is being performed manually, by adding robotic arms in a custom cell that collaborates with the human worker. The robotic solution of the proposal will perform all the spot tapings of the process, while the worker performs only the positioning of the cable. It reduces the processing time of the task and allows workers to work on two harnesses at the same time. The results of the present proposal will have a positive impact on companies dedicated to the production of wiring and the sectors to which they supply their products. The solution will also impact on the ergonomic conditions of workers through an innovative work environment that removes the most tedious and repetitive tasks of the operator.",
                "year": 2021,
                "publisher": "Computers in Industry"
            }
        }
    },
    "Automatic piano performance interaction system based on greedy algorithm for dexterous manipulator": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2096579624000548/pdfft?md5=e672f505cada3a54a42454aa69d9e666&pid=1-s2.0-S2096579624000548-main.pdf",
                "title": "Automatic piano performance interaction system based on greedy algorithm for dexterous manipulator",
                "abstract": "With continuous advancements in artificial intelligence (AI), automatic piano-playing robots have become subjects of cross-disciplinary interest. However, in most studies, these robots served merely as objects of observation with limited user engagement or interaction. To address this issue, we propose a user-friendly and innovative interaction system based on the principles of greedy algorithms. This system features three modules: score management, performance control, and keyboard interactions. Upon importing a custom score or playing a note via an external device, the system performs on a virtual piano in line with user inputs. This system has been successfully integrated into our dexterous manipulator-based piano-playing device, which significantly enhances user interactions.",
                "year": 2024,
                "publisher": "Virtual Reality & Intelligent Hardware"
            }
        }
    },
    "A survey on semantic-based methods for the understanding of human movements": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889018303932/pdfft?md5=e04695f5c92a4b367dce1701a5403a9e&pid=1-s2.0-S0921889018303932-main.pdf",
                "title": "A survey on semantic-based methods for the understanding of human movements",
                "abstract": "This paper presents semantic-based methods for the understanding of human movements in robotic applications. To understand human movements, robots need to first, recognize the observed or demonstrated human activities, and secondly, learn different parameters to execute an action or robot behavior. In order to achieve that, several challenges need to be addressed such as the automatic segmentation of human activities, identification of important features of actions, determine the correct sequencing between activities, and obtain the correct mapping between the continuous data and the symbolic and semantic interpretations of the human movements. This paper aims to present state-of-the-art semantic-based approaches, especially the new emerging approaches that tackle the challenges of finding generic and compact semantic models for the robotics domain. Finally, we will highlight potential breakthroughs and challenges for the next years such as achieving scalability, better generalization, compact and flexible models, and higher system accuracy.",
                "year": 2019,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "A new paradigm to study social and physical affordances as model-based reinforcement learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2667241324000107/pdfft?md5=08931f6c821eaa8f89deeabf14ab3737&pid=1-s2.0-S2667241324000107-main.pdf",
                "title": "A new paradigm to study social and physical affordances as model-based reinforcement learning",
                "abstract": "Social affordances, although key in human-robot interaction processes, have received little attention in robotics. Hence, it remains unclear whether the prevailing mechanisms to exploit and learn affordances in the absence of human interaction can be extended to affordances in social contexts. This study provides a review of the concept of affordance in psychology and robotics and proposes a new view on social affordances in robotics and their differences from physical affordances. We moreover show how the model-based reinforcement learning theory provides a useful framework to study and compare social and physical affordances. To further study their differences, we present a new benchmark task mixing navigation and social interaction, in which a robot has to make a human follow and reach different goal positions in a row. This new task is solved in simulation using a modular architecture and reinforcement learning.",
                "year": 2024,
                "publisher": "Cognitive Robotics"
            }
        }
    },
    "A hierarchical attention-based neural network architecture, based on human brain guidance, for perception, conceptualisation, action and reasoning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885609000407/pdfft?md5=2204b3bab3295fbfc64b22af3a8d9db0&pid=1-s2.0-S0262885609000407-main.pdf",
                "title": "A hierarchical attention-based neural network architecture, based on human brain guidance, for perception, conceptualisation, action and reasoning",
                "abstract": "We present a neural network software architecture, guided by that of the human and more generally primate brain, for the construction of an autonomous cognitive system (which we have named GNOSYS). GNOSYS is created so as to be able to attend to stimuli, to conceptualise them, to learn their predicted reward value and reason about them so as to attain those stimuli in the environment with greatest predicted value. We apply this software system to an embodied version in a robot, and describe the activities in the various component modules of GNOSYS, as well as the overall results. We briefly compare our system with some others proposed to have cognitive powers, and finish by discussion of future developments we propose for our system, as well as expanding on the arguments for and against our approach to creating such a software system.",
                "year": 2009,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Human inspired fall prediction method for humanoid robots": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889018309187/pdfft?md5=1c88ad3bcfaffddb0a8bad593511a4b6&pid=1-s2.0-S0921889018309187-main.pdf",
                "title": "Human inspired fall prediction method for humanoid robots",
                "abstract": "Humanoid robots are anticipated to work like humans in unstructured environments, and in such cases, falling over is inevitable due to the inherent postural instabilities and external disturbances arising from the environments. Since falling over may annihilate both the robot and its surroundings, we introduce in this work a generic method to predict the falling over of humanoid robots in a reliable, robust, and agile manner across various terrains, and also amidst arbitrary disturbances. The aforementioned characteristics have been strived to attain by proposing a prediction principle inspired by the human balance sensory systems. Accordingly, the fusion of multiple sensors such as inertial measurement unit and gyroscope (IMU), foot pressure sensor (FPS), joint encoders, and stereo vision sensor, which are equivalent to the human’s vestibular, proprioception, and vision systems are considered. For the prediction process, we first define a set of feature-based fall indicator variables (FIVs) from the different sensors, introduce prediction window parameters, and the thresholds for the FIVs are extracted using those parameters for four major disturbance scenarios. Further, an online threshold interpolation technique and an impulse adaptive counter limit are proposed to manage more generic disturbances. Finally, an instantaneous integer value is computed for each FIVs using their respective thresholds and the cumulative sum of them are normalized to predict the fall over by setting a suitable value as the critical limit. To determine the best combination and the usefulness of multiple sensors, the prediction performance is evaluated on four different types of terrains, in three unique combinations: first, each feature individually with their respective FIVs; second, an intuitive performance-based (PF); and finally, Kalman filter based (KF) techniques, which involve the usage of multiple features. For PF and KF techniques, prediction performance evaluations are carried out with and without adding noise to ascertain the influence of sensor noise. Overall, it is reported that KF performed better than PF and individual sensor features under different conditions. Also, the method’s ability to predict fall overs during the robot’s simple dynamic motion is also tested and verified through simulations. Experimental verification of the proposed prediction method on flat and uneven terrains is carried out with the WALK-MAN humanoid robot.",
                "year": 2019,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Visual object-action recognition: Inferring object affordances from human demonstration": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S107731421000175X/pdfft?md5=f63e93b9a0d928f62b4a46031623c270&pid=1-s2.0-S107731421000175X-main.pdf",
                "title": "Visual object-action recognition: Inferring object affordances from human demonstration",
                "abstract": "This paper investigates object categorization according to function, i.e., learning the affordances of objects from human demonstration. Object affordances (functionality) are inferred from observations of humans using the objects in different types of actions. The intended application is learning from demonstration, in which a robot learns to employ objects in household tasks, from observing a human performing the same tasks with the objects. We present a method for categorizing manipulated objects and human manipulation actions in context of each other. The method is able to simultaneously segment and classify human hand actions, and detect and classify the objects involved in the action. This can serve as an initial step in a learning from demonstration method. Experiments show that the contextual information improves the classification of both objects and actions.",
                "year": 2011,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Robots visual servo control with features constraint employing Kalman-neural-network filtering scheme": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231214012478/pdfft?md5=268c51f4b146c4d307307dea3b3a94e7&pid=1-s2.0-S0925231214012478-main.pdf",
                "title": "Robots visual servo control with features constraint employing Kalman-neural-network filtering scheme",
                "abstract": "This paper presents an image-based servo control approach with a Kalman-neural-network filtering scheme for robots manipulation in uncalibrated environment. The image Jacobian on-line identification problems are firstly addressed by introducing the state estimation techniques, which have been incorporated neural network assists Kalman filtering (NNAKF). In fact, this is, the neural network (NN) can serve to play exactly the role of the error estimator, has the task of compensate the errors of Kalman filtering (KF). Then, by employing the NNAKF scheme, the proposed image-based servo control approach has guaranteed the robustness with respect to destabilized system attached dynamic noises, as well as the image features are constrained in field-of-view (FOV) of the camera. Furthermore, it is without requiring the intrinsic and extrinsic parameters of the camera during visual servoing tasks. To demonstrate further the validity and practicality of proposed approach, various simulation and experimental results have been presented using a six-degree-of-freedom robotic manipulator with eye-in-hand configurations.",
                "year": 2015,
                "publisher": "Neurocomputing"
            }
        }
    },
    "GAM: General affordance-based manipulation for contact-rich object disentangling tasks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231224001577/pdfft?md5=7473df0742d82adf8e0ff9883cff107e&pid=1-s2.0-S0925231224001577-main.pdf",
                "title": "GAM: General affordance-based manipulation for contact-rich object disentangling tasks",
                "abstract": "Picking up an entangled object is a difficult manipulation task due to its rich contact dynamics. Most existing solutions fail to produce grasp poses to enable reliable manipulation due to the dependence on simplified assumptions for the motion policies. Grasps generated by these methods tend to drop objects or cause undesired movements of non-grasped objects. To improve such object-disentangling tasks, we propose to extend the concept of reinforcement learning (RL)-based affordance to include arbitrary action consequences and implement a general affordance-based manipulation (GAM) framework. In the GAM, we train an RL agent that uses more fine-grained actions and outperforms previous methods with a smaller chance of dropping objects and making contact with non-grasped hooks. Then, a manipulation affordance prediction (MAP) model is trained to estimate the performances of the RL agent. Finally, the manipulation affordance-based grasp filter (MAGF) selects grasp poses that afford the desired manipulation performances, showing substantial improvements in five challenging hook disentangling tasks in simulation. The experiments show (1) the limitation of TAG generators, (2) the effectiveness of filtering TAGs with predicted manipulation performances based on the general affordance theory, and (3) the importance of avoiding contact with non-grasped objects in contact-rich manipulation.",
                "year": 2024,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Robotics and computational thinking in primary school": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Robotics and computational thinking in primary school",
                "abstract": "This paper reports on a research study that examined how Australian primary school teachers integrated robotics and coding in their classrooms and the perceived impact this had on students’ computational thinking skills. The study involved four primary school teachers, (Years 1–6) from four schools, introducing LEGO® WeDo® 2.0 robotics kits in their classrooms. The data collected from questionnaires, journal entries, and semi-structured interviews were analyzed using computational thinking and teaching frameworks. The results demonstrate that exploring with and using the robot kits, and activities, helped the teachers build their confidence and knowledge to introduce young students to computational thinking. The study identified that teacher professional development (PD) needs to focus explicitly on how to teach developmentally appropriate robotics-based STEM activities that further promote computational concepts, practices, and perspectives.",
                "year": 2018,
                "publisher": "International Journal of Child-Computer Interaction"
            }
        }
    },
    "Interactive robot trajectory planning and simulation using Augmented Reality": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584511001116/pdfft?md5=796b22950090ad3f92ed27e4e1108a63&pid=1-s2.0-S0736584511001116-main.pdf",
                "title": "Interactive robot trajectory planning and simulation using Augmented Reality",
                "abstract": "Human–robot interaction in industrial robotics has largely been confined to finding better ways to reconfigure or program the robots. In this paper, an Augmented Reality based (RPAR-II) system is proposed to facilitate robot programming and trajectory planning considering the dynamic constraints of the robots. Through the various simulation capabilities provided in the proposed AR environment, the users are able to preview the simulated motion, perceive any possible overshoot, and resolve discrepancies between the planned and simulated paths prior to the execution of a task. By performing the simulation, the performance of the trajectory planning and the fitness of the selection of the robot controller model/parameters in the robot programming process can be visually evaluated. Practical issues concerning the system implementation are also discussed.",
                "year": 2012,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Consistent attack: Universal adversarial perturbation on embodied vision navigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000661/pdfft?md5=1f79893a63346bb6f59cc153492195fd&pid=1-s2.0-S0167865523000661-main.pdf",
                "title": "Consistent attack: Universal adversarial perturbation on embodied vision navigation",
                "abstract": "Embodied agents in vision navigation coupled with deep neural networks have attracted increasing attention. However, deep neural networks have been shown vulnerable to malicious adversarial noises, which may potentially cause catastrophic failures in Embodied Vision Navigation. Among different adversarial noises, universal adversarial perturbations (UAP), i.e., a constant image-agnostic perturbation applied on every input frame of the agent, play a critical role in Embodied Vision Navigation since they are computation-efficient and application-practical during the attack. However, existing UAP methods ignore the system dynamics of Embodied Vision Navigation and might be sub-optimal. In order to extend UAP to the sequential decision setting, we formulate the disturbed environment under the universal noise δ, as a δ-disturbed Markov Decision Process (δ-MDP). Based on the formulation, we analyze the properties of δ-MDP and propose two novel Consistent Attack methods, named Reward UAP and Trajectory UAP, for attacking Embodied agents, which consider the dynamic of the MDP and calculate universal noises by estimating the disturbed distribution and the disturbed Q function. For various victim models, our Consistent Attack can cause a significant drop in their performance in the PointGoal task in Habitat with different datasets and different scenes. Extensive experimental results indicate that there exist serious potential risks for applying Embodied Vision Navigation methods to the real world.",
                "year": 2023,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "Active estimation of distance in a robotic system that replicates human eye movement": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889006001187/pdfft?md5=e83e26b2e1321e565a822971d957fe5c&pid=1-s2.0-S0921889006001187-main.pdf",
                "title": "Active estimation of distance in a robotic system that replicates human eye movement",
                "abstract": "In a moving agent, the different apparent motion of objects located at various distances provides an important source of depth information. While motion parallax is evident for large translations of the agent, a small parallax also occurs in most head/eye systems during rotations of the cameras. A similar parallax is also present in the human eye, so that a redirection of gaze shifts the projection of an object on the retina by an amount that depends not only on the amplitude of the rotation, but also on the distance of the object with respect to the observer. This study examines the accuracy of distance estimation on the basis of the parallax produced by camera rotations. Sequences of human eye movements were used to control the motion of a pan/tilt system specifically designed to reproduce the oculomotor parallax present in the human eye. We show that the oculomotor strategies by which humans scan visual scenes produce parallaxes that provide accurate estimation of distance. This information simplifies challenging visual tasks such as image segmentation and figure/ground segregation.",
                "year": 2007,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Facial behaviour mapping—From video footage to a robot head": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889008001401/pdfft?md5=3ab5f8bf6076b68b566935e35365fdd1&pid=1-s2.0-S0921889008001401-main.pdf",
                "title": "Facial behaviour mapping—From video footage to a robot head",
                "abstract": "As autonomous robotic systems advance, they will be required and designed for interaction with humans in order to exchange information, which is essential for fulfilling their tasks. It is well established that human–machine interactions are more believable and memorable when a physical entity is present, provided that the machine behaves in a realistic manner. It is desirable to adopt face-to-face communication, because it is the most natural and efficient way of exchanging information, and does not require users to alter their habits. In this context, this paper describes a process for animating a robot head, based on video input of a human head. We map from the 2D coordinates of feature points into the robot’s servo space, using Partial Least Squares (PLS). Learning is done using a small set of keyframes manually created by an animator. The method is efficient, robust to tracking errors and independent of the scale of the face being tracked.",
                "year": 2008,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Semiotic schemas: A framework for grounding language in action and perception": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0004370205001037/pdfft?md5=2f31e33c3b15e4a87294c5118dd57f52&pid=1-s2.0-S0004370205001037-main.pdf",
                "title": "Semiotic schemas: A framework for grounding language in action and perception",
                "abstract": "A theoretical framework for grounding language is introduced that provides a computational path from sensing and motor action to words and speech acts. The approach combines concepts from semiotics and schema theory to develop a holistic approach to linguistic meaning. Schemas serve as structured beliefs that are grounded in an agent's physical environment through a causal-predictive cycle of action and perception. Words and basic speech acts are interpreted in terms of grounded schemas. The framework reflects lessons learned from implementations of several language processing robots. It provides a basis for the analysis and design of situated, multimodal communication systems that straddle symbolic and non-symbolic realms.",
                "year": 2005,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Object–Action Complexes: Grounded abstractions of sensory–motor processes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889011000935/pdfft?md5=04e90f94f82bfa2bbfad8db1ea8ae856&pid=1-s2.0-S0921889011000935-main.pdf",
                "title": "Object–Action Complexes: Grounded abstractions of sensory–motor processes",
                "abstract": "This paper formalises Object–Action Complexes (OACs) as a basis for symbolic representations of sensory–motor experience and behaviours. OACs are designed to capture the interaction between objects and associated actions in artificial cognitive systems. This paper gives a formal definition of OACs, provides examples of their use for autonomous cognitive robots, and enumerates a number of critical learning problems in terms of OACs.",
                "year": 2011,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Computational model of enactive visuospatial mental imagery using saccadic perceptual actions": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Computational model of enactive visuospatial mental imagery using saccadic perceptual actions",
                "abstract": "From the onset of cognitive revolution, the concept of mental imagery has been given different, many times opposing, theoretical accounts. Mental imagery appears to be a ubiquitous, yet wholly individual, easy to explain experience on the one hand, being hard to deal with scientifically on the other hand. The focus of this research is on an enactive approach to visuospatial mental imagery, inspired by Sima’s perceptual instantiation theory. We designed a hybrid computational model, composed of a forward model, an inverse model, both implemented as neural networks, and a memory/controller module, that grounds simple mental concepts, such as a triangle and a square, in perceptual actions, and is able to reimagine these objects by performing the necessary perceptual actions in a simulated humanoid robot. We tested the model on three tasks – salience-based object recognition, imagination-based object recognition and object imagination – and achieved very good results showing, as a proof of concept, that perceptual actions are a viable candidate for grounding the visuospatial mental concepts as well as the credible substrate of visuospatial mental imagery.",
                "year": 2018,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Robot programming using augmented reality: An interactive method for planning collision-free paths": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584508000665/pdfft?md5=0028426b2dc52853e375bfb03da0e8c8&pid=1-s2.0-S0736584508000665-main.pdf",
                "title": "Robot programming using augmented reality: An interactive method for planning collision-free paths",
                "abstract": "Current robot programming approaches lack the intuitiveness required for quick and simple applications. As new robotic applications are being identified, there is a greater need to be able to programme robots safely and quickly. This paper discusses the use of an augmented reality (AR) environment for facilitating intuitive robot programming, and presents a novel methodology for planning collision-free paths for an n-d.o.f. (degree-of-freedom) manipulator in a 3D AR environment. The methodology is interactive because the human is involved in defining the free space or collision-free volume (CFV), and selecting the start and goal configurations. The methodology uses a heuristic beam search algorithm to generate the paths. A number of possible scenarios are discussed.",
                "year": 2009,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Automatic surface roughing with 3D machine vision and cooperative robot control": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889007000152/pdfft?md5=1884a96d1ef490e35abfd8e645b0e8c5&pid=1-s2.0-S0921889007000152-main.pdf",
                "title": "Automatic surface roughing with 3D machine vision and cooperative robot control",
                "abstract": "This paper presents an innovative and practical strategy for automated leather surface roughing, using structured light 3D machine vision for object profile perception, and NURBS interpolation for accurate and smooth trajectory generation. As high pressure grit blasting is used for roughing, considering the spacial constraints in the blasting chamber, an additional degree of freedom is introduced using a rotary table, which supports the workpiece. Cooperative control is implemented between a 6-DOF robot and the rotary table to minimize robot movements, while satisfying the requirements of variable velocity control, accurate trajectory tracking and orientation control. Experimental results of consistent roughing performance have shown the efficiency of the proposed method.",
                "year": 2007,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "[I3] Imitation, Iteration and Improvisation: Embodied interaction in making and learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X1500071X/pdfft?md5=10d09337f39dcc071172e4f73a80f50e&pid=1-s2.0-S0142694X1500071X-main.pdf",
                "title": "[I3] Imitation, Iteration and Improvisation: Embodied interaction in making and learning",
                "abstract": "I introduce in this paper a new learning and making process that fosters a new ability to make things through the body's direct, iterative engagement with materials, tools, machines and objects. Tested in a variety of educational settings, this method, which I call ‘I3’ for its three-layer operation of ‘Imitation, Iteration and Improvisation’, allows learners to develop their sensory experiences to improvise and create on their own. I introduce case studies in order to test I3. I challenge the separation of design and construction often reinforced by the use of digital fabrication. I show that learning to make and learning from making emerge together through a situated and embodied interaction among the learner, the materials, the tools and the object in-the-making.",
                "year": 2015,
                "publisher": "Design Studies"
            }
        }
    },
    "Verified simulation for robotics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642318301655/pdfft?md5=052b401613c936cb5627b0ec21ad3ca0&pid=1-s2.0-S0167642318301655-main.pdf",
                "title": "Verified simulation for robotics",
                "abstract": "Simulation is a favoured technique for analysis of robotic systems. Currently, however, simulations are programmed in an ad hoc way, for specific simulators, using either proprietary languages or general languages like C or C++. Even when a higher-level language is used, no clear relation between the simulation and a design model is established. We describe a tool-independent notation called RoboSim, designed specifically for modelling of (verified) simulations. We describe the syntax, well-formedness conditions, and semantics of RoboSim. We also show how we can use RoboSim models to check if a simulation is consistent with a functional design written in a UML-like notation akin to those often used by practitioners on an informal basis. We show how to check whether the design enables a feasible scheduling of behaviours in cycles as needed for a simulation, and formalise implicit assumptions routinely made when programming simulations. We develop a running example and three additional case studies to illustrate RoboSim and the proposed verification techniques. Tool support is also briefly discussed. Our results enable the description of simulations using tool-independent diagrammatic models amenable to verification and automatic generation of code.",
                "year": 2019,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "A novel AR-based robot programming and path planning methodology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584509001100/pdfft?md5=7e3228f60ffc253a8b86e394f959ab89&pid=1-s2.0-S0736584509001100-main.pdf",
                "title": "A novel AR-based robot programming and path planning methodology",
                "abstract": "This paper discusses the benefits of applying Augmented Reality (AR) to facilitate intuitive robot programming, and presents a novel methodology for planning collision-free paths for an n degree-of-freedom (DOF) manipulator in an unknown environment. The targeted applications are where the end-effector is constrained to move along a visible 3D path/curve, which position is unknown, at a particular orientation with respect to the path, such as arc welding and laser cutting. The methodology is interactive as the human is involved in obtaining the 3D data points of the desired curve to be followed through performing a number of demonstrations, defining the free space relevant to the task, and planning the orientations of the end-effector along the curve. A Piecewise Linear Parameterization (PLP) algorithm is used to parameterize the data points using an interactively generated piecewise linear approximation of the desired curve. A curve learning method based on Bayesian neural networks and reparameterization is used to learn and generate 3D parametric curves from the parameterized data points. Finally, the orientation of the end-effector along the learnt curve is planned with the aid of AR. Two case studies are presented and discussed.",
                "year": 2010,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Multimodal attention networks for low-level vision-and-language navigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314221000990/pdfft?md5=f52b3af3a617e792c106047294751caf&pid=1-s2.0-S1077314221000990-main.pdf",
                "title": "Multimodal attention networks for low-level vision-and-language navigation",
                "abstract": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent needs to follow a language-specified path to reach a target destination. The goal gets even harder as the actions available to the agent get simpler and move towards low-level, atomic interactions with the environment. This setting takes the name of low-level VLN. In this paper, we strive for the creation of an agent able to tackle three key issues: multi-modality, long-term dependencies, and adaptability towards different locomotive settings. To that end, we devise “Perceive, Transform, and Act” (PTA): a fully-attentive VLN architecture that leaves the recurrent approach behind and the first Transformer-like architecture incorporating three different modalities — natural language, images, and low-level actions for the agent control. In particular, we adopt an early fusion strategy to merge lingual and visual information efficiently in our encoder. We then propose to refine the decoding phase with a late fusion extension between the agent’s history of actions and the perceptual modalities. We experimentally validate our model on two datasets: PTA achieves promising results in low-level VLN on R2R and achieves good performance in the recently proposed R4R benchmark.",
                "year": 2021,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Industrial robot efficient trajectory generation without collision through the evolution of the optimal trajectory": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889015301998/pdfft?md5=bdc4ccf927e48c73200af96ccc687f1d&pid=1-s2.0-S0921889015301998-main.pdf",
                "title": "Industrial robot efficient trajectory generation without collision through the evolution of the optimal trajectory",
                "abstract": "An efficient algorithm is presented to obtain trajectories for industrial robots working in industrial environments. The procedure starts with the obtaining of an optimal time trajectory neglecting the presence of obstacles. When obstacles are considered, the initial trajectory (obtained by neglecting obstacles) will not be feasible and will have to evolve so that it can become a solution. In this paper, the way that it evolves until a new feasible collision-free trajectory is obtained considering the possible obstacles is described. This is a direct algorithm that works in a discrete space of trajectories, approaching the global solution as the discretization is refined. The solutions obtained are efficient trajectories near to the minimum time one and they meet the physical limitations of the robot (the maximum values of torque, power and jerk are considered for each actuator), avoid collisions, and take into account the constraint of energy consumed. Examples already published and new examples in real industrial environments have been solved to verify the working of the algorithm.",
                "year": 2016,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "More things than are dreamt of in your biology: Information-processing in biologically inspired robots": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041704000348/pdfft?md5=a2904cf15d9b8960d48ac79faa892ddd&pid=1-s2.0-S1389041704000348-main.pdf",
                "title": "More things than are dreamt of in your biology: Information-processing in biologically inspired robots",
                "abstract": "Animals and robots perceiving and acting in a world require an ontology that accommodates entities, processes, states of affairs, etc., in their environment. If the perceived environment includes information-processing systems, the ontology should reflect that. Scientists studying such systems need an ontology that includes the first-order ontology characterising physical phenomena, the second-order ontology characterising perceivers of physical phenomena, and a (recursive) third order ontology characterising perceivers of perceivers, including introspectors. We argue that second- and third-order ontologies refer to contents of virtual machines and examine requirements for scientific investigation of combined virtual and physical machines, such as animals and robots. We show how the CogAff architecture schema, combining reactive, deliberative, and meta-management categories, provides a first draft schematic third-order ontology for describing a wide range of natural and artificial agents. Many previously proposed architectures use only a subset of CogAff, including subsumption architectures, contention-scheduling systems, architectures with ‘executive functions’ and a variety of types of ‘Omega’ architectures. Adding a multiply-connected, fast-acting ‘alarm’ mechanism within the CogAff framework accounts for several varieties of emotions. H-CogAff, a special case of CogAff, is postulated as a minimal architecture specification for a human-like system. We illustrate use of the CogAff schema in comparing H-CogAff with Clarion, a well known architecture. One implication is that reliance on concepts tied to observation and experiment can harmfully restrict explanatory theorising, since what an information processor is doing cannot, in general, be determined by using the standard observational techniques of the physical sciences or laboratory experiments. Like theoretical physics, cognitive science needs to be highly speculative to make progress.",
                "year": 2005,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Adaptive control of underactuated robots with unmodeled dynamics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889014002309/pdfft?md5=edaf26f39f692972f073b75b780233c2&pid=1-s2.0-S0921889014002309-main.pdf",
                "title": "Adaptive control of underactuated robots with unmodeled dynamics",
                "abstract": "This paper develops an adaptive controller for underactuated robotic systems with unmodeled dynamics. The control scheme is motivated by the applications of manipulators operating on dynamic platforms. The design decouples the system’s adaptation and control loops to allow for fast estimation rates, while guaranteeing bounded deviation from a nonadaptive reference system. The proposed formulation is independent of detailed information about the system model. The control scheme is tested in different trajectory-tracking scenarios: (i) a manipulator installed on a ship operating in a high-sea state with uncertain environmental disturbances and (ii) a mobile manipulator moving across a rough terrain of unknown geometry. The simulation results illustrate the tracking performance of the proposed control algorithm, its ability to deal with unmodeled dynamics, and its robustness to measurement noise and time delay, while maintaining smooth control signals.",
                "year": 2015,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Formally verified animation for RoboChart using interaction trees": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352220823000949/pdfft?md5=5312a452edca31ae541827e9147a0d84&pid=1-s2.0-S2352220823000949-main.pdf",
                "title": "Formally verified animation for RoboChart using interaction trees",
                "abstract": "RoboChart is a core notation in the RoboStar framework. It is a timed and probabilistic domain-specific and state machine-based language for robotics. RoboChart supports shared variables and communication across entities in its component model. It has formal denotational semantics given in CSP. The semantic technique of Interaction Trees (ITrees) represents behaviours of reactive and concurrent programs interacting with their environments. Recent mechanisation of ITrees, ITree-based CSP semantics and a Z mathematical toolkit in Isabelle/HOL bring new applications of verification and animation for state-rich process languages, such as RoboChart. In this paper, we use ITrees to give RoboChart novel operational semantics, implement it in Isabelle, and use Isabelle's code generator to generate verified and executable animations. We illustrate our approach using an autonomous chemical detector and patrol robot models, exhibiting nondeterminism and using shared variables. With animation, we show two concrete scenarios for the chemical detector when the robot encounters different environmental inputs and three for the patrol robot when its calibrated position is in other corridor sections. We also verify that the animated scenarios are trace refinements of the CSP denotational semantics of the RoboChart models using FDR, a refinement model checker for CSP. This ensures that our approach to resolve nondeterminism using CSP operators with priority is sound and correct.",
                "year": 2024,
                "publisher": "Journal of Logical and Algebraic Methods in Programming"
            }
        }
    },
    "What the body knows: Exploring the benefits of embodied metaphors in hybrid physical digital environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543808000647/pdfft?md5=adad71ee51708802e47dd84f1e8fceed&pid=1-s2.0-S0953543808000647-main.pdf",
                "title": "What the body knows: Exploring the benefits of embodied metaphors in hybrid physical digital environments",
                "abstract": "A recent trend in ubiquitous computing is the development of new forms of interfaces, which rely on embodied interaction. We focus on the definition of embodiment that refers to the ways in which abstract concepts rely on metaphorical extensions of embodied schemata shaped by processes below the level of conscious awareness as explored by Lakoff and Johnson [Lakoff, G., Johnson, M., 1980. Metaphors We Live By. Chicago Press, Chicago, IL, USA]. Our inquiry focuses on understanding the role embodied metaphors may play in supporting people to understand the possibilities for physical interaction in augmented spaces. We explore this issue through the development and evaluation of an interactive audio environment. We instantiate metaphor theory by using embodied schemata as the basis for the interactional metaphor that relates full-body input actions to audio output responses. We demonstrate and explore the benefits of this approach through a comparative experiment in which adults and children learn to use our audio environment. The results from our experiment indicated that embodied metaphors improve usability however, other factors including discoverability, perceivability of feedback and duplicity of structural isomorphism may mediate these metaphor-based benefits. We have generalized our main findings as a set of suggestions for the design of embodied style interfaces that rely on physical interaction.",
                "year": 2009,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Teleoperation by seamless transitions in real and virtual world environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889023000441/pdfft?md5=e3e65910c5d8ccfa87b6393c62555b6a&pid=1-s2.0-S0921889023000441-main.pdf",
                "title": "Teleoperation by seamless transitions in real and virtual world environments",
                "abstract": "This study investigates operability and acceptability issues in the teleoperation of robots. Prior studies have proposed efficient approaches to increase human perceptual ability and robot autonomy but with reduced operability and acceptance. We propose a novel teleoperation method that overcomes the weaknesses of existing approaches while inheriting their strengths. The key feature of our method is switching the teleoperated robot world from real to virtual. The user study results showed that the proposed method offered an improved user experience compared to the conventional methods, while task efficiency was equivalent in all methods. The contributions of this paper include the proposal of the teleoperation method by seamless switching between real and virtual space, the proposal of an image transformation method and visual effect to achieve seamless switching, and verification of the practicality of the proposed system through experiments on actual mobile robots.",
                "year": 2023,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Affordances, effectivities, and assisted imitation: Caregivers and the directing of attention": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231206005108/pdfft?md5=3d99fa97812ca95f78d361ca0c1601bf&pid=1-s2.0-S0925231206005108-main.pdf",
                "title": "Affordances, effectivities, and assisted imitation: Caregivers and the directing of attention",
                "abstract": "We focus on how infants’ discovery of a range of affordances and effectivities contributes to participating in a new activity. We emphasize how caregivers bracket ongoing actions with gestures that direct the infant's attention to perceptual information embodied in action sequences. Such supervised learning narrows the search space and enhances the speed of engaging adeptly in a new activity and provides a basis for achieving a common understanding of ongoing events. These caregiver practices during assisted imitation may illuminate how automata might detect and learn new affordances for action by observing and interacting with other intelligent agents.",
                "year": 2007,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Quantifying patterns of agent–environment interaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889005001545/pdfft?md5=489f4b4ac117b6c0b98202f5ea454e57&pid=1-s2.0-S0921889005001545-main.pdf",
                "title": "Quantifying patterns of agent–environment interaction",
                "abstract": "This article explores the assumption that a deeper (quantitative) understanding of the information-theoretic implications of sensory–motor coordination can help endow robots not only with better sensory morphologies, but also with better exploration strategies. Specifically, we investigate by means of statistical and information-theoretic measures to what extent sensory–motor coordinated activity can generate and structure information in the sensory channels of a simulated agent interacting with its surrounding environment. The results show how the usage of correlation, entropy, and mutual information can be employed (a) to segment an observed behavior into distinct behavioral states; (b) to analyze the informational relationship between the different components of the sensory–motor apparatus; and (c) to identify patterns (or fingerprints) in the sensory–motor interaction between the agent and its local environment.",
                "year": 2006,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Touchable pixels: Examining the affordance effect between an on-screen object and a user-elicited gesture on the touchscreen": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222004083/pdfft?md5=dc185c675e1339293e5ebb49d0e3df12&pid=1-s2.0-S0747563222004083-main.pdf",
                "title": "Touchable pixels: Examining the affordance effect between an on-screen object and a user-elicited gesture on the touchscreen",
                "abstract": "Touching a smartphone screen is one of the most frequent activities in our everyday life. These touch gestures have recently been investigated under the user-centered approach, which collects gestures proposed by end-users. Despite its advantages in cognitive and behavioral performance, these gestural studies neglect the effect of visual properties of interface objects, i.e., digital affordances. This study aims to explore the effect of digital affordances on the screen through two sequential studies. In the preliminary study, an online questionnaire and interview were conducted to investigate user-defined gestures. Twenty participants responded to the survey to select the most appropriate gesture in the combination between four functions and five gestures for eight stimuli manipulated in several visual properties. The collected gestures for each function were statistically compared in terms of visual properties. In the main study, a behavioral experiment was conducted to examine the object-based correspondence effect. Twenty-six participants executed real touch gestures on the eight stimuli, and the gestural response time was collected to compare corresponding and non-corresponding conditions between gestures and stimuli. The results of both the preliminary and main studies indicated significant effects of visual properties in gesture execution. We concluded that digital affordance exists on the touchscreen; furthermore, it has unique characteristics grounded on the hybrid materiality of a digital environment consisting of both hardware and software. The new findings on digital affordance can be summarized as 1) a digital entity is perceived as freely manipulable regardless of physical laws, 2) a visuo-perceptible entity can be only perceptually constrained by other visual obstacles, and 3) the gestural priority depends on the embodied direction inherent in particular content. This study contributes theoretically to a better understanding of digital affordance and practically designing touch gestures based on the characteristics of affordances on the screen.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "On-line near minimum-time path planning and control of an industrial robot for picking fruits": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169904000912/pdfft?md5=57d3071005219328b40b3a372fdade56&pid=1-s2.0-S0168169904000912-main.pdf",
                "title": "On-line near minimum-time path planning and control of an industrial robot for picking fruits",
                "abstract": "To be competitive, an industrial robot picking fruits must be able to perform this task in an amount of time which compares to that needed by humans. Because the location of a fruit changes due to the picking of others, the determination of their location has to be performed on-line and also the associated path planning for the robot. This poses two major problems in the development of fruit-picking robots since locating fruits and path planning, in general, are computationally expensive operations. This paper contributes to relaxing the second problem. Using the fact that the motions of the links of an industrial robot are approximately decoupled, this paper proposes a new method for near minimum-time path planning and control in the presence of obstacles. This method is computationally cheap compared to methods that solve the more general problem. Experimental results are presented which indicate the feasibility of this approach to make the robot competitive.",
                "year": 2004,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Control of perceptual attention in robot driving": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370295000291/pdfft?md5=e787437f1773c97fb0d92f8c627b2bb1&pid=1-s2.0-0004370295000291-main.pdf",
                "title": "Control of perceptual attention in robot driving",
                "abstract": "Computer vision research aimed at performing general scene understanding has proven to be conceptually difficult and computationally complex. Active vision is a promising approach to solving this problem. Active vision systems use optimized sensor settings, reduced fields of view, and relatively simple algorithms to efficiently extract specific information from a scene. This approach is only appropriate in the context of a task that motivates the selection of the information to extract. While there has been a fair amount of research that describes the extraction processes, there has been little work that investigates how active vision could be used for a realistic task in a dynamic domain. We are studying such a task: driving an autonomous vehicle in traffic.\n\nIn this paper we present a method for controlling visual attention as part of the reasoning process for driving, and analyze the efficiency gained in doing so. We first describe a model of driving and the driving environment, and estimate the complexity of performing the required sensing with a general driving-scene understanding system. We then introduce three programs that use increasingly sophisticated perceptual control techniques to select perceptual actions. The first program, called Ulysses-1, uses perceptual routines, which use known reference objects to guide the search for new objects. The second program, Ulysses-2, creates an inference tree to infer the effect of uncertain input data on action choices, and searches this tree to decide which data to sense. Finally, Ulysses-3 uses domain knowledge to reason about how dynamic objects will move or change over time; objects that do not move enough to affect the robot's decisions are not selected as perceptual targets. For each technique we have run experiments in simulation to measure the cost savings realized by using selective perception. We estimate that the techniques included in Ulysses-3 reduce the computational cost of perception by 9 to 12 orders of magnitude when compared to a general perception system.",
                "year": 1995,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Survey on model-based manipulation planning of deformable objects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584511000986/pdfft?md5=5ad5d90ec29b4d43d42ffdf73ff9a76c&pid=1-s2.0-S0736584511000986-main.pdf",
                "title": "Survey on model-based manipulation planning of deformable objects",
                "abstract": "A systematic overview on the subject of model-based manipulation planning of deformable objects is presented. Existing modeling techniques of volumetric, planar and linear deformable objects are described, emphasizing the different types of deformation. Planning strategies are categorized according to the type of manipulation goal: path planning, folding/unfolding, topology modifications and assembly. Most current contributions fit naturally into these categories, and thus the presented algorithms constitute an adequate basis for future developments.",
                "year": 2012,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Model-based furniture recognition for building semantic object maps": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S000437021400157X/pdfft?md5=b5aa037a59eebeeb97409ff8c0f00639&pid=1-s2.0-S000437021400157X-main.pdf",
                "title": "Model-based furniture recognition for building semantic object maps",
                "abstract": "This paper presents an approach to creating a semantic map of an indoor environment incrementally and in closed loop, based on a series of 3D point clouds captured by a mobile robot using an RGB-D camera. Based on a semantic model about furniture objects (represented in an OWL-DL ontology with rules attached), we generate hypotheses for locations and 6DoF poses of object instances and verify them by matching a geometric model of the object (given as a CAD model) into the point cloud. The result, in addition to the registered point cloud, is a consistent mesh representation of the environment, further enriched by object models corresponding to the detected pieces of furniture. We demonstrate the robustness of our approach against occlusion and aperture limitations of the RGB-D frames, and against differences between the CAD models and the real objects. We evaluate the complete system on two challenging datasets featuring partial visibility and totaling over 800 frames. The results show complementary strengths and weaknesses of processing each frame directly vs. processing the fully registered scene, which accord with intuitive expectations.",
                "year": 2017,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Heading direction of a mobile robot from the optical flow": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885699000360/pdfft?md5=7831b489dfb6fa1c2ad6a70ca480d439&pid=1-s2.0-S0262885699000360-main.pdf",
                "title": "Heading direction of a mobile robot from the optical flow",
                "abstract": "If a camera, mounted on a mobile robot, moves on a straight line, the optic flow field is a diverging vector field, of which the singularity is called “focus of expansion” (FOE). An object that is seen in this FOE is located on the future path of the camera. However, a mobile robot usually moves on a curved path such that the future path is no longer a point in the image domain, but a line. All objects which are on the future path (and thus will cause collisions) are projected on this line. Not necessarily the reverse is true: not all points on the line result in collisions. In this paper we derive how the optic flow can be used to compute which objects in the image are projections of future collisions. Experiments under controlled conditions are carried out to test the theory.",
                "year": 2000,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Tool-body assimilation model considering grasping motion through deep learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889016303852/pdfft?md5=b92133ba8f6b4c59b12b50148c5a7acc&pid=1-s2.0-S0921889016303852-main.pdf",
                "title": "Tool-body assimilation model considering grasping motion through deep learning",
                "abstract": "We propose a tool-body assimilation model that considers grasping during motor babbling for using tools. A robot with tool-use skills can be useful in human–robot symbiosis because this allows the robot to expand its task performing abilities. Past studies that included tool-body assimilation approaches were mainly focused on obtaining the functions of the tools, and demonstrated the robot starting its motions with a tool pre-attached to the robot. This implies that the robot would not be able to decide whether and where to grasp the tool. In real life environments, robots would need to consider the possibilities of tool-grasping positions, and then grasp the tool. To address these issues, the robot performs motor babbling by grasping and nongrasping the tools to learn the robot’s body model and tool functions. In addition, the robot grasps various parts of the tools to learn different tool functions from different grasping positions. The motion experiences are learned using deep learning. In model evaluation, the robot manipulates an object task without tools, and with several tools of different shapes. The robot generates motions after being shown the initial state and a target image, by deciding whether and where to grasp the tool. Therefore, the robot is capable of generating the correct motion and grasping decision when the initial state and a target image are provided to the robot.",
                "year": 2017,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Visual affordance detection using an efficient attention convolutional neural network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231221000278/pdfft?md5=268206f5942a2788b6095cff10825da1&pid=1-s2.0-S0925231221000278-main.pdf",
                "title": "Visual affordance detection using an efficient attention convolutional neural network",
                "abstract": "Visual affordance detection is an important issue in the field of robotics and computer vision. This paper proposes a novel and practical convolutional neural network architecture that adopts an encoder-decoder architecture for pixel-wise affordance detection. The encoder network comprises two modules: a dilated residual network that is the backbone for feature extraction, and an attention mechanism that is used for modeling long-range, multi-level dependency relations. The decoder network consists of a novel up-sampling layer that maps the low-resolution encoder feature to a high-resolution pixel-wise prediction map. Specifically, integrating an attention mechanism into our network reduces the loss of salient details and improves the feature representation performance of the model. The results of experiments conducted on the University of Maryland dataset (UMD) verify that the proposed network with the attention mechanism and up-sampling layer improved performance compared with classical methods. The proposed method lays the foundation for subsequent research on multi-task learning by physical robots.",
                "year": 2021,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Online learning of task-driven object-based visual attention control": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885609002224/pdfft?md5=eed0dde139c6df91638d4bd5bed02dfe&pid=1-s2.0-S0262885609002224-main.pdf",
                "title": "Online learning of task-driven object-based visual attention control",
                "abstract": "We propose a biologically-motivated computational model for learning task-driven and object-based visual attention control in interactive environments. In this model, top-down attention is learned interactively and is used to search for a desired object in the scene through biasing the bottom-up attention in order to form a need-based and object-driven state representation of the environment. Our model consists of three layers. First, in the early visual processing layer, most salient location of a scene is derived using the biased saliency-based bottom-up model of visual attention. Then a cognitive component in the higher visual processing layer performs an application specific operation like object recognition at the focus of attention. From this information, a state is derived in the decision making and learning layer. Top-down attention is learned by the U-TREE algorithm which successively grows an object-based binary tree. Internal nodes in this tree check the existence of a specific object in the scene by biasing the early vision and the object recognition parts. Its leaves point to states in the action value table. Motor actions are associated with the leaves. After performing a motor action, the agent receives a reinforcement signal from the critic. This signal is alternately used for modifying the tree or updating the action selection policy. The proposed model is evaluated on visual navigation tasks, where obtained results lend support to the applicability and usefulness of the developed method for robotics.",
                "year": 2010,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Memory-Adaptive Vision-and-Language Navigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002620/pdfft?md5=57066187cccfd024da7c4c7db08a34d3&pid=1-s2.0-S0031320324002620-main.pdf",
                "title": "Memory-Adaptive Vision-and-Language Navigation",
                "abstract": "Vision-and-Language Navigation (VLN) requests an agent to navigate in 3D environments following given instructions, where history is critical for decision-making in dynamic navigation process. Particularly, a memory bank storing histories is widely used in existing methods to incorporate with multimodel representations in current scenes for better decision-making. However, by weighting each history with a simple scalar, those methods cannot purely utilize the informative cues that co-exist with detrimental contents in each history, thereby inevitably introducing noises into decision-making. To that end, we propose a novel Memory-Adaptive Model (MAM) that can dynamically restrain the detrimental contents in histories for retaining contents that benefit navigation only. Specifically, two key modules, Visual and Textual Adaptive Modules, are designed to restrain history noises based on scene-related vision and text, respectively. A Reliability Estimator Module is further introduced to refine above adaptation operations. Our experiments on the widely used RxR and R2R datasets show that MAM outperforms its baseline method by 4.0%/2.5% and 2%/1% on the validation unseen/test split, respectively, wrt the SR metric.",
                "year": 2024,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "An integrated system for the sensor processing and control of robot systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0141933198000532/pdfft?md5=bdb4d3932c51d2480383aec421eaef64&pid=1-s2.0-S0141933198000532-main.pdf",
                "title": "An integrated system for the sensor processing and control of robot systems",
                "abstract": "For a mobile robot to operate with some degree of autonomy it needs an awareness of its environment. This awareness may range from a few stored coordinates to a detailed map which is continuously updated. Within a mobile robot controller a considerable amount of processing effort can be consumed to provide this data. Despite the increasing performance of digital electronic components, analogue electronics can still provide significant performance gains. However, analogue systems lack flexibility—one solution to this is a hybrid analogue/digital signal processing system. This paper describes the development of the hardware and software for such a system.",
                "year": 1998,
                "publisher": "Microprocessors and Microsystems"
            }
        }
    },
    "Mapless navigation via Hierarchical Reinforcement Learning with memory-decaying novelty": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889024001994/pdfft?md5=d8582d1a21a5b1405a794b5c34b147fa&pid=1-s2.0-S0921889024001994-main.pdf",
                "title": "Mapless navigation via Hierarchical Reinforcement Learning with memory-decaying novelty",
                "abstract": "Hierarchical Reinforcement Learning (HRL) has shown superior performance for mapless navigation tasks. However, it remains limited in unstructured environments that might contain terrains like long corridors and dead corners, which can lead to local minima. This is because most HRL-based mapless navigation methods employ a simplified reward setting and exploration strategy. In this work, we propose a novel reward function for training the high-level (HL) policy, which contains two components: extrinsic reward and intrinsic reward. The extrinsic reward encourages the robot to move towards the target location, while the intrinsic reward is computed based on novelty, episode memory and memory decaying, making the agent capable of accomplishing spontaneous exploration. We also design a novel neural network structure that incorporates an LSTM network to augment the agent with memory and reasoning capabilities. We test our method in unknown environments and specific scenarios prone to the local minimum problem to evaluate the navigation performance and local minimum resolution ability. The results show that our method significantly increases the success rate when compared to advanced RL-based methods, achieving a maximum improvement of nearly 28%. Our method demonstrates effective improvement in addressing the local minimum issue, especially in cases where the baselines fail completely. Additionally, numerous ablation studies consistently confirm the effectiveness of our proposed reward function and neural network structure.",
                "year": 2024,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Autonomous Object Modeling based on Affordances in a Dynamic Environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S187705091503642X/pdfft?md5=2de8b7b41ab835767fd40d1e7ac7fe54&pid=1-s2.0-S187705091503642X-main.pdf",
                "title": "Autonomous Object Modeling based on Affordances in a Dynamic Environment",
                "abstract": "We present an architecture for self-motivated agents to generate behaviors in a dynamic envi- ronment according to its possibilities of interactions. Some interactions have predefined valences that specify inborn behavioral preferences. Over time, the agent learns to recognize affordances in its surrounding environment under the form of structures called signatures of interactions. The agent keeps track of enacted interactions in a spatial memory to generate a completed con- text in which it can use signatures to recognize and localize distant possibilities of interactions, and generates behaviors that satisfy its motivation principles.",
                "year": 2015,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Abnormal lower limb posture recognition based on spatial gait feature dynamic threshold detection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157824002507/pdfft?md5=27cec39130c542af88b8b1f0132833cd&pid=1-s2.0-S1319157824002507-main.pdf",
                "title": "Abnormal lower limb posture recognition based on spatial gait feature dynamic threshold detection",
                "abstract": "Lower limb rehabilitation training often involves the use of assistive standing devices. However, elderly individuals frequently experience reduced exercise effectiveness or suffer muscle injuries when utilizing these devices. The ability to recognize abnormal lower limb postures can significantly enhance training efficiency and minimize the risk of injury. To address this, we propose a model based on dynamic threshold detection of spatial gait features to identify such abnormal postures. A human-assisted standing rehabilitation device platform was developed to build a lower limb gait depth dataset. RGB data is employed for keypoint detection, enabling the establishment of a 3D lower limb posture recognition model that extracts gait, time, spatial features, and keypoints. The predicted joint angles, stride length, and step frequency demonstrate errors of 4 %, 8 %, and 1.3 %, respectively, with an average confidence of 0.95 for 3D key points. We employed the WOA-BP neural network to develop a dynamic threshold algorithm based on gait features and propose a model for recognizing abnormal postures. Compared to other models, our model achieves a 96 % accuracy rate in recognizing abnormal postures, with a recall rate of 83 % and an F1 score of 90 %. ROC curve analysis and AUC values reveal that the WOA-BP algorithm performs farthest from the pure chance line, with the highest AUC value of 0.89, indicating its superior performance over other models. Experimental results demonstrate that this model possesses a strong capability in recognizing abnormal lower limb postures, encouraging patients to correct these postures, thereby reducing muscle injuries and improving exercise effectiveness.",
                "year": 2024,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "An Intelligent Virtual Environment for Designers with Reduced Motor Abilities": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050923000315/pdfft?md5=73d35fb6a6b4670e456ffca7a9c24788&pid=1-s2.0-S1877050923000315-main.pdf",
                "title": "An Intelligent Virtual Environment for Designers with Reduced Motor Abilities",
                "abstract": "Conventional CAD modelling software demands substantial utilisation of input modalities like the keyboard and mouse for creating 3-dimensional (3D) models. The dexterity measures involved in controlling input modalities could pose challenges to users with motor disabilities—including the inability to move their limbs, particularly their upper and lower arms, and fingers, due to traumatic damage or congenital problems. In order to meet these challenges, this paper proposes a virtual reality (VR)-based medium to help users with motor disabilities build simple 3D models for architectural design. The concept of operating buttons using head-gaze in the VR environment has been utilised to perform scaling—a 3D object manipulation method—to create simplified building models. Moreover, navigation in the VR space using tilting of the head has been employed with the user seated on a revolving chair, thus eliminating the need for any limbic movement. Unity game engine was used to develop two variations of the VR model with a different button layout for creating simple cuboidal volumes mimicking buildings in the virtual environment. Both variations have been tested with 32 individuals against a specific performance indicator (i.e., task completion time) and self-reported metrics, such as the perception of effort applied and degree of visual clutter, followed by retrospective participant feedback sessions. One of the VR application's variants (i.e., variant 1) produced promising results regarding overall usability and effort demand. This paper also proposes a methodological framework for an AI-based, intelligent, and adaptive VR application interface that caters to the user's abilities and pain points in real-time. In the future, this framework could be instrumental in creating a comprehensive gaze-based VR tool for 3D modelling having multiple functions to help users with motor disabilities.",
                "year": 2023,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Learning grasping points with shape context": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889009001699/pdfft?md5=9326645fb35b37f69c896f533b7d76a7&pid=1-s2.0-S0921889009001699-main.pdf",
                "title": "Learning grasping points with shape context",
                "abstract": "This paper presents work on vision based robotic grasping. The proposed method adopts a learning framework where prototypical grasping points are learnt from several examples and then used on novel objects. For representation purposes, we apply the concept of shape context and for learning we use a supervised learning approach in which the classifier is trained with labelled synthetic images. We evaluate and compare the performance of linear and non-linear classifiers. Our results show that a combination of a descriptor based on shape context with a non-linear classification algorithm leads to a stable detection of grasping points for a variety of objects.",
                "year": 2010,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Beyond sensorimotor segregation: On mirror neurons and social affordance space tracking": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041715000066/pdfft?md5=12e00539492368df37eff877aaea842b&pid=1-s2.0-S1389041715000066-main.pdf",
                "title": "Beyond sensorimotor segregation: On mirror neurons and social affordance space tracking",
                "abstract": "Mirror neuron research has come a long way since the early 1990s, and many theorists are now stressing the heterogeneity and complexity of the sensorimotor properties of fronto-parietal circuits. However, core aspects of the initial ‘mirror mechanism’ theory, i.e. the idea of a symmetric encapsulated mirroring function translating sensory action perceptions into motor formats, still appears to be shaping much of the debate. This article challenges the empirical plausibility of the sensorimotor segregation implicit in the original mirror metaphor. It is proposed instead that the teleological organization found in the broader fronto-parietal circuits might be inherently sensorimotor. Thus the idea of an independent ‘purely perceptual’ goal understanding process is questioned. Further, it is hypothesized that the often asymmetric, heterogeneous and contextually modulated mirror and canonical neurons support a function of multisensory mapping and tracking of the perceiving agents affordance space. Such a shift in the interpretative framework offers a different theoretical handle on how sensorimotor processes might ground various aspects of intentional action choice and social cognition. Mirror neurons would under the proposed “social affordance model” be seen as dynamic parts of larger circuits, which support tracking of currently shared and competing action possibilities. These circuits support action selection processes—but also our understanding of the options and action potentials that we and perhaps others have in the affordance space. In terms of social cognition ‘mirror’ circuits might thus help us understand not only the intentional actions others are actually performing—but also what they could have done, did not do and might do shortly.",
                "year": 2015,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Fine-grained action plausibility rating": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889019305536/pdfft?md5=1c85072d5e952d7d3c5fed7d11dc7a53&pid=1-s2.0-S0921889019305536-main.pdf",
                "title": "Fine-grained action plausibility rating",
                "abstract": "An essential capability of humans is the effortless identification of useful tasks based on visual cues in everyday situations. Objects and their surroundings are integrated and processed to differentiate plausible from implausible actions. In this work, we study how to teach this ability to robots. In contrast to many tasks in computer vision where the goal is an accurate description (object labels, caption, scene class) of the present situation here the challenge is to make reasonable guesses about which forms of plausible and implausible actions can be conducted. To this end, we collect a dataset that associates images with probabilities over a set of actions. A convolutional neural network is trained to match these ground truth plausibility scores using this dataset. We compare the performance of state-of-the-art encoder architectures and specifically analyze the role of contextual cues quantitatively. While the object recognition capabilities of the encoder have a strong impact on performance, using context did not lead to substantial improvements. We show qualitatively the utility of such a system for robotic action selection in a household setting.",
                "year": 2020,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Principles of animate vision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/104996609290081D/pdfft?md5=770bed6e82fafeb95ac42edac5ca157e&pid=1-s2.0-104996609290081D-main.pdf",
                "title": "Principles of animate vision",
                "abstract": "Vision theories can be categorized in terms of the amount of explicit representation postulated in the perceiver. Gibson's precomputational theory eschewed any explicit representation. In contrast, Marr used layers of explicit representation, hoping to simplify vision computations. Current technological advances in robotic hardware and computer architectures have allowed the building of anthropomorphic devices that capture important technical features of human vision. Experience with these devices suggests that cooperative sensorimotor behaviors can reduce the need for explicit representation. This view is captured in the notion of “animate vision,” which is a framework for sequential decision-making, gaze control, and visual learning.",
                "year": 1992,
                "publisher": "CVGIP: Image Understanding"
            }
        }
    },
    "Determining maximum payloads for cooperating robots under time-optimal control": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0736584593900077/pdfft?md5=f9eefceb8e7338a1c92632702b4e38fa&pid=1-s2.0-0736584593900077-main.pdf",
                "title": "Determining maximum payloads for cooperating robots under time-optimal control",
                "abstract": "This work presents an algorithm which evaluates the dynamic performance limit of a cooperating robotic system using movements planned for minimum time. Minimum-time movements characteristically require that a set of motors in the robot be driven at their maximum torque throughout the motion. These movements are limited by the combination of motor performance, mechanical advantage of the kinematic chain, and the location of the start and goal positions. By increasing the payload for a motion until a minimum-time solution is no longer feasible the payload limit of the system for the associated path is obtained. To illustrate the algorithm a detailed analysis of a robotic arm developed at Odetics Inc. is presented. The analysis includes numerical results for cooperating Odetics robotic arms using their maximum payload under time-optimal control. Furthermore, the maximum payload for the cooperating robotic system to perform the same motion with a 1 sec time constraint is determined.",
                "year": 1993,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "A DESIGN PROCEDURE FOR A NOVEL CONCEPT OF SHAPE-MEMORY-ALLOY-ACTUATED FINGER EXOSKELETON": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889025000053/pdfft?md5=fcebf17642288ecd3fe059ab31ca3b2b&pid=1-s2.0-S0921889025000053-main.pdf",
                "title": "A DESIGN PROCEDURE FOR A NOVEL CONCEPT OF SHAPE-MEMORY-ALLOY-ACTUATED FINGER EXOSKELETON",
                "abstract": "This paper provides a thorough investigation demonstrating the feasibility of an innovative finger exoskeleton design concept in a compact, self-contained shape using Shape Memory Alloy (SMA) wires. This novel design integrates SMA wires directly onto the finger body, eliminating the need for bulky wrist actuators. Strategically positioned, the SMA wires serve as both actuators and sensors. The proposed exoskeleton can be used for rehabilitation purposes or for hand augmentation during manufacturing tasks. To properly design the exoskeleton, multiple simulation models have been developed. Kinematic and dynamic analyses were conducted to determine the sufficient motion ranges and forces as a function of SMA wire locations. Both models and experiments confirm the feasibility of integrating SMA wires within the limited space near the fingers. Several experimental tests were conducted to validate the simulation model results. Moreover, thermal-camera measurements confirm that SMA wires can be safely isolated and attached to the finger, preventing skin overheating. All simulation and experimental outcomes indicate the reliability of the proposed design procedure and the engineering feasibility of our novel finger.",
                "year": 2025,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Objects, spatial compatibility, and affordances: A connectionist study": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S138904171000029X/pdfft?md5=f68f76f7402ee55872499c27d42c2b53&pid=1-s2.0-S138904171000029X-main.pdf",
                "title": "Objects, spatial compatibility, and affordances: A connectionist study",
                "abstract": "In two Artificial Life simulations we evolved artificial organisms possessing a visual and a motor system, and whose nervous system was simulated with a neural network. Each organism could see four objects, either upright or reversed, with a left or a right handle. In Task 1 they learned to reach the object handle independently of the handle’s position. In Task 2 they learned to reach one of two buttons located below the handle either to decide where the handle was (Simulation 1) or whether the object was upright or reversed (Simulation 2). Task 1 simulated real life experience, Task 2 replicated either a classic spatial compatibility task (Simulation 1) or an experiment by Tucker and Ellis (1998) (Simulation 2). In both simulations learning occurred earlier in the Compatible condition, when the button to reach and the handle were on the same side, than in the Incompatible condition.",
                "year": 2011,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "COSMO: Contextualized scene modeling with Boltzmann Machines": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889018303427/pdfft?md5=2f2e373aa6a1bb76bbf4dac41c6c1800&pid=1-s2.0-S0921889018303427-main.pdf",
                "title": "COSMO: Contextualized scene modeling with Boltzmann Machines",
                "abstract": "Scene modeling is very crucial for robots that need to perceive, reason about and manipulate the objects in their environments. In this paper, we adapt and extend Boltzmann Machines (BMs) for contextualized scene modeling. Although there are many models on the subject, ours is the first to bring together objects, relations, and affordances in a highly-capable generative model. For this end, we introduce a hybrid version of BMs where relations and affordances are incorporated with shared, tri-way connections into the model. Moreover, we introduce a dataset for relation estimation and modeling studies. We evaluate our method in comparison with several baselines on object estimation, out-of-context object detection, relation estimation, and affordance estimation tasks. Moreover, to illustrate the generative capability of the model, we show several example scenes that the model is able to generate, and demonstrate the benefits of the model on a humanoid robot. The code and the dataset are publicly made available at: https://github.com/bozcani/COSMO.",
                "year": 2019,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Automatic graspability map generation based on shape-primitives for unknown and familiar objects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889018300940/pdfft?md5=7a6800bde4366f9b95b8654eb097cb29&pid=1-s2.0-S0921889018300940-main.pdf",
                "title": "Automatic graspability map generation based on shape-primitives for unknown and familiar objects",
                "abstract": "Determining goal configurations that lead to successful grasps is a critical, time-consuming stage in reach-to-grasp planning, especially in unstructured, cluttered environments. While traditional, analytic algorithms are computation intensive and susceptible to uncertainty, modern, data-driven algorithms do not offer success guarantees and require large datasets for learning models of reach-to-grasp motion. Graspability maps are data structures which store wrist configurations that lead to successful grasps of an object. They are suitable for both direct use in reach-to-grasp motion planning, and as grasp databases for gripper design analysis and for learning grasp models. The computation of graspability maps can be based on analytical models. This facilitates the integration of analytical grasp quality guarantees with data-driven grasp planning. Yet, current graspability map computation methods are prohibitively time-consuming for many application scenarios. In the current work, we suggest a method for adaptation of graspability maps of known objects (shape primitives) to familiar and to unknown objects. The method facilitates run-time generation of graspability maps and significantly enhances their usability. Adapted maps are generated based on detecting shape primitives in the object to be grasped, scaling the a-priori generated maps to the required dimensions, and combining the scaled maps to form a compound graspability map. Simulation results confirm that map adaption does not critically reduce quality while significantly reducing computation time. A case study evaluation with objects from a public point-cloud image database corroborates the method’s ability to quickly and accurately generate high-quality graspability maps for familiar and unknown objects.",
                "year": 2018,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "A cell mapping method for general optimum trajectory planning of multiple robotic arms": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0921889094900442/pdfft?md5=3e06e9bd7a3e1e16bfb696a6c16b879c&pid=1-s2.0-0921889094900442-main.pdf",
                "title": "A cell mapping method for general optimum trajectory planning of multiple robotic arms",
                "abstract": "This paper proposes a method that uses cell state space and cell mapping based techniques for planning general optimum trajectories along given geometric paths for coordinated multiple robotic arm systems. The major advantages of this method include its simplicity and applicability to a wide range of problem formulations. In particular, three performance indices for optimum trajectory specification are investigated, i.e., minimum-energy, minimum-jerk, and minimum-time formulations. A simple search strategy is constructed using cell-to-cell mapping to find optimum trajectories. A special feature of this search algorithm is its ability to generate all optimum trajectories for all possible initial conditions through a single search. The computational complexity is analyzed for the search algorithm and its hierarchical implementation. Parallel execution of the hierarchical search method is discussed and the results indicate that it can improve the cell-mapping search efficiency significantly.",
                "year": 1994,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Smartphone use can modify the body schema: An ERP study based on hand mental rotation task": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S074756322100457X/pdfft?md5=5e769086e2eb55620ef2218fc9f2e9d2&pid=1-s2.0-S074756322100457X-main.pdf",
                "title": "Smartphone use can modify the body schema: An ERP study based on hand mental rotation task",
                "abstract": "A substantial body of prior work has documented the relationship between extensive smartphone use and individuals' psychological features, yet little is known about how smartphone influences people's body representation. Based on previous studies on tool embodiment, this study adapted the hand mental rotation paradigm to determine whether smartphone use would change the body schema. We compared smartphone users' behavioral performances and electrophysiological activities when presented with different stimuli. We found that people had faster and more accurate responses to smartphone in hand stimuli than other two stimuli. ERP results showed that N200 amplitude elicited by gesture of holding a phone stimuli was smaller than smartphone in hand stimuli, which was smaller than non-smartphone in hand stimuli. For the P300 component, gesture of holding a phone stimuli evoked larger P300 amplitude than non-smartphone in hand stimuli, and there was no significant difference in P300 amplitude between gesture of holding a phone stimuli and smartphone in hand stimuli. These results provide new evidence that smartphone is embodied into the body schema. It not only expands the research scope and field of tool embodiment but also provides insight into the impact of smartphone on individuals' body self.",
                "year": 2022,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "LightDepth: A resource efficient depth estimation approach for dealing with ground truth sparsity via curriculum learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889024001684/pdfft?md5=c0a9dd5245148e4b8f4a301d9e0fca01&pid=1-s2.0-S0921889024001684-main.pdf",
                "title": "LightDepth: A resource efficient depth estimation approach for dealing with ground truth sparsity via curriculum learning",
                "abstract": "Accurate depth estimation from monocular images is critical for various applications such as robotics, augmented reality, and autonomous navigation. However, achieving high accuracy while maintaining computational efficiency is a major challenge, particularly for resource-constrained devices. In this paper, we present LightDepth, an approach that leverages curriculum learning to estimate depth efficiently while taking into account resource constraints. It modifies the ground truth sparse depth maps from the KITTI dataset by resizing them to 31 extents during training to reduce sparsity and control complexity. The resulting model achieves comparable accuracy to state-of-the-art large models while outperforming them in response time by 71%. Our approach outperforms resource-efficient models regarding depth accuracy (measured by RMSE), achieving a 56% improvement. LightDepth is designed to be fast and resource-efficient, making it suitable for deployment in resource-constrained devices. It also balances the trade-off between accuracy and resource efficiency. All codes are available online at https://github.com/fatemehkarimii/lightdepth.",
                "year": 2024,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Dynamic analysis of a composite-material flexible robot arm": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/004579499390111P/pdfft?md5=2e63138f6780376e5512cdee9fdcdc4c&pid=1-s2.0-004579499390111P-main.pdf",
                "title": "Dynamic analysis of a composite-material flexible robot arm",
                "abstract": "Increased demands for higher productivity and improved quality of goods have required industrial robots to operate at high speed with greater precision. To meet these demands, robots should be lightweight, quick, and accurate. In this study these requirements are satisfied by inclusion of structural flexibility in the dynamic model of the robotic manipulator and implementation of advanced composite materials in the structural design. The focus of this study is a three-dimensional, revolute, compositematerial robot arm. A displacement finite element dynamic model is employed which includes all the coupling terms between the rigid and flexible motions and takes into consideration the axial, in-plane, and out-of-plane transverse deflections. The material damping of the laminated flexible link in both transverse directions is considered. The digital simulation results clearly demonstrate the advantage of incorporating advanced composite materials in the structural design of robotic manipulators. The effect of flexible motion on the rigid body motion is proven to be very important. It is also shown that there is a significant difference between the behavior of the geometrically linear and nonlinear models. Effects of fiber orientation and material orthotropy on the bending stress and displacements are also assessed. It is demonstrated that the inclusion of material damping in the dynamic model is an important factor in the design of flexible robot arms made of advanced composite materials.",
                "year": 1993,
                "publisher": "Computers & Structures"
            }
        }
    },
    "Tracking in object action space": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314213000313/pdfft?md5=fd9e650fb567a541f0d52faf83ba7d34&pid=1-s2.0-S1077314213000313-main.pdf",
                "title": "Tracking in object action space",
                "abstract": "In this paper we focus on the joint problem of tracking humans and recognizing human action in scenarios such as a kitchen scenario or a scenario where a robot cooperates with a human, e.g., for a manufacturing task. In these scenarios, the human directly interacts with objects physically by using/manipulating them or by, e.g., pointing at them such as in “Give me that…”. To recognize these types of human actions is difficult because (a) they ought to be recognized independent of scene parameters such as viewing direction and (b) the actions are parametric, where the parameters are either object-dependent or as, e.g., in the case of a pointing direction convey important information. One common way to achieve recognition is by using 3D human body tracking followed by action recognition based on the captured tracking data. For the kind of scenarios considered here we would like to argue that 3D body tracking and action recognition should be seen as an intertwined problem that is primed by the objects on which the actions are applied. In this paper, we are looking at human body tracking and action recognition from a object-driven perspective. Instead of the space of human body poses we consider the space of the object affordances, i.e., the space of possible actions that are applied on a given object. This way, 3D body tracking reduces to action tracking in the object (and context) primed parameter space of the object affordances. This reduces the high-dimensional joint-space to a low-dimensional action space. In our approach, we use parametric hidden Markov models to represent parametric movements; particle filtering is used to track in the space of action parameters. We demonstrate its effectiveness on synthetic and on real image sequences using human-upper body single arm actions that involve objects.",
                "year": 2013,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Tangible data visualization of physical activity for children and adolescents: A qualitative study of temporal transition of experiences": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2212868923000028/pdfft?md5=8a5ea543f83cccede0004fa21136035f&pid=1-s2.0-S2212868923000028-main.pdf",
                "title": "Tangible data visualization of physical activity for children and adolescents: A qualitative study of temporal transition of experiences",
                "abstract": "Children and adolescents in the UK are increasingly at risk of significant health problems due to physical inactivity. While activity trackers and fitness applications have focused on addressing this problem in youth, poor wear-time compliance and usability and accessibility issues have been frequently reported in the literature as barriers to engagement. Physicalization of data offers an alternative approach to engage with physical activity (PA). In this paper, we present the results of a seven-week qualitative study with 97 primary and secondary school children (8–14 years old). We took a temporal approach to collect children’s and adolescents’ perspectives in short video interviews as they received 3D-printed models representing their faded-weekly PA levels. Our findings showed that children’s and adolescents’ emotional engagement with the models remained high throughout the study, while their reflection on the models and their knowledge of what constitutes PA and its different types evolved over time. The findings from this temporal study suggest that tangible data visualization of PA evokes experiences such as embodied reflection, active learning, emotional engagement, and temporality of PA experience. Therefore, we argue that the motivational impact of regular tangible visualizations as a form of feedback should be considered alongside wearable trackers in addressing childhood inactivity.",
                "year": 2023,
                "publisher": "International Journal of Child-Computer Interaction"
            }
        }
    },
    "Complex self-driving behaviors emerging from affordance competition in layered control architectures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041722001097/pdfft?md5=d66e9a04b9d4b37b5ca81ecdedc24879&pid=1-s2.0-S1389041722001097-main.pdf",
                "title": "Complex self-driving behaviors emerging from affordance competition in layered control architectures",
                "abstract": "The deployment of autonomous driving technology is hindered by “corner cases”: unusual nuanced conditions that the self-driving software cannot understand and act fully. We argue that some corner cases originate from a “narrow AI” approach, which lacks the general knowledge that humans exploit when dealing with these cases. We propose an alternative that can be seen as a step toward features of Artificial General Intelligence.\n\nWe exploit the biological principle of affordance competition in layered control architectures to create an artificial agent that realizes emergent, adaptive, and logical behaviors without programming case-specific rules or algorithms. We give six different examples of simple and complex emergent behaviors. For the case study of merge scenarios, we contrast the approach of this paper with an algorithmic solution of the literature. The ideas presented here (if not the whole agent’s sensorimotor organization) could be used to improve the robustness and flexibility of self-driving technology.",
                "year": 2023,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Near-time optimal feedrate planning for the NURBS curve considering interpolation error constraints": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584523001540/pdfft?md5=da3fa167de9bb1dded6ab1d4d9cf0589&pid=1-s2.0-S0736584523001540-main.pdf",
                "title": "Near-time optimal feedrate planning for the NURBS curve considering interpolation error constraints",
                "abstract": "Compared with general time-optimal robot feedrate planning, trajectory machining tasks require consideration of feedrate command constraints and the complexity of parameter spline curves. The interpolation error generated during the interpolation stage affects the command accuracy and contour accuracy of the trajectory. To ensure that the robot end-effector feedrate maintains consistency with the feedrate command and to improve the contour accuracy of the trajectory, this study proposes a two-stage method: near-time optimal feedrate planning for the NURBS curve considering interpolation error constraints. In the first stage, the end-effector velocity constraint under joint velocity constraint is calculated, and the feedrate command is corrected based on that constraint. Then, the corrected feedrate command is transformed into parameter form, and feedrate planning is carried out utilizing the direct transcription method. In the second stage, interpolation is carried out after feedrate planning, and interpolation error is calculated to detect any over-limit positions. If any over-limit positions are detected, the first over-limit position is recorded, and feedrate planning with reduced feedrate limit is carried out at that position. This process is repeated until the interpolation error constraint is satisfied. A series of simulations and experiments were conducted in this study. The simulation results show that the proposed method can consider feedrate command constraints and limit interpolation errors within specified constraints. The experimental results show that the proposed method can reduce contour errors, particularly at low feedrate command.",
                "year": 2024,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "What factors contribute to the acceptance of artificial intelligence? A systematic review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585322001587/pdfft?md5=598c59106fa381c6f8391ad5897ee7c8&pid=1-s2.0-S0736585322001587-main.pdf",
                "title": "What factors contribute to the acceptance of artificial intelligence? A systematic review",
                "abstract": "Artificial Intelligence (AI) agents are predicted to infiltrate most industries within the next decade, creating a personal, industrial, and social shift towards the new technology. As a result, there has been a surge of interest and research towards user acceptance of AI technology in recent years. However, the existing research appears dispersed and lacks systematic synthesis, limiting our understanding of user acceptance of AI technologies. To address this gap in the literature, we conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and meta-Analysis guidelines using five databases: EBSCO host, Embase, Inspec (Engineering Village host), Scopus, and Web of Science. Papers were required to focus on both user acceptance and AI technology. Acceptance was defined as the behavioural intention or willingness to use, buy, or try a good or service. A total of 7912 articles were identified in the database search. Sixty articles were included in the review. Most studies (n = 31) did not define AI in their papers, and 38 studies did not define AI for their participants. The extended Technology Acceptance Model (TAM) was the most frequently used theory to assess user acceptance of AI technologies. Perceived usefulness, performance expectancy, attitudes, trust, and effort expectancy significantly and positively predicted behavioural intention, willingness, and use behaviour of AI across multiple industries. However, in some cultural scenarios, it appears that the need for human contact cannot be replicated or replaced by AI, no matter the perceived usefulness or perceived ease of use. Given that most of the methodological approaches present in the literature have relied on self-reported data, further research using naturalistic methods is needed to validate the theoretical model/s that best predict the adoption of AI technologies.",
                "year": 2023,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "50 Years of object recognition: Directions forward": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S107731421300091X/pdfft?md5=5e3fdaef9223edb01efd476f77b4eb7f&pid=1-s2.0-S107731421300091X-main.pdf",
                "title": "50 Years of object recognition: Directions forward",
                "abstract": "Object recognition systems constitute a deeply entrenched and omnipresent component of modern intelligent systems. Research on object recognition algorithms has led to advances in factory and office automation through the creation of optical character recognition systems, assembly-line industrial inspection systems, as well as chip defect identification systems. It has also led to significant advances in medical imaging, defence and biometrics. In this paper we discuss the evolution of computer-based object recognition systems over the last fifty years, and overview the successes and failures of proposed solutions to the problem. We survey the breadth of approaches adopted over the years in attempting to solve the problem, and highlight the important role that active and attentive approaches must play in any solution that bridges the semantic gap in the proposed object representations, while simultaneously leading to efficient learning and inference algorithms. From the earliest systems which dealt with the character recognition problem, to modern visually-guided agents that can purposively search entire rooms for objects, we argue that a common thread of all such systems is their fragility and their inability to generalize as well as the human visual system can. At the same time, however, we demonstrate that the performance of such systems in strictly controlled environments often vastly outperforms the capabilities of the human visual system. We conclude our survey by arguing that the next step in the evolution of object recognition algorithms will require radical and bold steps forward in terms of the object representations, as well as the learning and inference algorithms used.",
                "year": 2013,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "An intelligent model approach for leakage detection of modified atmosphere pillow bags": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S095219762401769X/pdfft?md5=ba4008a7dc3acc4733070e4c3dbe5fa9&pid=1-s2.0-S095219762401769X-main.pdf",
                "title": "An intelligent model approach for leakage detection of modified atmosphere pillow bags",
                "abstract": "Modified atmosphere pillow bags have been widely used to package various food products due to their advantages for preservation and shipment. Sealing defects are statistically inevitable, although modern packaging machinery and manual inspection utilized by manufacturers continue reducing the leakage probability. Hence the bag contents may spoil if the seal is broken. Instead of manual inspection and various destructive methods utilized by factories, this study introduces non-destructive leakage detection using deep learning methods. Firstly, a squeezing method is developed to aggravate the feature difference between positive samples and negative samples without destroying the bag content, thus 2160 images of three different pillow bags are acquired to establish dataset. Secondly, the deep learning model Vision Transformer (ViT) is deployed and studied so that feasibility of computer vision method is verified. Then the Semantic segmentation and Contour Extraction model combining ViT (SCE-ViT) is proposed and improved to the Multi-dimensional Fusion model (SCE-MdF). The accuracies of SCE-MdF reached 97.5%, 97.5%, and 97.5%, respectively. The F1-scores of SCE-MdF reached 97.6%, 97.6%, and 97.4%, respectively. Compared to averaged accuracies of SCE-ViT, accuracies introduced in the ultimate model SCE-MdF improved by 19.17%, 5.84%, and 11.67%, respectively. Therefore, combination of unique squeezing method and Semantic segmentation Contour Extraction with Multi-dimensional Fused ViT, is eventually validated viable on leakage detection of modified atmosphere pillow bags. Hence a cost-effective, efficient and non-destructive leakage detection method for modified atmosphere pillow bags in relevant industry is introduced, filling a gap between artificial intelligence and food packaging industry.",
                "year": 2025,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Good vibrations: Exploiting reflector motion to partition an acoustic environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889098000219/pdfft?md5=c73626760c8bb640e109d8eaa9af9ad2&pid=1-s2.0-S0921889098000219-main.pdf",
                "title": "Good vibrations: Exploiting reflector motion to partition an acoustic environment",
                "abstract": "Much of Artificial Intelligence studies agent perception by exploring architectures for linking (often abstract) sensors and actuators so as to give rise to particular behaviour. By contrast, the approach presented here proposes that perceptual investigations should begin with a characterisation of the underlying physical laws which govern the specific interaction of a sensor (or actuator) with its environment throughout the execution of a task. Moreover, it demonstrates that, through an understanding of task and environment physics, problems for which architectural solutions or explanations are often proposed may be solved more simply at the sensory interface - thereby minimising subsequent computation.\n\nAs an example of this approach, we isolate the relevant physics governing an agent-environment interaction by reverse-engineering an echolocation system from existing ethological descriptions. The biological reference for this work includes several species of tone emitting insectivorous bats which hunt moving targets (i.e. fluttering insects) in acoustically cluttered environments (e.g. under the forest canopy). The simple perceptual mechanism that we hypothesise underlies this behaviour involves a binaural comparison of the spectral sideband energy contained in echoes reflected by targets with periodic motion. This perceptual mechanism can be exploited by any echolocator (with sufficiently narrow acoustical filters) to disambiguate the location of moving targets in the presence of strong stationary reflectors. When this localisation strategy is made to operate on energy in the particular sidebands reflected by targets with particular motions, target selective localisation behaviour emerges.\n\nThe echolocation sensor designed to investigate this hypothesis is demonstrated via computer simulation and tested aboard a mobile robot.",
                "year": 1998,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Human tracking from a mobile agent: Optical flow and Kalman filter arbitration": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0923596511000713/pdfft?md5=b99373438658275d07ba37595229800d&pid=1-s2.0-S0923596511000713-main.pdf",
                "title": "Human tracking from a mobile agent: Optical flow and Kalman filter arbitration",
                "abstract": "Tracking moving objects is one of the most important but problematic features of motion analysis and understanding. The Kalman filter (KF) has commonly been used for estimation and prediction of the target position in succeeding frames. In this paper, we propose a novel and efficient method of tracking, which performs well even when the target takes a sudden turn during its motion. The proposed method arbitrates between KF and Optical flow (OF) to improve the tracking performance. Our system utilizes a laser to measure the distance to the nearest obstacle and an infrared camera to find the target. The relative data is then fused with the Arbitrate OFKF filter to perform real-time tracking. Experimental results show our suggested approach is very effective and reliable for estimating and tracking moving objects.",
                "year": 2012,
                "publisher": "Signal Processing: Image Communication"
            }
        }
    },
    "Investigating the relationship between three-dimensional perception and presence in virtual reality-reconstructed architecture": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687022002769/pdfft?md5=403820db31b7af016cf752f8d27dfd29&pid=1-s2.0-S0003687022002769-main.pdf",
                "title": "Investigating the relationship between three-dimensional perception and presence in virtual reality-reconstructed architecture",
                "abstract": "Identifying and characterizing the factors that affect presence in virtual environments has been acknowledged as a critical step to improving Virtual Reality (VR) applications in the built environment domain. In the search to identify those factors, the research objective was to test whether three-dimensional perception affects presence in virtual environments. A controlled within-group experiment utilizing perception and presence questionnaires was conducted, followed by data analysis, to test the hypothesized unidirectional association between three-dimensional perception and presence in two different virtual environments (non-immersive and immersive). Results indicate no association in either of the systems studied, contrary to the assumption of many scholars in the field but in line with recent studies on the topic. Consequently, VR applications in architectural design may not necessarily need to incorporate advanced stereoscopic visualization techniques to deliver highly immersive experiences, which may be achieved by addressing factors other than depth realism. As findings suggest that the levels of presence experienced by users are not subject to the display mode of a 3D model (whether immersive or non-immersive display), it may still be possible for professionals involved in the review of 3D models (e.g., designers, contractors, clients) to experience high levels of presence through non-stereoscopic VR systems provided that other presence-promoting factors are included.",
                "year": 2023,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Evolution of robust high speed optical-flow-based landing for autonomous MAVs": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889019302404/pdfft?md5=ef9fa81f754e53a76437ef284bc43be6&pid=1-s2.0-S0921889019302404-main.pdf",
                "title": "Evolution of robust high speed optical-flow-based landing for autonomous MAVs",
                "abstract": "Automatic optimization of robotic behavior has been the long-standing goal of Evolutionary Robotics. Allowing the problem at hand to be solved by automation often leads to novel approaches and new insights. A common problem encountered with this approach is that when this optimization occurs in a simulated environment, the optimized policies are subject to the reality gap when implemented in the real world. This often results in sub-optimal behavior, if it works at all. This paper investigates the automatic optimization of neurocontrollers to perform quick but safe landing maneuvers for a quadrotor micro air vehicle using the divergence of the optical flow field of a downward looking camera. The optimized policies showed that a piece-wise linear control scheme is more effective than the simple linear scheme commonly used, something not yet considered by human designers. Additionally, we show the utility in using abstraction on the input and output of the controller as a tool to improve the robustness of the optimized policies to the reality gap by testing our policies optimized in simulation on real world vehicles. We tested the neurocontrollers using two different methods to generate and process the visual input, one using a conventional CMOS camera and one a dynamic vision sensor, both of which perform significantly differently than the simulated sensor. The use of the abstracted input resulted in near seamless transfer to the real world with the controllers showing high robustness to a clear reality gap.",
                "year": 2020,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "How should intelligent agents apologize to restore trust? Interaction effects between anthropomorphism and apology attribution on trust repair": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585321000344/pdfft?md5=6fb9ab249c94b5cd907ad6ebe3cb0b0e&pid=1-s2.0-S0736585321000344-main.pdf",
                "title": "How should intelligent agents apologize to restore trust? Interaction effects between anthropomorphism and apology attribution on trust repair",
                "abstract": "Trust is essential in individuals’ perception, behavior, and evaluation of intelligent agents. Because, it is the primary motive for people to accept new technology, it is crucial to repair trust when damaged. This study investigated how intelligent agents should apologize to recover trust and how the effectiveness of the apology is different when the agent is human-like compared to machine-like based on two seemingly competing frameworks of the Computers-Are-Social-Actors paradigm and automation bias. A 2 (agent: Human-like vs. Machine-like) X 2 (apology attribution: Internal vs. External) between-subject design experiment was conducted (N = 193) in the context of the stock market. Participants were presented with a scenario to make investment choices based on an artificial intelligence agent’s advice. To see the trajectory of the initial trust-building, trust violation, and trust repair process, we designed an investment game that consists of five rounds of eight investment choices (40 investment choices in total). The results show that trust was repaired more efficiently when a human-like agent apologizes with internal rather than external attribution. However, the opposite pattern was observed among participants who had machine-like agents; the external rather than internal attribution condition showed better trust repair. Both theoretical and practical implications are discussed.",
                "year": 2021,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Muddy irrigation ditch understanding for agriculture environmental monitoring": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Muddy irrigation ditch understanding for agriculture environmental monitoring",
                "abstract": "Understanding an irrigation ditch plays an important role in intelligent agriculture environmental monitoring, especially in field environments where large chunks of ditches are particularly covered by various types of natural unstructured soil, vegetation and weeds. However, due to the diverse and unstructured muddy ditches, understanding them remains a challenge. Traditional approaches of understanding a scene from three-dimensional (3D) point clouds or multi-sensor fusion are energy intensive and computationally complex, making them quite laborious in application on a resource-constrained system. In this study, we propose a methodology to understand irrigation ditches and reconstruct them in a 3D scene, using only a resource-constrained monocular camera, without prior training. Spatial similar textures projections are extracted and clustered. Through geometric constraints of distribution and orientation, similar texture projections are refined and their corresponding surfaces are shaped. By contours and evidence lines, the ditch bottom surfaces are represented. Thus an irrigation ditch can be understood and reconstructed in a 3D environment, which can be used in agricultural automatic control system, agricultural robots, and precise agriculture. Unlike machine learning-based algorithms, the proposed method requires no prior training nor knowledge of the camera’s internal parameters such as focal length, field angle, and aperture. Additionally, pure geometric features make the presented method robust to varying illumination and colour. The percentage of incorrectly classified pixels was compared to the ground truth. Experimental results demonstrated that the approach can successfully elucidate irrigation ditches, meeting requirements in safety monitoring in an agriculture environment.",
                "year": 2024,
                "publisher": "Sustainable Computing: Informatics and Systems"
            }
        }
    },
    "Two grids are better than one: Hybrid indoor scene reconstruction framework with adaptive priors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231224018897/pdfft?md5=eb576618106c4d66d7a58c857beeac3e&pid=1-s2.0-S0925231224018897-main.pdf",
                "title": "Two grids are better than one: Hybrid indoor scene reconstruction framework with adaptive priors",
                "abstract": "Indoor scene reconstruction from multi-view images is a pivotal technology within the field of robotics and augmented reality. Previous researches have predominantly focused on neural radiance fields aided by geometric monocular priors. However, due to the inductive smoothness bias introduced by deep Multi-Layer Perceptron (MLP) networks, these methods struggle to recover the scene surface with complex and fine geometry details. Additionally, when used as additional supervision signals during optimization, priors in different regions make different contributions. Simply incorporating them in all regions may lead to a decrease in the accuracy. To tackle these issues, we present a generic end-to-end framework named AdaptSurf, which combines Signed Distance Field (SDF) voxel grids and feature voxel grids to enhance the capability of reconstructing accurate geometry details, respectively. Furthermore, we design a policy network to adaptively enable the estimated depth or normal priors to supervise the learning process, which improves the reconstruction accuracy and accelerates neural surface reconstruction. Qualitative and quantitative experiments show that AdaptSurf yields high-quality surfaces, especially for fine-grained details and smooth regions. Furthermore, the policy network exhibits an interpretable behavior that depends on the voxel features, which helps to improve the quality of surface reconstruction.",
                "year": 2025,
                "publisher": "Neurocomputing"
            }
        }
    },
    "A linear-complexity reparameterisation strategy for the hierarchical bootstrapping of capabilities within perception–action architectures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885608002552/pdfft?md5=a30d08c8d4bb9267c56356170a0540ab&pid=1-s2.0-S0262885608002552-main.pdf",
                "title": "A linear-complexity reparameterisation strategy for the hierarchical bootstrapping of capabilities within perception–action architectures",
                "abstract": "Perception–action (PA) architectures are capable of solving a number of problems associated with artificial cognition, in particular, difficulties concerned with framing and symbol grounding. Existing PA algorithms tend to be ‘horizontal’ in the sense that learners maintain their prior percept–motor competences unchanged throughout learning. We here present a methodology for simultaneous ‘horizontal’ and ‘vertical’ perception–action learning in which there additionally exists the capability for incremental accumulation of novel percept–motor competences in a hierarchical fashion.\n\nThe proposed learning mechanism commences with a set of primitive ‘innate’ capabilities and progressively modifies itself via recursive generalising of parametric spaces within the linked perceptual and motor domains so as to represent environmental affordances in maximally-compact manner. Efficient reparameterising of the percept domain is here accomplished by the exploratory elimination of dimensional redundancy and environmental context.\n\nExperimental results demonstrate that this approach exhibits an approximately linear increase in computational requirements when learning in a typical unconstrained environment, as compared with at least polynomially-increasing requirements for a classical perception–action system.",
                "year": 2009,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Perceiving affordances: A computational investigation of grasping affordances": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041710000434/pdfft?md5=8ee1908bb72b4cdf6057a78c686ee9a1&pid=1-s2.0-S1389041710000434-main.pdf",
                "title": "Perceiving affordances: A computational investigation of grasping affordances",
                "abstract": "The Grasping Affordance Model (GAM) introduced here provides a computational account of perceptual processes enabling one to identify grasping action possibilities from visual scenes. GAM identifies the core of affordance perception with visuo-motor transformations enabling one to associate features of visually presented objects to a collection of hand grasping configurations. This account is coherent with neuroscientific models of relevant visuo-motor functions and their localization in the monkey brain. GAM differs from other computational models of biological grasping affordances in the way of modeling focus, functional account, and tested abilities. Notably, by learning to associate object features to hand shapes, GAM generalizes its grasp identification abilities to a variety of previously unseen objects. Even though GAM information processing does not involve semantic memory access and full-fledged object recognition, perceptions of (grasping) affordances are mediated there by substantive computational mechanisms which include learning of object parts, selective analysis of visual scenes, and guessing from experience.",
                "year": 2011,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Multi goals and multi scenes visual mapless navigation in indoor using meta-learning and scene priors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231221004707/pdfft?md5=8057ebd30c0939cd046373be0fdbdf46&pid=1-s2.0-S0925231221004707-main.pdf",
                "title": "Multi goals and multi scenes visual mapless navigation in indoor using meta-learning and scene priors",
                "abstract": "The goal of visual mapless navigation is to navigate from a random starting point in a scene to a specified target in an unknown environment. A fundamental challenge in visual mapless navigation is generalizing to a novel environment, where the layout of the scenes and appearance of targets are unfamiliar. Furthermore, traditional navigation models are frozen during inference resulting in poor adaptability. To address these issues, we propose a multi goals and multi scenes visual mapless navigation model, which integrates meta learning with spatial relationships between different object categories. In this way, our method not only improves the generalization on multi goals in multi scenes but also encourages effective navigation. Experimental results on AI2-THOR dataset show that our approach significantly outperforms the state-of-the-art model SAVN by >27.05% for the average success rate and by >31.7% for the average SPL. Our source code and data of this paper are available at: https://github.com/zhiyu-tech/WHU-VMN.",
                "year": 2021,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Unifying perceptual and behavioral learning with a correlative subspace learning rule": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231210001426/pdfft?md5=552866326005ebafa9581fd2aefaedc3&pid=1-s2.0-S0925231210001426-main.pdf",
                "title": "Unifying perceptual and behavioral learning with a correlative subspace learning rule",
                "abstract": "For an animal to survive it has to excel in a twofold task: It has to perceive the world and execute adequate actions. These skills are acquired and adapted through perceptual and behavioral learning, respectively. Perceptual and behavioral learning are tightly interwoven, choosing the adequate action is only possible in the presence off accurate perceptions. Learning to perceive accurately does, however, happen while acting in the world. The nature of this interaction is not well understood as theoretical work does mostly investigate the two forms of learning separately. To overcome this limitation we combine perceptual and behavioral learning in a subspace learning algorithm. In a formal analysis and in numerical simulations we show that the proposed subspace learning algorithm allows us to integrate both learning systems and to smoothly change form perceptual learning only to behavioral learning only. Further we show that in a robot open area foraging task an active adaptation of the balance between perceptual and behavioral learning is necessary in order to stabilize the performance of the robot. This alludes to a fundamental argument for the necessity of a task dependent modulation of perceptual and behavioral learning as found in biological systems.",
                "year": 2010,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Where brain, body, and world collide": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041799000029/pdfft?md5=8a309b3c47167a2c22cb3ae914f46737&pid=1-s2.0-S1389041799000029-main.pdf",
                "title": "Where brain, body, and world collide",
                "abstract": "The brain fascinates because it is the biological organ of mindfulness itself. It is the inner engine that drives intelligent behavior. Such a depiction provides a worthy antidote to the once-popular vision of the mind as somehow lying outside the natural order. However, it is a vision with a price. For it has concentrated much theoretical attention on an uncomfortably restricted space; the space of the inner neural machine, divorced from the wider world which then enters the story only via the hygienic gateways of perception and action. Recent work in neuroscience, robotics and psychology casts doubt on the effectiveness of such a shrunken perspective. Instead, it stresses the unexpected intimacy of brain, body and world and invites us to attend to the structure and dynamics of extended adaptive systems — ones involving a much wider variety of factors and forces. Whilst it needs to be handled with some caution, I believe there is much to be learnt from this broader vision. The mind itself, if such a vision is correct, is best understood as the activity of an essentially situated brain: a brain at home in its proper bodily, cultural and environmental niche.",
                "year": 1999,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Insights on obstacle avoidance for small unmanned aerial systems from a study of flying animal behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889017301197/pdfft?md5=cf929578c3498029498e259b45f6b42b&pid=1-s2.0-S0921889017301197-main.pdf",
                "title": "Insights on obstacle avoidance for small unmanned aerial systems from a study of flying animal behavior",
                "abstract": "Thirty-five papers from the ethological literature were surveyed with the perception and reaction of flying animals to autonomous navigation tasks organized and analyzed using a schema theoretic framework. Flying animals are an existence proof of autonomous collision-free flight in unknown and disordered environments. Because they successfully avoid obstacles, self-orient, and evade predators and capture prey to survive, the collected information could inform the design of biologically-inspired behaviors for control of a small unmanned aerial system (SUAS) to improve the current state-of-the art in autonomous obstacle avoidance. Five observations were derived from the surveyed papers: sensing is done by vision in lighted scenarios and sonar in darkness, one sensor is always dominant, adaptive sensing is beneficial, no preference was identified for lateral versus vertical avoidance maneuvers, and reducing speed is consistently seen across species in response to objects in the flight path. Additionally, the questions of defining clutter and scale of speed reduction left unanswered by the literature were discussed. Finally, three rules for control of a SUAS in an unknown environment that restricts maneuverability were identified. These are the distance to begin maneuvers to avoid an obstacle in the flight path, the direction to adjust the flight path, and the role of centered flight in determining the adjustment.",
                "year": 2018,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "A survey on machine learning from few samples": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001802/pdfft?md5=1fd2455889b1869dbe2d6de960934b84&pid=1-s2.0-S0031320323001802-main.pdf",
                "title": "A survey on machine learning from few samples",
                "abstract": "The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning, few surveys for few sample learning (FSL) are available. We extensively study almost all papers of FSL spanning from the 2000s to now and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history and current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review their latest advances. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.",
                "year": 2023,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "From dynamic movement primitives to associative skill memories": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889012001716/pdfft?md5=56528bf501f4f0897cffd9ffb92ad0e7&pid=1-s2.0-S0921889012001716-main.pdf",
                "title": "From dynamic movement primitives to associative skill memories",
                "abstract": "In recent years, research on movement primitives has gained increasing popularity. The original goals of movement primitives are based on the desire to have a sufficiently rich and abstract representation for movement generation, which allows for efficient teaching, trial-and-error learning, and generalization of motor skills (Schaal 1999). Thus, motor skills in robots should be acquired in a natural dialog with humans, e.g., by imitation learning and shaping, while skill refinement and generalization should be accomplished autonomously by the robot. Such a scenario resembles the way we teach children and connects to the bigger question of how the human brain accomplishes skill learning. In this paper, we review how a particular computational approach to movement primitives, called dynamic movement primitives, can contribute to learning motor skills. We will address imitation learning, generalization, trial-and-error learning by reinforcement learning, movement recognition, and control based on movement primitives. But we also want to go beyond the standard goals of movement primitives. The stereotypical movement generation with movement primitives entails predicting of sensory events in the environment. Indeed, all the sensory events associated with a movement primitive form an associative skill memory that has the potential of forming a most powerful representation of a complete motor skill.",
                "year": 2013,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Co-constructing intersubjectivity with artificial conversational agents: People are more likely to initiate repairs of misunderstandings with agents represented as human": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563215303101/pdfft?md5=4afaa8f1636e2ae5afdb22274e3555fe&pid=1-s2.0-S0747563215303101-main.pdf",
                "title": "Co-constructing intersubjectivity with artificial conversational agents: People are more likely to initiate repairs of misunderstandings with agents represented as human",
                "abstract": "This article explores whether people more frequently attempt to repair misunderstandings when speaking to an artificial conversational agent if it is represented as fully human. Interactants in dyadic conversations with an agent (the chat bot Cleverbot) spoke to either a text screen interface (agent's responses shown on a screen) or a human body interface (agent's responses vocalized by a human speech shadower via the echoborg method) and were either informed or not informed prior to interlocution that their interlocutor's responses would be agent-generated. Results show that an interactant is less likely to initiate repairs when an agent-interlocutor communicates via a text screen interface as well as when they explicitly know their interlocutor's words to be agent-generated. That is to say, people demonstrate the most “intersubjective effort” toward establishing common ground when they engage an agent under the same social psychological conditions as face-to-face human–human interaction (i.e., when they both encounter another human body and assume that they are speaking to an autonomously-communicating person). This article's methodology presents a novel means of benchmarking intersubjectivity and intersubjective effort in human-agent interaction.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Is Wearable Technology Becoming Part of Us? Developing and Validating a Measurement Scale for Wearable Technology Embodiment": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Is Wearable Technology Becoming Part of Us? Developing and Validating a Measurement Scale for Wearable Technology Embodiment",
                "abstract": "Background\n\nTo experience external objects in such a way that they are perceived as an integral part of one’s own body is called embodiment. Wearable technology is a category of objects, which, due to its intrinsic properties (eg, close to the body, inviting frequent interaction, and access to personal information), is likely to be embodied. This phenomenon, which is referred to in this paper as wearable technology embodiment, has led to extensive conceptual considerations in various research fields. These considerations and further possibilities with regard to quantifying wearable technology embodiment are of particular value to the mobile health (mHealth) field. For example, the ability to predict the effectiveness of mHealth interventions and knowing the extent to which people embody the technology might be crucial for improving mHealth adherence. To facilitate examining wearable technology embodiment, we developed a measurement scale for this construct.",
                "year": 2019,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Group Technology: A systematic approach to the design of flexible robot grippers for metal-cutting cells": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167849386900495/pdfft?md5=045fb5b2ef45f7de6c2d1eb56f75224f&pid=1-s2.0-0167849386900495-main.pdf",
                "title": "Group Technology: A systematic approach to the design of flexible robot grippers for metal-cutting cells",
                "abstract": "The principles and techniques of Group Technology have the scope for universal application, being adaptable to the required areas in manufacturing system planning. This report suggests the extension of existing Classification and Coding systems, and approaches for the creation of manufacturing cells using techniques of machine-component grouping, to the design to robot grippers possessing the flexibility to handle a large variety of part shapes. It proposes the use of appropriate part attributes and the inclusion of a part attribute ranking scheme in the Classification and Coding exercise to systematise this effort, using data originally intended for use in machine tool selection, process-planning and design retrieval.",
                "year": 1986,
                "publisher": "Robotics"
            }
        }
    },
    "CoLiTecVS software for the automated reduction of photometric observations in CCD-frames": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2213133722000385/pdfft?md5=60262dfc4ff475b5379f8e30721ec7e5&pid=1-s2.0-S2213133722000385-main.pdf",
                "title": "CoLiTecVS software for the automated reduction of photometric observations in CCD-frames",
                "abstract": "In this article, we present the new product and its description developed by the CoLiTec (Collection Light Technology) project (Khlamov & Savanevych 2020), called CoLiTecVS (CoLiTec Variable Stars) software as a set of tools for the automated light curve creation of variable stars using the sensor data received from the different sensors.\n\nCoLiTecVS allows performing the photometry and light curve creation with minimal user interaction in two different modes. The first one is an observation mode when the data are received online from the sensors directly and immediately processed by the OnLine Data Analysis System (OLDAS). This mode is called “OLDAS-Night” mode. The second one is processing the existing observations when sensor data are already received and saved. This mode is called ”CoLiTec-Day” mode.\n\nNowadays, the modern sensors (robotic ground-based telescopes equipped with different cameras with charge-coupled device (CCD)) still have a lack of capabilities, like non-uniform sensitivity of the matrix pixels, non-linearity of the pixel response to the number of photons that have fallen, oversaturation of bright objects leads to charge transfer to neighboring pixels and the presence of cold or hot pixels. All these shortcomings of modern sensors complicate the process of obtaining the accurate photometric measurements. Such lack of capabilities of modern sensors also makes the plotting of light curves a difficult and routine task. The latter shifts astronomer’s main attention from research to the very time-consuming tasks of processing sensor data and light curve creation. To solve this problem, we developed a new tool for the automated reduction of photometric observations, which includes the computational method for the brightness assessment of the investigated and comparison stars; brightness equalization of astronomical images using an inverse median filter; and light curve plotting and its processing using different tools.\n\nThis new CoLiTecVS software provides a tool for high accurate stellar brightness measuring and automated light curve creation with aperture photometry uncertainties less than 0.04 magnitude, the value of which generally depends on various imaging characteristics of telescopes, detectors, and sensors.\n\nCoLiTecVS has been tested on many different time series of 20-600 CCD-frames with variable stars and has produced high-quality photometric light curves in a fraction of the usual processing time.",
                "year": 2022,
                "publisher": "Astronomy and Computing"
            }
        }
    },
    "EEG-based texture roughness classification in active tactile exploration with invariant representation learning networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S174680942100104X/pdfft?md5=ecb1c039a9a696aeae8f8f358aaeeb7d&pid=1-s2.0-S174680942100104X-main.pdf",
                "title": "EEG-based texture roughness classification in active tactile exploration with invariant representation learning networks",
                "abstract": "During daily activities, humans use their hands to grasp surrounding objects and perceive sensory information which are also employed for perceptual and motor goals. Multiple cortical brain regions are known to be responsible for sensory recognition, perception and motor execution during sensorimotor processing. While various research studies particularly focus on the domain of human sensorimotor control, the relation and processing between motor execution and sensory processing is not yet fully understood. Main goal of our work is to discriminate textured surfaces varying in their roughness levels during active tactile exploration using simultaneously recorded electroencephalogram (EEG) data, while minimizing the variance of distinct motor exploration movement patterns. We perform an experimental study with eight healthy participants who were instructed to use the tip of their dominant hand index finger while rubbing or tapping three different textured surfaces with varying levels of roughness. We use an adversarial invariant representation learning neural network architecture that performs EEG-based classification of different textured surfaces, while simultaneously minimizing the discriminability of motor movement conditions (i.e., rub or tap). Results show that the proposed approach can discriminate between three different textured surfaces with accuracies up to 70%, while suppressing movement related variability from learned representations.",
                "year": 2021,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "3D Human pose estimation: A review of the literature and analysis of covariates": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314216301369/pdfft?md5=077331d5d1d33b9853e36f49c5008faa&pid=1-s2.0-S1077314216301369-main.pdf",
                "title": "3D Human pose estimation: A review of the literature and analysis of covariates",
                "abstract": "Estimating the pose of a human in 3D given an image or a video has recently received significant attention from the scientific community. The main reasons for this trend are the ever increasing new range of applications (e.g., human-robot interaction, gaming, sports performance analysis) which are driven by current technological advances. Although recent approaches have dealt with several challenges and have reported remarkable results, 3D pose estimation remains a largely unsolved problem because real-life applications impose several challenges which are not fully addressed by existing methods. For example, estimating the 3D pose of multiple people in an outdoor environment remains a largely unsolved problem. In this paper, we review the recent advances in 3D human pose estimation from RGB images or image sequences. We propose a taxonomy of the approaches based on the input (e.g., single image or video, monocular or multi-view) and in each case we categorize the methods according to their key characteristics. To provide an overview of the current capabilities, we conducted an extensive experimental evaluation of state-of-the-art approaches in a synthetic dataset created specifically for this task, which along with its ground truth is made publicly available for research purposes. Finally, we provide an in-depth discussion of the insights obtained from reviewing the literature and the results of our experiments. Future directions and challenges are identified.",
                "year": 2016,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Tactile sensory substitution: Models for enaction in HCI": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543808000490/pdfft?md5=52da4deac7ca9509602054b95cc537d9&pid=1-s2.0-S0953543808000490-main.pdf",
                "title": "Tactile sensory substitution: Models for enaction in HCI",
                "abstract": "To apply enactive principles within human–computer interaction poses interesting challenges to the way that we design and evaluate interfaces, particularly those that possess a strong sensorimotor character. This article surveys the field of tactile sensory substitution, an area of science and engineering that lies at the intersection of such research domains as neuroscience, haptics, and sensory prosthetics. It is argued that this area of research is of high relevance to the design and understanding of enactive interfaces that make use of touch, and is also a fertile arena for revealing fundamental issues at stake in the design and implementation of enactive interfaces, ranging from engineering, to human sensory physiology, and the function and plasticity of perception. A survey of these questions is provided, alongside a range of current and historical examples.",
                "year": 2009,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Bio-inspired cognitive model of motor learning by imitation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Bio-inspired cognitive model of motor learning by imitation",
                "abstract": "Learning, and even more so by imitation, is an essential Cognitive Functions because it is carried out throughout life and allows us to adapt our behaviors from other beings through observation. In this work, we propose a model, and implementation of the cognitive function of imitation motor learning (IML), based on psychological and neuroscientific evidence. According to the evidence, learning by imitation includes imitation of action and imitation of action over an object sub-processes. The imitation of action consists of the movement of the limbs. The imitation of action over an object consists of the interaction with an object within the environment. We achieve an implementation of the proposed model for IML and endow a virtual entity with it. In order to validate the proposal, we use a case study to analyze the sub-processes performance. From results, we conclude that both imitation of action and imitation of action over an object sub-processes play an essential role in getting the agent to interact with stimuli within the environment.",
                "year": 2021,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Spatial memory-augmented visual navigation based on hierarchical deep reinforcement learning in unknown environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705123011061/pdfft?md5=622e2f683043e06cea79381bd7575626&pid=1-s2.0-S0950705123011061-main.pdf",
                "title": "Spatial memory-augmented visual navigation based on hierarchical deep reinforcement learning in unknown environments",
                "abstract": "Visual navigation in unknown environments poses significant challenges due to the presence of many obstacles and low-texture scenes. These factors may cause frequent collisions and tracking failure of feature-based visual Simultaneous Localization and Mapping (vSLAM). To avoid these issues, this paper proposes a spatial memory-augmented visual navigation system that combines a vSLAM module, a conventional global planner module, and a Hierarchical Reinforcement Learning (HRL)-based local planner module. Firstly, a real-time vSLAM named Salient-SLAM is proposed to augment the performance of visual navigation. Salient-SLAM creates a navigation mapping thread by combining a saliency prediction model to build a navigation map that categorizes environmental regions as occupied, explored, or noticeable. Spatial memory that contains spatial abstraction and saliency information of the environment can be further formed by encoding navigation maps, which helps the agent determine an optimal path towards its destination. An open-sourced saliency dataset is proposed to train the saliency prediction model by mimicking the visual attention mechanism. Secondly, a HRL method is proposed to automatically decompose local planning into a high-level policy selector and several low-level policies, where the latter produces actions to interact with the environment. We maximize entropy and minimize option correlation in learning low-level policies, aiming at acquiring diverse and independent behaviors. The simulation results show that the proposed HRL method outperforms competitive baselines by 6.29–10.85 % on Success Rate (SR) and 3.87–11.1 % on Success weighted by Path Length (SPL) metrics. By incorporating the spatial memory, SR, and SPL metrics can be augmented by an average of 9.85 % and 10.89 %, respectively.",
                "year": 2024,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Detection of semantic inconsistencies of motor actions: From language to praxis": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Detection of semantic inconsistencies of motor actions: From language to praxis",
                "abstract": "Semantics of actions includes three types of knowledge: a) of the function of objects and tools, b) of the actions independently of the tools, and c) of the organization of simple actions in sequences. These types of knowledge might be structured as thematic roles into the semantics of actions. To test these hypothesis, 125 illustrations, divided into five conditions: i) Congruent (C), ii) Agent Inconsistent (AI), iii) Instrument Inconsistent (II), iv) Patient Inconsistent (PI), and v) Location Inconsistent (LI), were presented to 23 volunteers (50 % women), aged 20–25, who were asked to respond whether the image was congruent or incongruent. Electrical brain activity was recorded through 20 channels to obtain the event-related potentials (ERP) associated. Lower reaction times for II and PI than C, and a greater number of incorrect trials for C were found. A N300/N400 effect appeared for AI and LI conditions with respect to C. Finally, II and LI conditions present a deflected P600 in reference to C. These findings suggest semantic of actions is sensitive to thematic role manipulations and constitute evidence in favor of a semantic processing shared between visually observed praxis and words.",
                "year": 2024,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Interactive information systems: Toward perception based computing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0304397512003635/pdfft?md5=8c29ce9fd1b41bfd92d9ebdf5eee4479&pid=1-s2.0-S0304397512003635-main.pdf",
                "title": "Interactive information systems: Toward perception based computing",
                "abstract": "We discuss the role of interactions in the modeling of perception processes. Interactive information systems, introduced in this paper, play the important role in this modeling. Moreover, the proposed approach opens a new research direction in rough set theory. In this approach, partial information about the environment used for the approximation of concepts is changing dynamically in a network of interacting information systems contrary to static information systems typically used in rough set theory so far. In particular, we illustrate the use of such information systems for representation of actions or plans, their (changing in time) pre and post conditions. These information systems create a starting point for perception modeling, i.e., modeling of the process of understanding of sensory measurements. We also propose interactive grammars as a tool for modeling interactive computations in perception based computing.",
                "year": 2012,
                "publisher": "Theoretical Computer Science"
            }
        }
    },
    "An ecological approach to embodiment and cognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S138904170200044X/pdfft?md5=bc45197aec9afc27776b093396948177&pid=1-s2.0-S138904170200044X-main.pdf",
                "title": "An ecological approach to embodiment and cognition",
                "abstract": "The purpose of this article is to explore the relation between embodiment and cognition from an ecological point of view, which has been given little attention in current studies on embodiment. To begin with, two basic meanings of embodiment are distinguished: the state of being embodied and the act of embodying. This article gives more attention to embodying than to being embodied. Next, the ecological framework to investigate embodiment are presented, with focusing on affordances, tool use, and the body. On this view, it is argued that tools extend action and perception capabilities, which implies that the boundary of the body can be extended beyond the surface of the skin. Then, the empirical studies on perception of affordances, on limb proprioception, and on tool use are outlined. These studies support the idea that the boundary of the body can shift. Finally, the boundary of the body is discussed in reference to perception–action systems, suggesting that research on embodiment should pay more attention to the dynamic nature of the body.",
                "year": 2002,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Learning landmark triples by experimentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889097000493/pdfft?md5=1497ef9164a7acdb793c9b7350af1911&pid=1-s2.0-S0921889097000493-main.pdf",
                "title": "Learning landmark triples by experimentation",
                "abstract": "This article describes a method for learning a set of landmarks suitable for place navigation. The approach is novel in that it exploits the ability of a robot to learn through active perception in the task environment, similar to the learning by experimentation technique developed for LEX (Mitchell et al., 1990). The proposed strategy uses heuristics to select and rank candidate triples, then generates test cases to confirm that the best triple is sufficient. The method supports the use of multiple sensors with different computational and energy costs, where a utility function captures the tradeoff between navigational performance ranking and cost.\n\nOver 100 data points were collected on a mobile robot using a laser barcode reader and computer vision to identify landmarks. The results indicated that active perception and experimentation identified triples with better navigational properties. Furthermore, the learning process is proactive: it was shown to prevent the robot from learning a triple which was not visible over the entire navigational space and/or was not sufficient in practice.",
                "year": 1997,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Computational Approaches for Tactile Information Processing and Analysis": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Computational Approaches for Tactile Information Processing and Analysis",
                "abstract": "This chapter reviews various tactile data processing techniques adopted so far. Tactile information is quite valuable while performing tasks such as object recognition, grasping, or surface exploration. A systematic presentation of the robotic tactile perception capability should include discussion of two important issues. The first one is associated with the tactile sensor hardware; and the second one deals with the computational approaches for extracting useful information from tactile sensors. In this chapter, the emphasis is on the second aspect. The chapter discusses about the issues involved in endowing tactual or touch sensing capability to robots. The chapter reviews some of the issues affecting tactile sensing, the choice of techniques, and the extent of processing required. Static or passive tactile sensing approach is elaborated. The chapter discusses the active tactile sensing approach or dynamic sensing that is more common to humans. The chapter presents some experimental results showing the effectiveness of tactile sensing for object recognition.",
                "year": 1992,
                "publisher": "Advances in Computers"
            }
        }
    },
    "Autonomous construction of ecologically and socially relevant semantics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041708000326/pdfft?md5=0397752305f3fc6dad9a067ba3397384&pid=1-s2.0-S1389041708000326-main.pdf",
                "title": "Autonomous construction of ecologically and socially relevant semantics",
                "abstract": "This article presents a synthetic modeling approach to the problem of grounded construction of concepts. In many computational models of grounded language acquisition and evolution, meanings are created in the process of discrimination between a chosen object and other objects present on the scene of communication. We argue that categories constructed for the purpose of identification rather than discrimination are more suitable for the detached language use (talking about things not present here and now). We describe a semantics based on so-called identification criteria constructed by extracting cross-situational similarities among instances of a category, and present several computational models. In the model of individual category construction, the instances are grouped to categories by common motor programs (affordances), while in the model of social learning, focused on the influence of naming on category formation, entities are considered members of the same category, if they are labeled with the same word by an external teacher. By these two mechanisms, the learner can construct interactionally grounded representation of objects, properties, relations, changes, complex situations and events. We also report and analyze simulation results of an experiment focused on the dynamics of meanings in iterated intergenerational transmission.",
                "year": 2008,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Learning in LIDA": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Learning in LIDA",
                "abstract": "LIDA is a systems-level, biologically-inspired cognitive architecture. More than a decade of research on LIDA has seen much conceptual work on its learning mechanisms, and resulted in a set of conceptual commitments that constrain those mechanisms; perhaps the most essential of these constraints is the Conscious Learning Hypothesis from Global Workspace Theory, which asserts that all significant learning requires consciousness. Despite these successes, many conceptual challenges remain, and bridging the divide between LIDA’s conceptual model and its implementations has been challenging.\n\nThe contributions of this paper are threefold: We present a detailed survey of learning in LIDA, during which we clarify, elaborate on, and synthesize together ideas from numerous papers, using updated terminology that reflects the continuing evolution of LIDA. We explore foundational issues in learning, such as, “What must be innate or built-in?” versus “What can be learned?”, the nature of LIDA’s representations, and the relationship between the LIDA conceptual model and its computational realizations. Finally, we provide a roadmap for future work. We believe that this paper will direct and catalyze our research endeavors, and provide a thorough introduction to the conceptual foundations of LIDA’s learning mechanisms that will be useful to anyone that would like a deeper understanding of LIDA or for those that plan to implement LIDA-based agents.",
                "year": 2021,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Mediators in visual interaction: An analysis of the “Poietic Generator” and “Open Studio”": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1045926X05000236/pdfft?md5=0b9ae94052fe3c7a4e1c103c64da3c49&pid=1-s2.0-S1045926X05000236-main.pdf",
                "title": "Mediators in visual interaction: An analysis of the “Poietic Generator” and “Open Studio”",
                "abstract": "The Poietic Generator and Open Studio are examples of interactive art, a form of art intended for the viewer's direct participation. They are based on distributed applications for visual interaction enabling to collaborate on the creation of visual images and narratives. This paper reports the analysis of the visual activity generated by their users. Such an analysis is founded on the phenomenological hypothesis that the visual activity generated by the participants in the Poietic Generator and Open Studio allows the study of the interaction process in terms of a co-determining relationship between perception and action. The results of this analysis indicate five classes of mediators capable of tuning the development of the interaction process according to the context and emotional state of the participants. These classes are based on: (1) spatial relationships, (2) chromatic relationships, (3) figurative elements, (4) textual elements, and (5) temporal events.\n\nBy sustaining the intersubjective processing of information among participants, mediators sustain their socially intelligent ability of constructing and sharing meaningful activities; that is, they sustain co-creation. For this reason, mediators are particularly important in the design of social interactive systems that have purposes but not explicit goals (as in the case of art and creative activities in general).",
                "year": 2006,
                "publisher": "Journal of Visual Languages & Computing"
            }
        }
    },
    "Flow-based fabrication: An integrated computational workflow for design and digital additive manufacturing of multifunctional heterogeneously structured objects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448515000846/pdfft?md5=e7381f5efb18d3fcfb2ded408c57683e&pid=1-s2.0-S0010448515000846-main.pdf",
                "title": "Flow-based fabrication: An integrated computational workflow for design and digital additive manufacturing of multifunctional heterogeneously structured objects",
                "abstract": "Structural hierarchy and material organization in design are traditionally achieved by combining discrete homogeneous parts into functional assemblies where the shape or surface is the determining factor in achieving function. In contrast, biological structures express higher levels of functionality on a finer scale through volumetric cellular constructs that are heterogeneous and complex. Despite recent advancements in additive manufacturing of functionally graded materials, the limitations associated with computational design and digital fabrication of heterogeneous materials and structures frame and limit further progress. Conventional computer-aided design tools typically contain geometric and topologic data of virtual constructs, but lack robust means to integrate material composition properties within virtual models. We present a seamless computational workflow for the design and direct digital fabrication of multi-material and multi-scale structured objects. The workflow encodes for and integrates domain-specific meta-data relating to local, regional and global feature resolution of heterogeneous material organizations. We focus on water-based materials and demonstrate our approach by additively manufacturing diverse constructs associating shape-informing variable flow rates and material properties to mesh-free geometric primitives. The proposed workflow enables virtual-to-physical control of constructs where structural, mechanical and optical gradients are achieved through a seamless design-to-fabrication tool with localized control. An enabling technology combining a robotic arm and a multi-syringe multi nozzle deposition system is presented. Proposed methodology is implemented and full-scale demonstrations are included.",
                "year": 2015,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Towards computational models of intention detection and intention prediction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041713000399/pdfft?md5=1aee0f7ca1a846b4394b7db7f87fd715&pid=1-s2.0-S1389041713000399-main.pdf",
                "title": "Towards computational models of intention detection and intention prediction",
                "abstract": "Intention recognition is one of the core components of mindreading, an important process in social cognition. Human beings, from age of 18 months, have been shown to be able to extrapolate intentions from observed actions, even when the performer failed at achieving the goal. Existing accounts of intention recognition emphasize the use of an intent (plan) library, which is matched against observed actions for recognition. These therefore cannot account for recognition of failed sequences of actions, nor novel actions. In this paper, we begin to tackle these open questions by examining computational models for components of human intention recognition, which emphasize the ability of humans to detect and identify intentions in a sequence of observed actions, based solely on the rationality of movement (its efficiency). We provide a high-level overview of intention recognition as a whole, and then elaborate on two components of the model, which we believe to be at its core, namely, those of intention detection and intention prediction. By intention detection we mean the ability to discern whether a sequence of actions has any underlying intention at all, or whether it was performed in an arbitrary manner with no goal in mind. By intention prediction we mean the ability to extend an incomplete sequence of actions to its most likely intended goal. We evaluate the model, and these two components, in context of existing literature, and in a number of experiments with more than 140 human subjects. For intention detection, our model was able to attribute high levels of intention to those traces perceived by humans as intentional, and vice versa. For intention prediction as well, our model performed in a way that closely matched that of humans. The work highlights the intimate relationship between the ability to generate plans, and the ability to recognize intentions.",
                "year": 2014,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Collision avoidance using a model of the locust LGMD neuron": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889099000639/pdfft?md5=2e52bd68f6c2c558661404496fc16aff&pid=1-s2.0-S0921889099000639-main.pdf",
                "title": "Collision avoidance using a model of the locust LGMD neuron",
                "abstract": "The lobula giant movement detector (LGMD) system in the locust responds selectively to objects approaching the animal on a collision course. In earlier work we have presented a neural network model based on the LGMD system which shared this preference for approaching objects.\n\nWe have extended this model in order to evaluate its responses in a real-world environment using a miniature mobile robot. This extended model shows reliable obstacle detection over an eight-fold range of speeds, and raises interesting questions about basic properties of the biological system.",
                "year": 2000,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "From natural to artificial life: Biomimetic mechanisms in animat designs": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889097000134/pdfft?md5=72e60a489ac915bbe0ec11452a005aa3&pid=1-s2.0-S0921889097000134-main.pdf",
                "title": "From natural to artificial life: Biomimetic mechanisms in animat designs",
                "abstract": "This paper describes several models that incorporate some biomimetic mechanisms into the control architecture of an animat that has to survive in a changing environment. The adaptive capacities of these mechanisms are compared to engineering solutions, in application domains that involve navigation, action selection, and evolutionary design. It is argued that, although impressive adaptive capacities are already afforded by such biomimetic mechanisms, these capacities are nevertheless limited by the shortcomings of current biological knowledge. It is also argued that these shortcomings could be remedied were biologists willing to take advantage of the bottom-up and synthetic point of view with which artificial life might complement their traditional top-down and analytic approach.",
                "year": 1997,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Monitoring behavioral symptoms of dementia using activity trackers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046420301489/pdfft?md5=92b2d675fcbf7c05161ea93175e69f03&pid=1-s2.0-S1532046420301489-main.pdf",
                "title": "Monitoring behavioral symptoms of dementia using activity trackers",
                "abstract": "Tertiary disease prevention for dementia focuses on improving the quality of life of the patient. The quality of life of people with dementia (PwD) and their caregivers is hampered by the presence of behavioral and psychological symptoms of dementia (BPSD), such as anxiety and depression. Non-pharmacological interventions have proved useful in dealing with these symptoms. However, while most PwD exhibit BPSD, their manifestation (in frequency, intensity and type) varies widely among patients, thus the need to personalize the intervention and its assessment. Traditionally, instruments to measure behavioral symptoms of dementia, such as NPI-NH and CMAI, are used to evaluate these interventions. We propose the use of activity trackers as a complement to monitor behavioral symptoms in dementia research. To illustrate this approach we describe a nine week Cognitive Stimulation Therapy conducted with the assistance of a social robot, in which the ten participants wore an activity tracker. We describe how data gathered from these wearables complements the assessment of traditional behavior assessment instruments with the advantage that this assessment can be conducted continuously and thus be used to tailor the intervention to each PwD.",
                "year": 2020,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Understanding of indoor scenes based on projection of spatial rectangles": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320318301523/pdfft?md5=a6a52396516166cb53a9f9844486a1d5&pid=1-s2.0-S0031320318301523-main.pdf",
                "title": "Understanding of indoor scenes based on projection of spatial rectangles",
                "abstract": "Understanding of indoor scenes has considerable value in mission planning and monitoring in robots. This has become one of the biggest challenges in computer vision because of the diversity and changeability of 3D indoor scenes. Indoor scenes can be considered compositions of many planes in which most common external surfaces are rectangles, such as doors, windows, walls, tables. These spatial rectangles are projected into 2D projections with special geometric configurations, which may enable us to estimate their original orientation and position in 3D scenes. In this paper, the study presents a method to efficiently reconstruct 3D indoor scene without any knowledge of camera’s internal calibration. The approach first found quadrangles composed of lines. Through the projection of spatial rectangles, our method not only can estimate room layout of scene, but also can reconstruct excellent details of scene. Due to simple geometric inferences, our method can cope with clutter without prior training, making it practical and efficient for a navigating robot. We compare the room layout estimated by our algorithm against room box ground truth, measuring the percentage of pixels that were classified correctly. Furthermore, we evaluate our ability to fit the indoor scene by comparing against the details that were reconstructed correctly in scene. The experiments showed that our method is capable of reconstructing various structures of indoor environments and that the accuracy and speed of this method meet the requirements a of indoor robot navigation.",
                "year": 2018,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Sensorimotor coordination generates extended agency": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Sensorimotor coordination generates extended agency",
                "abstract": "When we synchronize finger tapping with a visual metronome, we experience a strikingly robust phenomenon of extended agency known as Spizzo’s effect. This effect is the compelling sense that we are controlling the metronome. The effect arises even though the agent knows that the metronome operates autonomously. We propose that the extended agency here established over metronome pulses results from sensorimotor coordination. To test this hypothesis, we operationalize sensorimotor coordination in terms of the correlation structures in series of asynchronies or reaction times from two finger-tapping tasks. Analyses reveal that, whereas correlation structures vary across individuals and show a systematic drift towards nonstationarity with increasing metronome frequency conditions, the presence of correlation structure is co-extensive with Spizzo’s effect. We interpret this result as supporting the view that extended agency relies on sensorimotor coordination. Sensorimotor coordination, we suggest, may induce the effect by integrating the perception of visual pulses and the agency over tapping into a synesthetic experience.",
                "year": 2019,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Visual spatial learning of complex object morphologies through the interaction with virtual and real-world data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X10000128/pdfft?md5=0bc3125203844ea803a65b51c6b4fa73&pid=1-s2.0-S0142694X10000128-main.pdf",
                "title": "Visual spatial learning of complex object morphologies through the interaction with virtual and real-world data",
                "abstract": "Conceptual design relies on extensive manipulation of the morphological properties of real or virtual objects. This study aims to investigate the nature of the perceptual information that could be retrieved in different representation modalities to learn a complex structure. An abstract and complex object was presented to two study populations, experts and non-experts, in three different representation modalities: 2D view; digital 3D model; real object. After viewing, observers had to draw some parts of the structure into a 2D reference frame. The results reveal a considerable performance advantage of digital 3D compared with real 3D, especially in the expert population. The results are discussed in terms of the nature of the morphological cues made available in the different representation modalities.",
                "year": 2010,
                "publisher": "Design Studies"
            }
        }
    },
    "Revisiting 3D visual grounding with Context-aware Feature Aggregation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231224009664/pdfft?md5=e38abe41df0eefcad1895841522c4822&pid=1-s2.0-S0925231224009664-main.pdf",
                "title": "Revisiting 3D visual grounding with Context-aware Feature Aggregation",
                "abstract": "3D visual grounding is the task of accurately locating objects in a three-dimensional scene based on textual descriptions. Current approaches mainly depend on downsampling and extracting the point features for fusion with text features. However, the main challenges of these methods are the poor point feature resolution and limited local context during multi-modal fusion, causing visual-linguistic misalignment, particularly for small objects described in the text. The intuitive solution is to get additional object-related point features, gathering more contextual information to augment the representation capability of multimodal features, thereby promoting the representation capabilities of multimodal features. Based on this, we introduce a novel 3D visual grounding framework named Context-aware Feature Aggregation (CFA). The CFA framework includes two key modules: (1) Point Augmented Aggregation Module (PAM), designed to compensate for downsampling-induced information loss by augmenting sampled points with neighboring context for more discriminative features; and (2) Dual Contextual Grouping Attention Module (DCGAM), which iteratively refines features and geometry coordinates from PAM, capturing more global context. We assess the performance of our CFA framework on two point-based datasets: ScanRefer and Nr3D/Sr3D. The CFA framework exhibits efficiency in 3D visual grounding, surpassing the performance of previous methods by experimental results.",
                "year": 2024,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Integrating behavioral, perceptual, and world knowledge in reactive navigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889005800314/pdfft?md5=9a86377257f79ca0cef9a3f877c40ec4&pid=1-s2.0-S0921889005800314-main.pdf",
                "title": "Integrating behavioral, perceptual, and world knowledge in reactive navigation",
                "abstract": "Reactive navigation based on task decomposition is an effective means for producing robust navigation in complex domains. By incorporating various forms of knowledge, this technique can be made considerably more flexible. Behavioral and perceptual strategies which are represented in a modular form and configured to meet the robot's mission and environment add considerable versatility. A priori world knowledge, when available, can be used to configure these strategies in an efficient form. Dynamically acquired world models can be used to circumvent certain pitfalls that representationless methods are subject to.\n\nThe Autonomous Robot Architecture (AuRA) is the framework within which experiments in the application of knowledge to reactive control are conducted. Actual robot experiments and simulation studies demonstrate the flexibility and feasibility of this approach over a wide range of navigational domains.",
                "year": 1990,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Identifying interaction types and functionality for automated vehicle virtual assistants: An exploratory study using speech acts cluster analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687023001904/pdfft?md5=a5d44509c0f7a564f04e60bf1fc4ff4a&pid=1-s2.0-S0003687023001904-main.pdf",
                "title": "Identifying interaction types and functionality for automated vehicle virtual assistants: An exploratory study using speech acts cluster analysis",
                "abstract": "Onboard virtual assistants with the ability to converse with users are gaining favour in supporting effective human-machine interaction to meet safe standards of operation in automated vehicles (AVs). Previous studies have highlighted the need to communicate situation information to effectively support the transfer of control and responsibility of the driving task. This study explores ‘interaction types’ used for this complex human-machine transaction, by analysing how situation information is conveyed and reciprocated during a transfer of control scenario. Two human drivers alternated control in a bespoke, dual controlled driving simulator with the transfer of control being entirely reliant on verbal communication. Handover dialogues were coded based on speech-act classifications, and a cluster analysis was conducted. Four interaction types were identified for both virtual assistants (i.e., agent handing over control) - Supervisor, Information Desk, Interrogator and Converser, and drivers (i.e., agent taking control) - Coordinator, Perceiver, Inquirer and Silent Receiver. Each interaction type provides a framework of characteristics that can be used to define driver requirements and implemented in the design of future virtual assistants to support the driver in maintaining and rebuilding timely situation awareness, whilst ensuring a positive user experience. This study also provides additional insight into the role of dialogue turns and takeover time and provides recommendations for future virtual assistant designs in AVs.",
                "year": 2024,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Remote Agent: to boldly go where no AI system has gone before": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S000437029800068X/pdfft?md5=7fc864da036971d03ad5d2d74c87ad57&pid=1-s2.0-S000437029800068X-main.pdf",
                "title": "Remote Agent: to boldly go where no AI system has gone before",
                "abstract": "Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous fleets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraintbased temporal planning and scheduling, robust multi-threaded execution, and model-based mode identification and reconfiguration. The demonstration of the integrated system as an on-board controller for Deep Space One, NASA's first New Millennium mission, is scheduled for a period of a week in mid 1999. The development of the Remote Agent also provided the opportunity to reassess some of AI's conventional wisdom about the challenges of implementing embedded systems, tractable reasoning, and knowledge representation. We discuss these issues, and our often contrary experiences, throughout the paper.",
                "year": 1998,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "A head movement propensity model for animating gaze shifts and blinks of virtual characters": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849310001408/pdfft?md5=912543a14ae29ba07f13adbe843cebff&pid=1-s2.0-S0097849310001408-main.pdf",
                "title": "A head movement propensity model for animating gaze shifts and blinks of virtual characters",
                "abstract": "An automatic model is presented for animating gaze shifts of virtual characters towards target locations in a virtual environment. Two connected components are described: an eye–head controller and a blinking controller. The gaze control model is based on results from neuroscience, and dictates the contributions of the eyes and head to a gaze shift according to an individual's head movement propensity; that is, their tendency to recruit their head when making gaze motions under different conditions. The blink controller simulates gaze-evoked blinking, a specific category of behaviours that accompany gaze shifts. The probability of occurrence of such blinks, and their amplitude, is related to the gaze shift. The model forms the basis for a number of experiments investigating the impact of blinking, eye–head ratio and direction of head movements on user perception. In addition to other application domains, the findings are of significance to serious games environments, where the perceived quality of a character's gaze may affect engagement, immersion and learning outcomes.",
                "year": 2010,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Conceptual descriptions from monitoring and watching image sequences": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885699000256/pdfft?md5=de7998fbbc395337871480d245676f98&pid=1-s2.0-S0262885699000256-main.pdf",
                "title": "Conceptual descriptions from monitoring and watching image sequences",
                "abstract": "This paper contrasts two ways of forming conceptual descriptions from images. The first, called “monitoring”, just follows the flow of data from images to interpretation, having little need for top-level control. The second, called “watching”, emphasizes the use of top-level control and actively selects evidence for task-based descriptions of the dynamic scenes. Here we look at the effect this has on forming conceptual descriptions. First, we look at how motion verbs and the perception of events contribute to an effective representational scheme. Then we go on to discuss illustrated examples of computing conceptual descriptions from images in our implementations of the monitoring and watching systems. Finally, we discuss future plans and related work.",
                "year": 2000,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "What drives technology-enhanced storytelling immersion? The role of digital humans": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222000681/pdfft?md5=1a8cb5fc3e7f12e859f626fbbbe59a9b&pid=1-s2.0-S0747563222000681-main.pdf",
                "title": "What drives technology-enhanced storytelling immersion? The role of digital humans",
                "abstract": "In this research, we investigate consumer responses to technology-enhanced storytelling marketing via augmented digital humans in two different contexts. We test the role of an augmented digital human stimulus as a moderator for storytelling satisfaction in a technology-enhanced retail complex. Building on visual perception theory and information processing theory, the findings from our study reveal sequential links between the four realms of experience economy theory in a mixed reality environment and subsequent effects on storytelling satisfaction, which in turn are boosted by digital human storytelling. Overall, our findings reveal that digital human storytelling is an effective long-term marketing strategy in technology-enhanced environments.",
                "year": 2022,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "An optimization-based approach to tailor the mechanical response of soft metamaterials undergoing rate-dependent instabilities": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782524009332/pdfft?md5=b387a0831aca6a1088aa3d0cf557130a&pid=1-s2.0-S0045782524009332-main.pdf",
                "title": "An optimization-based approach to tailor the mechanical response of soft metamaterials undergoing rate-dependent instabilities",
                "abstract": "An optimization-based design framework is proposed to tune the response of soft metamaterials involving both geometric instabilities and nonlinear viscoelastic material behavior. Designing the response of soft metmaterials to harness instabilities and undergo large, tailored configuration changes will enable advancements in soft robotics, shock and vibration mitigation, and flexible electronics. In line with the metamaterial concept, the response of these materials is governed to a large extent by the geometric and topological makeup of their small-scale features. However, the link between structure and response is less intuitive for soft metamaterials due to their reliance upon highly nonlinear responses triggered by geometric instabilities. This is further complicated by the effects of viscoelastic relaxation, which recent studies have shown to alter the emergence of instabilities in non-intuitive ways. hese effects are accounted for in our framework to achieve various design objectives, including tailored force–displacement response and maximized energy absorption from both geometric and material effects. To fully automate this process, it is essential to have a completely robust equation solver for forward problems involving instabilities and viscoelastic relaxation. We achieve this by casting the search for stable mechanical equilibrium — i.e. the forward problem — as a minimization problem and utilize a trust region algorithm to robustly handle instabilities and follow energetically-favorable equilibrium paths through critical points.",
                "year": 2025,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Analyzing part functionality via multi-modal latent space embedding and interweaving": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Analyzing part functionality via multi-modal latent space embedding and interweaving",
                "abstract": "In this paper, we advocate a novel method for analyzing the functionality of parts in 3D objects. In contrast to prior research, our method no longer characterizes the functionality of an object part using its single type of qualities (or attributes), e.g., geometry or interactions, nor by weighing the significance of various qualities. Instead, we consider the latent space of part functions as a semantic feature space comprehensively defined by part qualities. To learn such a space by parameterizing and encoding semantic features from multi-channel, we begin by learning multi-modal latent space using shapes, textures, and interaction scenes. Next, the latent space of part functions is generated by embedding and interweaving these multi-modal spaces into a space with a higher dimension. We devise loss functions to direct the embedding and interweaving of multi-modal spaces while preserving their manifolds. Consequently, the learned functionality latent space can capture the similarities between semantic features related to functionality and encode them into high-level functional representations. We assess this innovative approach on diverse categories of textured 3D shapes. Extensive experiments have exhibited our method’s parametric and encoding capability towards functionality-centric shape analysis and synthesis, including shape functionality analysis, functionally-similar shape retrieval, and functionality-aware modeling, all of which are of the essence to new graphics techniques and applications.",
                "year": 2023,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Model-driven grouping and recognition of generic object parts from single images": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889097000109/pdfft?md5=ec107324a3b2c0a3267d0e9c25302479&pid=1-s2.0-S0921889097000109-main.pdf",
                "title": "Model-driven grouping and recognition of generic object parts from single images",
                "abstract": "Grouping is often intended as a general-purpose early vision stage which gathers together image features of perceptual salience, usually having a well-definable structure. This work addresses the problem of generic part-based grouping and recognition from single two-dimensional edge images following a strategy that employs generic part models at all stages: the key underlying idea is to perform a purposive grouping of simple parts and these parts can be conveniently represented by generic deformable part models. This paper describes the proposed computational method, which is more extensively treated in Pilu (1996).",
                "year": 1997,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Visuo-inertial stabilization in space-variant binocular systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S092188909900072X/pdfft?md5=6d4a61ffe03f1986d8b88bc88f7b6ef9&pid=1-s2.0-S092188909900072X-main.pdf",
                "title": "Visuo-inertial stabilization in space-variant binocular systems",
                "abstract": "Stabilization of gaze is a major functional prerequisite for robots exploring the environment. The main reason for a “steady-image” requirement is to prevent the robot’s own motion to compromise its “visual functions”. In this paper we present an artificial system, the LIRA robot head, capable of controlling its cameras/eyes to stabilize gaze. The system features a stabilization mechanism relying on principles exploited by natural systems: an inertial sensory apparatus and images of space-variant resolution. The inertial device measures angular velocities and linear acceleration along the vertical and horizontal fronto-parallel axes. The space-variant image geometry facilitates real-time computation of optic flow and the extraction of first-order motion parameters. Experiments which describe the performance of the LIRA robot head are presented. The results show that the stabilization mechanism improves the reactivity of the system to changes occurring suddenly at new spotted locations.",
                "year": 2000,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Modeling visual attention via selective tuning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370295000259/pdfft?md5=f15204049d1ca85f6ecf929c5c966929&pid=1-s2.0-0004370295000259-main.pdf",
                "title": "Modeling visual attention via selective tuning",
                "abstract": "A model for aspects of visual attention based on the concept of selective tuning is presented. It provides for a solution to the problems of selection in an image, information routing through the visual processing hierarchy and task-specific attentional bias. The central thesis is that attention acts to optimize the search procedure inherent in a solution to vision. It does so by selectively tuning the visual processing network which is accomplished by a top-down hierarchy of winner-take-all processes embedded within the visual processing pyramid. Comparisons to other major computational models of attention and to the relevant neurobiology are included in detail throughout the paper. The model has been implemented; several examples of its performance are shown. This model is a hypothesis for primate visual attention, but it also outperforms existing computational solutions for attention in machine vision and is highly appropriate to solving the problem in a robot vision system.",
                "year": 1995,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Haptic rendering of objects with rigid and deformable parts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849310001317/pdfft?md5=e2095b9672bee011bc113a81e1d32277&pid=1-s2.0-S0097849310001317-main.pdf",
                "title": "Haptic rendering of objects with rigid and deformable parts",
                "abstract": "In many haptic applications, the user interacts with the virtual environment through a rigid tool. Tool-based interaction is suitable in many applications, but the constraint of using rigid tools is not applicable to some situations, such as the use of catheters in virtual surgery, or of a rubber part in an assembly simulation. Rigid-tool-based interaction is also unable to provide force feedback regarding interaction through the human hand, due to the soft nature of human flesh. In this paper, we address some of the computational challenges of haptic interaction through deformable tools, which forms the basis for direct-hand haptic interaction. We describe a haptic rendering algorithm that enables interactive contact between deformable objects, including self-collisions and friction. This algorithm relies on a deformable tool model that combines rigid and deformable components, and we present the efficient simulation of such a model under robust implicit integration.",
                "year": 2010,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Passive navigation using egomotion estimates": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885699000827/pdfft?md5=b07afec9131283307cf95bb38c43ee16&pid=1-s2.0-S0262885699000827-main.pdf",
                "title": "Passive navigation using egomotion estimates",
                "abstract": "The goal of this work is to propose a method to solve the problem of passive navigation with visual means. Passive navigation is the ability of an autonomous agent to determine its motion with respect to the environment. The two main egomotion parameters allowing performing passive navigation are the heading direction and the time to collision with the environment. A lot of approaches have been proposed in literature in order to estimate the above parameters, most of which work well only if the motion is a predominant forward translation and small amounts of noise are present in the input data.\n\nThe method we propose is a two-state approach: matching of features extracted from 2D images of a sequence at different times and egomotion parameter computation. Both algorithms are based on optimization approaches minimizing appropriate energy functions. The novelty of the proposed approach is to formulate the matching energy function in order to englobe invariant cues of the scene. The matching stage recovers correspondences between sparse high interest feature points of two successive images useful to perform the second stage of egomotion parameter estimation. Experimental results obtained in real context show the robustness of the method.",
                "year": 2000,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Grid coding: A preprocessing technique for robot and machine vision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370271900154/pdfft?md5=87ed00bb83761ba1909b410ca26f9759&pid=1-s2.0-0004370271900154-main.pdf",
                "title": "Grid coding: A preprocessing technique for robot and machine vision",
                "abstract": "The problem of machine vision as evidenced in the various robot projects in existence is attacked by analogy with the supposed nature of human visual processing in that edges are enhanced, texture is examined and various heuristic approaches are studied. This paper describes a non-anthropomorphically based method of decomposing a scene subjected to a special form of illumination into elementary planar areas. The method consists in coding the various planar areas as the modulation on a spatial frequency carrier grid so that the extraction of the planar areas becomes a matter of linear frequency domain filtering. The paper also addresses the application of grid coding to other problems in recording and extracting information from 3-D images.",
                "year": 1971,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Multi-Modality guidance based surgical navigation for percutaneous endoscopic transforaminal discectomy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260721005344/pdfft?md5=fd261f63a4f54aeeade131a1064aa351&pid=1-s2.0-S0169260721005344-main.pdf",
                "title": "Multi-Modality guidance based surgical navigation for percutaneous endoscopic transforaminal discectomy",
                "abstract": "Objective\n\nFluoroscopic guidance is a critical step for the puncture procedure in percutaneous endoscopic transforaminal discectomy (PETD). However, two-dimensional observations of the three-dimensional anatomic structure suffer from the effects of projective simplification. To accurately assess the spatial relations between the patient vertebra tissues and puncture needle, a considerable number of fluoroscopic images from different orientations need to be acquired by the surgeons. This process significantly increases the radiation risk for both the patient and surgeons.",
                "year": 2021,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Today the earwig, tomorrow man?": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/000437029190054N/pdfft?md5=c1b9d333e02c19172aae5d51ba2e06e0&pid=1-s2.0-000437029190054N-main.pdf",
                "title": "Today the earwig, tomorrow man?",
                "abstract": "A startling amount of intelligent activity can be controlled without reasoning or thought. By tuning the perceptual system to task relevant properties a creature can cope with relatively sophisticated environments without concepts. There is a limit, however, to how far a creature without concepts can go. Rod Brooks, like many ecologically oriented scientists, argues that the vast majority of intelligent behaviour is concept-free. To evaluate this position I consider what special benefits accrue to concept-using creatures. Concepts are either necessary for certain types of perception, learning, and control, or they make those processes computationally simpler. Once a creature has concepts its capacities are vastly multiplied.",
                "year": 1991,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "A strategy for grasping unknown objects based on co-planarity and colour information": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889010000047/pdfft?md5=08969b8a57a4c1f16a470d49e628378f&pid=1-s2.0-S0921889010000047-main.pdf",
                "title": "A strategy for grasping unknown objects based on co-planarity and colour information",
                "abstract": "In this work, we describe and evaluate a grasping mechanism that does not make use of any specific object prior knowledge. The mechanism makes use of second-order relations between visually extracted multi-modal 3D features provided by an early cognitive vision system. More specifically, the algorithm is based on two relations covering geometric information in terms of a co-planarity constraint as well as appearance based information in terms of co-occurrence of colour properties. We show that our algorithm, although making use of such rather simple constraints, is able to grasp objects with a reasonable success rate in rather complex environments (i.e., cluttered scenes with multiple objects).\n\nMoreover, we have embedded the algorithm within a cognitive system that allows for autonomous exploration and learning in different contexts. First, the system is able to perform long action sequences which, although the grasping attempts not being always successful, can recover from mistakes and more importantly, is able to evaluate the success of the grasps autonomously by haptic feedback (i.e., by a force torque sensor at the wrist and proprioceptive information about the distance of the gripper after a gasping attempt). Such labelled data is then used for improving the initially hard-wired algorithm by learning. Moreover, the grasping behaviour has been used in a cognitive system to trigger higher level processes such as object learning and learning of object specific grasping.",
                "year": 2010,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "The CLEARSY safety platform: 5 years of research, development and deployment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642320301325/pdfft?md5=2f038d721c3089833af7004d5de0f99a&pid=1-s2.0-S0167642320301325-main.pdf",
                "title": "The CLEARSY safety platform: 5 years of research, development and deployment",
                "abstract": "The CLEARSY Safety Platform (CSSP) was designed to ease the development of safety critical systems and to reduce the overall costs (development, deployment, and certification) under the pressure of the worldwide market. A smart combination of hardware features (double processor) and formal method (B method and code generators) was used to produce a SIL4-ready platform where safety principles are built-in and cannot be altered by the developer. Summarizing a 5-year return of experience in the effective application in the railways, this article explains how this approach is a game-changer and tries to anticipate the future of this platform for safety critical systems. In particular, the education of future engineers and the seamless integration in existing engineering processes with the support of Domain Specific Languages are key topics for a successful deployment in other domains. DSL like Robosim to program mobile robots and relay circuits to design railway signaling systems are connected to the platform.",
                "year": 2020,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Estimating optical flow: A comprehensive review of the state of the art": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314224002418/pdfft?md5=0e040acf6e4116194d80885aeb4b2b49&pid=1-s2.0-S1077314224002418-main.pdf",
                "title": "Estimating optical flow: A comprehensive review of the state of the art",
                "abstract": "Optical flow estimation is a crucial task in computer vision that provides low-level motion information. Despite recent advances, real-world applications still present significant challenges. This survey provides an overview of optical flow techniques and their application. For a comprehensive review, this survey covers both classical frameworks and the latest AI-based techniques. In doing so, we highlight the limitations of current benchmarks and metrics, underscoring the need for more representative datasets and comprehensive evaluation methods. The survey also highlights the importance of integrating industry knowledge and adopting training practices optimized for deep learning-based models. By addressing these issues, future research can aid the development of robust and efficient optical flow methods that can effectively address real-world scenarios.",
                "year": 2024,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "An intrinsically-motivated schema mechanism to model and simulate emergent cognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041711000398/pdfft?md5=b55121b72f380a1b026de56c7491bc61&pid=1-s2.0-S1389041711000398-main.pdf",
                "title": "An intrinsically-motivated schema mechanism to model and simulate emergent cognition",
                "abstract": "We introduce an approach to simulate the early mechanisms of emergent cognition based on theories of enactive cognition and on constructivist epistemology. The agent has intrinsic motivations implemented as inborn proclivities that drive the agent in a proactive way. Following these drives, the agent autonomously learns regularities afforded by the environment, and hierarchical sequences of behaviors adapted to these regularities. The agent represents its current situation in terms of perceived affordances that develop through the agent’s experience. This situational representation works as an emerging situation awareness that is grounded in the agent’s interaction with its environment and that in turn generates expectations and activates adapted behaviors. Through its activity and these aspects of behavior (behavioral proclivity, situation awareness, and hierarchical sequential learning), the agent starts to exhibit emergent sensibility, intrinsic motivation, and autonomous learning. Following theories of cognitive development, we argue that this initial autonomous mechanism provides a basis for implementing autonomously developing cognitive systems.",
                "year": 2012,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "On the assessment of the mechanical properties of additively manufactured lattice structures": {
        "accordingTo": {
            "scienceDirect": {
                "title": "On the assessment of the mechanical properties of additively manufactured lattice structures",
                "abstract": "Lattice structures fabricated by additive manufacturing (AM) technology have many excellent properties, such as lightweight, high strength, energy absorption, and vibration reduction, which have been extensively researched and made a breakthrough. Lattice structures have been commonly used in aviation, bioengineering, robotics, and other industrial fiber because of their outstanding properties. The first part of this article provides a short review on the assessment of mechanical properties of various lattice structures in terms of their classification, applications, materials and fabrication techniques, and complexity of designing, fabrication, and post-processing as well as some of the numerical models to predict the mechanical properties of the lattice structures. The second part of the article proposes a deep learning (DL) model for a highly accurate stress-strain behavior assessment of numerous lattice structures such as namely: the octet, face center-cubic, body-centered cubic, diamond, rhombic, cubic, truncated cube, and truncated cuboctahedron, etc, which were fabricated using many different materials via various approaches and methods. Using the proposed DL model, an accuracy in terms of R2 = 0.999 (correlation coefficient), MSE = 0.0017 (mean squared error), and MAE = 0.0312 (mean absolute error) can be achieved for the prediction of the deemed mechanical property of the lattice structures. The model contains simple, quick and precise predictability that makes it ideal for the use of lattice structures in various practical applications, including heater and heat exchangers, engine hood, biomedical implant, wings, gas turbine, vibration absorber, robotic device, etc.",
                "year": 2022,
                "publisher": "Engineering Analysis with Boundary Elements"
            }
        }
    },
    "Enabling affordances for AI Governance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S266665962400012X/pdfft?md5=9bf6cc548743ad7d2d5c0830773f5145&pid=1-s2.0-S266665962400012X-main.pdf",
                "title": "Enabling affordances for AI Governance",
                "abstract": "Organizations dealing with mission-critical AI based autonomous systems may need to provide continuous risk management controls and establish means for their governance. To achieve this, organizations are required to embed trustworthiness and transparency in these systems, with human overseeing and accountability. Autonomous systems gain trustworthiness, transparency, quality, and maintainability through the assurance of outcomes, explanations of behavior, and interpretations of intent. However, technical, commercial, and market challenges during the software development lifecycle (SDLC) of autonomous systems can lead to compromises in their quality, maintainability, interpretability and explainability. This paper conceptually models transformation of SDLC to enable affordances for assurance, explanations, interpretations, and overall governance in autonomous systems. We argue that opportunities for transformation of SDLC are available through concerted interventions such as technical debt management, shift-left approach and non-ephemeral artifacts. This paper contributes to the theory and practice of governance of autonomous systems, and in building trustworthiness incrementally and hierarchically.",
                "year": 2024,
                "publisher": "Journal of Responsible Technology"
            }
        }
    },
    "The design of RIA accessibility evaluation tool": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0965997812001512/pdfft?md5=f976ac4f6478786cec87e6011e8c7fb2&pid=1-s2.0-S0965997812001512-main.pdf",
                "title": "The design of RIA accessibility evaluation tool",
                "abstract": "Most of the accessibility evaluation tools available check the accessibility of static traditional web pages (e.g., web pages encoded with only HTML) according to the accessibility guidelines (e.g., WCAG). There is an increased need to check the accessibility of Rich Internet Applications (RIAs) using automatic tools. In this paper, we present a conceptual framework for automatic evaluation of accessibility of rich Internet contents. The framework includes: web robot, RIA events controller, WAI-ARIA accessibility specifications, evaluator, and results handler. An example to demonstrate how the framework can be applied has been presented in detail.",
                "year": 2013,
                "publisher": "Advances in Engineering Software"
            }
        }
    },
    "Representing information – Classifying the Augmented Reality presentation space": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849313001313/pdfft?md5=0fd63643f66d274cd32fad3ccb602ba8&pid=1-s2.0-S0097849313001313-main.pdf",
                "title": "Representing information – Classifying the Augmented Reality presentation space",
                "abstract": "Augmented Reality has a wide-ranging presentation space. In addition to presenting virtual information in a 3D space, such information can also be placed in relation to physical objects, locations or events.\n\nDecomposing this presentation space – or more exactly, the principles of how information is represented in Augmented Reality – into unique and independent dimensions provides a fundamental spectrum of options. First, this decomposition facilitates a fine-grained analysis of effects on human understanding. Second, multiple factors, given by multiple differences between different presentation systems with respect to more than one such principle, can be determined and properly addressed. Third, this decomposition facilitates a determination of new fields of research by identifying not-yet-used concepts.\n\nSince the beginning of Augmented Reality research, a growing number of applications have emerged that exploit various ways to represent information. This paper resumes this development and presents a set of independent dimensions covering representation principles of virtual information related to a physical environment: the temporality of virtual information, dimensionality, the frame of reference, mounting/registration and the type of reference. The suitability of the devised dimensions is tested by categorizing a wide variety of AR applications. The categorized data is analyzed for the most-often and less-frequently used combinations of classes. In particular, the classes that have not yet been used exhibit the potential to allow future work that investigates new options for information presentation.",
                "year": 2013,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Dynamic aspects in active vision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/104996609290089L/pdfft?md5=41432c9a73b957f9668eb6d80b48ba16&pid=1-s2.0-104996609290089L-main.pdf",
                "title": "Dynamic aspects in active vision",
                "abstract": "The term active stresses the role of the motion or, generally speaking, the dynamic interaction of the observer with the environment. This concept emphasizes the relevance of determining scene properties from the temporal evolution of image features. Within the active vision approach, two different aspects are considered: the advantages of space-variant vision for dynamic visual processing and the qualitative analysis of optical flow. A space-variant sampling of the image plane has many good properties in relation to active vision. In particular, two examples are presented in which a log-polar representation is used for active vergence control and to estimate the time-to-impact during tracking egomotion. These are just two modules which could fit into a complete active vision system, but already highlight the advantages of space-variant sensing within the active vision paradigm. In the second part of the paper the extraction of qualitative properties from the optical flow is discussed. A new methodology is proposed in which optical flows are analyzed in terms of anomalies as unexpected velocity patterns or inconsistencies with respect to some predicted dynamic feature. Two kinds of knowledge are necessary: about the dynamic of the scene (for example, the approximate motion of the camera) and about the task to be accomplished, which is analogous to (qualitatively) knowing the kind of scene I would expect to see. The first simply implies the measurement of some motion parameters (for example, with inertial sensors) directly on the camera, or to put some constraints in the egomotion. The second requirement implies that the visual process is task-driven. Some examples are presented in which the method is successfully applied to robotic tasks.",
                "year": 1992,
                "publisher": "CVGIP: Image Understanding"
            }
        }
    },
    "From LiDAR point cloud towards digital twin city: Clustering city objects based on Gestalt principles": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0924271620302070/pdfft?md5=d4fd23c940d72d5930189b8c37bbbe83&pid=1-s2.0-S0924271620302070-main.pdf",
                "title": "From LiDAR point cloud towards digital twin city: Clustering city objects based on Gestalt principles",
                "abstract": "Recent advancement of remote sensing technologies has brought in accurate, dense, and inexpensive city-scale Light Detection And Ranging (LiDAR) point clouds, which can be utilized to model city objects (e.g., buildings, roads, and automobiles) for creating Digital Twin Cities (DTCs). However, processing such unstructured point clouds is very challenging, epitomized by high cost, movable objects, limited object classes, and high information inadequacy/redundancy. We noticed that many city objects are not in random shapes; rather, they have invariant cross-sections following the Gestalt design principles, including proximity, connectivity, symmetry, and similarity. In this paper, we present a novel unsupervised method, called Clustering Of Symmetric Cross-sections of Objects (COSCO), to process urban LiDAR point clouds to a hierarchy of objects based on their characteristic cross-sections. First, city objects are segmented as connected patches of proximate 3D points. Then, symmetric cross-sections are detected for symmetric city objects. Finally, the taxonomy and groups of city objects are recognized from a hierarchical clustering analysis of the dissimilarity matrix. Experimental results showed that COSCO detected the correct taxonomy and types of 12 cars from 24,126 LiDAR points in 8.28 s. Based on the cross-sections and taxonomy, a digital twin was created by registering online free 3D car models in 29.58 s. The contribution of this paper is twofold. First, it presents an effective unsupervised method for understanding and developing DTC objects in LiDAR point clouds by harnessing innate Gestalt design principles. Secondly, COSCO can be an efficient LiDAR pre-processing tool for recognizing symmetric city objects’ cross-sections, positions, heading directions, dimensions, and possible types for smart city applications in GIScience, Architecture, Engineering, Construction and Operation (AECO), and autonomous vehicles.",
                "year": 2020,
                "publisher": "ISPRS Journal of Photogrammetry and Remote Sensing"
            }
        }
    },
    "Developing an immersive game-based learning platform with generative artificial intelligence and virtual reality technologies – “LearningverseVR”": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2949678024000199/pdfft?md5=b5c7815c990cbacc10e7f6074f3bd423&pid=1-s2.0-S2949678024000199-main.pdf",
                "title": "Developing an immersive game-based learning platform with generative artificial intelligence and virtual reality technologies – “LearningverseVR”",
                "abstract": "The rapid evolution of generative artificial intelligence (AI) and virtual reality (VR) technologies are revolutionising various fields, including education and gaming industries. However, studies on how to enhance immersive game-based learning with AI and VR technologies remain scant. Given this, the article presents the creation of “LearningverseVR,” an immersive game-based learning platform developed using generative AI and VR technologies, which is based on “Learningverse,” a metaverse platform developed by the lead author and her research team. The “LearningverseVR” platform uses Unity as the client and Python, Flask and MySQL as the backend. Unity's multiplayer service provides multiplayer online functionality, supporting learners to engage in immersive and interactive learning activities. The design framework of the platform consists of two main components: Game-based learning with generative AI and immersion with VR technologies. First, generative AI is used to create NPCs with diverse personalities and life backgrounds, and enable learners to interact with NPCs without scripted dialogues, creating an interactive and immersive game-based learning environment. Secondly, such a learning experience is enhanced by leveraging the Large Language Model (LLM) ecosystem with VR technology. The creation of the “LearningverseVR” platform provides novel perspectives on digital game-based learning.",
                "year": 2024,
                "publisher": "Computers & Education: X Reality"
            }
        }
    },
    "Detecting intention to execute the next movement while performing current movement from EEG using global optimal constrained ICA": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482518301379/pdfft?md5=f96465e1b554c7aa113624da4f9dc2fd&pid=1-s2.0-S0010482518301379-main.pdf",
                "title": "Detecting intention to execute the next movement while performing current movement from EEG using global optimal constrained ICA",
                "abstract": "Background\n\nBrain–computer interfaces (BCIs) are a promising tool in neurorehabilitation. The intention to perform a motor action can be detected from brain signals and used to control robotic devices. Most previous studies have focused on the starting of movements from a resting state, while in daily life activities, motions occur continuously and the neural activities correlated to the evolving movements are yet to be investigated.",
                "year": 2018,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Interpreting a dynamic and uncertain world: task-based control": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0004370298000046/pdfft?md5=c114a6cf00a766b7ebd2b471e678f200&pid=1-s2.0-S0004370298000046-main.pdf",
                "title": "Interpreting a dynamic and uncertain world: task-based control",
                "abstract": "In this paper we show that it can be beneficial to have a high-level vision component that guides the reasoning of the whole vision system when interpreting a dynamic and uncertain world. This guidance is provided by an attentional mechanism that exploits knowledge of the specific problem being solved. Here we develop a general framework for such an attentional mechanism and its application to understanding dynamic scenes. This attentional mechanism can enable a vision system to perform a given domain task while expending minimal resources. We have developed a component that uses Bayesian networks combined with a deictic representation to select what, when and how to use processed data from a fixed camera. We apply two forms of Bayesian network, which (1) create a dynamic structure to reflect the spatial organisation of the data and (2) measure task relatedness. Together these give attentional focus making the reasoning performed relevant to the task.",
                "year": 1998,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Animate vision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370291900804/pdfft?md5=1413bc86d7a1cb9a462c9dab05e82c62&pid=1-s2.0-0004370291900804-main.pdf",
                "title": "Animate vision",
                "abstract": "Animate vision systems have gaze control mechanisms that can actively position the camera coordinate system in response to physical stimuli. Compared to passive systems, animate systems show that visual computation can be vastly less expensive when considered in the larger context of behavior. The most important visual behavior is the ability to control the direction of gaze. This allows the use of very low resolution imaging that has a high virtual resolution. Using such a system in a controlled way provides additional constraints that dramatically simplify the computations of early vision. Another important behavior is the way the environment “behaves”. Animate systems under real-time constraints can further reduce their computational burden by using environmental cues that are perspicuous in the local context. A third source of economy is introduced when behaviors are learned. Because errors are rarely fatal, systems using learning algorithms can amortize computational cost over extended periods. Further economies can be achieved when the learning system uses indexical reference, which is a form of dynamic variable binding. Animate vision is a natural way of implementing this dynamic binding.",
                "year": 1991,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Task frames: Primitives for sensory-motor coordination": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X86900794/pdfft?md5=f8fc6a0fcf7561a2d22e6201f754c6bf&pid=1-s2.0-0734189X86900794-main.pdf",
                "title": "Task frames: Primitives for sensory-motor coordination",
                "abstract": "In order to navigate and manipulate objects in the environment, a robot must have a model of itself, its surround, and how it interacts with the surround. The key issues are: what form should these models take; how are they constructed from sensory input; and how are they used? We argue that an extension of object-centered coordinate frames, termed task frames, is an essential ingredient in such models.",
                "year": 1986,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Embodiment of Wearable Technology: Qualitative Longitudinal Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Embodiment of Wearable Technology: Qualitative Longitudinal Study",
                "abstract": "Background\n\nCurrent technology innovations, such as wearables, have caused surprising reactions and feelings of deep connection to devices. Some researchers are calling mobile and wearable technologies cognitive prostheses, which are intrinsically connected to individuals as if they are part of the body, similar to a physical prosthesis. Additionally, while several studies have been performed on the phenomenology of receiving and wearing a physical prosthesis, it is unknown whether similar subjective experiences arise with technology.",
                "year": 2020,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Dual humanness and trust in conversational AI: A person-centered approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563221000492/pdfft?md5=d381aa2bbfc00cbdaef7f15a5f94d191&pid=1-s2.0-S0747563221000492-main.pdf",
                "title": "Dual humanness and trust in conversational AI: A person-centered approach",
                "abstract": "Conversational Artificial Intelligence (AI) is digital agents that interact with users by natural language. To advance the understanding of trust in conversational AI, this study focused on two humanness factors manifested by conversational AI: speaking and listening. First, we explored users' heterogeneous perception patterns based on the two humanness factors. Next, we examined how this heterogeneity relates to trust in conversational AI. A two-stage survey was conducted to collect data. Latent profile analysis revealed three distinct patterns: para-human perception, para-machine perception, and asymmetric perception. Finite mixture modeling demonstrated that the benefit of humanizing AI's voice for competence-related trust can evaporate once AI's language understanding is perceived as poor. Interestingly, the asymmetry between humanness perceptions in speaking and listening can impede morality-related trust. By adopting a person-centered approach to address the relationship between dual humanness and user trust, this study contributes to the literature on trust in conversational AI and the practice of trust-inducing AI design.",
                "year": 2021,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Minimal cognition and stigmergic coordination: An everyday tale of building and bacteria": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Minimal cognition and stigmergic coordination: An everyday tale of building and bacteria",
                "abstract": "Van Duijn and colleagues propose that minimal cognition can be understood in terms of sensorimotor coordination – that is the coordination between sensory apparatus of an organism and the motor processes that move it around the environment (2006). Despite there being much to recommend this account it still faces some challenges. For example, it does not accord with the intuition that there are cognitive processes that have little to do with sensorimotor coupling, it relies on a strong distinction between metabolic and cognitive processes, and perhaps counterintuitively it denies cognition to plants but grants it to the bacteria on their roots that perform various functions for them. Moreover, it is difficult to see how to account for cognition over the transition from single to multicellular organisms. This paper proposes taking a more radical view of cognition to be what happens in swarms – the coordination of multiple processes through the traces of their actions in the environment. Key to this approach is the observation that the structure of the environment plays an active coordinative role and that this structure results in part from the actions of the system that it helps coordinate. Systems develop sensitivity to the appropriate trace variables in the environment. Viewed as collections of processes bacteria, biofilms, plants, animals can be viewed as cognitive systems in this framework and it has the potential to be applied to the social world.",
                "year": 2023,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Towards an AI-Driven User Interface Design for Web Applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050924011104/pdfft?md5=d3aee0c3ac346f170241c55d4b3e7306&pid=1-s2.0-S1877050924011104-main.pdf",
                "title": "Towards an AI-Driven User Interface Design for Web Applications",
                "abstract": "The increasing exploitation of Artificial Intelligence (AI) technologies has enabled the design of user interfaces in a way that integrating artificial intelligence capabilities has become crucial in the modern digital landscape. Exploring the main features and best practices for designing user interfaces for Web applications, which effectively support and leverage AI functionalities, is currently one of the relevant topics in this context. This research work discusses the fundamental principles of user interface (UI) design, and the challenges posed by the integration of AI into web applications. It emphasizes the need to strike a balance between the AI advanced capabilities and the users' ability to understand and control the system. Furthermore, the paper highlights the importance of creating intuitive and engaging UI designs that empower users to interact with AI-driven features effortlessly. The study presents a comprehensive analysis of various UI design techniques specifically tailored for AI-enabled web applications user interfaces. Additionally, the paper explores the incorporation of AI-driven recommendation systems, personalized interfaces, and adaptive designs, which dynamically adapt to users' preferences and behavior. To validate the proposed user interface design principles, the study presents a proposal for a guidelines structure that promotes empirical evaluations through user studies and usability testing. Results collected via a survey based on measuring the effectiveness and user satisfaction of AI-enabled Web interfaces. User interfaces in real-life scenarios are presented and provides information on the impact of UI design decisions on user interaction and overall experience. The outcomes of this research work contribute to a deeper understanding of UI design for AI-supported Web applications user interfaces and offer practical guidelines for designers and developers. By embracing the suggested principles, organizations and designers can create Web interfaces that effectively harness the power of AI while prioritizing user-centricity, accessibility, and ethical considerations.",
                "year": 2024,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Technology meets tradition: The perceived impact of the introduction of information and communication technology on ward rounds in the intensive care unit": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505617300825/pdfft?md5=75a72942dec8cc978d0db6debdf1c61e&pid=1-s2.0-S1386505617300825-main.pdf",
                "title": "Technology meets tradition: The perceived impact of the introduction of information and communication technology on ward rounds in the intensive care unit",
                "abstract": "Background\n\nPublic policy in many health systems is currently dominated by the quest to find ways to ‘do more with less’—to achieve better outcomes at a reduced cost. The success or failure of initiatives in support of this quest are often understood in terms of an adversarial dynamic or struggle between the professional logics of medicine and of management. Here, we use the case of the introduction of information and communication technology (ICT) to a well-established ritual of medical autonomy (the medical ward round) to articulate a more nuanced explanation of how and why new ways of working with technology are accepted and adopted (or not).",
                "year": 2017,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "Sensory representation spaces in neuroscience and computation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231208004682/pdfft?md5=b5dcb4f04f16d36cb3e5ab2a2b170629&pid=1-s2.0-S0925231208004682-main.pdf",
                "title": "Sensory representation spaces in neuroscience and computation",
                "abstract": "Physics, Neuroscience and Computation are concerned with finding the most appropriate representation spaces to describe the interaction of a dynamic system with its environment. In this work first we review the two basic conceptual approaches to the problem of representing an environment, Marr's ascending “constructivism” and Gibson's “direct perception” hypothesis. Later we review the basic neural mechanisms associated with creating meaning in both approaches: lateral inhibition and the creation of cortical maps by resonance to patterns of stimuli of families of spatially ordered neurons. We end by considering the usefulness in artificial intelligence of knowledge about the way in which biological systems construct their representation spaces. We stress the idea regarding events as representation entities and, consequently, using an event time, different from physical time. Semantics emerges from the mechanisms that detect these relevant events in each organisational level and their composition rules to specify the constitutive entities of the next level. This semantic is distributed in the cortical maps of the neuron groups that resound to the corresponding events.",
                "year": 2009,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Optical flow and scene flow estimation: A survey": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000480/pdfft?md5=5228fc3e406a205d6e046d29410038cf&pid=1-s2.0-S0031320321000480-main.pdf",
                "title": "Optical flow and scene flow estimation: A survey",
                "abstract": "Motion analysis is one of the most fundamental and challenging problems in the field of computer vision, which can be widely applied in many areas, such as autonomous driving, action recognition, scene understanding, and robotics. In general, the displacement field between subsequent frames can be divided into two types: optical flow and scene flow. The optical flow represents the pixel motion of adjacent frames. In contrast, the scene flow is a 3D motion field of the dynamic scene between two frames. Traditional approaches for the estimation of optical flow and scene flow usually leverage the variational technique, which can be solved as an energy minimization process. In recent years, deep learning has emerged as a powerful technique for learning feature representations directly from data. It has led to remarkable progress in the field of optical flow and scene flow estimation. In this paper, we provide a comprehensive survey of optical flow and scene flow estimation. First, we briefly review the pioneering approaches that use variational technique and then we delve in detail into the deep learning-based approaches. Furthermore, we present insightful observations on evaluation issues, specifically benchmark datasets, evaluation metrics, and state-of-the-art performance. Finally, we give the promising directions for future research. To the best of our knowledge, we are the first to review both optical flow and scene flow estimation, and the first to cover both traditional and deep learning-based approaches.",
                "year": 2021,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "An agent that learns to support users of a Web site": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494603000565/pdfft?md5=e8fdde8ab669995bb51a9509c5efb3e9&pid=1-s2.0-S1568494603000565-main.pdf",
                "title": "An agent that learns to support users of a Web site",
                "abstract": "The terms agent, intelligent agent and 3D agent are becoming more and more frequently used in literature. A key issue in the web community is that a web site must be equipped with a virtual agent able to support users in a natural way. Following this trend, we decided to implement scenographic agents mimic intelligent reasoning (SAMIR), a prototype of a tool for animating 3D intelligent agents, mainly founded on a genetic algorithms-based learning system, namely an XCS, and on an FFD-based technique for the facial deformation of the character. SAMIR is based on an object-oriented architecture, and it adopts the MPEG-4 facial animation parameters (FAPs) as a description format for facial expressions and animations.",
                "year": 2004,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Manipulating subjective realism and its impact on presence: Preliminary results on feasibility and neuroanatomical correlates": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543812000458/pdfft?md5=9fc1576e52737550de994425c18d5146&pid=1-s2.0-S0953543812000458-main.pdf",
                "title": "Manipulating subjective realism and its impact on presence: Preliminary results on feasibility and neuroanatomical correlates",
                "abstract": "The feeling of presence has been shown to be an important concept in several clinical applications of virtual reality. Among the factors influencing presence, realism factors have been examined extensively from the angle of objective realism. Objective realism has been manipulated by altering numerous technological characteristics such as pictorial quality, texture and shading, or by adding more sensory information (i.e., smell, touch). Much less studied is the subjective (or perceived) realism, the focus of the two pilot studies reported in this article. In Study 1, subjective realism was manipulated in order to assess the impact on the feeling of presence. Method: Presence was measured in 31 adults after two immersions in virtual reality. Participants were immersed in a neutral/irrelevant virtual environment and subsequently subjected to the experimental manipulation. Participants in the experimental condition were falsely led to believe that they were immersed live in real time in a “real” room with a “real” mouse in a cage. In the control condition, participants believed they were immersed in a replica of the nearby room. All participants were actually immersed in the exact same virtual environment. Results: A manipulation check revealed that 80% of the participants believed in the deception. A 2 Times by 2 Conditions repeated measure ANOVA revealed that leading people to believe they were seeing a real environment digitized live in virtual reality increased their feeling of presence compared to the control condition. In Study 2, the same experimental design was used but with simultaneous functional magnetic resonance imaging (fMRI) in order to assess brain areas potentially related to the feeling of presence. fMRI data from five participants were subjected to a within subject fixed effect analysis to verify differences between the experimental immersion (higher presence) and the control immersion (lower presence). Results revealed a statistically significant difference in left and right parahippocampus areas. Conclusion: Results are discussed according to layers of presence and consciousness and the meaning given to experiences occurring in virtual reality. Some suggestions are formulated to target core presence and extended presence.",
                "year": 2012,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Externalized memory in slime mould and the extended (non-neuronal) mind": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Externalized memory in slime mould and the extended (non-neuronal) mind",
                "abstract": "The hypothesis of extended cognition (HEC) claims that the cognitive processes that materially realise thinking are sometimes partially constituted by entities that are located external to an agent’s body in its local environment. We show how proponents of HEC need not claim that an agent must have a central nervous system, or physically instantiate processes organised in such a way as to play a causal role equivalent to that of the brain if that agent is to be capable of cognition. Focusing on the case of spatial memory, we make our argument by taking a close look at the striking example of Physarum Polycephalum plasmodium (i.e., slime mould) which uses self-produced non-living extracellular slime trails to navigate its environment. We will argue that the use of externalized spatial memory by basal organisms like Physarum is an example of extended cognition. Moreover, it is a possible evolutionary precursor to the use of internal spatial memory and recall in animals thus demonstrating how extended cognition may have emerged early in evolutionary history.",
                "year": 2022,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Semantics, experience and time": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041702000451/pdfft?md5=85c15ce3f5cc1c51f6a13685fc0e9d3e&pid=1-s2.0-S1389041702000451-main.pdf",
                "title": "Semantics, experience and time",
                "abstract": "The computational hypothesis, with its inherent representationalism, and the dynamical hypothesis, with its apparent absence of representations and its commitment to continuous time, stand at an impasse. It is unclear how the dynamical stance can handle representational thought, or how computationalism can deal effectively with a tightly coupled, reciprocally causative agent–environment system in continuous transformation. Underlying this dilemma is the complex relation of mind to time, a relation encoded in the word experience. We must ask if any hypothesis describes a ‘device’ capable of experience? Yet what is an intelligence and its thought without experience? Is a computational device, whether supporting a symbolic processor or connectionist net, intrinsically condemned to a zero degree of experience? What is required of a dynamical device? It is argued here that ‘semantic’ intelligence and thought rests upon experience, fundamentally upon the invariance laws defined over time within conscious perception. The structure of experience is intrinsically unavailable to the computational device, limiting it to a ‘syntactic’ intelligence. An alternative conception of a device is offered, based on Bergson conjoined with Gibson, which supports the qualitative and structural aspects of experience and the semantic. It frames a dynamical model of perception and memory in which invariance laws are intrinsic, creates a deeper notion of situatedness, and supports a concept of semantically based, representative thought founded upon perception.",
                "year": 2002,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "A deep generic to specific recognition model for group membership analysis using non-verbal cues": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A deep generic to specific recognition model for group membership analysis using non-verbal cues",
                "abstract": "Automatic understanding and analysis of groups has attracted increasing attention in the vision and multimedia communities in recent years. However, little attention has been paid to the automatic analysis of the non-verbal behaviors and how this can be utilized for analysis of group membership, i.e., recognizing which group each individual is part of. This paper presents a novel Support Vector Machine (SVM) based Deep Specific Recognition Model (DeepSRM) that is learned based on a generic recognition model. The generic recognition model refers to the model trained with data across different conditions, i.e., when people are watching movies of different types. Although the generic recognition model can provide a baseline for the recognition model trained for each specific condition, the different behaviors people exhibit in different conditions limit the recognition performance of the generic model. Therefore, the specific recognition model is proposed for each condition separately and built on top of the generic recognition model. A number of experiments are conducted using a database aiming to study group analysis while each group (i.e., four participants together) were watching a number of long movie segments. Our experimental results show that the proposed deep specific recognition model (44%) outperforms the generic recognition model (26%). The recognition of group membership also indicates that the non-verbal behaviors of individuals within a group share commonalities.",
                "year": 2019,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Intelligent agents for feature modelling in computer aided design": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2288430017300520/pdfft?md5=fd2c2b97c4f2a934c6dbaa5374cdfbde&pid=1-s2.0-S2288430017300520-mainext.pdf",
                "title": "Intelligent agents for feature modelling in computer aided design",
                "abstract": "CAD modelling can be referred to as the process of generating an integrated multiple view model as a representation of multiple views of engineering design. In many situations, a change in the model of one view may conflict with the models of other views. In such situations, the model of some views needs to be adapted in order to make all models consistent. Thus, CAD models should be capable of adapting themselves to new situations. Recently, agent based technologies have been considered in order to increase both knowledge level and intelligence of real and virtual objects. The contribution of this paper consists in introducing the intelligent agents in intelligent CAD modelling. The proposed agents are elementary geometrical and topological objects. They incorporate the functions of observation, decision and action, and possess their own knowledge. Agents have the capacity of communication and inference based on the feature grammars. They are modelled as bio-dynamic objects that enjoy the properties of fusion, division and multiplication. Being aware of the context, the proposed agents interact to form potential regional transitory communities, called regions. Being aware of their belonging in a region, agents interact by generating virtual links (virtual extensions). These virtual links produce: (a) fusion of agents, (b) division of agents and c) multiplication of agents. The emerged agents interact with the other agents in a region to recognize each other and to form specific sub-communities, called intelligent features. From a CAD software development point of view, this paper advocates the idea of a new phase of CAD system development based on the agent-oriented programming (AOP) paradigm.",
                "year": 2018,
                "publisher": "Journal of Computational Design and Engineering"
            }
        }
    },
    "Making use: Attitudes to human-artifact engagements": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X15000563/pdfft?md5=72a6d1e380b6c10fd48fcb171d114d8b&pid=1-s2.0-S0142694X15000563-main.pdf",
                "title": "Making use: Attitudes to human-artifact engagements",
                "abstract": "‘Function’ and ‘use’ are keywords that design researchers customarily employ when referring to human-artifact engagements. However, there is little consensus about how the concepts of function and use relate to each other, to the intentions of ‘designers’ and ‘users’, or to their actions and encompassing contexts. In this paper, I synthesize literature from design research, material culture studies, design anthropology, and function theory in order to critically compare different attitudes to human-artifact engagements, implicit in characterizations of function and use. I identify design-centric, communicative, and use-centric attitudes, and discuss their assumptions and implications for design theory. I conclude by outlining principles for theoretically and computationally approaching use as an embodied and temporally contingent process – as a form of ‘making’.",
                "year": 2015,
                "publisher": "Design Studies"
            }
        }
    },
    "An alternative view of positioning observations from low cost sensors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S019897151100069X/pdfft?md5=92660c1d684110d60fdc0af3e3221fc5&pid=1-s2.0-S019897151100069X-main.pdf",
                "title": "An alternative view of positioning observations from low cost sensors",
                "abstract": "The measurement uncertainty of low-cost, low-quality positioning sensors in consumer electronics, such as smart phones, is well-known and prevents reliable location awareness in mobile applications, for example. In this paper we argue that location awareness arises from qualitative spatial descriptions which are only partially reliant on the absolute accuracy of the positioning system used. Qualitative descriptions would relate the actual position qualitatively to a location. To facilitate qualitative spatial descriptions we suggest an integration of different positioning sensors by searching for event patterns within the sensor readings. It is hypothesized here that integrating qualitative information derived from traditional measurement sensors into the position computation process will improve the overall reliability of the location awareness information generated.",
                "year": 2012,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "Real-time oriented behavior-driven 3D freehand tracking for direct interaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320312003354/pdfft?md5=87a9032b349c8fffa434c4917ebe92e6&pid=1-s2.0-S0031320312003354-main.pdf",
                "title": "Real-time oriented behavior-driven 3D freehand tracking for direct interaction",
                "abstract": "Articulated hand tracking systems have been commonly used in virtual reality applications, including selection–move–release systems that use freehand as input device. However, building an effective real-time hand tracker remains a challenge. Motivated by the need for three-dimensional (3D) freehand-based human–computer interface in the selection–move–release systems, the current study aims to develop 3D freehand tracking, in which the high dimensionality of articulated hand models is one of the dominating obstacles for tracking 3D hand in real-time. This study focuses on forming the behavioral model for users in selection–move–release systems to build a natural, direct, and effective human–computer interface. First, the statistical models for users were learned. Second, the behavioral models were derived from the statistical models to form a general behavioral model database. Third, a freehand tracking algorithm for interaction between human and computer was presented based on the behavioral models. Fourth, the proposed approach was tested in several real selection–move–release systems, and the experimental results were provided. Unlike previous studies on this subject, the proposed behavioral model was composed of Correlation among Local Motion Models and cognitive information. Experimental results show that the proposed algorithm can achieve satisfactory results in relation to time cost and accuracy compared with some previous studies.",
                "year": 2013,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Three-dimensional object recognition from single two-dimensional images": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370287900701/pdfft?md5=514a487ebde440c4d58009ff7e10c95b&pid=1-s2.0-0004370287900701-main.pdf",
                "title": "Three-dimensional object recognition from single two-dimensional images",
                "abstract": "A computer vision system has been implemented that can recognize three-dimensional objects from unknown viewpoints in single gray-scale images. Unlike most other approaches, the recognition is accomplished without any attempt to reconstruct depth information bottom-up from the visual input. Instead, three other mechanisms are used that can bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization is used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Second, a probabilistic ranking method is used to reduce the size of the search space during model-based matching. Finally, a process of spatial correspondence brings the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. A high level of robustness in the presence of occlusion and missing data can be achieved through full application of a viewpoint consistency constraint. It is argued that similar mechanisms and constraints form the basis for recognition in human vision.",
                "year": 1987,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Meme-affordance tourism: The power of imitation and self-presentation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Meme-affordance tourism: The power of imitation and self-presentation",
                "abstract": "To respond to information systems (IS) researchers' on-going call for understanding what happens in new, social media-enabled processes in diverse contexts, this research investigated how Internet memes facilitate the emergence of new online travel activities and influence visit intentions in new forms of meme tourism. Meme tourism involves new forms of visit intention, where individuals are motivated to visit a destination by viral memes, and once on location, they recreate and re-enact elements of the meme before sharing the results via social media. Such use of memes in the tourism context – often considered as a casual reflection of individuality, pop culture, and a new way of communicating in social media – raises the question of how to influence and nurture travelers' behaviors and actions during an actual trip. To empirically test the ability of memes to induce specific actions during visits, this study developed an integrated model using affordance as an overarching theoretical framework to provide insights into the actualization of two types of meme affordances – Imitation and Self-presentation – during tourism visits. With survey data collected from individual tourists using a crowdsourcing platform, Amazon Mechanical Turk (MTurk), this research found that the relationship between both affordances (imitation and self-presentation) and travel intention was significant. The results of the study offer alternative explanations of the possible psychological influence of memes in inducing physical behaviors, which adds insight to behavioral research fields such as IS. Both the research and the practical implications of the results are discussed.",
                "year": 2024,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Time to collision from first-order spherical image motion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889099000779/pdfft?md5=3073b6d3b86bbe2548669dbcf1808a53&pid=1-s2.0-S0921889099000779-main.pdf",
                "title": "Time to collision from first-order spherical image motion",
                "abstract": "In the absence of contraints on either object motion or surface slant, a narrow field of view constraint has to be assumed to compute time to collision as a scaled depth from first-order image motion measurements. In this work, time to collision and scaled depth are regarded as different visual entities, and it is shown that a bound for time to collision can always be computed regardless of the field of view, thus extending the range of applicability of time to collision based techniques in areas such as mobile robotics and visual surveillance. The method relies on computing in closed form the spherical motion field and the associated parallax from image plane measurements obtained with either conventional cameras or space-variant sensors. An experimental validation of the main theoretical results highlights the difference between time to collision and scaled depth, and addresses a comparison of time to collision approaches using both dense and sparse motion estimates.",
                "year": 2000,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Active and exploratory perception": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/104996609290083F/pdfft?md5=a495d261331f85676c2cf91ea60fed32&pid=1-s2.0-104996609290083F-main.pdf",
                "title": "Active and exploratory perception",
                "abstract": "The main goal of this paper is to show that there is a natural flow from active perception through exploration to perceptual learning. W have attempted to conceptualize the perceptual process of an organism that has the top-level task of surviving in an unknown environment. During this conceptualization process. Four necessary ingredients have emerged for either artificial or biological organisms. First, the sensory apparatus and processing of the organism must be active and flexible. Second, the organism must have exploratory capabilities. Third, the organism must be selective in its data acquisition process. Fourth, the organism must be able to learn. In the section on learning, we have clearly delineated the difference between what must be innate and what must be learned. In order to test our theory, we present the system's architecture that follows from the perceptual task decomposition. The predictions of this theory are that an artificial system can explore and learn about its environment modulo its sensors, manipulators, end effectors, and exploratory procedures/attribute extractors. It can describe its world with respect to the built-in alphabet, that is, the set of perceptual primitives.",
                "year": 1992,
                "publisher": "CVGIP: Image Understanding"
            }
        }
    },
    "Futures of artificial intelligence through technology readiness levels": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585320301842/pdfft?md5=fb9d14e6d0604e686bfd9b72aa247ce3&pid=1-s2.0-S0736585320301842-main.pdf",
                "title": "Futures of artificial intelligence through technology readiness levels",
                "abstract": "Artificial Intelligence (AI) offers the potential to transform our lives in radical ways. However, the main unanswered questions about this foreseen transformation are its depth, breadth and timelines. To answer them, not only do we lack the tools to determine what achievements will be attained in the near future, but we even ignore what various technologies in present-day AI are capable of. Many so-called breakthroughs in AI are associated with highly-cited research papers or good performance in some particular benchmarks. However, research breakthroughs do not directly translate into a technology that is ready to use in real-world environments. In this paper, we present a novel exemplar-based methodology to categorise and assess several AI technologies, by mapping them onto Technology Readiness Levels (TRL) (representing their depth in maturity and availability). We first interpret the nine TRLs in the context of AI, and identify several categories in AI to which they can be assigned. We then introduce a generality dimension, which represents increasing layers of breadth of the technology. These two dimensions lead to the new readiness-vs-generality charts, which show that higher TRLs are achievable for low-generality technologies, focusing on narrow or specific abilities, while high TRLs are still out of reach for more general capabilities. We include numerous examples of AI technologies in a variety of fields, and show their readiness-vs-generality charts, serving as exemplars. Finally, we show how the timelines of several AI technology exemplars at different generality layers can help forecast some short-term and mid-term trends for AI.",
                "year": 2021,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Upper-Limb Motion Recognition Based on Hybrid Feature Selection: Algorithm Development and Validation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Upper-Limb Motion Recognition Based on Hybrid Feature Selection: Algorithm Development and Validation",
                "abstract": "Background\n\nFor rehabilitation training systems, it is essential to automatically record and recognize exercises, especially when more than one type of exercise is performed without a predefined sequence. Most motion recognition methods are based on feature engineering and machine learning algorithms. Time-domain and frequency-domain features are extracted from original time series data collected by sensor nodes. For high-dimensional data, feature selection plays an important role in improving the performance of motion recognition. Existing feature selection methods can be categorized into filter and wrapper methods. Wrapper methods usually achieve better performance than filter methods; however, in most cases, they are computationally intensive, and the feature subset obtained is usually optimized only for the specific learning algorithm.",
                "year": 2021,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "The implications for understanding high-level cognition of a grounding in elementary adaptive systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0921889095000437/pdfft?md5=97abde4adc6c1aa55c3f20b36c283c30&pid=1-s2.0-0921889095000437-main.pdf",
                "title": "The implications for understanding high-level cognition of a grounding in elementary adaptive systems",
                "abstract": "There is a certain tendency to consider that, whereas approaches based on artificial life may be appropriate for lower-level cognition, the computational theory of mind based primarily on the manipulation of symbolic representations is the only possible approach to high-level cognition. This article contests that view, and argues that a constructivist approach to cognition rooted in elementary living organisms leads to a radically new understanding of phenomena such as communication, representation, intentional action and language. Whatever the level of cognition, the computational paradigm is necessarily objectivist, whereas the constructivist paradigm is non-objectivist. It is suggested that within the field of Artificial Life, objectivism is appropriate from an engineering perspective, but that constructivism is appropriate from a biological perspective aimed at modelling living organisms as autonomous systems.",
                "year": 1995,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "AlabOS: a Python-based reconfigurable workflow management framework for autonomous laboratories": {
        "accordingTo": {
            "scienceDirect": {
                "title": "AlabOS: a Python-based reconfigurable workflow management framework for autonomous laboratories",
                "abstract": "The recent advent of autonomous laboratories, coupled with algorithms for high-throughput screening and active learning, promises to accelerate materials discovery and innovation. As these autonomous systems grow in complexity, the demand for robust and efficient workflow management software becomes increasingly critical. In this paper, we introduce AlabOS, a general-purpose software framework for orchestrating experiments and managing resources, with an emphasis on automated laboratories for materials synthesis and characterization. AlabOS features a reconfigurable experiment workflow model and a resource reservation mechanism, enabling the simultaneous execution of varied workflows composed of modular tasks while eliminating conflicts between tasks. To showcase its capability, we demonstrate the implementation of AlabOS in a prototype autonomous materials laboratory, the A-Lab, with around 3500 samples synthesized over 1.5 years.",
                "year": 2024,
                "publisher": "Digital Discovery"
            }
        }
    },
    "When to use vibrotactile displays? A meta-analysis for the role of vibrotactile displays in human–computer interaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687022001259/pdfft?md5=b8c6109509e45bb523e6dba16b75abd1&pid=1-s2.0-S0003687022001259-main.pdf",
                "title": "When to use vibrotactile displays? A meta-analysis for the role of vibrotactile displays in human–computer interaction",
                "abstract": "Objective\n\nThis study aims to investigate the benefits of unimodal tactile displays relative to other modal displays and the performance gains of adding redundant tactile displays by integrating empirical studies.",
                "year": 2022,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Consistent estimation of rotational optical flow in real environments using a biologically-inspired vision algorithm on embedded hardware": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Consistent estimation of rotational optical flow in real environments using a biologically-inspired vision algorithm on embedded hardware",
                "abstract": "Insects are able to freely navigate through ever-changing environments using predominantly visual inputs, while possessing very minimal processing power compared to humans. Not only are they able to move at high velocities and accelerations, but they are also able to achieve extraordinary levels of obstacle avoidance. We begin to emulate this biological behaviour in a robotic application by first modelling how these visual pathways react to separable degrees of freedom within the motion field, specifically rotation in this case. We have developed upon an existing biologically-inspired algorithm based on the visual pathway of the hoverfly, and statistically compare results to current state-of-the-art algorithms, all the while performing this on computationally-constrained embedded hardware. We have shown that, using a complex, highly-elaborated representation of the hoverfly visual pathway, rotation optical flow estimations can be achieved with a high level of accuracy, at a level of consistency previously unseen in dense-flow algorithms, and they can be achieved at 100 frames per second on an embedded system. This work forms a fundamental basis to understanding one of the two separable components of insect egomotion (rotation and translation), allowing for consistently accurate rotational velocity estimation, providing a building block towards understanding the translational component of insect vision and the application of biologically-inspired egomotion estimation in autonomous vehicles.",
                "year": 2019,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "On optimal constrained trajectory planning in 3D environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889000000956/pdfft?md5=1733e861ee4c1564cdfe62ee474654ae&pid=1-s2.0-S0921889000000956-main.pdf",
                "title": "On optimal constrained trajectory planning in 3D environments",
                "abstract": "A novel approach to generating acceleration-based optimal smooth piecewise trajectories is proposed. Given two configurations (position and orientation) in 3D, we search for the minimal energy trajectory that minimizes the integral of the squared acceleration, opposed to curvature, which is widely investigated. The variation in both components of acceleration: tangential (forces on gas pedal or brakes) and normal (forces that tend to drive a car on the road while making a turn) controls the smoothness of generated trajectories. In the optimization process, our objective is to search for the trajectory along which a free moving robot is able to accelerate (decelerate) to a safe speed in an optimal way. A numerical iterative procedure is devised for computing the optimal piecewise trajectory as a solution of a constrained boundary value problem. The resulting trajectories are not only smooth but also safe with optimal velocity (acceleration) profiles and therefore suitable for robot motion planning applications. Experimental results demonstrate this fact.",
                "year": 2000,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Affordance as context": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543805000408/pdfft?md5=da35c64f1db7f7ad4d0c75eefb5cd295&pid=1-s2.0-S0953543805000408-main.pdf",
                "title": "Affordance as context",
                "abstract": "The concept of affordance is relatively easy to define, but has proved to be remarkably difficult to engineer. This paradox has sparked numerous debates as to its true nature. The discussion presented here begins with a review of the use of the term from which emerges evidence for a two-fold classification—simple affordance and complex affordance. Simple affordance corresponds to Gibson's original formulation, while complex affordances embody such things as history and practice. In trying to account for complex affordance, two contrasting, but complementary philosophical treatments are considered. The first of these is Ilyenkov's account of significances which he claims are ‘ideal’ phenomena. Ideal phenomena occupy are objective characteristics of things and are the product of human purposive activity. This makes them objective, but not independent (of any particular mind or perception) hence their similarity to affordances.\n\nThe second perspective is Heidegger's phenomenological treatment of ‘familiarity’ and ‘equipment’. As will be seen, Heidegger has argued that familiarity underpins our ability to cope in the world. A world, in turn, which itself comprises the totality of equipment. We cope by making use of equipment. Despite the different philosophical traditions both Ilyenkov and Heidegger have independently concluded that a thing is identified by its use and that use, in turn, is revealed by way of its affordances/significances. Finally, both authors—Heidegger directly and Ilyenkov indirectly—equate context and use, leading to the conclusion that affordance and context are one and the same.",
                "year": 2005,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Computing a representation of the local environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0004370298001118/pdfft?md5=94a266d4779a212016ea7a2790426529&pid=1-s2.0-S0004370298001118-main.pdf",
                "title": "Computing a representation of the local environment",
                "abstract": "Yeap (1988) argued that an important basis for computing a cognitive map is the ability to compute and recognise local environments. Although he has demonstrated how such local environments could be used to construct a raw cognitive map, he failed to produce an adequate algorithm for computing them. In this paper, a detailed study of this problem is presented. We argue that although each local environment computed forms a natural basis for constructing a raw cognitive map, it is not computed primarily to do so. Instead, it is computed for one's immediate needs (such as hunting a prey or escaping from danger). This change in perspective argues for a very different cognitive mapping process, namely one that computes local environments as the individual moves through the environment but these representations are not necessarily used to construct a raw map. The individual does not do so until there is evidence that it is going to stay. Consequently this simplifies the algorithm for computing a local environment and a new algorithm is thus proposed. Some results of our implementation are shown.",
                "year": 1999,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Emoji and visual complexity in health information design: A moderated serial mediation model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585323001296/pdfft?md5=724e43e574255cb4e4d6b89c20662658&pid=1-s2.0-S0736585323001296-main.pdf",
                "title": "Emoji and visual complexity in health information design: A moderated serial mediation model",
                "abstract": "Social media is a valuable tool that enables public health organizations to communicate effectively. To enhance the reach of health communication on social media, scholars have proposed that emoji be used to convey scientific information. The current study explored the influence of emoji on the effectiveness of health communication on social media. Automated content analysis revealed that the presence of emoji in online health information resulted in higher levels of social media engagement (SME) than the absence of emoji did. Additionally, a 2 (emoji: present versus absent) × 3 (visual complexity of information design: low versus medium versus high) online experiments revealed that the presence of emoji in health information sequentially increased perceived enjoyment and perceived interactivity, thereby promoting SME. However, this effect is influenced by the visual complexity of health information designs. The presence of emoji is only effective in increasing SME with health information presented using a design with low or medium visual complexity. This study provides theoretical and practical insights into visual health communication and health information design.",
                "year": 2023,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Kinematic modeling of Exechon parallel kinematic machine": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584510000852/pdfft?md5=2782725d627ec57702b23184956bc1b0&pid=1-s2.0-S0736584510000852-main.pdf",
                "title": "Kinematic modeling of Exechon parallel kinematic machine",
                "abstract": "The studies on PKMs have attracted a great attention to robotics community. By deploying a parallel kinematic structure, a parallel kinematic machine (PKM) is expected to possess the advantages of heavier working load, higher speed, and higher precision. Hundreds of new PKMs have been proposed. However, due to the considerable gaps between the desired and actual performances, the majorities of the developed PKMs were the prototypes in research laboratories and only a few of them have been practically applied for various applications; among the successful PKMs, the Exechon machine tool is recently developed. The Exechon adopts unique over-constrained structure, and it has been improved based on the success of the Tricept parallel kinematic machine. Note that the quantifiable theoretical studies have yet been conducted to validate its superior performances, and its kinematic model is not publically available. In this paper, the kinematic characteristics of this new machine tool is investigated, the concise models of forward and inverse kinematics have been developed. These models can be used to evaluate the performances of an existing Exechon machine tool and to optimize new structures of an Exechon machine to accomplish some specific tasks.",
                "year": 2011,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Preventive mental health care: A complex systems framework for ambient smart environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S138904172300133X/pdfft?md5=0f8a535d260cff4e2692d459919764c5&pid=1-s2.0-S138904172300133X-main.pdf",
                "title": "Preventive mental health care: A complex systems framework for ambient smart environments",
                "abstract": "We offer a framework for the design and use of Ambient Smart Environments (ASEs) for preventive mental health care support. Drawing from Complex Systems Theory (CST) and ‘E’ Cognitive Science (ECS), we claim that ASEs have the potential to act in a preventive capacity in support of good mental health, i.e. supporting dynamics that avoid so-called “struck states” (which are, according to CST, thought generally to underpin forms of psychopathology). Here, we frame our discussion with what has recently been termed the “mind-technology problem”. We define and characterise ASE systems, present some examples, and briefly survey some existing theoretical work. After introducing the essential CST terminology, the paper goes on to apply CST to explain developmental adaptation to continuously changing (smart) environments. Understanding the ASE’s navigation in terms of a dynamic geometry between attracting and repelling points (or local minima/local maxima), allows us to develop neurotechnology that can augment clinical interventions by predicting upcoming shifts for good symptomatic outcomes, i.e. when a preventive intervention (i.e. destabilisation) should take place. We further offer clear directions for the development and design of such neurotechnology.",
                "year": 2024,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "A survey on RGB-D datasets": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314222000923/pdfft?md5=5cf7efe6727a1541ffad751c964167e2&pid=1-s2.0-S1077314222000923-main.pdf",
                "title": "A survey on RGB-D datasets",
                "abstract": "RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field.",
                "year": 2022,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Theoretical and hypothetical pathways to real-time neuromorphic AGI/post-AGI ecosystems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050920302453/pdfft?md5=201f875457b42a7d6cbe684eb2186111&pid=1-s2.0-S1877050920302453-main.pdf",
                "title": "Theoretical and hypothetical pathways to real-time neuromorphic AGI/post-AGI ecosystems",
                "abstract": "While Homo sapiens is without doubt our planet’s most advanced species capable of imagining, creating and implementing tools, one of the many observable trends in evolution is the accelerating merger of biology and technology at increasing levels of scale. This is not surprising, given that our technology can be seen from a perspective in which the sensorimotor and, subsequently, prefrontal areas of our brain increasingly extending its motor (as did our evolutionary predecessors), perceptual, and—with computational advances, cognitive and memory capacities—into the exogenous environment. As such, this trajectory has taken us to a point in the above-mentioned merger at which the brain itself is beginning to meld with its physically expressed hardware and software counterparts—functionally at first, but increasingly structurally as well, initially by way of neural prostheses and brain-machine interfaces. Envisioning the extension of this trend, I propose theoretical technological pathways to a point at which humans and non-biological human counterparts may have the option to have identical neural substrates that—when integrated with Artificial General Intelligence (AGI), counterfactual quantum communications and computation, and AGI ecosystems—provide a global advance in shared knowledge and cognitive function while ameliorating current concerns associated with advanced AGI, as well as suggesting (and, if realized, accelerating) the far-future emergence of Transentity Universal Intelligence (TUI).",
                "year": 2020,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Single-view-based high-fidelity three-dimensional reconstruction of leaves": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169924010731/pdfft?md5=6be0a2a18331b83356ae81dfa03ab261&pid=1-s2.0-S0168169924010731-main.pdf",
                "title": "Single-view-based high-fidelity three-dimensional reconstruction of leaves",
                "abstract": "In modern agricultural science research, high-fidelity three-dimensional (3D) leaf models are crucial for crop growth analysis. However, reconstructing the complex morphology and texture of leaves from a single viewpoint under varying natural lighting conditions poses a significant challenge. To address the issues associated with this challenge, this paper presents a diffusion model-based method for single-view leaf reconstruction using potato leaves as the experimental subject. In the camera prediction process, the combination of an explicit point cloud generation technique and an implicit 3D Gaussian rendering technique enables the accurate prediction of camera parameters and the effective capture of leaf phenotypic features. In the synthesis of the 3D model of the leaf, a strategy for optimizing the coarse model UV texture is designed with the objective of achieving spatial consistency of texture details. Furthermore, the model was successfully applied to the reconstruction of other crop leaves and lamellar structural objects, and innovatively constructed a leaf reconstruction model with disease characteristics, aiming to provide a reference for the early 3D detection of crop diseases, as well as a reference for the 3D reconstruction and visualization of other lamellar objects. The results demonstrate that the method is effective in reconstructing the morphological structure and texture details of leaves, as well as thin sheet-like structured objects, achieving fast and high-fidelity single-view reconstruction.",
                "year": 2024,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Bounds on time-to-collision and rotational component from first-order derivatives of image flow": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X9090151K/pdfft?md5=419681e8eaa94a2e358927a6a24432cd&pid=1-s2.0-0734189X9090151K-main.pdf",
                "title": "Bounds on time-to-collision and rotational component from first-order derivatives of image flow",
                "abstract": "A moving rigid object produces a moving image on the retina of an observer. It is shown that only the first-order spatial derivatives of image motion are sufficient to determine (i) the maximum and minimum time-to-collision of the observer and the object and (ii) the maximum and minimum angular velocity of the object along the direction of view. The second or higher order derivatives whose estimation is expensive and unreliable are not necessary. (The second-order derivatives are necessary to determine the actual motion of the object.) These results are interpreted in the image domain in terms of three differential invariants of the image flow field: divergence, curl, and shear magnitude. In the world domain, the above results are interpreted in terms of the motion and local surface orientation of the object. In particular, the result that the minimum time-to-collision could be determined from only the first order derivatives has a fundamental significance to both biological and machine vision systems. It implies that an organism (or a robot) can quickly respond to avoid collision with a moving object from only coarse information. This capability exists irrespective of the shape or motion of the object. The only restriction is that motion should be rigid.",
                "year": 1990,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Segmentation of images by color features: A survey": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231218302364/pdfft?md5=e6e2ce3afbb5ae5a9628fae6a87bbbf4&pid=1-s2.0-S0925231218302364-main.pdf",
                "title": "Segmentation of images by color features: A survey",
                "abstract": "Image segmentation is an important stage for object recognition. Many methods have been proposed in the last few years for grayscale and color images. In this paper, we present a deep review of the state of the art on color image segmentation methods; through this paper, we explain the techniques based on edge detection, thresholding, histogram-thresholding, region, feature clustering and neural networks. Because color spaces play a key role in the methods reviewed, we also explain in detail the most commonly color spaces to represent and process colors. In addition, we present some important applications that use the methods of image segmentation reviewed. Finally, a set of metrics frequently used to evaluate quantitatively the segmented images is shown.",
                "year": 2018,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Sensor Planning for 3D Object Search": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314298907366/pdfft?md5=30df5a38fb284287c9f8e538fd8b08ea&pid=1-s2.0-S1077314298907366-main.pdf",
                "title": "Sensor Planning for 3D Object Search",
                "abstract": "In this paper, we provide a systematic study of the task of sensor planning for object search. The search agent's knowledge of object location is encoded as a discrete probability density which is updated whenever a sensing action occurs. Each sensing action of the agent is defined by a viewpoint, a viewing direction, a field-of-view, and the application of a recognition algorithm. The formulation casts sensor planning as an optimization problem: the goal is to maximize the probability of detecting the target with minimum cost. This problem is proved to be NP-Complete, thus a heuristic strategy is favored. To port the theoretical framework to a real working system, we propose a sensor planning strategy for a robot equipped with a camera that can pan, tilt, and zoom. In order to efficiently determine the sensing actions over time, the huge space of possible actions with fixed camera position is decomposed into a finite set of actions that must be considered. The next action is then selected from among these by comparing the likelihood of detection and the cost of each action. When detection is unlikely at the current position, the robot is moved to another position for which the probability of target detection is the highest.",
                "year": 1999,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Robust face recognition using the GAP feature": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320313001441/pdfft?md5=395f5dfcdf0bfd27bd0ff348108ecbb5&pid=1-s2.0-S0031320313001441-main.pdf",
                "title": "Robust face recognition using the GAP feature",
                "abstract": "In this paper, we propose a novel approach based on Grayscale Arranging Pairs (GAP) for face recognition. A facial model is built by using the stable point pairs of the GAP feature. Then the similarity between the facial model and the input facial image is calculated by checking whether the intensity relationship of these point pairs is the same. Different from current face recognition algorithms, GAP is a robust holistic feature without losing its local property. By using a stable intensity relationship of multiple point pairs, the GAP feature shows a great invariance property of facial features, and exhibits high robustness to resist illumination variations. Meanwhile, it describes the holistic information in the entire facial image, which is more similar to the human recognition mechanism. In addition, a novel weighting model that exploits the local characteristics of faces is applied in the framework, leading to a higher accuracy in face recognition. We compare the proposed method with four other famous methods on the Extended Yale B face and PIE face databases. The experimental results showed that the proposed method provides outstanding results in recognizing faces.",
                "year": 2013,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Direct manipulation of graph-based decision models": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167923693900499/pdfft?md5=80620ae7b88e04fca62fe86b8ca07a80&pid=1-s2.0-0167923693900499-main.pdf",
                "title": "Direct manipulation of graph-based decision models",
                "abstract": "This paper builds on a long tradition in operations research and systems analysis: building models by drawing pictures. While pictorial representations are certainly useful as a communication medium, we focus on the learnability of modeling applications through graphical interfaces. Using the combined perspectives of direct perception (especially affordances) and direct manipulation, we focus on the dynamics of model building and model manipulation. The GX Shell is a DSS interface generator for direct manipulation of graph-based decision models, which are processed either by routines coded in Prolog or by external solvers.",
                "year": 1993,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "A new reality: Fan perceptions of augmented reality readiness in sport marketing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563219304509/pdfft?md5=f96c2de52b962dafd0004052343cb76a&pid=1-s2.0-S0747563219304509-main.pdf",
                "title": "A new reality: Fan perceptions of augmented reality readiness in sport marketing",
                "abstract": "Sport marketers have begun to adopt augmented reality (AR) in an effort to enhance the fan experience. Hence, there is a need to investigate fan perspectives of this new marketing tool. The current study investigated fans' perceptions AR activations within a sport setting. Findings indicated the visual appeal of AR is very influential on fans' intention to use an activation or provide positive word of mouth. While previous research in sport indicated the perceived ease of use of a technology to significantly influence consumers’ attitudes toward a technology (Kim and Chiu, 2019), the current study utilized smartphones as the vehicle to provide the AR and ease of use was not found to be significant. Hence, marketers should note that smartphone based AR activations will likely be deemed relatively easy to use as most people are intimately familiar with their phone and there should be a significant focus on ensuring the AR is visually appealing.",
                "year": 2020,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "The information infrastructures of 1985 and 2018: The sociotechnical context of computer law & security": {
        "accordingTo": {
            "scienceDirect": {
                "title": "The information infrastructures of 1985 and 2018: The sociotechnical context of computer law & security",
                "abstract": "This article identifies key features of the sociotechnical contexts of computer law and security at the times of this journal's establishment in 1985, and of its 200th Issue in 2018. The infrastructural elements of devices, communications, data and actuator technologies are considered first. Social actors as individuals, and in groups, communities, societies and polities, together with organisations and economies, are then interleaved with those technical elements. This provides a basis for appreciation of the very different challenges that confront us now in comparison with the early years of post-industrialism.",
                "year": 2018,
                "publisher": "Computer Law & Security Review"
            }
        }
    },
    "Manipulating the design space to resolve trade-offs: Theory and evidence": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Manipulating the design space to resolve trade-offs: Theory and evidence",
                "abstract": "Trade-offs between design goals have traditionally been studied using optimization approaches, which assume a fixed formulation and framing of the design problem. We propose a novel set-theory framework of design spaces to model the role of problem reformulation and reframing in resolving design trade-offs. The framework predicts mechanisms by which the designer can alter the boundaries and structure of that space to alter or avoid Pareto frontiers in the original space. Empirical evidence from interviews with designers identifies eight distinct trade-off response mechanisms aligned with the framework's predictions. The framework and interview results provide a foundation for developing methodologies that encourage design space restructuring to avoid unnecessary design compromises and sacrifices.",
                "year": 2022,
                "publisher": "Design Studies"
            }
        }
    },
    "Information and representation in autonomous agents": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041799000078/pdfft?md5=3599b1c18c3cda46607878614c3fc23b&pid=1-s2.0-S1389041799000078-main.pdf",
                "title": "Information and representation in autonomous agents",
                "abstract": "Information and representation are thought to be intimately related. Representation, in fact, is commonly considered to be a special kind of information. It must be a special kind, because otherwise all of the myriad instances of informational relationships in the universe would be representational — some restrictions must be placed on informational relationships in order to refine the vast set into those that are truly representational. I will argue that information in this general sense is important to genuine agents, but that it is a blind alley with regard to the attempt to understand representation. On the other hand, I will also argue that a different, quite non-standard, form of information is central to genuine representation. First, I turn to some of the reasons why information as usually considered is the wrong category for understanding representation; second, to an alternative model of representation — one that is naturally emergent in autonomous agents, and that does involve information, but not in standard form; and third, I return to standard notions of informational relationships and show what they are in fact useful for.",
                "year": 2000,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Presence and general principles of brain function": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543812000380/pdfft?md5=b9a4963087f7b3e246405556724e717e&pid=1-s2.0-S0953543812000380-main.pdf",
                "title": "Presence and general principles of brain function",
                "abstract": "Recent developments in general theories of cognition and brain function make it possible to consider the concept of presence from a new perspective, based in general principles of brain function. The importance of interaction with reality for the development and function of the brain and human cognition is increasingly emphasized. The brain is explained as implementing a generative model of the current environment. Whether this environment is real or virtual does not matter. Mental simulations are created for whatever one interacts with, when possible. This view provides a basis for relating human experiences in virtual environments to several theories that explain cognition and brain function on many levels, from ultimate evolutionary motivations to plausible neural implementations. The purpose of this paper is not to provide yet another definition of presence but to suggest explanations of phenomena commonly related to presence, with a basis in general principles of brain function. Such principles are employed to explain how, and why, interaction with our environment, and internalization of objects and tools therein, play an essential role in human cognition. This provides a rich basis for further analysis of how central aspects of presence, such as breaks in presence or the perceptual illusion of non-mediation, may work on a fundamental level. More general descriptions of such phenomena have advantages such as being easier to relate to new contexts and technologies, and opening up for additional inspiration and confirmation from other disciplines such as cognitive neuroscience. In addition to an account of general principles for brain function and a discussion about the concept of presence in light of these, this paper also relates this discussion to a number of previous accounts of presence, and to practical implications and applications for interaction design.",
                "year": 2012,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "The intelligent use of space": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/000437029400017U/pdfft?md5=7eb870dd03c0cddc93f84b13e5c2b651&pid=1-s2.0-000437029400017U-main.pdf",
                "title": "The intelligent use of space",
                "abstract": "The objective of this essay is to provide the beginning of a principled classification of some of the ways space is intelligently used. Studies of planning have typically focused on the temporal ordering of action, leaving as unaddressed, questions of where to lay down instruments, ingredients, work-in-progress, and the like. But, in having a body, we are spatially located creatures: we must always be facing some direction, have only certain objects in view, be within reach of certain others. How we manage the spatial arrangement of items around us, is not an afterthought; it is an integral part of the way we think, plan and behave. The proposed classification has three main categories: spatial arrangements that simplify choice; spatial arrangements that simplify perception; and spatial dynamics that simplify internal computation. The data for such a classification is drawn from videos of cooking, assembly and packing, everyday observations in supermarkets, workshops and playrooms, and experimental studies of subjects playing Tetris, the computer game. This study, therefore, focusses on interactive processes in the medium and short term: on how agents set up their workplace for particular tasks, and how they continuously manage that workplace.",
                "year": 1995,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Machine learning applied to acoustic-based road traffic monitoring": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050922010468/pdfft?md5=3ae32396ae97d796cedc3a0b0fff6ed7&pid=1-s2.0-S1877050922010468-main.pdf",
                "title": "Machine learning applied to acoustic-based road traffic monitoring",
                "abstract": "The motivation behind this study lies in adapting acoustic noise monitoring systems for road traffic monitoring for driver's safety. Such a system should recognize a vehicle type and weather-related pavement conditions based on the audio level measurement. The study presents the effectiveness of the selected machine learning algorithms in acoustic-based road traffic monitoring. Bases of the operation of the acoustic road traffic detector are briefly described. Principles of several machine learning algorithms, data acquisition process, and information about the dataset built are explained. The study is conducted using the audio recordings prepared by the authors, registered in several locations and under different meteorological conditions of the road surface. For each recording containing a single-vehicle passage, a vector of 67 parameters extracted from the audio signal is calculated. Fisher Linear Discriminant Analysis and Regression Analysis, the fastest among algorithms employed, return the following values of accuracy: 0.968 and 0.978, precision: 0.919 and 0.853, recall: 0.882 and 0.974, and F1-score: 0.898 and 0.868 for vehicle type classification. In the case of the road pavement conditions, the obtained metrics are as follows: accuracy of 0.933, precision of 0.898, recall of 0.9, and F1-score of 0.884.",
                "year": 2022,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Robust implementation of foreground extraction and vessel segmentation for X-ray coronary angiography image sequence": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006246/pdfft?md5=503707c79058ff1862eee0f5f0b2070b&pid=1-s2.0-S0031320323006246-main.pdf",
                "title": "Robust implementation of foreground extraction and vessel segmentation for X-ray coronary angiography image sequence",
                "abstract": "The extraction of contrast-filled vessels from X-ray coronary angiography (XCA) image sequence has important clinical significance for intuitively diagnosis and therapy. In this study, the XCA image sequence is regarded as a 3D tensor input, the vessel layer is regarded as a sparse tensor, and the background layer is regarded as a low-rank tensor. Using tensor nuclear norm (TNN) minimization, a novel method for vessel layer extraction based on tensor robust principal component analysis (TRPCA) is proposed. Furthermore, considering the irregular movement of vessels and the low-frequency dynamic disturbance of surrounding irrelevant tissues, the total variation (TV) regularized spatial–temporal constraint is introduced to smooth the foreground layer. Subsequently, for vessel layer images with uneven contrast distribution, a two-stage region growing (TSRG) method is utilized for vessel enhancement and segmentation. A global threshold method is used as the preprocessing to obtain main branches, and the Radon-Like features (RLF) filter is used to enhance and connect broken minor segments. The final binary vessel mask is constructed by combining the two intermediate results. The visibility of TV-TRPCA algorithm for foreground extraction is evaluated on clinical XCA image sequences and third-party dataset, which can effectively improve the performance of commonly used vessel segmentation algorithms. Based on TV-TRPCA, the accuracy of TSRG algorithm for vessel segmentation is further evaluated. Both qualitative and quantitative results validate the superiority of the proposed method over existing state-of-the-art approaches.",
                "year": 2024,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "On integration of interface design methods: Can debates be resolved?": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543805001104/pdfft?md5=b273d7fe472a6d853f01146eb1d84445&pid=1-s2.0-S0953543805001104-main.pdf",
                "title": "On integration of interface design methods: Can debates be resolved?",
                "abstract": "There have been many debates on how to design the human–computer interface (HCI). Often, one can find that different views in a debate are simply because these views are attached to different aspects which embody the same thing. In other words, prior to giving an effective judgment of a debate, one needs to establish an understanding of the ‘total’ aspects of a thing the debate is about. Following this line of thinking, in this paper, we propose an understanding of the ‘total’ aspects of designing HCI, which is called the total interface design framework. We then judge several debates under this framework with the purpose of exemplifying the judgment process for any other debate related to designing HCI. At the end, the debates used for exemplifying our judgment process can be resolved. The effectiveness of the total interface design framework for integrating the different HCI approaches is also demonstrated.",
                "year": 2006,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "UrduHope: Analysis of hope and hopelessness in Urdu texts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705124013807/pdfft?md5=fef38a4a7a020d1d73be684abc58dccf&pid=1-s2.0-S0950705124013807-main.pdf",
                "title": "UrduHope: Analysis of hope and hopelessness in Urdu texts",
                "abstract": "Hope is a crucial aspect of human psychology that has received considerable attention due to its role in facing challenges in human life. However, current research predominantly focuses on hope as positive anticipation, overlooking its counterpart, hopelessness. This paper addresses this gap by presenting an expanded framework for analyzing hope speech in social media, incorporating hope and hopelessness. Drawing on insights from psychology and Natural Language Processing (NLP), we argue that a comprehensive understanding of human emotions necessitates considering both constructs. We introduce the concept of hopelessness as a distinct category in hope speech analysis and develop a novel dataset for Urdu, an underrepresented language in NLP research. We proposed a semi-supervised annotation procedure by utilizing Large Language Models (LLMs) along with human annotators to annotate the dataset and explored various learning approaches for hope speech detection, including traditional machine learning models, neural networks, and state-of-the-art transformers. The findings demonstrate the effectiveness of different learning approaches in capturing the nuances of hope speech in Urdu social media discourse. The hope speech detection task was modeled in two subtasks: a binary classification of Urdu tweets to Hope and Not Hope classes and then a multiclass classification of Urdu tweets into Generalized, Realistic, and Unrealistic Hopes, along with Hopelessness, and Not Hope (Neutral) categories. The best results for binary classification were obtained with Logistic Regression (LR) with an averaged macro F1 score of 0.7593, and for the multiclass classification experiments, transformers outperformed other experiments with an averaged macro F1 score of 0.4801.",
                "year": 2025,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Beyond Pathogen Filtration: Possibility of Smart Masks as Wearable Devices for Personal and Group Health and Safety Management": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Beyond Pathogen Filtration: Possibility of Smart Masks as Wearable Devices for Personal and Group Health and Safety Management",
                "abstract": "Face masks are an important way to combat the COVID-19 pandemic. However, the prolonged pandemic has revealed confounding problems with the current face masks, including not only the spread of the disease but also concurrent psychological, social, and economic complications. As face masks have been worn for a long time, people have been interested in expanding the purpose of masks from protection to comfort and health, leading to the release of various “smart” mask products around the world. To envision how the smart masks will be extended, this paper reviewed 25 smart masks (12 from commercial products and 13 from academic prototypes) that emerged after the pandemic. While most smart masks presented in the market focus on resolving problems with user breathing discomfort, which arise from prolonged use, academic prototypes were designed for not only sensing COVID-19 but also general health monitoring aspects. Further, we investigated several specific sensors that can be incorporated into the mask for expanding biophysical features. On a larger scale, we discussed the architecture and possible applications with the help of connected smart masks. Namely, beyond a personal sensing application, a group or community sensing application may share an aggregate version of information with the broader population. In addition, this kind of collaborative sensing will also address the challenges of individual sensing, such as reliability and coverage. Lastly, we identified possible service application fields and further considerations for actual use. Along with daily-life health monitoring, smart masks may function as a general respiratory health tool for sports training, in an emergency room or ambulatory setting, as protection for industry workers and firefighters, and for soldier safety and survivability. For further considerations, we investigated design aspects in terms of sensor reliability and reproducibility, ergonomic design for user acceptance, and privacy-aware data-handling. Overall, we aim to explore new possibilities by examining the latest research, sensor technologies, and application platform perspectives for smart masks as one of the promising wearable devices. By integrating biomarkers of respiration symptoms, a smart mask can be a truly cutting-edge device that expands further knowledge on health monitoring to reach the next level of wearables.",
                "year": 2022,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Fractal dimension and clinical neurophysiology fusion to gain a deeper brain signal understanding: A systematic review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253525000090/pdfft?md5=f4436ea0ec2f4cd99fe8e1d31f4f8aad&pid=1-s2.0-S1566253525000090-main.pdf",
                "title": "Fractal dimension and clinical neurophysiology fusion to gain a deeper brain signal understanding: A systematic review",
                "abstract": "Fractal dimension (FD) analysis, a powerful tool that has significantly advanced our understanding of brain complexity, evolving from basic geometrical characterization to the nuanced analysis of neurophysiological signals. This review integrates the theoretical foundations of FD calculation with its practical applications in clinical neurophysiology, focusing on the Higuchi method. This method, widely recognized for its effectiveness in analyzing clinical time series datasets, is a crucial aspect of our research. Emphasizing the importance of fractal properties in interpreting brain function, we explore how FD analysis reveals the brain’s physiological and pathological states.\n\nThe review systematically examines FD analysis’s role across various neurological conditions, drawing on a meta-analysis of existing literature, including studies on Alzheimer’s disease, Parkinson’s disease, multiple sclerosis, stroke, and schizophrenia. Additionally, we discuss its implications in aging and developmental research, particularly in elderly and young populations. By establishing FD analysis, particularly the Higuchi method, as an indispensable tool for evaluating brain dynamics, we highlight its potential for providing new insights and identifying biomarkers for these conditions. This exploration also underscores the ongoing challenges in synthesizing a unified model of brain function and the need for continued development of computational models that emulate the biological brain.",
                "year": 2025,
                "publisher": "Information Fusion"
            }
        }
    },
    "Engineering collective intelligence at the edge with aggregate processes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197620303389/pdfft?md5=b39b3be6059848ba11afbfb3dc30a925&pid=1-s2.0-S0952197620303389-main.pdf",
                "title": "Engineering collective intelligence at the edge with aggregate processes",
                "abstract": "Edge computing promotes the execution of complex computational processes without the cloud, i.e., on top of the heterogeneous, articulated, and possibly mobile systems composed of IoT and edge devices. Such a pervasive smart fabric augments our environment with computing and networking capabilities. This leads to a complex and dynamic ecosystem of devices that should not only exhibit individual intelligence but also collective intelligence—the ability to take group decisions or process knowledge among autonomous units of a distributed environment. Self-adaptation and self-organisation mechanisms are also typically required to ensure continuous and inherent toleration of changes of various kinds, to distribution of devices, energy available, computational load, as well as faults. To achieve this behaviour in a massively distributed setting like edge computing demands, we seek for identifying proper abstractions, and engineering tools therefore, to smoothly capture collective behaviour, adaptivity, and dynamic injection and execution of concurrent distributed activities. Accordingly, we elaborate on a notion of “aggregate process” as a concurrent collective computation whose execution and interactions are sustained by a dynamic team of devices, whose spatial region can opportunistically vary over time. We ground this notion by extending the aggregate computing model and toolchain with new constructs to instantiate aggregate processes and regulate key aspects of their lifecycle. By virtue of an open-source implementation in the ScaFi framework, we show basic programming examples as well as case studies of edge computing, evaluated by simulation in realistic settings.",
                "year": 2021,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Accurate object contour tracking based on boundary edge selection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320306002949/pdfft?md5=911dc76b9d840bc6a3e2232f8dc0d4c3&pid=1-s2.0-S0031320306002949-main.pdf",
                "title": "Accurate object contour tracking based on boundary edge selection",
                "abstract": "In this paper, a novel method for accurate subject tracking, by selecting only tracked subject boundary edges in a video stream with a changing background and moving camera, is proposed. This boundary edge selection is achieved in two steps: (1) removing background edges using edge motion, and from the output of the previous step, (2) selecting boundary edges using a normal direction derivative of the tracked contour. Accurate tracking is based on reduction of the effects of irrelevant edges, by only selecting boundary edge pixels. In order to remove background edges using edge motion, the tracked subject motion is computed and edge motions and edges having different motion directions from the subjects are removed. In selecting boundary edges using the normal contour direction, the image gradient values on every edge pixel are computed, and edge pixels with large gradient values are selected. Multi-level Canny edge maps are used to obtain proper details of a scene. Multi-level edge maps allow tracking, even though the tracked object boundary has complex edges, since the detail level of an edge map for the scene can be adjusted. A process of final routing is deployed in order to obtain a detailed contour. The computed contour is improved by checking against a strong Canny edge map and hiring strong Canny edge pixels around the computed contour using Dijkstra's minimum cost routing. The experimental results demonstrate that the proposed tracking approach is robust enough to handle a complex-textured scene in a mobile camera environment.",
                "year": 2007,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "AI-powered chatbot communication with customers: Dialogic interactions, satisfaction, engagement, and customer behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222001510/pdfft?md5=901c9ce3f050edee98bc707f55133a01&pid=1-s2.0-S0747563222001510-main.pdf",
                "title": "AI-powered chatbot communication with customers: Dialogic interactions, satisfaction, engagement, and customer behavior",
                "abstract": "The present study is grounded in social exchange theory and resource exchange theory. By exploring customers' satisfaction with chatbot services and their social media engagement, it examined the effects of responsiveness and a conversational tone in dialogic chatbot communication on customers. To test the proposed mediation model, we surveyed a representative sample of customers (N = 965) living in the U.S. After examining the validity and reliability of our measurement model, we tested the hypothesized model using structural equation modeling (SEM) procedures. All proposed hypotheses were supported, indicating the significant direct effects of (1) responsiveness and a conversational tone on customers' satisfaction with chatbot services, (2) customers' chatbot use satisfaction on social media engagement, (3) customers’ social media engagement on price premium and purchase intention, and (4) purchase intention on price premium. In addition, we examined satisfaction, social media engagement, and purchase intention as significant mediators in the proposed model. Theoretical and practical implications of the study were then discussed.",
                "year": 2022,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "How much time do drivers need to obtain situation awareness? A laboratory-based study of automated driving": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687016302630/pdfft?md5=da38637f1b56753a0972ac17e3537042&pid=1-s2.0-S0003687016302630-main.pdf",
                "title": "How much time do drivers need to obtain situation awareness? A laboratory-based study of automated driving",
                "abstract": "Drivers of automated cars may occasionally need to take back manual control after a period of inattentiveness. At present, it is unknown how long it takes to build up situation awareness of a traffic situation. In this study, 34 participants were presented with animated video clips of traffic situations on a three-lane road, from an egocentric viewpoint on a monitor equipped with eye tracker. Each participant viewed 24 videos of different durations (1, 3, 7, 9, 12, or 20 s). After each video, participants reproduced the end of the video by placing cars in a top-down view, and indicated the relative speeds of the placed cars with respect to the ego-vehicle. Results showed that the longer the video length, the lower the absolute error of the number of placed cars, the lower the total distance error between the placed cars and actual cars, and the lower the geometric difference between the placed cars and the actual cars. These effects appeared to be saturated at video lengths of 7–12 s. The total speed error between placed and actual cars also reduced with video length, but showed no saturation up to 20 s. Glance frequencies to the mirrors decreased with observation time, which is consistent with the notion that participants first estimated the spatial pattern of cars after which they directed their attention to individual cars. In conclusion, observers are able to reproduce the layout of a situation quickly, but the assessment of relative speeds takes 20 s or more.",
                "year": 2017,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Modelling Devices for Natural Interaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1571066108002107/pdfft?md5=c3bc97a95323898939897966457c7218&pid=1-s2.0-S1571066108002107-main.pdf",
                "title": "Modelling Devices for Natural Interaction",
                "abstract": "We do not interact with systems without first performing some physical action on a physical device. This paper shows how formal notations and formal models can be developed to account for the relationship between the physical devices that we actually press, twist or pull and their effects on systems. We use state diagrams of each but find we have to extend these in order to account for features such as bounce-back, where buttons or other controls are sprung. Critical to all is the fact that we are physical creatures and so formal models have to take into account that physicality.",
                "year": 2008,
                "publisher": "Electronic Notes in Theoretical Computer Science"
            }
        }
    },
    "Real time pose recognition of covered human for diagnosis of sleep apnoea": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0895611109001360/pdfft?md5=59064ea97e898660d00cdb756b0d399b&pid=1-s2.0-S0895611109001360-main.pdf",
                "title": "Real time pose recognition of covered human for diagnosis of sleep apnoea",
                "abstract": "Existing video monitoring techniques for sleep apnoea require clinicians to analyze substantial amounts of video data. Analysis of the covered human body from video is a challenging task as traditional computer vision methods such as correlation, template matching, background subtraction, contour models and related techniques for object tracking become ineffective because of the large degree of occlusion for long periods. To the authors’ best knowledge, there is no previously published method to estimate pose from persistently covered human body. This paper presents an automated monocular video monitoring approach to recover the human pose in conditions with persistently heavy obscuration, allowing for further analysis of covered human activity. In evaluation, we demonstrate that the proposed technique is able to identify human configurations with various poses and occlusion levels in two different environments.",
                "year": 2010,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "Semi-supervised Learning for Segmentation of Bleeding Regions in Video Capsule Endoscopy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050923017994/pdfft?md5=6caf842a4068d8a8672676f38d93131e&pid=1-s2.0-S1877050923017994-main.pdf",
                "title": "Semi-supervised Learning for Segmentation of Bleeding Regions in Video Capsule Endoscopy",
                "abstract": "In the realm of modern diagnostic technology, video capsule endoscopy (VCE) is a standout for its high efficacy and non-invasive nature in diagnosing various gastrointestinal (GI) conditions, including obscure bleeding. Importantly, for the successful diagnosis and treatment of these conditions, accurate recognition of bleeding regions in VCE images is crucial. While deep learning-based methods have emerged as powerful tools for the automated analysis of VCE images, they often demand large training datasets with comprehensive annotations. Acquiring these labeled datasets tends to be time-consuming, costly, and requires significant domain expertise. To mitigate this issue, we have embraced a semi-supervised learning (SSL) approach for the bleeding regions segmentation within VCE. By adopting the ‘Mean Teacher’ method, we construct a student U-Net equipped with an scSE attention block, alongside a teacher model of the same architecture. These models’ parameters are alternately updated throughout the training process. We use the Kvasir-Capsule dataset for our experiments, which encompasses various GI bleeding conditions. Notably, we develop the segmentation annotations for this dataset ourselves. The findings from our experiments endorse the efficacy of the SSL-based segmentation strategy, demonstrating its capacity to reduce reliance on large volumes of annotations for model training, without compromising on the accuracy of identification.",
                "year": 2023,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Not Propositions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041799000042/pdfft?md5=643cda53c7f0e5813be95924b087458a&pid=1-s2.0-S1389041799000042-main.pdf",
                "title": "Not Propositions",
                "abstract": "Current computational accounts of meaning in the cognitive sciences are based on abstract, amodal symbols (e.g., nodes, links, propositions) that are arbitrarily related to their referents. We argue that such accounts lack convincing empirical support and that they do not provide a satisfactory account for linguistic meaning. One historic set of results supporting the abstract symbol view has come from investigation into comprehension of negated sentences, such as “The buttons are not black.” These sentences are presumed to be understood as two propositions composed of abstract symbols. One proposition corresponds to “the buttons are black,” and it is embedded in another proposition corresponding to “it is not true.” Thus, the propositional account predicts (a) that comprehension of negated sentences should take longer than comprehension of the corresponding positive sentence (because of the time needed to construct the embedding), but (b) that the resulting embedded propositions are informationally equivalent (but of opposite valence) to the simple proposition underlying the positive sentence. Contrary to these predictions, Experiment 1 demonstrates that negated sentences out of context are interpreted as situationally ambiguous, that is, as conveying less specific information than positive sentences. Furthermore, Experiment 2 demonstrates that when negated sentences are used in an appropriate context, readers do not take longer to understand them. Thus, difficulty with negation is demonstrated to be an artifact of presentation out of context. After discussing other serious problems with the use of abstract symbols, we describe the Indexical Hypothesis. This embodied account of meaning does not depend on abstract symbols, and hence it provides a more satisfactory account of meaning.",
                "year": 1999,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Co-orchestration of multiple instruments to uncover structure–property relationships in combinatorial libraries": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Co-orchestration of multiple instruments to uncover structure–property relationships in combinatorial libraries",
                "abstract": "The rapid growth of automated and autonomous instrumentation brings forth opportunities for the co-orchestration of multimodal tools that are equipped with multiple sequential detection methods or several characterization techniques to explore identical samples. This is exemplified by combinatorial libraries that can be explored in multiple locations via multiple tools simultaneously or downstream characterization in automated synthesis systems. In co-orchestration approaches, information gained in one modality should accelerate the discovery of other modalities. Correspondingly, an orchestrating agent should select the measurement modality based on the anticipated knowledge gain and measurement cost. Herein, we propose and implement a co-orchestration approach for conducting measurements with complex observables, such as spectra or images. The method relies on combining dimensionality reduction by variational autoencoders with representation learning for control over the latent space structure and integration into an iterative workflow via multi-task Gaussian Processes (GPs). This approach further allows for the native incorporation of the system's physics via a probabilistic model as a mean function of the GPs. We illustrate this method for different modes of piezoresponse force microscopy and micro-Raman spectroscopy on a combinatorial Sm-BiFeO3 library. However, the proposed framework is general and can be extended to multiple measurement modalities and arbitrary dimensionality of the measured signals.",
                "year": 2024,
                "publisher": "Digital Discovery"
            }
        }
    },
    "On surface orientation detection in 3-D": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167865505800066/pdfft?md5=7f2464d7e668074f8810c68c57bf1b0c&pid=1-s2.0-S0167865505800066-main.pdf",
                "title": "On surface orientation detection in 3-D",
                "abstract": "Theoretical foundations of an advanced vision system capable of detecting the three-dimensional orientation of planar surfaces from a single view of a scene are presented. The standard 2-D picture records the light intensity of points registered at the point of observation, whereas data for the new vision system is represented by the direction and intensity of light rays originating from points on objects in a scene, and registered at the point of observation. The collection of these rays, i.e., light intensities and associated directions, is called an Ambient Light Array and is modelled by a vector function. In the new system the image of a flat surface is a sphere. An algorithm for the detection of orientation of a planar surface exhibiting the Lambertian reflectance characteristic and illuminated by a point light source is described, and the results of simulation in the presence of random noise are shown.",
                "year": 1991,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "Lateral interaction in accumulative computation: a model for motion detection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231202005714/pdfft?md5=1bd4a6a09849394fb3af0ee23616b2e4&pid=1-s2.0-S0925231202005714-main.pdf",
                "title": "Lateral interaction in accumulative computation: a model for motion detection",
                "abstract": "Some of the major computer vision techniques make use of neural nets. In this paper we present a novel model based on neural networks denominated lateral interaction in accumulative computation (LIAC). This model is based on a series of neuronal models in one layer, namely the local accumulative computation model, the double time scale model and the recurrent lateral interaction model. The LIAC model usefulness in the general task of motion detection may be appreciated by means of some significant examples of object detection in indefinite sequences of synthetic and real images.",
                "year": 2003,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Developing a Knowledge Graph for Pharmacokinetic Natural Product-Drug Interactions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S153204642300062X/pdfft?md5=d1d9b156e0f433dc8be27d9751dbb7f5&pid=1-s2.0-S153204642300062X-main.pdf",
                "title": "Developing a Knowledge Graph for Pharmacokinetic Natural Product-Drug Interactions",
                "abstract": "Background\n\nPharmacokinetic natural product-drug interactions (NPDIs) occur when botanical or other natural products are co-consumed with pharmaceutical drugs. With the growing use of natural products, the risk for potential NPDIs and consequent adverse events has increased. Understanding mechanisms of NPDIs is key to preventing or minimizing adverse events. Although biomedical knowledge graphs (KGs) have been widely used for drug-drug interaction applications, computational investigation of NPDIs is novel. We constructed NP-KG as a first step toward computational discovery of plausible mechanistic explanations for pharmacokinetic NPDIs that can be used to guide scientific research.",
                "year": 2023,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Entropy-based gaze planning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885600001037/pdfft?md5=6bcd104db3c0db5ba467899b8fe3866f&pid=1-s2.0-S0262885600001037-main.pdf",
                "title": "Entropy-based gaze planning",
                "abstract": "This paper describes an algorithm for recognizing known objects in an unstructured environment (e.g. landmarks) from measurements acquired with a single monochrome television camera mounted on a mobile observer. The approach is based on the concept of an entropy map, which is used to guide the mobile observer along an optimal trajectory that minimizes the ambiguity of recognition as well as the amount of data that must be gathered. Recognition itself is based on the optical flow signatures that result from the camera motion — signatures that are inherently ambiguous due to the confounding of motion, structure and imaging parameters. We show how gaze planning partially alleviates this problem by generating trajectories that maximize discriminability. A sequential Bayes approach is used to handle the remaining ambiguity by accumulating evidence for different object hypotheses over time until a clear assertion can be made. Results from an experimental recognition system using a gantry-mounted television camera are presented to show the effectiveness of the algorithm on a large class of common objects.",
                "year": 2001,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Non-conformal field-only boundary integral method for modeling EM scattering problems": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Non-conformal field-only boundary integral method for modeling EM scattering problems",
                "abstract": "Recently, a novel boundary integral equation method, namely field-only boundary integral equation (FOBIE) method, has been proposed in analyzing the electromagnetic (EM) scattering from perfectly electrically conducting (PEC) and homogeneous dielectric objects in the frequency domain. The main feature of the FOBIE method is that the Cartesian components of its electric fields can be obtained directly by solving a set of singularity free boundary integral equations. This paper will give a brief review of the FOBIE method from the EM point of view first, then extend it to handle non-conformal discretization since the continuity conditions of electric fields have already been imposed by the boundary conditions and no additional conditions are needed. A block matrix solution scheme is proposed to accelerate the solution process of the discretized matrix equation, which makes the numerical framework much more efficient. The accuracy and efficiency of the proposed method are demonstrated by analyzing the EM scattering from several canonical objects with non-conformal discretization. In addition, the time domain transient EM characteristic can be obtained by using the inverse Fourier transform to reflect the physical process of the interaction between the EM fields and object conveniently in the time domain.",
                "year": 2022,
                "publisher": "Engineering Analysis with Boundary Elements"
            }
        }
    },
    "Recent advances of Captcha security analysis: a short literature review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050923002296/pdfft?md5=8f0da3fe5f4da7be28892bf20f855858&pid=1-s2.0-S1877050923002296-main.pdf",
                "title": "Recent advances of Captcha security analysis: a short literature review",
                "abstract": "CAPTCHA has long been used to keep bots from misusing web services. Various CAPTCHA schemes have been proposed over the years, principally to increase usability and security against emerging bots and hackers performing malicious operations. However, automated attacks have effectively cracked all common conventional schemes, and the majority of present CAPTCHA methods are also vulnerable to human-assisted relay attacks. Invisible reCAPTCHA and some approaches have not yet been cracked. However, with the introduction of fourth generation bots accurately mimicking human behavior, a secure CAPTCHA would be hardly designed without additional special devices. In this paper, we presented a short literature review of the current CAPTCHA schemes, as well as highlighting new trends and open issues, the challenges, and the opportunities as a solid starting point for designing the future secure and usable CAPTCHA schemes.",
                "year": 2023,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Sensor-based continuous user authentication on smartphone through machine learning": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Sensor-based continuous user authentication on smartphone through machine learning",
                "abstract": "With the rapid development of hardware and software technology, many end-users have often stored their data on smartphones through several in-built sensors. Thus, the security of stored data on smartphones has become a significant concern. This has fueled the importance of entry-point authentication methods on smartphones. Many entry-point authentication methods have failed to offer security due to insider or side-channel attacks. This article introduces a sensor-based continuous user authentication approach on the smartphone through multi-modal behavioral biometrics and a machine learning model to tackle the aforementioned issues. The proposed approach captures the touch and motion-based behavioral biometrics through the touchscreen and inertial sensors of the device. Then, the proposed approach extracts several features from captured behavioral data and selects the best set of features through a filter-based feature selection technique. Further, we implement a nonlinear support vector machine with optimized hyperparameters for training and predicting the features to generate the scores. We apply score-level fusion on generated scores of several sensors to compute the final score for identification of the genuine user. In this article, we systematically evaluate the proposed approach with the most commonly used behavioral activities of the smartphone user in our daily life. The experiment results on all behavioral activities show that the proposed approach obtained the best authentication score compared to the other machine learning models, and state-of-the-art methods. Finally, we conclude our article by addressing the limitations of the proposed approach and practical research issues for future exploration.",
                "year": 2023,
                "publisher": "Microprocessors and Microsystems"
            }
        }
    },
    "Patients’ and Clinicians’ Perceived Trust in Internet-of-Things Systems to Support Asthma Self-management: Qualitative Interview Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Patients’ and Clinicians’ Perceived Trust in Internet-of-Things Systems to Support Asthma Self-management: Qualitative Interview Study",
                "abstract": "Background\n\nAsthma affects 235 million people worldwide. Supported self-management, including an action plan agreed with clinicians, improves asthma outcomes. Internet-of-things (IoT) systems with artificial intelligence (AI) can provide customized support for a range of self-management functions, but trust is vital to encourage patients’ adoption of such systems. Many models for understanding trust exist, some explicitly designed for eHealth, but no studies have used these models to explore trust in the context of using IoT systems to support asthma self-management.",
                "year": 2021,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Underwater image de-scattering and classification by deep neural network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045790616302075/pdfft?md5=963e52bf1899f579a1aa96770547c682&pid=1-s2.0-S0045790616302075-main.pdf",
                "title": "Underwater image de-scattering and classification by deep neural network",
                "abstract": "Vision-based underwater navigation and object detection requires robust computer vision algorithms to operate in turbid water. Many conventional methods aimed at improving visibility in low turbid water. High turbid underwater image enhancement is still an opening issue. Meanwhile, we find that the de-scattering and color correction of underwater images affect classification results. In this paper, we correspondingly propose a novel joint guidance image de-scattering and physical spectral characteristics-based color correction method to enhance high turbidity underwater images. The proposed enhancement method removes the scatter and preserves colors. In addition, as a rule to compare the performance of different image enhancement algorithms, a more comprehensive image quality assessment index Qu is proposed. The index combines the benefits of SSIM index and color distance index. We also use different machine learning methods for classification, such as support vector machine, convolutional neural network. Experimental results show that the proposed approach statistically outperforms state-of-the-art general purpose underwater image contrast enhancement algorithms. The experiment also demonstrated that the proposed method performs well for image classification.",
                "year": 2016,
                "publisher": "Computers & Electrical Engineering"
            }
        }
    },
    "A hardware framework for fall detection using inertial sensors and compressed sensing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A hardware framework for fall detection using inertial sensors and compressed sensing",
                "abstract": "Falls are the leading cause of sudden and accidental injuries in the elderly. With the aging of the world’s population, one concern of the elderly increase as well. Notably, the rate of fall accidents and hospitalizations related to falls is increasing year after year. Monitoring human activity to detect falls using intelligent portable systems is an efficient and economical solution that enables faster intervention and immediate action. This article proposes a hardware framework for fall detection using accelerometer and gyroscope data implemented on a Zedboard FPGA (Field Programmable Gate Array). Hardware components are designed, tested and simulated using the Xilinx Vivado tool. On this proposed design, the innovation lies in the insertion and integration of the compressed sensing technique (CS) in the fall detection system and human activities to reduce a number of samples and thus reduce energy consumption. To this end, four hardware blocks (compression, recovery, feature extraction and prediction) are designed and tested and validated against the software implementation. In the same system, we implemented the design by exploiting pipeline optimization, achieving very low latency compared to the unoptimized configuration.",
                "year": 2022,
                "publisher": "Microprocessors and Microsystems"
            }
        }
    },
    "The robustness of behavior-verification-based slider CAPTCHAs": {
        "accordingTo": {
            "scienceDirect": {
                "title": "The robustness of behavior-verification-based slider CAPTCHAs",
                "abstract": "CAPTCHAs represent a vital technique for protecting the security of websites by differentiating between humans and bots. There have been many successful attacks against text and image CAPTCHAs, demonstrating that they are no longer secure, and blindly using methods to enhance CAPTCHAs robustness may reduce user-friendliness. Slider CAPTCHAs, which are based on behavioral verification, are being used on more and more websites as an alternative to text CAPTCHAs and image CAPTCHAs. However, despite the widespread use of slider CAPTCHAs, their security remains largely unknown. In this paper, we introduce a simple, efficient and generic approach for attacking five widely deployed slider CAPTCHAs, with success rates ranging from 87.5% to 100%. Experimental results proving that behavior-verification-based slider CAPTCHAs do not fully differentiate between humans and bots, especially with regard to mouse operation behavior. Simultaneously, this paper also discusses whether deep learning technology can further improve the security of slider CAPTCHAs by analyzing user behavior trajectories, hoping that our work can provide new inspiration for designers to design more effective CAPTCHAs.",
                "year": 2024,
                "publisher": "Journal of Information Security and Applications"
            }
        }
    },
    "Understanding roles of virtual reality in radiology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2667345222000189/pdfft?md5=b2b0bcf876edf626e15b185d7b9835e0&pid=1-s2.0-S2667345222000189-main.pdf",
                "title": "Understanding roles of virtual reality in radiology",
                "abstract": "Radiology includes a wide range of imaging technologies that use different technologies to capture patients' data. Virtual Reality (VR) is an innovative technology that provides a clear virtual image of a patient. We see the vast potential of VR to provide a positive impact in radiology. Most relevant papers on Virtual reality in Healthcare/Radiology are identified from Scopus, ScienceDirect, Google Scholar and ResearchGate. Paper tries to technologically explore VR and its applications to improve radiological training and learners' participation levels. Paper briefs about Virtual reality and its working process steps for radiology based clinical treatments. Supportive features of virtual reality in the broad radiology domain are discussed diagrammatically. This paper's primary strength is identifying and discussing thirteen significant VR applications in radiology. VR is an important technology that is viable for healthcare. With the integration of various components and software, it provides a complete virtual display. In the starting years, this technology was for entertainment purposes. Nowadays, this technology is used to visualise the internal structure. The patient's internal or external parts are used to create more integrated interaction, providing a better procedure for guidance. A radiologist can benefit from this technology for trainees' procedural planning, in-process guidance, detection, diagnosis, and education.",
                "year": 2022,
                "publisher": "Internet of Things and Cyber-Physical Systems"
            }
        }
    },
    "Applications of affordance and semantics in product design": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X06000494/pdfft?md5=81b72845fa96159e5d60f27f24dab781&pid=1-s2.0-S0142694X06000494-main.pdf",
                "title": "Applications of affordance and semantics in product design",
                "abstract": "This paper aims to clarify the concept and elucidate the role of affordances in the interaction design for physical products by making a parallel comparison to product semantics. This study argues that the core of affordance concept in design lies not in expressing the design intent, but constructing the actions required in the user–product interaction. A framework consisting of three design dimensions: affordance, perceptual information, and symbol, is suggested to deal with different aspects in physical interaction design, in particular, the motor, perceptual, and cognitive factors with an illustrative example. In conclusion, implications for interaction design and future research are suggested.",
                "year": 2007,
                "publisher": "Design Studies"
            }
        }
    },
    "Combined path and force control for elastic manipulators": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/088832709290027G/pdfft?md5=19e1b60f7ae1915cac3e973f9c1afb59&pid=1-s2.0-088832709290027G-main.pdf",
                "title": "Combined path and force control for elastic manipulators",
                "abstract": "The basic idea for realising a combined path and force control system consists in putting most control efforts into a feedforward loop with off-line properties and only minor control activities into the on-line loops. From this given arbitrary paths and forces together with quasi-static elastic corrections are established in an extended feedforward non-linear decoupling procedure, whereas elastic vibrations and force deviations are controlled on-line. Theory and test coincide very well. Prescribed forces could be realised by a robot with elastic links and joints up to an accuracy of about 3–5%.",
                "year": 1992,
                "publisher": "Mechanical Systems and Signal Processing"
            }
        }
    },
    "The dark side of AI identity: Investigating when and why AI identity entitles unethical behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563223000201/pdfft?md5=0abcea01db2ff649b99b86dec804055f&pid=1-s2.0-S0747563223000201-main.pdf",
                "title": "The dark side of AI identity: Investigating when and why AI identity entitles unethical behavior",
                "abstract": "With its rapid development and significant benefits, increasingly more organizations have adopted artificial intelligence (AI) and taken various ways to promote employees’ AI usage and AI-related support behaviors. As a primary way to promote AI usage and AI-related supportive behaviors, building an AI identity is widely recognized as having beneficial effects on employees’ work attitudes and outcomes. Drawing upon role identity theory, we challenge this general conclusion by identifying a potential dark side of AI identity and investigating how and when AI identity promotes unethical behavior. Based on an experiment and a multi-wave field study, we found that AI identity had a positive indirect effect on unethical behavior via psychological entitlement. Furthermore, perceived rarity of AI identity moderated the observed effects—that is, when perceived rarity of AI identity was high, employees with AI identity were more likely to have psychological entitlement, which increased unethical behavior. Taken together, our findings provide new insights into the consequences of AI identity as well as reveal the importance of the rarity of AI identity and psychological entitlement in this process.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Algorithmic clustering based on string compression to extract P300 structure in EEG signals": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260718316535/pdfft?md5=fe2f2be9286044bc960b88e1ee65c5b1&pid=1-s2.0-S0169260718316535-main.pdf",
                "title": "Algorithmic clustering based on string compression to extract P300 structure in EEG signals",
                "abstract": "Background and objectives: P300 is an Event Related Potential control signal widely used in Brain Computer Interfaces. Using the oddball paradigm, a P300 speller allows a human to spell letters through P300 events produced by his/her brain. One of the most common issues in the detection of this event is that its structure may differ between different subjects and over time for a specific subject. The main purpose of this work is to deal with this inherent variability and identify the main structure of P300 using algorithmic clustering based on string compression.\n\nMethods: In this work, we make use of the Normalized Compression Distance (NCD) to extract the main structure of the signal regardless of its inherent variability. In order to apply compression distances, we carry out a novel signal-to-ASCII process that transforms and merges different events into suitable objects to be used by a compression algorithm. Once the ASCII objects are created, we use NCD-driven clustering as a tool to analyze if our object creation method suitably represents the information contained in the signals and to explore if compression distances are a valid tool for identifying P300 structure. With the purpose of increasing the level of generalization of our study, we apply two different clustering methods: a hierarchical clustering algorithm based on the minimum quartet tree method and a multidimensional projection method.\n\nResults:Our experimental results show good clustering performance over different experiments, showing the structure extraction capabilities of our procedure. Two datasets with recordings in different scenarios were used to analyze the problem and validate our results, respectively. It has to be pointed out that when the clustering performance over individual electrodes is analyzed, higher P300 activity is found in similar regions to other articles using the same datasets. This suggests that our approach might be used as an electrode-selection criteria.\n\nConclusions: The proposed NCD-driven clustering methodology can be used to discover the structural characteristics of EEG and thereby, it is suitable as a complementary methodology for the P300 analysis.",
                "year": 2019,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Augmented reality: An ecological blend": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Augmented reality: An ecological blend",
                "abstract": "In this article we present Ecological Augmented Reality (E-AR), an approach that questions the theoretical assumptions of mainstream Augmented Reality (AR). The development of AR systems to date presupposes an information-processing theory of perception that hinders the potential of the field.\n\nGenerally, in AR devices, virtual symbolic information is superimposed upon the environment in such a way that the real and the virtual may be processed, informationally speaking, in tandem. Thus, we find information in reality itself, as well as virtual symbolic information. But by increasing the burden of symbolic crunching, AR devices run the risk of saturating the user of the technology. AR systems developed under the principles of an ecological psychology may contribute to new and better levels of performance and adaptation to the user’s perceptual abilities. Our proposal is to develop AR devices such that reality itself is augmented non-symbolically by blending real and virtual layers/information. Although there are seldom AR devices in the market that are designed ecologically, two fields of research may well bring inspiration to AR developers. These are the design and manipulation of real objects, and ecological research in the field of sensory substitution. We consider them both in turn with an eye to putting forward a framework that eschews any type of information-processing regarding the nature of our psychological processes. Ultimately, our aim is to provide some guidelines for the exploration of an ecological trend in AR applications.",
                "year": 2017,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Modelling the effect of landmarks on pedestrian dynamics in urban environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0198971520303069/pdfft?md5=259410108853a40681a1f45cc6b14367&pid=1-s2.0-S0198971520303069-main.pdf",
                "title": "Modelling the effect of landmarks on pedestrian dynamics in urban environments",
                "abstract": "Landmarks have been identified as relevant and prominent urban elements, explicitly involved in human navigation processes. Despite the understanding accumulated around their functions, landmarks have not been included in simulation models of pedestrian movement in urban environments. In this paper, we describe an Agent-Based Model (ABM) for pedestrian movement simulation that incorporates the role of on-route and distant landmarks in agents' route choice behaviour. Route choice models with and without landmarks were compared by using four scenarios: road distance minimisation, least cumulative angular change, road distance minimisation and landmarks, least cumulative angular change and landmarks. The city centre of London was used as a case study and a set of GPS trajectories was employed to evaluate the model. The introduction of landmarks led to more heterogeneous patterns that diverge from the minimisation models. Landmark-based navigation brought about high pedestrian volumes along the river (up to 13% of agents) and the boundaries of the parks (around 8% of the agents). Moreover, the model evaluation showed that the results of the landmark-based scenarios were not significantly different from the GPS trajectories in terms of cumulative landmarkness, whereas the other scenarios were. This implies that our proposed landmark-based route choice approach was better able to reproduce human navigation. At the street-segment level, the pedestrian volumes emerging from the scenarios were comparable to the trajectories' volumes in most of the case study area; yet, under- and over-estimation were observed along the banks of the rivers and across green areas (up to +7%, −11% of volumes) in the landmark-based scenarios, and along major roads (up to +11% of volumes) in the least cumulative angular change scenario. While our model could be expanded in relation to the agents' cognitive representation of the environment, e.g. by considering other relevant urban elements and accounting for individual spatial knowledge differences, the inclusion of landmarks in route choice models results in more plausible agents that make use of relevant urban information.",
                "year": 2021,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "Stereokinematic analysis of visual data in active convergent stereoscopy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889098000335/pdfft?md5=2c71085611f9d843c368eb2f1c8dda02&pid=1-s2.0-S0921889098000335-main.pdf",
                "title": "Stereokinematic analysis of visual data in active convergent stereoscopy",
                "abstract": "The core contribution of this study is a mathematical model of combination of stereopsis and kineopsis under active viewing, a model to serve as a basis for algorithms implemental of active stereokinematic analysis of visual data.\n\nThe model is specified by two main groups of equations: (A) The fundamental equations of interpretation: they relate the unknowns of perception (depth and motion in space) to image variables (image positions and optical velocities) and viewing system control variables (angles of stereoscopic convergence and angular velocities of viewing system movement), in a neck-eyes as well as a neck-less mode of active viewing, (B) The equations of control regimes: they specify trategies of control of stereoscopic viewing system movement (neck-eyes and neck-less modes) in the form of expressions integratable with the fundamental equations.\n\nThe presentation of the model is prefaced by a motivational discussion, and the model's prerequisite background. It is postfaced by an account of some of its algorithmic implications.",
                "year": 1998,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Early powdery mildew detection system for application in greenhouse automation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169916304318/pdfft?md5=dc9e6ac96f52abdd2006dbb8394641e6&pid=1-s2.0-S0168169916304318-main.pdf",
                "title": "Early powdery mildew detection system for application in greenhouse automation",
                "abstract": "Early disease detection in greenhouses is an important component of an integrated management strategy. An automated process can be more accurate and cost effective. Automated disease detection of plants in the field is complex due to an uncontrolled environment, and the scale at which detection must be done. This paper presents a powdery mildew detection machine vision system for use in greenhouses. The system uses augmented lighting to remove clutter and reduce the search space, and a Hough forest to detect early powdery mildew. Field testing on greenhouse tomato plants has showed detection rates of 85%.",
                "year": 2016,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "A novel pedal musculoskeletal response based on differential spatio-temporal LSTM for human activity recognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705122012837/pdfft?md5=52f3619a76823550e8eb871e073288b2&pid=1-s2.0-S0950705122012837-main.pdf",
                "title": "A novel pedal musculoskeletal response based on differential spatio-temporal LSTM for human activity recognition",
                "abstract": "Human activity recognition (HAR) with wearable devices has shown superior usability in everyday life tracking and healthcare monitoring in recent years. To solve the existing challenges of weak sensor durability and difficulty capturing dynamic features of traditional wearable devices based on the long short-term memory neural network (LSTM), a new HAR system based on a pedal wearable device is proposed by utilizing a novel differential spatio-temporal LSTM (DST-LSTM) method. The pedal wearable device is embedded in the tongue area of the general footwear to obtain pedal musculoskeletal response (PMR) data of dorsum pedis. Based on the PMR data, the DST-LSTM is developed to classify five typical activity statuses as standing, sitting, floor walking, down the stairs and up the stairs. First, the dynamic differential information of the PMR data is taken into consideration to build a new LSTM unit. Second, the spatial relationship features of the PMR data are obtained by multi-head graph attention networks (GAT). Third, a novel spatial gate is added to the new LSTM unit to filter irrelevant spatial features and get valid information. The effectiveness and superiority of both the pedal wearable device and the DST-LSTM method are validated by comparison with the state-of-the-art methods and wearable devices.",
                "year": 2023,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "The relation between interaction aesthetics and affordances": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X12000361/pdfft?md5=10cb66ba180d8696cfd7738ecd449095&pid=1-s2.0-S0142694X12000361-main.pdf",
                "title": "The relation between interaction aesthetics and affordances",
                "abstract": "Even though aesthetics and affordances are two important factors based on which designers provide effective ways of interaction through their artifacts, there is no study or theoretical model that relates these two aspects of design. We suggest a theoretical explanation that relates the underlying functionality of aesthetics, in particular, of interaction aesthetics and of affordances in the design process. Our claim is that interaction aesthetics are one among other factors that allow users to enhance the detection of action possibilities and consequently, the detection of affordances. Our aim is first to discuss the role of interaction aesthetics in the design process, and second to suggest an explanation for their role in the detection of affordances when users interact with artifacts.",
                "year": 2013,
                "publisher": "Design Studies"
            }
        }
    },
    "Human activity recognition using deep electroencephalography learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809420302500/pdfft?md5=0383836eb4d99ef4f15925cfb59855fe&pid=1-s2.0-S1746809420302500-main.pdf",
                "title": "Human activity recognition using deep electroencephalography learning",
                "abstract": "Electroencephalography (EEG) signals can be contaminated by the noise raised from a non-cerebral artifact and vary in magnitude due to physiological differences between individuals. Hence, EEG has not been extensively applied to the design of Human Activity Recognition (HAR) systems. HAR involves classifying the activities of an individual that are captured by sensory technologies. To address this issue, this paper introduces a deep learning-based framework for classifying EEG artifacts (FCEA), based on a person’s physiological activity. The FCEA proposes an approach for designing a processing pipeline involving a Convolutional Neural Network and a Long Short-Term Memory Recurrent Neural Network, in order to classify raw EEG signals based on the EEG artifacts. To demonstrate the performance of the FCEA, a 3-class dataset of EEG activities was created. A consumer-grade EEG wearable device was used to collect the data from two prefrontal EEG channels, from 8 participants whilst speaking loudly, reading printed text and watching a TV program. These activities include jaw-clenching and head and eye movements, that are part of a wide range of daily human activities. Moreover, these activities cannot easily be detected by using the sensory technologies that have commonly been used in HAR. The comparative performance analysis results demonstrate that the 1- and 2-channel models trained by the FCEA outperform competitive state-of-the-art deep-learning models for HAR and raw EEG data. The deep-learning approach proposed by the FCEA improves raw data processing to obtain a better generalization performance.",
                "year": 2020,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Behavioral biometrics & continuous user authentication on mobile devices: A survey": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253520303493/pdfft?md5=b13cdbaf1ff4f79c3444c87263726ef6&pid=1-s2.0-S1566253520303493-main.pdf",
                "title": "Behavioral biometrics & continuous user authentication on mobile devices: A survey",
                "abstract": "This paper offers an up-to-date, comprehensive, extensive and targeted survey on Behavioral Biometrics and Continuous Authentication technologies for mobile devices. Our aim is to help interested researchers to effectively grasp the background in this field and to avoid pitfalls in their work. In our survey, we first present a classification of behavioral biometrics technologies and continuous authentication for mobile devices and an analysis for behavioral biometrics collection methodologies and feature extraction techniques. Then, we provide a state-of-the-art literature review focusing on the machine learning models performance in seven types of behavioral biometrics for continuous authentication. Further, we conduct another review that showed the vulnerability of machine learning models against well-designed adversarial attack vectors and we highlight relevant countermeasures. Finally, our discussions extend to lessons learned, current challenges and future trends.",
                "year": 2021,
                "publisher": "Information Fusion"
            }
        }
    },
    "A survey of variational and CNN-based optical flow techniques": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A survey of variational and CNN-based optical flow techniques",
                "abstract": "Dense motion estimations obtained from optical flow techniques play a significant role in many image processing and computer vision tasks. Remarkable progress has been made in both theory and its application in practice. In this paper, we provide a systematic review of recent optical flow techniques with a focus on the variational method and approaches based on Convolutional Neural Networks (CNNs). These two categories have led to state-of-the-art performance. We discuss recent modifications and extensions of the original model, and highlight remaining challenges. For the first time, we provide an overview of recent CNN-based optical flow methods and discuss their potential and current limitations.",
                "year": 2019,
                "publisher": "Signal Processing: Image Communication"
            }
        }
    },
    "Ground plane estimation, error analysis and applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889002001756/pdfft?md5=f211bae88751881d65d647c4d20e3f88&pid=1-s2.0-S0921889002001756-main.pdf",
                "title": "Ground plane estimation, error analysis and applications",
                "abstract": "Ground plane perception is of vital importance to human mobility. In order to develop a stereo-based mobility aid for the partially sighted, we model the ground plane based on disparity and analyze its uncertainty. Because the mobility aid is to be mounted on a person, the cameras will be moving around while the person is walking. By calibrating the ground plane at each frame, we show that a partial pose estimate can be recovered. Moreover, by keeping track of how the ground plane changes and analyzing the ground plane, we show that obstacles and curbs are detected. Detailed error analysis has been carried out as reliability is of utmost importance for human applications.",
                "year": 2002,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "DTAAD: Dual Tcn-attention networks for anomaly detection in multivariate time series data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705124004830/pdfft?md5=2f537151d04ecf4234f679e9242c1729&pid=1-s2.0-S0950705124004830-main.pdf",
                "title": "DTAAD: Dual Tcn-attention networks for anomaly detection in multivariate time series data",
                "abstract": "Anomaly detection techniques enable effective anomaly detection and diagnosis in multi-variate time series data, which are of major significance for today’s industrial applications. However, establishing an anomaly detection system that can be rapidly and accurately located is a challenging problem due to the lack of anomaly labels, the high dimensional complexity of the data, memory bottlenecks in actual hardware, and the need for fast reasoning. In this paper, we propose an anomaly detection and diagnosis model, DTAAD, based on Transformer and Dual Temporal Convolutional Network (TCN). Our overall model is an integrated design in which an autoregressive model (AR) combines with an autoencoder (AE) structure. Scaling methods and feedback mechanisms are introduced to improve prediction accuracy and expand correlation differences. Constructed by us, the Dual TCN-Attention Network (DTA) uses only a single layer of Transformer encoder in our baseline experiment, belonging to an ultra-lightweight model. Our extensive experiments on seven public datasets validate that DTAAD exceeds the majority of currently advanced baseline methods in both detection and diagnostic performance. Specifically, DTAAD improved F1 scores by 8.38% and reduced training time by 99% compared to the baseline. The code and training scripts are publicly available on GitHub at https://github.com/Yu-Lingrui/DTAAD.",
                "year": 2024,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Agens Faber: Toward a Theory of Artefacts for MAS": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1571066106003276/pdfft?md5=fba47eb344839e3e4fc47077b724db52&pid=1-s2.0-S1571066106003276-main.pdf",
                "title": "Agens Faber: Toward a Theory of Artefacts for MAS",
                "abstract": "Human intelligence has evolved along with the use of more and more sophisticated tools, allowing Homo Faber (from Homo Habilis to Homo Sapiens Sapiens) to cope with environment changes, as well as to adapt the environment to his needs. Analogously, in this seminal paper we introduce the notion of Agens Faber, conveying the idea that agent intelligence should not be considered as separated by the agent ability to perceive and affect the environment—and so, that agent intelligence is strictly related to the artefacts that enable, mediate and govern any agent (intelligent) activity.\n\nAlong this line, we first discuss the notion of artefact for MAS in general, then we try to devise out the admissible / required / desirable features of an artefact for MAS. We elaborate on the many sorts of possible relations between agents and artefacts, focusing in particular on the issue of the rational exploitation of artefacts, and also rough out a possible taxonomy of artefacts for MAS.",
                "year": 2006,
                "publisher": "Electronic Notes in Theoretical Computer Science"
            }
        }
    },
    "Complementing search engines with online web mining agents": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923602001069/pdfft?md5=9c126424ecf76ebc1a352c9313284b23&pid=1-s2.0-S0167923602001069-main.pdf",
                "title": "Complementing search engines with online web mining agents",
                "abstract": "While search engines have become the major decision support tools for the Internet, there is a growing disparity between the image of the World Wide Web stored in search engine repositories and the actual dynamic, distributed nature of Web data. We propose to attack this problem using an adaptive population of intelligent agents mining the Web online at query time. We discuss the benefits and shortcomings of using dynamic search strategies versus the traditional static methods in which search and retrieval are disjoint. This paper presents a public Web intelligence tool called MySpiders, a threaded multiagent system designed for information discovery. The performance of the system is evaluated by comparing its effectiveness in locating recent, relevant documents with that of search engines. We present results suggesting that augmenting search engines with adaptive populations of intelligent search agents can lead to a significant competitive advantage. We also discuss some of the challenges of evaluating such a system on current Web data, introduce three novel metrics for this purpose, and outline some of the lessons learned in the process.",
                "year": 2003,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Assembly sequence and path planning for monotone and nonmonotone assemblies with rigid and flexible parts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584521000636/pdfft?md5=0fa3261ed68df8f73c018cd212d0b56b&pid=1-s2.0-S0736584521000636-main.pdf",
                "title": "Assembly sequence and path planning for monotone and nonmonotone assemblies with rigid and flexible parts",
                "abstract": "The Assembly Sequence and Path Planning (ASPP) problem deals with finding a proper sequence of parts to be assembled into a finished product and short assembly paths for each part. The problem is a combination of Assembly Sequence Planning (ASP) and Assembly Path Planning (APP) subproblems, which are both NP-complete and therefore intractable at large sizes. Nearly in all works on ASPP, it is assumed that planning is monotone (i.e., parts are moved only once, without considering intermediate placements) and each part is completely rigid. These are simplifying, yet limiting assumptions, since most assembled products like ships, aircraft, and automobiles are composed of rigid and flexible parts, and the generation of assembly sequence and path plans for most real-world products requires intermediate placement of parts to be taken into account. None of the existing works in the literature, however, have handled nonmonotone ASPP problems for rigid and flexible parts, and this issue remains largely untouched. In this paper, we present a new method called SPP-Flex for solving monotone and nonmonotone ASPP for rigid and flexible parts. SPP-Flex first utilizes a Directional Assembly Stress Matrix (DASM) for describing interference relations between all pairs of parts and the amounts of compressive stresses needed for assembling flexible parts and then obtains an initial tentative assembly sequence using a simple new greedy heuristic. Next, short assembly paths are iteratively computed and planned from initial to goal configurations of all parts using a novel sampling-based path planner called BXXT. If finding a free path for an active part fails due to obstruction of a previously assembled part, then such a part is identified, relocated, and its path replanned until the active part is moved to its final position. In case of failure again, if the part is flexible, through finite element analysis, it is determined if the part can still be assembled by undergoing elastic deformation. To evaluate the performance of the SPP-Flex and its components, two new products were designed and solved by four combinations of ASP and APP methods 20 times each, and the means and standard deviations of five performance criteria (total path length, total number of generated nodes and edges in the search tree, total number of collision (interference) checks, and total runtime) were calculated. Analysis of the computational results showed that the proposed greedy heuristic sequence planner together with the BXXT path planner/replanner outperformed other variations with at most 4.6% average gap in path length and 2.1% average gap in runtime compared to the best-found solution in all runs.",
                "year": 2021,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "X-Net: Multi-branch UNet-like network for liver and tumor segmentation from 3D abdominal CT scans": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231221009255/pdfft?md5=2d54630991bac952e7b592720f177ed6&pid=1-s2.0-S0925231221009255-main.pdf",
                "title": "X-Net: Multi-branch UNet-like network for liver and tumor segmentation from 3D abdominal CT scans",
                "abstract": "The diagnosis of liver cancer is one of the most attractive fields in clinical practice for its high mortality. Accurate segmentation of liver and tumor has been publicly accepted to be an effective method to assist doctors in determining the disease condition and planning the subsequent treatments. Recently, deep learning based methods have been widely used in tumor segmentation and provided good performance. However, current methods cannot fully reflect the differences between tumor, inside-liver tissues and outside-liver organs simultaneously, while the extraction of features reflecting axial changes of liver and tumor is always discounted by the heavy computational burden, resulting in limited learning effects and efficiencies. To solve these problems, in this paper, we propose a novel framework to segment liver and tumors in abdominal CT volumes, which consists of two parts: 1) we propose a multi-branch network where an up-sampling branch for liver region recognition and a pyramid-like convolution structure for inner-liver feature extraction are integrated into the back-bone Dense UNet structure for better extracting intra-slice features of liver and tumors; 2) we simplify the traditional 3D UNet by using the convolutional kernels with the fixed size 3×3 in x-y plane and apply it as a 3D counterpart for aggregating contextual information along the z-axis from the stacked, filtered CT slices, with the advantages of inhibiting the influence from neighboring pixels and alleviating the computational burden greatly. The above two parts are formulated as a unified end-to-end network so that the intra-slice feature representation and the inter-slice information aggregation can be learned and optimized jointly. Furthermore, we novely define a loss function combining a modified dice loss and a contour-detection based loss, so that the region features and contour features of the predicted segmentation of liver and tumors are jointly considered for network training and parameters optimization. Experimental results on the MICCAI 2017 Liver Tumor Segmentation Challenge dataset and 3DIRCADb dataset have demonstrated that the proposed method can provide superior performance to the state-of-the-art methods with respect to the certain benchmarks for liver and tumor segmentation.",
                "year": 2021,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Operationalizing situated cognition and learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041702000487/pdfft?md5=3d4c6428c518fb8008981be0e478a0fe&pid=1-s2.0-S1389041702000487-main.pdf",
                "title": "Operationalizing situated cognition and learning",
                "abstract": "Vera and Simon [Cognitive Science, 17 (1993) 117] complain that the situational approach to cognition is insufficiently operationalized. This paper presents an interpreted system of diagrams that permit descriptions of tasks in both symbolic and situative terms, allowing differences to be operationally defined. It is argued that the basic language for describing psychological phenomena, shared by both behavioral and cognitive approaches, that of stimulus and response, is not adequate for the situational approach. Two examples, one drawn from published research in animal learning and one illustrating a re-analysis of standard tasks used in studies of problem solving, demonstrate differences between characterizing tasks in traditional stimulus–response terms as opposed to characterizing tasks in terms of situations, deeds, and observations. Connections are made to Reinforcement Learning and the study of Partially Observable Markov Decision Processes (POMDPs).",
                "year": 2002,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Wavelet based tone mapping (TM) enhancement to a detection system for faint and compact sources in HDR and large FOV radio scenes": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Wavelet based tone mapping (TM) enhancement to a detection system for faint and compact sources in HDR and large FOV radio scenes",
                "abstract": "For high dynamic range (HDR) and large field-of-view (FOV) radio scenes, detection of faint sources with compact support and tiny sizes in comparatively extensive celestial areas is a challenging task. In the era of the Square Kilometer Array (SKA), it will be of particular concern, and data-targeted systematic methods are put on the agenda. This paper proposes a systematic method to solve this problem. From the perspective of image processing, a multiscale image contrast enhancement (ICE) is proposed to normalize the HDR. Source representation is realized by a starlet framework, where a two-step tone mapping (TM) including an arc-tangent function compression and an aggregated gain function enhancement are embedded. According to two bionic principles, the gain function is designed to highlight faint sources, remove artifacts (e.g., halos), and avoid feature distortions. For source detection, an integration method is proposed for projections between the celestial and the planar coordinate systems. It serves for coordinate transform and source location. A detection indicator system is built, and elaborate discrimination principles are drawn up. In the experiments, this method is dedicated to the dataset of the TIFR GMRT Sky Survey (TGSS) alternative data release 1 (ADR1) to facilitate the scientific goal of discovering new sources. Results show its advantages over the state of the art methods in visual inspections and detection ratios, and preliminarily prove that it has prospects for future HDR and large FOV radio data.",
                "year": 2023,
                "publisher": "Astronomy and Computing"
            }
        }
    },
    "Monitoring of human body running training with wireless sensor based wearable devices": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Monitoring of human body running training with wireless sensor based wearable devices",
                "abstract": "With the rapid pace of today’s society, work pressure, less exercise time, people began to pay more attention to their health. Walking and running have become the first choice of moderate exercise for many young people. The recognition of human running state based on wireless acceleration sensor will play an increasingly important role in the fields of motion detection, energy consumption evaluation and health care. It is of great significance to develop and design a kind of wearable multi-functional wireless sensor which can monitor the running state of human body. In this paper, a wearable human body monitoring system based on wireless acceleration sensor technology is proposed for real-time monitoring of daily running volume of human body. Hardware and upper computer design: stm32f405 is used as the main control chip, and ma8451q is used to collect human motion data. In this paper, aiming at the problem that three kinds of motion states of human body are easy to be confused and difficult to distinguish, based on the in-depth study of the complex structure mode and self similarity characteristics of non-stationary acceleration signal, a method of human body motion state recognition based on single fractal and multi fractal is proposed. In this method, the fractal dimension and the generalized dimension are used as the feature variables, and the correlation judgment method is used to distinguish and recognize different motion states. Experiments show the validity and feasibility of single fractal and multifractal in walking and going up and down three kinds of motion state recognition. On the basis of multifractal motion state recognition, this paper combines fractal theory with wavelet multiresolution analysis, and proposes a matrix fractal human motion state recognition method based on wavelet transform. The fractal matrix based on wavelet transform quantifies the fractal characteristics of the component signals of walking and going up and down in different wavelet scales, and then describes the complexity and self similarity of the original acceleration signals. Experimental results show that the average recognition rate of walking, jogging and fast running can reach over 93% under the premise of less prior information.",
                "year": 2020,
                "publisher": "Computer Communications"
            }
        }
    },
    "Computational model for identifying stereotyped behaviors and determining the activation level of pseudo-autistic": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494620308152/pdfft?md5=71f6880be7bbe3670e9be3c9c0da3fa8&pid=1-s2.0-S1568494620308152-main.pdf",
                "title": "Computational model for identifying stereotyped behaviors and determining the activation level of pseudo-autistic",
                "abstract": "Affective state recognition of an individual is based on the emotional cues, such as the activation level. Body expression is a modal able to convey emotions and can be used for autism diagnosis through the presence of stereotyped behaviors (SBs). These behaviors are atypical and repetitive movements of the body, which can be related to a low mental health condition. The development of systems able to both recognize SBs and inferring activation level can automatically aid some therapeutic approaches. In this paper, a computational model of low intrusiveness is proposed to infer activation levels from recognized SBs, Machine Learning Algorithms (MLAs) are for identifying the SBs and for determining the related activation levels. A metric performance is also proposed to evaluate the performance of MLAs considering the time for classification of the SBs, accuracy, and precision. For classifying the SBs, the Hidden Markov Models and Multilayer Perceptron presented the best performance than Support Vector Machine and Convolutional Neural Network. The Adaptive Neuro-Fuzzy technique based on the Fuzzy C-Means algorithm allowed one to determine and differentiate the activation levels of the stereotyped behaviors considered in the present study. The experiments were performed with non-autistic participants, here referred to as pseudo-autistic.",
                "year": 2021,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "External multi-modal imaging sensor calibration for sensor fusion: A review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S156625352300115X/pdfft?md5=ad9851579d4e556cc3470f4cba4155b5&pid=1-s2.0-S156625352300115X-main.pdf",
                "title": "External multi-modal imaging sensor calibration for sensor fusion: A review",
                "abstract": "Multi-modal data fusion has gained popularity due to its diverse applications, leading to an increased demand for external sensor calibration. Despite several proven calibration solutions, they fail to fully satisfy all the evaluation criteria, including accuracy, automation, and robustness. Thus, this review aims to contribute to this growing field by examining recent research on multi-modal imaging sensor calibration and proposing future research directions. The literature review comprehensively explains the various characteristics and conditions of different multi-modal external calibration methods, including traditional motion-based calibration and feature-based calibration. Target-based calibration and targetless calibration are two types of feature-based calibration, which are discussed in detail. Furthermore, the paper highlights systematic calibration as an emerging research direction. Finally, this review concludes crucial factors for evaluating calibration methods and provides a comprehensive discussion on their applications, with the aim of providing valuable insights to guide future research directions. Future research should focus primarily on the capability of online targetless calibration and systematic multi-modal sensor calibration.",
                "year": 2023,
                "publisher": "Information Fusion"
            }
        }
    },
    "A codesign study exploring needs, strategies, and opportunities for digital health platforms to address pandemic-related impacts on children and families": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A codesign study exploring needs, strategies, and opportunities for digital health platforms to address pandemic-related impacts on children and families",
                "abstract": "In this paper we contribute seven design opportunities for future digital health platforms, like Private Social Networks (PSNs), focused on supporting the (un)met mental health and socioemotional needs of children (∼8-12 years old) and their supporting adults (parents and teachers) in the wake of the Covid-19 pandemic. These were derived from the thematic analysis of a two-phase co-design study with children, their parents, and their teachers (Phase 1), and employees at our industry partner Curatio.me (Phase 2). Our thematic findings contribute understanding about the types of experiences children, families and educators have had, and open the conversation around designing digital health platforms that can support mental health and socioemotional wellbeing in children and their supporting adults. Through individualized tracking, social capabilities, and secure, vetted sources of support, PSNs offer unique opportunities to (1) provide children with a safe space to share, reflect and come together, (2) extend existing practices related to SEL across children’s changing contexts and developmental needs, (2) support an integrated digital ecosystem of care across different stakeholders that allows for engagement and targeted interventions, and (3) support niche or marginalized communities in gaining access to relevant, meaningful and identity-specific support that may not otherwise be available.",
                "year": 2023,
                "publisher": "International Journal of Child-Computer Interaction"
            }
        }
    },
    "Drone mapping of damage information in GPS-Denied disaster sites": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1474034621002020/pdfft?md5=e649144d6a8d37b0c00a217dc18d3625&pid=1-s2.0-S1474034621002020-main.pdf",
                "title": "Drone mapping of damage information in GPS-Denied disaster sites",
                "abstract": "The increasing number and severity of natural hazard events in recent years, and their devastating impact on human life, local economies, and the built environment has called governments around the world into action and created a new mandate for a paradigm shift in disaster management and mitigation policies. To this end, new affordable technologies with mobile connectivity (e.g., smartphones, unmanned systems, reality capture devices) have scaled up tasks such as data collection and curation, leading to a significant increase in the volume of data gathered and shared in the aftermath of disasters. In the meantime, advancements in high-power and distributed computing have created new opportunities in fast and reliable data analytics. In particular to the application of drones in disaster response, past research has primarily focused on aerial data collection and more recently, ground object detection. Geolocalization of drone data (i.e., the process of determining the geographical position of objects in drone’s field of view), however, is a complex task that relies on prior knowledge of the drone’s geolocation (e.g., flight path coordinates, inertial sensors, camera gaze). Such metadata may not be always available or shared across platforms especially with the increased use of crowdsourcing in disaster response, damage assessment, and recovery. This paper presents a methodology for spatial mapping of disaster impact information in drone videos without reliance on GPS data of the aerial camera. We perform progressive mapping using scale-invariant visual features in red–greenblue (RGB) videos of disaster-affected sites in two major hurricanes in North America, namely Harvey (2017) and Dorian (2019). Results indicate that the proposed methodology can project objects from the perspective view of a drone camera onto an orthogonal map with 32.7–36.9 ft of average root mean square (RMS) error in a land area of 18–45 acres.",
                "year": 2022,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "Discrete element modeling of particle-based additive manufacturing processes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782516300536/pdfft?md5=0bc2722b256c3c9a0f996046cda8d14b&pid=1-s2.0-S0045782516300536-main.pdf",
                "title": "Discrete element modeling of particle-based additive manufacturing processes",
                "abstract": "A critical element for the design, characterization, and certification of materials and products produced by additive manufacturing processes is the ability to accurately and efficiently model the associated materials and processes. This is necessary for tailoring these processes to endow the associated products with proper geometrical and functional features. In an effort to address these needs in a computationally elegant and at the same time physically realistic manner, this paper presents the development of a methodology for simulating particle-based additive manufacturing processes, which employs the Discrete Element Method (DEM) extended to incorporate thermal physics. The details of the DEM-based methodology are presented first and the approach is demonstrated on a pair of test problems involving laser sintering of metal powders. The paper concludes with a discussion on how this approach may be generalized to broader classes of additive manufacturing systems, and details are given regarding future work necessary to further develop the present methodology.",
                "year": 2016,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Development of a participation assessment system based on multimodal evaluation of user responses for upper limb rehabilitation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809421006637/pdfft?md5=41263428b7959a2c04b4d9a5b9618473&pid=1-s2.0-S1746809421006637-main.pdf",
                "title": "Development of a participation assessment system based on multimodal evaluation of user responses for upper limb rehabilitation",
                "abstract": "The patient's active participation in exercises is a crucial factor to increase the functional outputs received from therapy. For improving the patient's active and voluntary involvement, the difficulty levels of therapy tasks and the device assistance are adjusted based on the patient's therapy performance. However, the existing performance evaluation methods are based on either some specific device designs or certain therapy tasks. In this work, a patient performance evaluation method is proposed based on a multimodal sensor fusion formed by the trajectory tracking error signal and the patient's physiological responses (heart rate and skin conductance) during the upper extremity rehabilitation. The novelty of the system is assessing the patient's performance independently from any device designs or therapy tasks. The developed system also evaluates the patient's tiredness and slacking, which are substantial factors affecting the therapy performance. The patient's upper limb joints' angles are measured via inertial measurement unit sensors. Arm movements of the patient are estimated by an upper limb kinematic module for evaluating the trajectory tracking. The patient's performance, tiredness, and slacking are assessed by a fuzzy inference system using physiological responses and exercise profile. The developed system is tested experimentally with healthy subjects on five therapy tasks. Also, for demonstrating the proposed method efficacy, additional experiments have been performed for different cases while measuring the sEMG signals of the subjects. The experimental results showed that the proposed system estimates subjects’ participation successfully and adjusts the therapy tasks according to subjects' performance and tiredness.",
                "year": 2021,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "All about uncertainties and traps: Statistical oracle-based attacks on a new CAPTCHA protection against oracle attacks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404820300420/pdfft?md5=75502f799800806ce5fa0b81c4d45469&pid=1-s2.0-S0167404820300420-main.pdf",
                "title": "All about uncertainties and traps: Statistical oracle-based attacks on a new CAPTCHA protection against oracle attacks",
                "abstract": "CAPTCHAs are security mechanisms that try to prevent automated abuse of computer services. Many CAPTCHAs have been proposed but most have known security flaws against advanced attacks. In order to avoid a kind of oracle attacks in which the attacker learns about ground truth labels via active interactions with the CAPTCHA service as an oracle, Kwon and Cha proposed a new CAPTCHA scheme that employ uncertainties and trap images to generate adaptive CAPTCHA challenges, which we call “Uncertainty and Trap Strengthened CAPTCHA” (UTS-CAPTCHA) in this paper. Adaptive CAPTCHA challenges are used widely (either explicitly or implicitly) but the role of such adaptive mechanisms in the security of CAPTCHAs has received little attention from researchers.\n\nIn this paper we present a statistical fundamental design flaw of UTS-CAPTCHA. This flaw leaks information regarding ground truth labels of images used. Exploiting this flaw, an attacker can use the UTS-CAPTCHA service as an oracle, and perform several different statistical learning-based attacks against UTS-CAPTCHA, increasing any reasonable initial success rate up to 100% according to our theoretical estimation and experimental simulations. Based on our proposed attacks, we discuss how the fundamental idea behind our attacks may be generalized to attack other CAPTCHA schemes and propose a new principle and a number of concrete guidelines for designing new CAPTCHA schemes in the future.",
                "year": 2020,
                "publisher": "Computers & Security"
            }
        }
    },
    "3-D object modeling from 2-D occluding contour correspondences by opti-acoustic stereo imaging": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314214002112/pdfft?md5=ea6b225648f92cfc52817568e04e3974&pid=1-s2.0-S1077314214002112-main.pdf",
                "title": "3-D object modeling from 2-D occluding contour correspondences by opti-acoustic stereo imaging",
                "abstract": "Utilizing in situ measurements to build 3-D volumetric object models under variety of turbidity conditions is highly desirable for marine sciences. To address the ineffectiveness of feature-based structure from motion and stereo methods under poor visibility, we explore a multi-modal stereo imaging technique that utilizes coincident optical and forward-scan sonar cameras, a so-called opti-acoustic stereo imaging system. The challenges of establishing dense feature correspondences in either opti-acoustic or low-contrast optical stereo images are avoided, by employing 2-D occluding contour correspondences, namely, the images of 3-D object occluding rims. Collecting opti-acoustic stereo pairs while circling an object, matching 2-D apparent contours in optical and sonar views to construct the 3-D occluding rim, and computing the stereo rig trajectory by opti-acoustic bundle adjustment, we generate registered samples of 3-D surface in a reference coordinate system. A surface interpolation gives the 3-D object model.\n\nIn addition to the key advantage of utilizing range measurements from sonar, the proposed paradigm requires no assumption about local surface curvature as traditionally made in 3-D shape reconstruction from occluding contours. The reconstruction accuracy is improved by computing both the 3-D positions and local surface normals of sampled contours. We also present (1) a simple calibration method to estimate and correct for small discrepancy from the desired relative stereo pose; (2) an analytical analysis of the degenerate configuration that enables special treatment in mapping (tall) elongated objects with dominantly vertical edges. We demonstrate the performance of our method based on the 3-D surface rendering of certain objects, imaged by an underwater opti-acoustic stereo system.",
                "year": 2015,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Teleonomic functions and intrinsic intentionality: Dretske’s theory as a test case": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041706000301/pdfft?md5=3de9e6b6c671df004f6db4cc0c9c78e8&pid=1-s2.0-S1389041706000301-main.pdf",
                "title": "Teleonomic functions and intrinsic intentionality: Dretske’s theory as a test case",
                "abstract": "Fred Dretske’s theory of indicatory functions [Dretske, F. (1988). Explaining behavior: reasons in a world of causes. Cambridge, MA: MIT/Bradford; Dretske, F. (1994). A recipe for thought. Originally published as “If You Can’t Make One, You Don’t Know How It Works.” In P. French, T. Uehling, & H. Wettstein (eds.), Midwest studies in philosophy: Vol. 19. Reprinted in D. J. Chalmers (2002) (pp. 468–482).] is undoubtedly one of the more ambitious attempts to articulate a sound naturalistic foundation for an adequate theory of intentional content. In what follows I argue that, contrary to Dretske’s explicit intentions, his theory fails a crucial adequacy test – that of accounting for mental content as a system-intrinsic property. Once examined in light of the first-person perspective of an embodied psychological agent, I argue, it becomes clear that neither ‘indication’, nor ‘function’, as used by Dretske, can be consistently applied. Dretske’s theory of indicatory functions is, thus, doubly incoherent. It is then argued that the problems identified here stretch far beyond Dretske’s specific theory – covering the better part of contemporary attempts to naturalize content. I conclude by suggesting that these general problems of representation, exemplified so vividly in Dretske’s theory, also testify to the inadequacy of the quest to reduce teleological phenomena (function and purpose) to predominantly mechanistic variables.",
                "year": 2007,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Redundant Arrays of Independent Libraries (RAIL): The StarFish tertiary storage system": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167819197001166/pdfft?md5=048721e73deb4513e3bc766813c93179&pid=1-s2.0-S0167819197001166-main.pdf",
                "title": "Redundant Arrays of Independent Libraries (RAIL): The StarFish tertiary storage system",
                "abstract": "Increased computer networking has sparked a resurgence of the ‘on-line’ revolution of the 1970's, making ever larger amounts of data available on a world wide basis and placing greater demands on the performance and availability of tertiary storage systems. In this paper, we argue for a new approach to tertiary storage system architecture that is obtained by coupling multiple small and inexpensive ‘building block’ libraries (or jukeboxes) together to create larger tertiary storage systems. We call the resulting system a RAIL and show that it has performance and availability characteristics superior to conventional tertiary storage systems, for almost the same dollar/megabyte cost. A RAIL system is the tertiary storage equivalent of a fixed magnetic disk RAID storage system, but with several additional features that enable the ideas of data striping and redundancy to function efficiently on dismountable media and robotic media mounting systems. We present the architecture of such a system called StarFish I and describe the implementation of a prototype. We also introduce the idea of creating a log-structured library array (LSLA) on top of a RAIL architecture (StarFish II) and show how it can have write performance equivalent to that of secondary storage, and improved read performance along with other advantages such as easier compression and the elimination of the 4 × RAID/RAIL write penalty.",
                "year": 1998,
                "publisher": "Parallel Computing"
            }
        }
    },
    "An eye tracker based on webcam and its preliminary application evaluation in Chinese reading tests": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S174680942200043X/pdfft?md5=61411403260d98b283d92cb685bf6f4c&pid=1-s2.0-S174680942200043X-mainext.pdf",
                "title": "An eye tracker based on webcam and its preliminary application evaluation in Chinese reading tests",
                "abstract": "There is increasing evidence that eye trackers can be used in neuroscience research to assess cognitive function although expensive prices or low availability still prevent its ubiquitous application. Meanwhile, in tasks such as reaction speed testing in cognitive function assessment, the requirements for spatial performance are less critical than temporal resolution. Therefore, in this report we propose an eye-tracking system based on a common web-camera, to evaluate if it's possible to achieve a decent temporal resolution for eye-tracking tasks in cognitive experiments and then further evaluate it in Chinese reading tests. The performance of the proposed setup are evaluated in comparison experiments including 62 healthy subjects, with a commercialized eye-tracker (SMI RED250) simultaneously deployed. It’s found in the results that the webcam-based system showed no significant difference in the recognition accuracy of occurrence and switching of the eye movement events compared with the SMI system though the spatial eye-tracking performance is significantly worse. Then in an application experiment, cognitive-related Chinese reading tests are adapted and EMs of 27 young (18–30 years) and 28 elderly (over 60 years) readers are recorded to study their difference in Chinese reading. In the results there are significant differences in reading-time-per-sentence between young and elderly subjects. Meanwhile, a study on the reading-time-per-sentence difference using reading materials of different difficulties shows that elderly subjects are more sensitive to difficulty changes. These results preliminary demonstrated the feasibility of a Chinese reading test with a web-camera setup as an alternative option when the expensive eye-tracker is not available.",
                "year": 2022,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Transparent object extraction from regular textured backgrounds by using binary parallel operations": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X87900557/pdfft?md5=83d6e8b8c8088cb89939146e813f803d&pid=1-s2.0-0734189X87900557-main.pdf",
                "title": "Transparent object extraction from regular textured backgrounds by using binary parallel operations",
                "abstract": "A transparent object is investigated from the viewpoint of automatic image analysis and recognition. A regularly textured background is proposed to utilize both the reflection and refraction effects of the object. A transparent ashtray is put onto a dotted background and observed from the top by a TV camera. The dot size of the background is known in advance. The observed image is thresholded and processed by conventional binary local parallel operations, i.e., dilation (expansion) and erosion (contraction). Both the dilation-erosion and erosion-dilation operations detect regions larger than the original dot. These regions are either reflecting areas or areas created by deformed or enlarged dots. These two results, which have a compensatory property, are unified. This method is threshold insensitive and is applicable also to opaque objects which have continuously changing gray levels.",
                "year": 1987,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "A label-free immunosensor based on E-SMS optical fiber structure for rapid Ag85B detection of tuberculosis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1068520024002487/pdfft?md5=c69027e7fdc868928706eb1f78003e2d&pid=1-s2.0-S1068520024002487-main.pdf",
                "title": "A label-free immunosensor based on E-SMS optical fiber structure for rapid Ag85B detection of tuberculosis",
                "abstract": "Tuberculosis, caused by Mycobacterium tuberculosis bacillus, is one of the most prevalent, widespread, infectious, and deadliest diseases in the world. In this work, a compact and sensitive biosensor based on an etched single-mode–multimode–single-mode (E-SMS) optical fiber structure was developed for the rapid detection of Mycobacterium tuberculosis Ag85B protein in phosphate buffered saline solution. This biosensor, based on the multimode interference produced into an E-SMS structure, was fabricated by chemical etching of the cladding region of the multimode fiber, followed by the immobilization of the anti-Ag85B antibody on the surface of the etched region. The functionalized E-SMS biosensor provided high selectivity in the detection of Ag85B protein, with the ability to detect biological interactions in a range of 1.31 µg/mL to 4.36 µg/mL, with a sensitivity of 2.3 nm/(µg/mL); also, signal was even obtained at concentrations as low as 80 ng/mL. This sensitive label-free biosensing technology could contribute significantly improve Mycobacterium tuberculosis detection, management in clinical diagnosis, and medical treatment of infected patients.",
                "year": 2024,
                "publisher": "Optical Fiber Technology"
            }
        }
    },
    "Sensory substitution using tactile pin arrays: Human factors, technology and applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0165168406001411/pdfft?md5=13242f715df8311a9b7b14bcdcaf572a&pid=1-s2.0-S0165168406001411-main.pdf",
                "title": "Sensory substitution using tactile pin arrays: Human factors, technology and applications",
                "abstract": "Tactile arrays use a matrix of individually controllable elements to present spatial and temporal patterns of cutaneous information. Early devices of this type were in the field of sensory substitution to replace vision or hearing for users with a sensory impairment. Many advances have been made due to the appropriation of tactile displays for telerobotics and virtual reality, to represent physical contact with a remote or simulated environment. However, many of these have been limited to engineering prototypes. The recent commercial availability of affordable, portable tactile pin arrays has provided renewed impetus to apply the technology to sensory substitution applications. Lack of access to digitally stored data can prove a significant barrier to blind people seeking careers in numerate disciplines. Tactile displays could potentially provide a discrete and portable means of accessing graphical information in an intuitive non-visual manner. Results are presented from experiments on tactual perception related to understanding graphs and simple visualisations with a commercially available tactile array device. It was found that subjects could discriminate positive or negative line gradient to within ±4.7° of the horizontal, compared to ±3.25° for results with a force feedback mouse and ±2.42° with a raised paper representation.",
                "year": 2006,
                "publisher": "Signal Processing"
            }
        }
    },
    "Efficient algorithm to detect collision between deformable B-spline surfaces for virtual sculpting": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448508001863/pdfft?md5=2803ff4450dc28e2c5d5092d247f9460&pid=1-s2.0-S0010448508001863-main.pdf",
                "title": "Efficient algorithm to detect collision between deformable B-spline surfaces for virtual sculpting",
                "abstract": "A structured computational framework to efficiently detect collision between deformable freeform shapes in a VR environment is proposed in this paper. The deformable shape is represented as a B-spline surface and no assumption is made with regard to the degree of the surface, extent of deformation or virtual material properties. The proposed technique calculates and stores transformation matrices and their inverse during preprocessing, which are then used to discretize the B-spline surfaces. It exploits the fact that the transformation matrices for calculating discrete points on the B-spline are independent of the position of control points and therefore can be pre-calculated. The intensity of the points is dynamically increased at lower levels of detail as per accuracy requirements, and finally the regions which are likely to undergo collision are tessellated using these points. Spheres are used to determine lower levels of detail which makes this algorithm highly suitable for multiple contact collision detection. The algorithm efficiently calculates tangents and surface normals at these points. The surface normals give inside/outside property to the triangulated region and tangents provide the necessary information to model tangential forces such as frictional forces. The proposed algorithm is especially suitable for sculpting during concept design and its validation before exchanging information with existing CAD softwares for detailed design. A comparison based on the worst case scenario is presented to prove the efficiency of the present algorithm.",
                "year": 2008,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "An exploratory study on the use of digital sculpting in conceptual product design": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X12000610/pdfft?md5=b600901afa01295b539843aa86f7a475&pid=1-s2.0-S0142694X12000610-main.pdf",
                "title": "An exploratory study on the use of digital sculpting in conceptual product design",
                "abstract": "The product design process involves intensive manipulation of graphical data, from pencil sketches to CAD files. The use of graphic software is common among professionals in this field. Despite this, the conceptual design stage remains intensive in paper and pencil work, as CAD systems are still too rigid to allow a creative production of concepts. In this paper the use of digital sculpting software is proposed as a way of producing 3D sketches in the early stages of the process. An experiment is conducted to determine to which extent 3D sculpt sketches can be considered as a suitable tool for conceptual design. The results show a better performance of 2D drawings, but support the complementary use of digital sculpting.",
                "year": 2013,
                "publisher": "Design Studies"
            }
        }
    },
    "Context-adaptive navigation of 3D model collections": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849318301961/pdfft?md5=f15f828ec84dfdd1c8dbde83b9fd65c4&pid=1-s2.0-S0097849318301961-main.pdf",
                "title": "Context-adaptive navigation of 3D model collections",
                "abstract": "When reasoning about similarity in a collection of objects with heterogeneous qualities, there are several aspects of interest that can be followed to explore the collection. Indeed, the notion of similarity among 3D models is not only grounded on the geometric shape but also, for instance, on the style, material, color, decorations, common parts. These are all important factors that concur to the concept of similarity. Search engines for visual content are expected to address similarity assessment in collections, providing a higher degree of flexibility with respect to the traditional 3D object retrieval operations. In this work, we describe the design and functioning of a search engine working on multiple factors and discuss the results on a number of collections, which challenge existing 3D object retrieval engines.",
                "year": 2019,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "ASIST: Annotation-free synthetic instance segmentation and tracking by adversarial simulations": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S001048252100295X/pdfft?md5=04b31d7370ccdc6d09d9a81f3a7ba157&pid=1-s2.0-S001048252100295X-main.pdf",
                "title": "ASIST: Annotation-free synthetic instance segmentation and tracking by adversarial simulations",
                "abstract": "Background\n\nThe quantitative analysis of microscope videos often requires instance segmentation and tracking of cellular and subcellular objects. The traditional method consists of two stages: (1) performing instance object segmentation of each frame, and (2) associating objects frame-by-frame. Recently, pixel-embedding-based deep learning approaches these two steps simultaneously as a single stage holistic solution. Pixel-embedding-based learning forces similar feature representation of pixels from the same object, while maximizing the difference of feature representations from different objects. However, such deep learning methods require consistent annotations not only spatially (for segmentation), but also temporally (for tracking). In computer vision, annotated training data with consistent segmentation and tracking is resource intensive, the severity of which is multiplied in microscopy imaging due to (1) dense objects (e.g., overlapping or touching), and (2) high dynamics (e.g., irregular motion and mitosis). Adversarial simulations have provided successful solutions to alleviate the lack of such annotations in dynamics scenes in computer vision, such as using simulated environments (e.g., computer games) to train real-world self-driving systems.",
                "year": 2021,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Geographic Object-Based Image Analysis – Towards a new paradigm": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0924271613002220/pdfft?md5=8bda8ea91a7e57ca60490ef3d5c6b2e0&pid=1-s2.0-S0924271613002220-main.pdf",
                "title": "Geographic Object-Based Image Analysis – Towards a new paradigm",
                "abstract": "The amount of scientific literature on (Geographic) Object-based Image Analysis – GEOBIA has been and still is sharply increasing. These approaches to analysing imagery have antecedents in earlier research on image segmentation and use GIS-like spatial analysis within classification and feature extraction approaches. This article investigates these development and its implications and asks whether or not this is a new paradigm in remote sensing and Geographic Information Science (GIScience). We first discuss several limitations of prevailing per-pixel methods when applied to high resolution images. Then we explore the paradigm concept developed by Kuhn (1962) and discuss whether GEOBIA can be regarded as a paradigm according to this definition. We crystallize core concepts of GEOBIA, including the role of objects, of ontologies and the multiplicity of scales and we discuss how these conceptual developments support important methods in remote sensing such as change detection and accuracy assessment. The ramifications of the different theoretical foundations between the ‘per-pixel paradigm’ and GEOBIA are analysed, as are some of the challenges along this path from pixels, to objects, to geo-intelligence. Based on several paradigm indications as defined by Kuhn and based on an analysis of peer-reviewed scientific literature we conclude that GEOBIA is a new and evolving paradigm.",
                "year": 2014,
                "publisher": "ISPRS Journal of Photogrammetry and Remote Sensing"
            }
        }
    },
    "An autowave based methodology for deformable object simulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448506000637/pdfft?md5=0239d882a3fc7b4d0576136084ab51f7&pid=1-s2.0-S0010448506000637-main.pdf",
                "title": "An autowave based methodology for deformable object simulation",
                "abstract": "This paper presents a new methodology for deformable object simulation by drawing an analogy between autowaves and elastic deformation. The potential energy stored in an elastic body as a result of a deformation caused by an external force is propagated among mass points by non-linear autowaves. The novelty of the methodology is that autowave techniques are established to describe the potential energy distribution of a deformation for extrapolating internal elastic forces, and non-linear material properties are modelled with non-linear autowaves other than geometric non-linearity. A haptic virtual reality system has been developed for deformation simulation with force feedback. The proposed methodology not only deals with large-range deformations, but also accommodates isotropic, anisotropic and inhomogeneous materials by simply modifying diffusion coefficients.",
                "year": 2006,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "A discriminative feature set in the fast phase of spikes for sorting oligo-unit discharges of arterial baroreceptors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231218309044/pdfft?md5=e3bf3e5cec82e4159b49e199f257b871&pid=1-s2.0-S0925231218309044-main.pdf",
                "title": "A discriminative feature set in the fast phase of spikes for sorting oligo-unit discharges of arterial baroreceptors",
                "abstract": "The present study was aimed to establish a simple and robust protocol that was more suitable for sorting discharges of the arterial baroreceptor. Oligo-unit (≤ 5) baroreceptor discharges were recorded in vitro from fine filaments of the rabbit carotid sinus nerve. Different time windows, covering the fast phase only or both the fast and slow phases of the spike were used to extract spike event data for sorting. Three measurements focusing on the fast phase of spikes—the maximum slope in the ascending limb from the half amplitude to the peak, the peak amplitude, and the width of the spike at the half amplitude—were selected as a feature set. The performance of this measurement-based analysis with subsequent K-means algorithm (MBAKM) in sorting oligo-unit discharges was compared with the performance of principal component analysis followed by K-means (PCAKM) and template matching (TM). The present study proved that: (1) MBAKM was more discriminative with less intervention than PCAKM and TM in determining the number of clusters and cluster attributions of spikes; (2) there was a higher consistency (larger intersection set) among the three algorithms with narrow windows of 0.45–0.65 ms than with 1.45 ms window. This study suggested that discriminative features were embodied in the fast phase of spikes and the oligo-unit discharges of baroreceptors could be sorted more robustly and accurately with less intervention by MBAKM than by PCAKM and TM. MBAKM with narrow time window would be promising in further studying baroreceptors and multiunit discharges from other neural structures.",
                "year": 2018,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Which parts of the road guide obstacle avoidance? Quantifying the driver's risk field": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687018307373/pdfft?md5=c959bbad278d49a4a0efb4fd868cd642&pid=1-s2.0-S0003687018307373-main.pdf",
                "title": "Which parts of the road guide obstacle avoidance? Quantifying the driver's risk field",
                "abstract": "Gibson and Crooks (1938) argued that a ‘field of safe travel’ could qualitatively explain drivers' steering behavior on straights, curved roads, and while avoiding obstacles. This study aims to quantitatively explain driver behavior while avoiding obstacles on a straight road, and quantify the ‘Driver's Risk Field’ (DRF). In a fixed-based driving simulator, 77 (7 longitudinal and 11 lateral) positions of the obstacles were used to quantify the subjectively perceived and objectively (maximum absolute steering angle) measured DRF for eight participants. The subjective response was a numerical answer to the question “How much steering do you think you need at this moment in time?” The results show that the propagation of the width of the DRF, along the longitudinal distance, resembled an hourglass shape, and all participants responded to obstacles that were placed beyond the width of the car. This implies that the Driver's Risk Field is wider than the car width.",
                "year": 2020,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Haptic rendering based on spatial run-length encoding": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584503001066/pdfft?md5=5680bb990ebf232a6e8282edadd97858&pid=1-s2.0-S0736584503001066-main.pdf",
                "title": "Haptic rendering based on spatial run-length encoding",
                "abstract": "In this paper, an extendable volumetric representation based on run-lengths called spatial run-length encoding (S-RLE) is presented. The S-RLE representation is developed for a haptic shape modeling system that is based on simulated machining processes. In the system, shape modeling is simulated as virtual material removal processes similar to machining processes with volume-based haptic rendering. The object and the tools are represented by S-RLE. The data structure of S-RLE consists of two cross-referenced databases: one is a stack of lists in geometrical domain, recording the runs describing the space occupation of the object; the other is a table in physical domain, describing the physical properties of each element. The latter is extendable to include more diverse physical properties such as parts composed of heterogeneous materials. Algorithms for geometric operations and haptic rendering based on S-RLE are developed. The proposed S-RLE data structure has the features of efficient memory usage, quick collision detection, inherent representation for heterogeneous objects, and fast visual rendering.",
                "year": 2004,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Sense-through-wall human detection based on UWB radar sensors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0165168415003242/pdfft?md5=1a869ac9725a2aeb21254aa3810b363c&pid=1-s2.0-S0165168415003242-main.pdf",
                "title": "Sense-through-wall human detection based on UWB radar sensors",
                "abstract": "In an emergency scenario, such as earthquake, victims can often be trapped in collapsed trenches or buildings. Search and rescue would be greatly simplified if first responders were equipped with a UltraWide Band (UWB) Radar sensor which has sense-through wall capability. Motivated by this challenge, we study sense-through-wall human detection based on UWB radar sensors. We observed that a Discrete Fourier Transform (DFT)-based approach could not work well in scenarios where signal attenuation is high, and the DFT-based approach has high computational load, which makes it difficult to be used in real-world. We propose a standard deviation (std)-based approach to sense-through wall and sense-through wooden door human detection, and make analysis on detection threshold selection. Our approach is very simple to be implemented, but it has high accuracy. It can achieve perfect detection (no detection error) with appropriate detection threshold.",
                "year": 2016,
                "publisher": "Signal Processing"
            }
        }
    },
    "An “optical flow” method based on pressure sensors data for quantification of Parkinson's disease characteristics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S174680942200831X/pdfft?md5=3e683cd64d821faafaf3e01e531894f5&pid=1-s2.0-S174680942200831X-main.pdf",
                "title": "An “optical flow” method based on pressure sensors data for quantification of Parkinson's disease characteristics",
                "abstract": "Most patients with Parkinson's disease (PD) have different degrees of movement disorders, and effective gait analysis is beneficial to find the abnormal gait of patients to achieve the diagnosis of patients with Parkinson's disease. In this paper, an “optical flow” method based on Vertical Ground Reaction Force (VGRF) time series data is proposed, the algorithm takes the force point as the detection target, regards the transfer of the force point on the sole as the optical flow, and combines the optical flow of the multi-level force points to form optic flow field. To quantify the optical flow direction, the direction histogram is used to extract the direction information, and the symmetry information is further extracted according to the optical flow difference between the left and right foot, which not only realizes the fusion of multi-sensors but also extracts highly interpretable motion information. Finally, the model is trained by combining optical flow features and spatial-temporal features. The results show that the proposed model has better performance in gait detection of Parkinson's disease patients than several other state-of-the-art methods previously studied. Among them, the accuracy of Parkinson's disease diagnosis reached 93.3%, and the accuracy of severity assessment of Parkinson's disease reached 91.4%.",
                "year": 2023,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Towards context aware data fusion: Modeling and integration of situationally qualified human observations to manage uncertainty in a hard+soft fusion process": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253513000675/pdfft?md5=93d1fa4d6ac7a81d0094bf0500c00d7e&pid=1-s2.0-S1566253513000675-main.pdf",
                "title": "Towards context aware data fusion: Modeling and integration of situationally qualified human observations to manage uncertainty in a hard+soft fusion process",
                "abstract": "This paper presents a framework for characterizing errors associated with different categories of human observation combined with a method for integrating these into a hard + soft data fusion system. Error characteristics of human observers (often referred to as soft data sensors) have typically been artificially generated and lack contextual considerations that in a real-world application can drastically change the accuracy and precision of these characteristics. The proposed framework and method relies on error values that change based upon known and unknown states of qualifying variables empirically shown to affect observation accuracy under different contexts. This approach allows fusion systems to perform uncertainty alignment on data coming from human observers. The preprocessed data yields a more complete and reliable situation assessment when it is processed by data association and stochastic graph matching algorithms. This paper also provides an approach and results of initial validation testing of the proposed methodology. The testing approach leverages error characterization models for several exemplar categories of observation in combination with simulated synthetic data. Results have shown significant performance improvements with respect to both data association and situation assessment fusion processes with an average F-measure improvement of 0.16 and 0.20 for data association and situation assessment respectively. These F-measure improvements are representative of fewer incorrect and missed associations and fewer graph matching results, which then must be considered by human analysts. These benefits are expected to translate into a reduction of the overall cognitive workload facing human analysts in situations where they are tasked with developing and maintaining situational awareness.",
                "year": 2015,
                "publisher": "Information Fusion"
            }
        }
    },
    "Enhanced LiteHRNet based sheep weight estimation using RGB-D images": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169923000558/pdfft?md5=a36024284f21683f03506648c151a9e4&pid=1-s2.0-S0168169923000558-main.pdf",
                "title": "Enhanced LiteHRNet based sheep weight estimation using RGB-D images",
                "abstract": "Sheep farming is a strategic sector of livestock husbandry, and its production has large market demand in many countries. The live weight of sheep provides important information about the health state and the time point for marketing. Manual weighing sheep is time-consuming for farmers even with the help of a ground scale. With the development of Artificial Intelligence (AI) and smart sensors, non-contact sheep weighing methods have gradually been used to estimate weight. However, the performance of prior studies tends to degenerate with varying postures and light conditions in practical natural environments. In this study, we propose a sheep live weight estimation approach based on LiteHRNet (a Lightweight High-Resolution Network) using RGB-D images. Class Activation Mapping (CAM) guided the design of efficient network heads embracing visual explanation and applicability in practical natural environments. Experiments are conducted on our challenging dataset (of 726 sheep RGB-D images, weight range between 19.5 to 94 kg). Comparative experiment results reveal that the lightweight Convolutional Neural Network (CNN) model trained on RGB-D images can reach an acceptable weight estimation result, Mean Average Percentage Error (MAPE) is 14.605% (95% confidence interval: [13.821%, 15.390%], t test) with only 1.06M parameters. Our works can be viewed as preliminary work that confirms the ability to use lightweight CNNs for sheep weight estimation on RGB-D data. The results of this study are potential to develop an embedded device to automatically estimate sheep live weight and would contribute to the development of precision livestock farming.",
                "year": 2023,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Intelligent virtual manufacturing cell formation in cloud-based design and manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197618301751/pdfft?md5=562622709ab9cb29fc96bbb5a9286988&pid=1-s2.0-S0952197618301751-main.pdf",
                "title": "Intelligent virtual manufacturing cell formation in cloud-based design and manufacturing",
                "abstract": "Cloud-based design and manufacturing (CBDM) can presumably stimulate greater intelligence in cloud-based models. This paper assumes that cloud-based design for cellular manufacturing can be referred to as a multiscale, uncertain, and dynamic service-oriented network where a set of CAD parts, modelled by set of features, can be manufactured in intelligent virtual manufacturing cells under certain constraints. Using the concepts of the holon and the attractor, integrating the uncertainty in the modelling of part design and part–manufacturing network, an approach to address intelligent virtual manufacturing cell formation in CBDM is proposed. The powerful role of the CAD features is exploited to organize and integrate part design and part–manufacturing engineering knowledge. Intelligent manufacturing features, modelled as fuzzy agents, are recognized in CAD part models and the distributed capabilities of machines in cloud manufacturing are evaluated through mobile intelligent agents. Then, intelligent virtual manufacturing cells, with holonic structure, emerge from the interactions of fuzzy machine holon agents and fuzzy part holon agents with holon agent attractors. The concepts of the holon and the attractor allow multi-scale cell formation with holonic structure: “intelligent virtual manufacturing cells within an intelligent virtual manufacturing cell”. These fuzzy cell holons overcome the distinction continuous–discontinuous of traditional cell design formation problem.",
                "year": 2018,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Analysing social-ecological interactions in disease control: An agent-based model on farmers’ decision making and potato late blight dynamics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815218305991/pdfft?md5=f93d35bb2be81057f4e46352ebaa728d&pid=1-s2.0-S1364815218305991-main.pdf",
                "title": "Analysing social-ecological interactions in disease control: An agent-based model on farmers’ decision making and potato late blight dynamics",
                "abstract": "In this paper we focus on potato late blight control in the Netherlands to analyse the social-ecological interactions between farmer behaviour and disease dynamics. An agent-based model was developed to analyse the use of crop resistance for sustainable disease control. The framework on farmers’ decision-making was based on a behavioural theory and supported by data from literature and interviews with Dutch potato farmers. This framework was integrated with a previously developed spatially explicit model on potato late blight dynamics. We assumed a scenario where a new resistant potato variety was introduced to the market. The model reproduced a boom-and-bust cycle: the percentage of farmers growing the resistant variety increased until resistance breakdown occurred by emergence and spread of a virulent strain, and in response farmers switched to other potato varieties and management strategies. Several factors and processes were identified that could contribute to the development of sustainable disease management strategies.",
                "year": 2019,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Monitoring elderly people at home with temporal Case-Based Reasoning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705117303477/pdfft?md5=37a88c282cb721a4dcdb3a03ae3bf2f7&pid=1-s2.0-S0950705117303477-main.pdf",
                "title": "Monitoring elderly people at home with temporal Case-Based Reasoning",
                "abstract": "This paper presents a study of why and how Case-Based Reasoning (CBR) can be used in the long term to help elderly people living alone in a Smart Home. The work focuses on the need to manage the temporal dimension and how the system must be maintained.\n\nThe proposal involves the integration of a CBR system in a commercial Smart Home architecture that includes sensors, data communication and data integration. The CBR system analyses the daily activity at home as temporal event sequences, using temporal edit distance to identify the most similar cases. Most common Case-Based Maintenance (CBM) algorithms adapted to the temporal problem (t-CNN, t-RENN, t-ICF, t-DROP1 and t-RCFP) are used to reduce the number of cases in the case base in order to contribute to its long term maintenance.\n\nThe experiments carried out analyse the effect of different temporal CBM algorithms in common risk scenarios (waking up during the night, falls and falls with loss of consciousness). Data experiments are generated synthetically based on real behaviour patterns of 12 hours’ and 24 hours’ duration. Algorithms are compared using a paired t-test analysis. The results show that the algorithms t-CNN and t-DROP1 are able to create case-bases that statistically present the same average results as the original case-base but with a 10–20% in size. Algorithms t-ICF, t-RCFP and t-RENN can build similar case-bases to the original with a 10–50% size reduction, although they are not totally equivalent since they have significantly different average results than the original case-base. Finally, algorithm t-RENN does not significantly reduce the size of the case-base because it commonly deletes cases describing abnormal scenarios.\n\nWe demonstrate that the proposed temporal CBR system is able to detect the different proposed risk scenarios when there is a large number of cases. That is, the CBR systems are useful in the long term. Experiments indicate that the temporal CBM algorithms analysed are able to reduce case-bases successfully to detect abnormal scenarios. However, success in creating a maintained case-base equivalent to the original depends on the number of cases.",
                "year": 2017,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "A framework for user experience, needs and affordances": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X13000756/pdfft?md5=59ad63db6717d2ba16c97e6306d08811&pid=1-s2.0-S0142694X13000756-main.pdf",
                "title": "A framework for user experience, needs and affordances",
                "abstract": "As design of interactive products started to address the whole user experience, User Experience (UX) became an established field of research. Nevertheless UX design presents some risks, such as providing users with experiences that they do not wish. Furthermore, UX methodologies lack prescriptive tools for guiding designers. This paper establishes a link between UX research and Affordance theory and postulates the concept of Experience Affordances. Affordances represent a first step toward the development of prescriptive methods and help preventing designer from imposing experiences to users. Thus, a framework for describing product experience in terms of affordances is exposed and discussed by analysing exemplary products. In concluding the paper, the implications of the framework are presented.",
                "year": 2014,
                "publisher": "Design Studies"
            }
        }
    },
    "A level-set based variational method for design and optimization of heterogeneous objects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S001044850400137X/pdfft?md5=e61220d4431f7edb762f13b77bcecf00&pid=1-s2.0-S001044850400137X-main.pdf",
                "title": "A level-set based variational method for design and optimization of heterogeneous objects",
                "abstract": "A heterogeneous object is referred to as a solid object made of different constituent materials. The object is of a finite collection of regions of a set of prescribed material classes of continuously varying material properties. These properties have a discontinuous change across the interface of the material regions. In this paper, we propose a level-set based variational approach for the design of this class of heterogeneous objects. Central to the approach is a variational framework for a well-posed formulation of the design problem. In particular, we adapt the Mumford–Shah model which specifies that any point of the object belongs to either of two types: inside a material region of a well-defined gradient or on the boundary edges and surfaces of discontinuities. Furthermore, the set of discontinuities is represented implicitly, using a multi-phase level set model. This level-set based variational approach yields a computational system of coupled geometric evolution and diffusion partial differential equations. Promising features of the proposed method include strong regularity in the problem formulation and inherent capabilities of geometric and material modeling, yielding a common framework for optimization of the heterogeneous objects that incorporates dimension, shape, topology, and material properties. The proposed method is illustrated with several 2D examples of optimal design of multi-material structures and materials.",
                "year": 2005,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "A judgment-based model for usability evaluating of interactive systems using fuzzy Multi Factors Evaluation (MFE)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494622000023/pdfft?md5=afc791f55b3a8073ad709667a06450fc&pid=1-s2.0-S1568494622000023-main.pdf",
                "title": "A judgment-based model for usability evaluating of interactive systems using fuzzy Multi Factors Evaluation (MFE)",
                "abstract": "The study aimed to propose a judgment-based evaluation model for usability evaluating of interactive systems. Human judgment is associated with uncertainty and gray information. We used the fuzzy technique for integration, summarization, and distance calculation of quality value judgment. The proposed model is an integrated fuzzy Multi Factors Evaluation (MFE) model based on experts’ judgments in HCI, ISPD, and AMLMs. We provided a Fuzzy Inference System (FIS) for scoring usability evaluation metrics in different interactive systems. A multi-model interactive system is implemented for experimental testing of the model. The achieved results from the proposed model and experimental tests are compared using statistical correlation tests. The results show the ability of the proposed model for usability evaluation of interactive systems without the need for conducting empirical tests. It is concluded that applying a dataset in a neuro-FIS and training system cause to produce more than a hundred effective rules. The findings indicate that the proposed model can be applied for interactive system evaluation, informative evaluation, and complex empirical tests. Future studies may improve the FIS with the integration of artificial neural networks.",
                "year": 2022,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Learning of control in a neural architecture of grounded language processing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041708000508/pdfft?md5=4b2d9d3901b06caeca27e58c5d2b9e19&pid=1-s2.0-S1389041708000508-main.pdf",
                "title": "Learning of control in a neural architecture of grounded language processing",
                "abstract": "Human cognition is characterized by three important features: productivity, dynamics and grounding. These features can be integrated in a neural architecture of language processing. The representations in this architecture always remain “in situ”, because they are grounded in perception, action, emotion, associations and (semantic) relations. The neural architecture shows how these representations can be combined in a productive manner, and how dynamics influences this process. The constraints that each of these features impose on each other result in an architecture in which local and global aspects interact in processing and learning. The architecture consists of neural “binding” mechanisms that produce (novel) sentence structures on the fly. Here, we discuss how the control of this binding process can be learned. We trained a feedforward network (FFN) for this task. The results show that information from the architecture is needed as input to learn control of binding. Thus, the control system is recurrent. We show that this recurrent system can learn control of binding for basic (but recursive) sentence structures. After learning, the binding process behaves well on a series of test sentences, including sentences with (unlimited) embeddings. However, for some of these sentences, difficulties arise due to dynamical binding conflicts in the architecture. We also discuss and illustrate the potential influence that the dynamics in the architecture could have on the binding process.",
                "year": 2010,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Seeing things: consumer response to the visual domain in product design": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X04000225/pdfft?md5=59134e800ffe2e762beb16c3b33f67b8&pid=1-s2.0-S0142694X04000225-main.pdf",
                "title": "Seeing things: consumer response to the visual domain in product design",
                "abstract": "This paper discusses consumer response to product visual form within the context of an integrated conceptual framework. Emphasis is placed on the aesthetic, semantic and symbolic aspects of cognitive response to design. The accompanying affective and behavioural responses are also discussed and the interaction between cognitive and affective response is considered. All aspects of response are presented as the final stage in a process of communication between the design team and the consumer. The role of external visual references is examined and the effects of moderating influences at each stage in the process of communication are discussed. In particular, the personal, situational and cultural factors that moderate response are considered. In concluding the paper, implications for design practice and design research are presented.",
                "year": 2004,
                "publisher": "Design Studies"
            }
        }
    },
    "Uniform offsetting of polygonal model based on Layered Depth-Normal Images": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448510001697/pdfft?md5=283eb76e9db7f3f3d3906d038e18f317&pid=1-s2.0-S0010448510001697-main.pdf",
                "title": "Uniform offsetting of polygonal model based on Layered Depth-Normal Images",
                "abstract": "Uniform offsetting is an important geometric operation for computer-aided design and manufacturing (CAD/CAM) applications such as rapid prototyping, NC machining, coordinate measuring machines, robot collision avoidance, and Hausdorff error calculation. We present a novel method for offsetting (grown and shrunk) a solid model by an arbitrary distance r. First, offset polygons are directly computed for each face, edge, and vertex of an input solid model. The computed polygonal meshes form a continuous boundary; however, such a boundary is invalid since there exist meshes that are closer to the original model than the given distance r as well as self-intersections. Based on the problematic polygonal meshes, we construct a well-structured point-based model, Layered Depth-Normal Image (LDNI), in three orthogonal directions. The accuracy of the generated point-based model can be controlled by setting the tessellation and sampling rates during the construction process. We then process all the sampling points in the model by using a set of point filters to delete all the invalid points. Based on the remaining points, we construct a two-manifold polygonal contour as the resulting offset boundary. Our method is general, simple and efficient. We report experimental results on a variety of CAD models and discuss various applications of the developed uniform offsetting method.",
                "year": 2011,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "A complex event processing framework for an adaptive language learning system": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X17312815/pdfft?md5=f2d704b7ab86dce30bd066aef0786ad3&pid=1-s2.0-S0167739X17312815-main.pdf",
                "title": "A complex event processing framework for an adaptive language learning system",
                "abstract": "Ubiquitous learning applications and worldwide educational websites such as MOOC (Massive Open Online Courses) are rapidly producing large volume of user data. Current delayed analysis processing in adaptive language learning systems is difficult to cope with the high-speed and high-volume data streams. To overcome this problem, we introduce a complex event processing (CEP) framework for an Adaptive Language Learning System. The system consists of an event adapter sub-system that can process various inputs such as voice, video, text and other interaction events. The event adapter extracts relevant data to support the operational events module, the learning activity events module and the learner knowledge space events module. These three modules in the event hierarchies provide support to the learner adaptation and learner visual analytics modules. In this study, we conduct three simulations to evaluate the initialization time, delay time and throughput of the proposed system. Each of the experiments simulates 1000 learners and 1000 rules and generates 10 events per second. The results indicate the CEP framework is efficient with a processing delay of less than 1.2μs and throughput of 80,000 events per second. We conclude by discussing the study’s implications and suggest ideas for future research.",
                "year": 2019,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Assembly-based conceptual 3D modeling with unlabeled components using probabilistic factor graph": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448515001554/pdfft?md5=1117b948c8051d3d9127abb47d81527c&pid=1-s2.0-S0010448515001554-main.pdf",
                "title": "Assembly-based conceptual 3D modeling with unlabeled components using probabilistic factor graph",
                "abstract": "This work presents a novel and intuitive assembly based 3D modeling interface to support conceptual design exploration activities. In the presented modeling interface, unlabeled segmented components of the objects are assembled to create new 3D models. The development of the interface is motivated by two aspects. First, the focus is on novice users since they stand to gain the most from intuitive interfaces. Second, the intent is on creative reuse of a growing number and variety of 3D models available on vast online repositories like Turbosquid and Trimble 3D warehouse. Specifically, we have devised an automated component suggestion algorithm based on a probabilistic factor graph. This algorithm helps the user to easily browse and select components from a database that are most compatible with the current state of 3D models being assembled. The component suggestion algorithm incorporates various aspects such as shape similarity, repetitions of shapes, and adjacency relationships. Our new suggestive interface overcomes several limitations of traditional CAD interfaces by helping the users to quickly create and explore new conceptual designs. We present results on the conceptual design of several products.",
                "year": 2016,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Reinforcement learning using expectation maximization based guided policy search for stochastic dynamics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231221015794/pdfft?md5=074c2a007a376f72d245fd8b35db6bbb&pid=1-s2.0-S0925231221015794-main.pdf",
                "title": "Reinforcement learning using expectation maximization based guided policy search for stochastic dynamics",
                "abstract": "Guided policy search algorithms have been proven to work with incredible accuracy not only for controlling complicated dynamical systems, but also in learning optimal policies from exploration of various unseen instances. This paper deals with a trajectory optimization problem for an unknown dynamical system subject to measurement noise using expectation maximization and extends it to learning (optimal) policies which have less stochasticity in trajectories because of the higher exploitation efficiency. Theoretical and empirical evidence of learned optimal policies for the new approach is depicted in comparison to some well known baselines which are evaluated on an autonomous system with widely used performance metrics.",
                "year": 2022,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Circular, linear, and curvilinear vection in a large-screen virtual environment with floor projection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849308001477/pdfft?md5=f058aa20dcd4d650871407811e294ea2&pid=1-s2.0-S0097849308001477-main.pdf",
                "title": "Circular, linear, and curvilinear vection in a large-screen virtual environment with floor projection",
                "abstract": "Vection is defined as the compelling sensation of illusory self-motion elicited by a moving sensory, usually visual, stimulus. This paper presents collected introspective data, user discomfort and perceived speed data for the experience of linear, circular, and curvilinear vection in a large-screen, immersive, virtual environment. As a first step we evaluated the effectiveness of a floor projection on the perception of vection for four trajectories: linear forward, linear backward, circular left, and circular right. The floor projection, which considerably extended the field of view, was found to significantly improve the introspective measures of linear, but not circular, vection experienced in a photo-realistic three-dimensional town. In a second study we investigated the differences between 12 different motion trajectories on the illusion of self-motion. In this study we found that linear translations to the left and right are perceived as the least convincing, while linear down is perceived as the most convincing of the linear trajectories. Second, we found that while linear forward vection is not perceived to be very convincing, curvilinear forward vection is reported to be as convincing as circular vection. In a third and final experiment we investigated the perceived speed for all different trajectories and acquired data based on simulator sickness questionnaires to compute a discomfort factor associated with each type of trajectory. Considering our experimental results, we offer suggestions for increasing the sense of self-motion in simulators and VE applications, specifically to increase the number of curvilinear trajectories (as opposed to linear ones) and, if possible, add floor projection in order to improve the illusory sense of self-motion.",
                "year": 2009,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Light curve classification with DistClassiPy: A new distance-based classifier": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Light curve classification with DistClassiPy: A new distance-based classifier",
                "abstract": "The rise of synoptic sky surveys has ushered in an era of big data in time-domain astronomy, making data science and machine learning essential tools for studying celestial objects. While tree-based models (e.g. Random Forests) and deep learning models dominate the field, we explore the use of different distance metrics to aid in the classification of astrophysical objects. We developed DistClassiPy, a new distance metric based classifier. The direct use of distance metrics is unexplored in time-domain astronomy, but distance-based methods can help make classification more interpretable and decrease computational costs. In particular, we applied DistClassiPy to classify light curves of variable stars, comparing the distances between objects of different classes. Using 18 distance metrics on a catalog of 6,000 variable stars across 10 classes, we demonstrate classification and dimensionality reduction. Our classifier meets state-of-the-art performance but has lower computational requirements and improved interpretability. Additionally, DistClassiPy can be tailored to specific objects by identifying the most effective distance metric for that classification. To facilitate broader applications within and beyond astronomy, we have made DistClassiPy open-source and available at https://pypi.org/project/distclassipy/.",
                "year": 2024,
                "publisher": "Astronomy and Computing"
            }
        }
    },
    "Negative effects of gamification in education software: Systematic mapping and practitioner perceptions": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Negative effects of gamification in education software: Systematic mapping and practitioner perceptions",
                "abstract": "Context:\n\nWhile most research shows positive effects of gamification, the focus on its adverse effects is considerably smaller and further understanding of these effects is needed.",
                "year": 2023,
                "publisher": "Information and Software Technology"
            }
        }
    },
    "The impact of atmospheric cues on consumers’ approach and avoidance behavioral intentions in social commerce websites": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563218304795/pdfft?md5=4f08e5be3b50bd7733d60096e00ead5c&pid=1-s2.0-S0747563218304795-main.pdf",
                "title": "The impact of atmospheric cues on consumers’ approach and avoidance behavioral intentions in social commerce websites",
                "abstract": "Approach and avoidance are typical behavioral tendencies of consumers when exposed to a new environment. Drawing on the stimulus-organism-response (S-O-R) framework, this study investigates the effects of atmospheric cues on consumers' approach and avoidance intentions in social commerce environments. We elucidate the interaction mechanisms between atmospheric cues (task, aesthetic and social cues) and consumers' evaluations through the lens of perceived affordances. Then we investigate how three types of perceived affordances (utilitarian, hedonic and connective) influence consumers’ approach and avoidance behavioral intentions. A three-way factorial experiment provides strong evidence that task cues, aesthetic cues and social cues elicit perceived utilitarian, hedonic and connective affordances, respectively. Approach behavioral intentions were affected by all three types of perceived affordances, yet avoidance intentions were only affected by perceived utilitarian and hedonic affordances. This paper discusses both theoretical and practical implications for social commerce research.",
                "year": 2020,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Bio-inspired computer vision: Towards a synergistic approach of artificial and biological vision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314216300339/pdfft?md5=4f11066c75e759ca85c419f162268056&pid=1-s2.0-S1077314216300339-main.pdf",
                "title": "Bio-inspired computer vision: Towards a synergistic approach of artificial and biological vision",
                "abstract": "Studies in biological vision have always been a great source of inspiration for design of computer vision algorithms. In the past, several successful methods were designed with varying degrees of correspondence with biological vision studies, ranging from purely functional inspiration to methods that utilise models that were primarily developed for explaining biological observations. Even though it seems well recognised that computational models of biological vision can help in design of computer vision algorithms, it is a non-trivial exercise for a computer vision researcher to mine relevant information from biological vision literature as very few studies in biology are organised at a task level. In this paper we aim to bridge this gap by providing a computer vision task centric presentation of models primarily originating in biological vision studies. Not only do we revisit some of the main features of biological vision and discuss the foundations of existing computational studies modelling biological vision, but also we consider three classical computer vision tasks from a biological perspective: image sensing, segmentation and optical flow. Using this task-centric approach, we discuss well-known biological functional principles and compare them with approaches taken by computer vision. Based on this comparative analysis of computer and biological vision, we present some recent models in biological vision and highlight a few models that we think are promising for future investigations in computer vision. To this extent, this paper provides new insights and a starting point for investigators interested in the design of biology-based computer vision algorithms and pave a way for much needed interaction between the two communities leading to the development of synergistic models of artificial and biological vision.",
                "year": 2016,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "A construction method of visual conceptual scenario for hydrological conceptual modeling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815221002322/pdfft?md5=71b8a43dd9b02986b8e2e765e620187c&pid=1-s2.0-S1364815221002322-main.pdf",
                "title": "A construction method of visual conceptual scenario for hydrological conceptual modeling",
                "abstract": "With the increasing complexity of hydrological systems, hydrological modeling by modelers from different research areas has been regarded as an effective way to solve complex hydrological issues. As the first step of hydrological modeling, conceptual modeling plays an important role in supporting modeling idea communication among interdisciplinary modelers. Currently, conceptual modeling methods usually show the modeling ideas by using the block-based diagrams. However, further research need to be explored to express the spatial and temporal distribution of these modeling elements and their dynamic interaction relationships, thus promoting the communication of modeling ideas and reaching a common understanding of the modeling system among modelers. Visual conceptual scenario is the production of hydrological conceptual modeling and can be used to express modelers’ cognition of modeling system. This article proposes a construction method of visual conceptual scenario. The conceptual component that represents the modeling elements, the rules that constrain the scenario construction, and a dynamic interaction method that supports the visualization of dynamic hydrological process are designed. Finally, a study case of identifying the impact of energy base water project on the groundwater is designed to illustrate the feasibility and practicability of the proposed construction method of visual conceptual scenario.",
                "year": 2021,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Global surface reconstruction by purposive control of observer motion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370295000275/pdfft?md5=62e57a10f76fb9deb412bd9442e10d70&pid=1-s2.0-0004370295000275-main.pdf",
                "title": "Global surface reconstruction by purposive control of observer motion",
                "abstract": "What viewpoint-control strategies are important for performing global visual exploration tasks such as searching for specific surface markings, building a global model of an arbitrary object, or recognizing an object? In this paper we consider the task of purposefully controlling the motion of an active, monocular observer in order to recover a global description of a smooth, arbitrarily-shaped object. We formulate global surface reconstruction as the task of controlling the motion of the observer so that the visible rim slides over the maximal, connected, reconstructible surface regions intersecting the visible rim at the initial viewpoint. We show that these regions are bounded by a subset of the visual event curves defined on the surface.\n\nBy studying the epipolar parameterization, we develop two basic strategies that allow reconstruction of a surface region around any point in a reconstructible surface region. These strategies control viewpoint to achieve and maintain a well-defined geometric relationship with the object's surface, rely only on information extracted directly from images (e.g., tangents to the occluding contour), and are simple enough to be performed in real time. We then show how global surface reconstruction can be provably achieved by (1) appropriately integrating these strategies to iteratively “grow” the reconstructed regions, and (2) obeying four simple rules.",
                "year": 1995,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "The roles that artefacts play: technical, social and aesthetic functions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X10000281/pdfft?md5=bc3a0a7938226b13a4026d2d2b0ac5a0&pid=1-s2.0-S0142694X10000281-main.pdf",
                "title": "The roles that artefacts play: technical, social and aesthetic functions",
                "abstract": "The concept of ‘function’ is often employed and sometimes defined in such a way that it only relates to how artefacts can be used to satisfy physical goals (e.g. transportation). Using artefacts to satisfy non-physical goals (e.g. social recognition) is typically described without reference to an artefact’s function. By drawing on the various disciplines that are concerned with function, this article demonstrates that there are many different kinds of function, some of which will account for non-physical uses. By referring to these different kinds of function we can reduce the conceptual distance between physical and non-physical uses. Furthermore, by applying the concept of function to non-physical uses our understanding of such uses can benefit from prior work on function.",
                "year": 2010,
                "publisher": "Design Studies"
            }
        }
    },
    "Examining the connection between preservice teachers’ discussion performance in a mixed reality teaching simulation with their self-reported goals and success in facilitating discussions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2949678024000217/pdfft?md5=3d865ac78e9e914b228d0d169d1e40a0&pid=1-s2.0-S2949678024000217-main.pdf",
                "title": "Examining the connection between preservice teachers’ discussion performance in a mixed reality teaching simulation with their self-reported goals and success in facilitating discussions",
                "abstract": "With the growing use of mixed reality teaching simulations in teacher education there is a need for researchers to examine how preservice teacher (PST) learning can be supported when using these simulations. To address this gap the current study explores how 47 PSTs used an online teaching simulation to facilitate a discussion focused on argumentation with five student avatars in the MursionTM mixed reality simulated classroom environment. We assessed PSTs' performance in the simulation using rubric-level scores assigned by trained raters and then compared the scores to PSTs' survey responses completed after their discussion asking them to self-report their goals for the discussion, how successful they thought they were across five dimensions of facilitating high-quality, argumentation-focused discussions, and their overall perceptions of the mixed reality teaching simulation. Findings suggest that PSTs' understanding of the discussion task's learning goals somewhat predicted their success in facilitating the discussion and that PSTs' self-assessment of their performance was not always consistent with raters' evaluation of the PSTs' performance. In particular, self-assessment was found to be most consistent with raters' evaluations for those PSTs with higher rater-assigned scores and least consistent for those with lower rater-assigned scores. The implications of these findings are as follows: (1) researchers should be cautious in relying on PST self-report of success when engaging in mixed reality teaching simulations, particularly because low performance may be obscured, (2) teacher educators should be aware that reliance on self-report from PSTs likely obscures the need for additional support for exactly those PSTs who need it most, and (3) the field, therefore, should expand efforts to measure PSTs' performance when using mixed reality teaching simulations.",
                "year": 2024,
                "publisher": "Computers & Education: X Reality"
            }
        }
    },
    "Cognitive aspects of tool use": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687005000979/pdfft?md5=58ec90ca317c58e65613fc89b52739ea&pid=1-s2.0-S0003687005000979-main.pdf",
                "title": "Cognitive aspects of tool use",
                "abstract": "Tool use has traditionally been viewed as primarily a physical activity, with little consideration given to the cognitive aspects that might be involved. In this paper, a new approach to considering tool use in terms of Forms of Engagement is presented and discussed. This approach combines notions of schema from cognitive psychology with the idea of task-specific devices to explain psychomotor aspects of using tools. From the perspective of Forms of Engagement, various aspects of craftwork and skilled tool use are considered.",
                "year": 2006,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Improved atmospheric correction and chlorophyll-a remote sensing models for turbid waters in a dusty environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0924271617301788/pdfft?md5=645a4ac8da3dc54cfc27828d06e6aecf&pid=1-s2.0-S0924271617301788-main.pdf",
                "title": "Improved atmospheric correction and chlorophyll-a remote sensing models for turbid waters in a dusty environment",
                "abstract": "This study presents a comprehensive assessment of the performance of the commonly used atmospheric correction models (NIR, SWIR, NIR-SWIR and FM) and ocean color products (OC3 and OC2) derived from MODIS images over the Arabian Gulf, Sea of Oman, and Arabian Sea. The considered atmospheric correction models have been used to derive MODIS normalized water-leaving radiances (nLw), which are compared to in situ water nLw(λ) data collected at different locations by Masdar Institute, United Arab of Emirates, and from AERONET-OC (the ocean color component of the Aerosol Robotic Network) database. From this comparison, the NIR model has been found to be the best performing model among the considered atmospheric correction models, which in turn shows disparity, especially at short wavelengths (400–500 nm) under high aerosol optical depth conditions (AOT (869) > 0.3) and over turbid waters. To reduce the error induced by these factors, a modified model taking into consideration the atmospheric and water turbidity conditions has been proposed. A turbidity index was used to identify the turbid water and a threshold of AOT (869) = 0.3 was used to identify the dusty atmosphere. Despite improved results in the MODIS nLw(λ) using the proposed approach, Chl-a models (OC3 and OC2) show low performance when compared to the in situ Chl-a measurements collected during several field campaigns organized by local, regional and international organizations. This discrepancy might be caused by the improper parametrization of these models or/and the improper selection of bands. Thus, an adaptive power fit algorithm (R2 = 0.95) has been proposed to improve the estimation of Chl-a concentration from 0.07 to 10 mg/m3 by using a new blue/red MODIS band ratio of (443,488)/645 instead of the default band ratio used for OC3(443,488)/547. The selection of this new band ratio (443,488)/645 has been based on using band 645 nm which has been found to represent both water turbidity and algal absorption.",
                "year": 2017,
                "publisher": "ISPRS Journal of Photogrammetry and Remote Sensing"
            }
        }
    },
    "Application of Automated Guided Vehicles in Smart Automated Warehouse Systems: A Survey": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Application of Automated Guided Vehicles in Smart Automated Warehouse Systems: A Survey",
                "abstract": "Automated Guided Vehicles (AGVs) have been introduced into various applications, such as automated warehouse systems, flexible manufacturing systems, and container terminal systems. However, few publications have outlined problems in need of attention in AGV applications comprehensively. In this paper, several key issues and essential models are presented. First, the advantages and disadvantages of centralized and decentralized AGVs systems were compared; second, warehouse layout and operation optimization were introduced, including some omitted areas, such as AGVs fleet size and electrical energy management; third, AGVs scheduling algorithms in chessboard-like environments were analyzed; fourth, the classical route-planning algorithms for single AGV and multiple AGVs were presented, and some Artificial Intelligence (AI)-based decision-making algorithms were reviewed. Furthermore, a novel idea for accelerating route planning by combining Reinforcement Learning (RL) and Dijkstra’s algorithm was presented, and a novel idea of the multi-AGV route-planning method of combining dynamic programming and Monte-Carlo tree search was proposed to reduce the energy cost of systems.",
                "year": 2022,
                "publisher": "CMES - Computer Modeling in Engineering and Sciences"
            }
        }
    },
    "Multi-layer cutting path planning for composite enclosed cavity in additive and subtractive hybrid manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584524001108/pdfft?md5=8bca0c099c39f4def69c3850150ec6c6&pid=1-s2.0-S0736584524001108-main.pdf",
                "title": "Multi-layer cutting path planning for composite enclosed cavity in additive and subtractive hybrid manufacturing",
                "abstract": "Additive and subtractive hybrid manufacturing (ASHM) refers to the hybrid manufacturing process where in-situ subtractive machining (SM) is introduced during additive manufacturing (AM). Its process characteristics dictate the necessity of planning multi-layer cutting paths in ASHM. Currently, the slice-based planning method cannot plan multi-axis cutting paths, and the machining accuracy is difficult to directly control. Meanwhile, the manual layering planning method is inefficient when dealing with complex models. Consequently, this paper presents an innovative automatic planning method for multi-layer, multi-axis, interference-free cutting paths with controllable precision in ASHM of composite enclosed cavity parts. To enhance the ASHM efficiency, criteria for the recognition of hybrid machining features (HMFs) have been defined to identify HMFs within the model. The identification of interference planes during cavity conversion has been achieved, and these interference planes are then utilized as the conversion planes for the ASHM process. Furthermore, a boundary-guided method is employed to automatically plan the overall cutting path for HMFs. According to the G-code standard, the overall cutting paths are then output to the corresponding cutting path file within the height interval of the conversion planes. Through practical machining, it has been demonstrated that the proposed method can significantly enhance the efficiency and automation of the data preparation process in ASHM, while also improving the surface quality and dimensional accuracy of the AM part.",
                "year": 2025,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "An Efficient Disassembly Sequencing Scheme Using the Shell Structure": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An Efficient Disassembly Sequencing Scheme Using the Shell Structure",
                "abstract": "This paper presents a part accessibility-based assembly partitioning scheme for efficient disassembly sequence planning. Accessibility of parts is shown to be an important consideration in product Disassembly Sequence Planning (DSP), which in turn has applications in Assembly Planning (AP), Maintenance Planning (MP), and End-of-Life (EOL) processing. A substantial amount of research exists on both DSP and MP. However, accessibility analysis of parts in an assembly remains largely unexplored. In this paper, we introduce the notion of shell structures associated with 3D CAD assemblies for accessibility analysis of parts. The proposed approach classifies the surface domains of parts as mating or non-mating domains. The non-mating domains are used to construct the shell structure, and the mating domains are used to generate the liaison information of the assembly. The method requires only one-time computation of the shells; throughout the disassembly process, the accessibility information in the shells is updated without any recomputation. Further applications of shell structures, such as in subassembly identification and parallel disassembly, which are important for maintenance planning and end-of-life processing, are also demonstrated. We propose a grid-based method for constructing the shell structure from the tessellated model of the assembly. Finally, results obtained for tessellated models demonstrate the efficacy of the proposed methods. The results show that the accessibility assessment can be done in very little time using the proposed approach. The dynamic update of shells after each disassembly operation results in efficient updating of accessibility information.",
                "year": 2023,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Picture processing: 1984": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X85900969/pdfft?md5=8b70755e11875f8b4c40ffa5ead3c612&pid=1-s2.0-0734189X85900969-main.pdf",
                "title": "Picture processing: 1984",
                "abstract": "This paper presents a bibliography of over 1250 references related to the computer processing of pictorial information, arranged by subject matter. Coverage is restricted, for the most part, to a selected set of U.S. journals and proceedings of specialized meetings. The topics covered include digitization, approximation, and compression; transforms, filtering, enhancement, restoration, and reconstruction; hardware and software; pictorial pattern recognition; feature detection, segmentation, and image analysis; matching and time-varying imagery; shape and pattern; texture; formal models; and three-dimensional scene analysis. No attempt is made to evaluate or summarize the items cited; the purpose is simply to provide a convenient compendium of references.",
                "year": 1985,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "A cloud-based condition monitoring system for fault detection in rotating machines using PROFINET process data": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A cloud-based condition monitoring system for fault detection in rotating machines using PROFINET process data",
                "abstract": "This work presents a methodology for a cloud-based condition monitoring system for fault detection and identification in rotating machines, such as uncoupling, angular and parallel misalignment, by data mining PROFINET network and PROFIdrive profile process data. The proposed methodology involves a new strategy for feature selection of unsupervised data set and employs SVM (Support Vector Machine) and OCSVM (One-Class Support Vector Machine) for operation status classification. The present diagnostic system represents a low-cost solution to the manufacturing process of small and medium enterprises, because it does not require dedicated sensors for fault detection and high featured hardware, and it employs an online cloud-based services. The experimental tests resulted in an accuracy between 87.5% and 100%, and high robustness among different operating conditions. In addition, the proposed feature selection strategy reduced the total execution time by 97.5%.",
                "year": 2021,
                "publisher": "Computers in Industry"
            }
        }
    },
    "Gait-Watch: A Gait-based context-aware authentication system for smart watch via sparse coding": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Gait-Watch: A Gait-based context-aware authentication system for smart watch via sparse coding",
                "abstract": "In recent years, wrist-worn smart devices such as smart wrist band and smart watch have pervaded our everyday life. Under this trend, the security issue of these wearable devices has received considerable attention as these devices usually store various private information. Conventional methods, however, do not provide a good user experience because they either depend on a secret PIN number input or require an explicit user authentication process. In this paper, we present Gait-watch, a context-aware authentication system for smart watch based on gait recognition. We address the problem of recognizing the user under various walking activities (e.g., walking normally, walking upstairs and walking with calling the phone), and propose a feature extraction method from gait signals to improve recognition accuracy. Extensive evaluations show that Gait-watchimproves recognition accuracy by up to 30.2% by leveraging the activity information, and can achieve 3.5% Equal Error Rate (EER). We also report a user study to demonstrate that Gait-watchcan accurately authenticate the user in real-world scenarios and require low system cost.",
                "year": 2020,
                "publisher": "Ad Hoc Networks"
            }
        }
    },
    "A high abstraction level approach for detecting feature interactions between telecommunication services": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157812000389/pdfft?md5=d86d4413340e72ec0d9582ad2c21c1bf&pid=1-s2.0-S1319157812000389-main.pdf",
                "title": "A high abstraction level approach for detecting feature interactions between telecommunication services",
                "abstract": "When several telecommunication services are running at the same time, undesirable behaviors may arise, which are commonly called feature interactions. Several methods have been developed for detecting and resolving feature interactions. However, most of these methods are based on detailed models of services, which make them suffer from state space explosion. Moreover, different telecommunication operators cannot cooperate to manage feature interactions by exchanging detailed service models because this violates the confidentiality principle. Our work is a part of the few attempts to develop feature interaction detection methods targeting to avoid or reduce significantly state space explosion. In order to reach this objective, we first develop a so called Cause–Restrict language to model subscribers of telecommunication services at a very high abstraction level. A Cause–Restrict model of a subscriber provides information such as: what is the cause of what, and what restricts (or forbids) what, and specifies coarsely the frequency of each operation “cause” or “restrict” by “always” or “sometimes”. Then, we develop a method that detects feature interactions between telecommunication services modeled in the Cause–Restrict language. We demonstrate the applicability of our approach by modeling several services and detecting several feature interactions between them. New feature interactions have been detected by our approach.",
                "year": 2013,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "The COVID-19 pandemic and deepening digital inequalities in China": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0308596123001556/pdfft?md5=2de86815fdaef5eddaa14c2a713cb077&pid=1-s2.0-S0308596123001556-main.pdf",
                "title": "The COVID-19 pandemic and deepening digital inequalities in China",
                "abstract": "As Internet usage reshapes our societies, digital inequalities have increased over the past few decades. During the COVID-19 pandemic, many countries accelerated their digital transformation processes, and it is widely believed the COVID-19 pandemic has deepened existing inequalities in the digital realm. Yet, few studies have empirically examined whether digital inequalities in the labor market increased during the pandemic. This analysis studies how the COVID-19 pandemic affected Chinese workers' Internet usage and how this influence varied across socioeconomic groups. By using the ordered probit model and leveraging the most recent data from the China Family Panel Studies and the Johns Hopkins Coronavirus Resource Center, we find that the pandemic significantly increased the overall level of Internet usage in the country, and the mediating effects of the perceived importance of the Internet and access to the Internet are confirmed. As Internet usage increased, digital inequalities in China's labor market deepened, especially among young and wealthy workers with high social status in urban areas, while older and poorer workers in rural areas benefited less from this new ‘digital wave.’ Moreover, during the pandemic, Internet usage increased among employees working in state-owned enterprises (SOEs), which suggests a growing digital inequality gap between SOEs and other sectors. Following a series of robustness tests, our research findings remain valid. We propose a policy redesign that embodies a comprehensive long-term vision and guarantees raising the levels of Internet usage for socially and economically disadvantaged groups in China.",
                "year": 2023,
                "publisher": "Telecommunications Policy"
            }
        }
    },
    "Fake news believability: The effects of political beliefs and espoused cultural values": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720622001537/pdfft?md5=ab2eb24bec9c28e58e777912beef71e5&pid=1-s2.0-S0378720622001537-main.pdf",
                "title": "Fake news believability: The effects of political beliefs and espoused cultural values",
                "abstract": "Fake news has led to a polarized society as evidenced by diametrically opposed perceptions of and reactions to global events such as the Coronavirus Disease 2019 (COVID-19) pandemic and presidential campaigns. Popular press has linked individuals’ political beliefs and cultural values to the extent to which they believe in false content shared on social networking sites (SNS). However, sweeping generalizations run the risk of helping exacerbate divisiveness in already polarized societies. This study examines the effects of individuals’ political beliefs and espoused cultural values on fake news believability using a repeated-measures design (that exposes individuals to a variety of fake news scenarios). Results from online questionnaire-based survey data collected from participants in the US and India help confirm that conservative individuals tend to exhibit increasing fake news believability and show that collectivists tend to do the same. This study advances knowledge on characteristics that make individuals more susceptible to lending credence to fake news. In addition, this study explores the influence exerted by control variables (i.e., age, sex, and Internet usage). Findings are used to provide implications for theory as well as actionable insights.",
                "year": 2023,
                "publisher": "Information & Management"
            }
        }
    },
    "Agent-based modeling of consumer decision making process based on power distance and personality": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705111000888/pdfft?md5=2b200fa0d3dff05449f432c790d8686f&pid=1-s2.0-S0950705111000888-main.pdf",
                "title": "Agent-based modeling of consumer decision making process based on power distance and personality",
                "abstract": "Simulating consumer decision making processes involves different disciplines such as: sociology, social psychology, marketing, and computer science. In this paper, we propose an agent-based conceptual and computational model of consumer decision-making based on culture, personality and human needs. It serves as a model for individual behavior in models that investigate system-level resulting behavior. Theoretical concepts operationalized in the model are the Power Distance dimension of Hofstede’s model of national culture; Extroversion, Agreeableness and Openness of Costa and McCrae’s five-factor model of personality, and social status and social responsibility needs. These factors are used to formulate the utility function, process and update the agent state, need recognition and action estimation modules of the consumer decision process. The model was validated against data on culture, personality, wealth and car purchasing from eleven European countries. It produces believable results for the differences of consumer purchasing across eleven European countries.",
                "year": 2011,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Fast computation with neural oscillators": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231205002687/pdfft?md5=b72d66079966c7739c4c4f4b3709d1f1&pid=1-s2.0-S0925231205002687-main.pdf",
                "title": "Fast computation with neural oscillators",
                "abstract": "This paper studies new spike-based models for winner-take-all computation and coincidence detection. In both cases, fast convergence is achieved independent of initial conditions, and network complexity is linear in the number of inputs. Fully distributed versions can be modelled based on groups of interneurons connected through electrical synapses.",
                "year": 2006,
                "publisher": "Neurocomputing"
            }
        }
    },
    "The electronic mirror: Human-computer interaction and change in self-appraisals": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563297000319/pdfft?md5=02864c07439f43717277e0c485a8c50b&pid=1-s2.0-S0747563297000319-main.pdf",
                "title": "The electronic mirror: Human-computer interaction and change in self-appraisals",
                "abstract": "As computers become increasingly powerful and complex, software designers are employing anthropomorphism to enhance the usability of computer interfaces (i.e., “user-centered” design). The potential for implementing a social mode of interface behavior, however, can only be realized through understanding the role anthropomorphism plays in modifying the behavior and perceptions of users. The present study compares human-like versus machine-like interactional styles of computer interfaces, testing hypotheses that evaluative feedback conveyed through a human-like interface will have greater impact on individuals' self-appraisals. College students received experimentally manipulated positive or negative computerized feedback in response to their performance on a purported “psychic ability” task. In general, computer feedback had considerable impact upon reflected appraisals (participants' perceptions of the computer's evaluations of their performance and ability) as well as upon their self-appraisals of performance and ability. Reflected appraisals were more influenced by computer feedback than were self-appraisals. Human-like and machine-like interface styles did not have significantly different effects.",
                "year": 1998,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Variable stiffness structural design of a dual-segment continuum manipulator with independent stiffness and angular position": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584520302118/pdfft?md5=12b5ea56c456638939490ab0d6cb77e6&pid=1-s2.0-S0736584520302118-main.pdf",
                "title": "Variable stiffness structural design of a dual-segment continuum manipulator with independent stiffness and angular position",
                "abstract": "This paper investigates the problem of the variable stiffness for a class of multisegment continuum manipulators. A simple design that mainly consists of two types of pneumatic muscle actuators (PMAs) is introduced to deal with the problem. To provide a detailed analysis, this characteristic of the variable stiffness is coupled with a mathematical analysis that is built upon the geometric mechanics and the performance testing of the stiffness in each PMA, thereby forming the stiffness model. Moreover, utilizing the simple design enables the continuum manipulator composed of two units to vary its stiffness independently from the angular position. In addition, using only the pressure variation of the PMAs in each unit, the dual-segment manipulator also has the capacity to implement independent unit locking. The experimental results further validate the expected performance of the proposed continuum manipulator.",
                "year": 2021,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "NeuralPlan: Neural floorplan radiance fields for accelerated view synthesis": {
        "accordingTo": {
            "scienceDirect": {
                "title": "NeuralPlan: Neural floorplan radiance fields for accelerated view synthesis",
                "abstract": "We propose an approach for quickly building a visual representation of a full indoor building. Our goal is to enable intelligent systems which frequently and regularly monitor buildings to assist personnel operating remotely, a need of special importance in these days. Prior work in neural scene representations for view synthesis focuses on single objects and small scenes and does not scale to full buildings in short timeframes. We propose introducing the floorplan and learning a neural floorplan radiance field, mapping floorplan 3D points and view directions to emitted radiance, and rendering via a sinusoidal multi-layer perceptron (MLP) neural renderer. To incorporate local priors and further accelerate the overall learning, we use a hypernetwork which maps a floorplan surface normal to the parameters of the neural renderer, thus defining the scene by a space of local neural rendering functions across the building. This allows shared knowledge, reasoned in function space, of performing the neural rendering from various vantage points in the scene based on similar building structure represented in the floorplan surface normal, and facilitates meta-knowledge pre-training across multiple buildings. The meta-knowledge is used to initialize the parameters of the hypernetwork at test time for the target building. Our approach performs significantly accelerated learning of neural floorplan radiance fields in around 15 min for full buildings on a single commodity GPU, and renders in real-time at 64 Hz, allowing for immersive visual experiences.",
                "year": 2021,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "CTArcade: Computational thinking with games in school age children": {
        "accordingTo": {
            "scienceDirect": {
                "title": "CTArcade: Computational thinking with games in school age children",
                "abstract": "We believe that children as young as ten can directly benefit from opportunities to engage in computational thinking. One approach to provide these opportunities is to focus on social game play. Understanding game play is common across a range of media and ages. Children can begin by solving puzzles on paper, continue on game boards, and ultimately complete their solutions on computers. Through this process, learners can be guided through increasingly complex algorithmic thinking activities that are built from their tacit knowledge and excitement about game play. This paper describes our approach to teaching computational thinking skills without traditional programming—but instead by building on children’s existing game playing interest and skills. We built a system called CTArcade, with an initial game (Tic-Tac-Toe), which we evaluated with 18 children aged 10–15. The study shows that our particular approach helped young children to better articulate algorithmic thinking patterns, which were tacitly present when they played naturally on paper, but not explicitly apparent to them until they used the CTArcade interface.",
                "year": 2014,
                "publisher": "International Journal of Child-Computer Interaction"
            }
        }
    },
    "An investigation of user attitudes toward search engines as an information retrieval tool": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563203000098/pdfft?md5=16c5c34014d37e3a9ffbdc72c4b91733&pid=1-s2.0-S0747563203000098-main.pdf",
                "title": "An investigation of user attitudes toward search engines as an information retrieval tool",
                "abstract": "After the Internet has gained great popularity at homes and schools, there is much information on the Web. Today, one of the primary uses of the Internet is information retrieval from search engines. The main purpose of the current study is to develop and examine an individual attitude model towards search engines as a tool for retrieving information. This model integrates individual computer experience with perceptions. In addition, it also combines perception theories, such as technology acceptance model (TAM) and motivation, in order to understand individual attitudes toward search engines. The results show that individual computer experience, quality of search systems, motivation, and perceptions of technology acceptance are all key factors that affect individual feelings to use search engines as an information retrieval tool.",
                "year": 2003,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "SINERGIA laparoscopic virtual reality simulator: Didactic design and technical development": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260706002975/pdfft?md5=b1d36d1fbf3c0cfde5eb748833c54df2&pid=1-s2.0-S0169260706002975-main.pdf",
                "title": "SINERGIA laparoscopic virtual reality simulator: Didactic design and technical development",
                "abstract": "VR laparoscopic simulators have demonstrated its validity in recent studies, and research should be directed towards a high training effectiveness and efficacy. In this direction, an insight into simulators’ didactic design and technical development is provided, by describing the methodology followed in the building of the SINERGIA simulator. It departs from a clear analysis of training needs driven by a surgical training curriculum. Existing solutions and validation studies are an important reference for the definition of specifications, which are described with a suitable use of simulation technologies. Five new didactic exercises are proposed to train some of the basic laparoscopic skills. Simulator construction has required existing algorithms and the development of a particle-based biomechanical model, called PARSYS, and a collision handling solution based in a multi-point strategy. The resulting VR laparoscopic simulator includes new exercises and enhanced simulation technologies, and is finding a very good acceptance among surgeons.",
                "year": 2007,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Shaping the contours of fractured landscapes: Extending the layering of an information perspective on refugee resettlement": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457318309038/pdfft?md5=92988b7cbcb54cbdd7d63db93e126080&pid=1-s2.0-S0306457318309038-main.pdf",
                "title": "Shaping the contours of fractured landscapes: Extending the layering of an information perspective on refugee resettlement",
                "abstract": "Refugee experience of resettlement into a third country is problematised by posing the question, what happens when an established information landscape fractures? Themes of disjuncture, intensification and liminality that have emerged from the author's research are described, using social theories as the analytical lens to shape the contours of fracture. Two other questions are posed How is digital space implicated in rebuilding information landscapes that have become fractured? and; What is the role of technology in enabling or constraining the conditions for remaking place?",
                "year": 2020,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "2D feature tracking algorithm for motion analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/003132039500006L/pdfft?md5=5ea6d066c9269df1e5fe0abb5e82f97c&pid=1-s2.0-003132039500006L-main.pdf",
                "title": "2D feature tracking algorithm for motion analysis",
                "abstract": "In this paper, we describe a local-neighborhood pixel-based adaptive algorithm to track image features, both spatially and temporally, over a sequence of monocular images. The algorithm assumes no a priori knowledge about the image features to be tracked, or the relative motion between the camera and the three dimensional (3D) objects. The features to be tracked are selected by the algorithm and they correspond to the peaks of a ‘correlation surface’ constructed from a local neighborhood in the first image of the sequence to be analysed. Any kind of motion, i.e., 6 DOF (translation and rotation), can be tolerated keeping in mind the pixels-per-frame motion limitations. No subpixel computations being necessary. Taking into account constraints of temporal continuity, the algorithm uses simple and efficient predictive tracking over multiple frames. Trajectories of features on multiple objects can also be computed. The algorithm accepts a slow, continuous change of brightness D.C. level in the pixels of the feature. Another important aspect of the algorithm is the use of an adaptive feature matching threshold that accounts for change in relative brightness of neighboring pixels. As applications of the feature tracking algorithm, and to test the accuracy of the tracking, we show how the algorithm has been used to extract the Focus of Expansion (FOE) and to compute the time-to-contact using real image sequences of unstructured, unknown environments. In both applications information from multiple frames is used.",
                "year": 1995,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Implications of data-driven product design: From information age towards intelligence age": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1474034622002518/pdfft?md5=149bef0257f8cdc46ea592f95ad7886e&pid=1-s2.0-S1474034622002518-main.pdf",
                "title": "Implications of data-driven product design: From information age towards intelligence age",
                "abstract": "Data-driven design (D3), a new design paradigm benefited from advanced data analytics and computational intelligence, has gradually promoted the research of data-driven product design (DDPD) ever since 2000 s. In today’s Intelligence Age, some theoretical and practical studies have tried to achieve the advanced intelligence capabilities in DDPD. However, to the authors' best knowledge, there is still a lack of a holistic summary of DDPD with chronological concern in the intelligence age. To bridge the gap, this research undertakes a literature review of DDPD publications from 2000 to date (19/09/2022), of which 172 relevant papers are discussed via bibliometric analysis and state-of-the-art analysis. The results shown that DDPD has vitality in the Intelligence Age by combining the cutting-edge digital technologies, such as AI, additive manufacturing, digital twin, and so on. Moreover, current DDPD studies could outperform the classical design methods on the well-defined tasks, but it still cannot master the creative/innovative design tasks which require the cognitive capability. This survey further highlights several future research potentials including cognitive intelligence-enabled design, end-to-end design integration, advanced design knowledge support, design for additive manufacturing, and sustainable smart product-service systems. It is hoped that this work can be regarded as a reference to understand the roadmap of DDPD and offer insights for the design practitioners to complete relevant tasks in today’s intelligence age.",
                "year": 2022,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "Video scene parsing: An overview of deep learning methods and datasets": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314220301120/pdfft?md5=ceec609570f86240d1dab238418bb65f&pid=1-s2.0-S1077314220301120-main.pdf",
                "title": "Video scene parsing: An overview of deep learning methods and datasets",
                "abstract": "Video scene parsing (VSP) has become a key problem in the field of computer vision in recent years due to its wide range of applications in numerous domains (e.g., autonomous driving). With the renaissance of deep learning (DL) techniques, various of VSP methods under this framework have demonstrated promising performance. However, no thorough review has been provided to comprehensively summarize the advantages and disadvantages of these methods, their datasets, or the directions for development. To remedy this, we provide an overview of the different DL methods applied to VSP in various scientific and engineering areas. Firstly, we describe several indispensable preliminaries of this field, defining essential background concepts as well as fundamental terminologies and differentiating between VSP and other similar problems. Then, according to their principles, contributions and importance, recent advanced DL methods for VSP are meticulously classified and thoroughly analyzed. Thirdly, we elaborate on the most frequently-used datasets and describe common evaluation metrics for VSP. Besides, extensive of experimental results for the aforementioned methods are presented to demonstrate their advantages and disadvantages. This is followed by further comparisons and discussions on the main challenges faced by researchers. Finally, we sum up the paper by drawing conclusions on the state-of-the-art methods for VSP and highlights potential research orientations as well as promising future work for DL techniques applied to VSP.",
                "year": 2020,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Modeling short-term adaptation processes of visual motion detectors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231211000567/pdfft?md5=d399e8d1c73f9860aa9664df39e4cefe&pid=1-s2.0-S0925231211000567-main.pdf",
                "title": "Modeling short-term adaptation processes of visual motion detectors",
                "abstract": "In the brain, both neural processing dynamics as well as the perceptual interpretation of a stimulus can depend on sensory history. The underlying principle is a sensory adaptation to the statistics of the input collected over some timespan, allowing the system to tune its detectors, e.g. by better sampling the input space and adjusting the response. Here, we show how a model for adaptation in visual motion processing can be set up from first principles using a generative formulation and casting the problem of adaptation in terms of optimal estimation over time. The model leads to an online adaptation of velocity tuning curves, inducing shifts in the velocity tuning and changes in the tuning curve widths that are compatible with observations from physiological experiments on macaque MT neurons. We also show how such an adaptation leads to a greater computational efficiency by a better sampling of the velocity space, requiring less motion detectors to achieve a desired level of estimation accuracy.",
                "year": 2011,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Toward a theory of the perceived spatial layout of scenes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X85900325/pdfft?md5=6ae58d71fd597494d7d8256a24e4d6b2&pid=1-s2.0-0734189X85900325-main.pdf",
                "title": "Toward a theory of the perceived spatial layout of scenes",
                "abstract": "When normally sighted people observe a natural scene, their perceptions include seeing the supporting ground surface and the arrangements of the objects on that surface: where each object is in relation to the others, to the ground surface, and to the observer. This is defined as the perceived layout of a scene. Most current perceptual theories do not consider perceived layout, preferring to focus on the much narrower concern of perceived radial distance of objects from the observer, ignoring concern for the relationships among the objects of the scene, and among the observer in relation to the objects. In contrast, current cognitive theories have focused on cognitive maps as a spatial representation of scenes previously seen, maps which include explicit representation of spatial layout. However, cognitive theories have paid little theoretical attention to the spatial relationships in scenes currently on view, so they can tell us little about perceived layout, only remembered layout. Perceived layout has been missing from perceptual theories, and needs to be studied in its own right. In addition, to understand spatial cognition, cognitive maps, or any kind of representation of knowledge about the layout of space, we must study the properties of perceived layout—the perception of the arrangements of objects in a scene currently on view. To provide an example of the assessment of perceived layout, experimental data are presented on the psychophysical measurement of the perceived layout of a natural scene containing 13 objects arranged on a large flat lawn. Subjects are asked to draw a map of the scene, to estimate the absolute interobjects distances between every pair of objects in feet, and to compare the relative magnitudes of every possible triplet of interobject distances. The matrices of the interobject distances derived from each of these three measures produce highly reliable and consistent 2-dimensional constructions of the scene through a multi-dimensional scaling analysis. These constructions resemble the physical layout of the actual scene quite closely. However, subjects tend to underestimate the distances between objects when the direction of the distances are parallel to their line of sight, compared to directions perpendicular to their line of sight. Not only does this tend to produce perceived layouts of the scene that are slightly more ellipical than in actuality, but more important, it means that subjects' perceived layouts change when they move to new viewing positions. These analyses and others are used to examine the usefulness and validity of these measures as indices of perceived layout. Following the psychophysical descriptions of the measures, as theoretical analysis of some of the properties of the perceived layout of space is provided, along with a consideration of some of the variables that are expected to affect these properties. The paper concludes with a return to the theoretical issues concerning the contents of a proper theory of space perception.",
                "year": 1985,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "A cloud endpoint coordinating CAPTCHA based on multi-view stacking ensemble": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016740482100002X/pdfft?md5=3190c534734968c30258459ec298d311&pid=1-s2.0-S016740482100002X-main.pdf",
                "title": "A cloud endpoint coordinating CAPTCHA based on multi-view stacking ensemble",
                "abstract": "Fully Autonomous Public Turing test to tell Computers and Humans Apart (CAPTCHA) is an essential component for network security resisting attacks, such as collision attack and password blasting.As a recently emerged CAPTCHA technology, drag-and-drop interactive CAPTCHA has been successfully employed in great number of practical applications. However, there are still some problems involved in the architecture and back-end anomaly detection model of the interactive CAPTCHA that need to be addressed: excessive concentration of computing pressure on cloud system, poor accuracy of anomaly detection model, and huge cost of the labelling for the attack sample. To this end, a novel cloud endpoint coordinating CAPTCHA based on multi-view stacking ensemble (MVSE) is proposed in this paper. In particular, a novel cloud endpoint coordinating CAPTCHA architecture is designed to make most use of the computing power of endpoint devices and reduce the calculation pressure of cloud system. Meanwhile, a multi-view stacking ensemble learning-based user action anomaly detection model is proposed for the cloud endpoint coordinating CAPTCHA architecture. Finally, an iterative top-k training (ITK-training) semi-supervised learning algorithm is employed for data enhancement and make the most use of un-labeled samples in order to reduce the deploy cost of drag-and-drop CAPTCHA system. A real-world data from one of the biggest Internet companies of China is used to validate the effectiveness of our proposed model. We can obtain that the computing pressure of the cloud can reduce nearly 95% and the accuracy of the proposed CAPTCHA system can reach 96.77% using MVSE learning and 98.67% using MVSE learning with the ITK-training based data enhancement.",
                "year": 2021,
                "publisher": "Computers & Security"
            }
        }
    },
    "Picture processing: 1985": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0734189X86800597/pdfft?md5=939ba5d2ead2c0937739e779252317fd&pid=1-s2.0-S0734189X86800597-main.pdf",
                "title": "Picture processing: 1985",
                "abstract": "This paper presents a bibliography of nearly 1100 references related to the computer processing of pictorial information, arranged by subject matter. Coverage is restricted, for the most part, to a selected set of U.S. journals and proceedings of specialized meetings. The topics covered include digitization, approximation, and compression; transforms, filtering, enhancement, restoration, and reconstruction; architectures, systems, software, and techniques; pictorial pattern recognition; feature detection, segmentation, and image analysis; matching and time-varying imagery; shape and pattern; geometry; texture; and three-dimensional scene analysis. No attempt is made to evaluate or summarize the items cited; the purpose is simply to provide a convenient compendium of references.",
                "year": 1986,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Molecular Computing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Molecular Computing",
                "abstract": "Molecular computers are information processing systems in which individual molecules play a vital functional role. This chapter discusses the simulation systems to comprehend the information processing capabilities of biological systems, to provide design guidance for molecular computers fabricated from bona fide molecular materials, or to serve as biologically motivated artificial intelligence systems. The effort to synthesize biomimetic or de novo molecular computing devices is an outgrowth of fundamental research in molecular and cellular biophysics, condensed-matter physics, polymer chemistry, neurophysiology, and computer science. Some major applications of molecular computing may involve the contribution of microphysical processes to information processing and control in biological cells, the dynamics of evolutionary processes, the development of new classes of chemical materials, a more precise concept of computation and the linkage between the structure and function of computing systems, and a broader concept of cognitive computation. As per the tradeoff principle, processes in which the biological systems perform well, such as pattern recognition and learning, cannot be efficiently automated in structurally programmable machines. To the extent that structurally nonprogrammable systems, in particular molecular computers, can be fabricated, it should make new inroads into the class of problems that can be efficiently automated. If they cannot be fabricated, the clear implication is that some human computational functions cannot be used for automation. In either case, the theoretical, experimental, or technical sides of molecular computing provide a new and more comprehensive framework for the comparative computational analysis of brain and machine.",
                "year": 1990,
                "publisher": "Advances in Computers"
            }
        }
    },
    "High quality 3D reconstruction based on fusion of polarization imaging and binocular stereo vision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253521001421/pdfft?md5=a30b8d0b1795caff5dad84f173859e68&pid=1-s2.0-S1566253521001421-main.pdf",
                "title": "High quality 3D reconstruction based on fusion of polarization imaging and binocular stereo vision",
                "abstract": "Polarization imaging can retrieve inaccurate objects’ 3D shapes with fine textures, whereas coarse but accurate depths can be provided by binocular stereo vision. To take full advantage of these two complementary techniques, we investigate a novel 3D reconstruction method based on the fusion of polarization imaging and binocular stereo vision for high quality 3D reconstruction. We first generate the polarization surface by correcting the azimuth angle errors on the basis of registered binocular depth, to solve the azimuthal ambiguity in the polarization imaging. Then we propose a joint 3D reconstruction model for depth fusion, including a data fitting term and a robust low-rank matrix factorization constraint. The former is to transfer textures from the polarization surface to the fused depth by assuming their relationship linear, whereas the latter is to utilize the low-frequency part of binocular depth to improve the accuracy of the fused depth considering the influences of missing-entries and outliers. To solve the optimization problem in the proposed model, we adopt an efficient solution based on the alternating direction method of multipliers. Extensive experiments have been conducted to demonstrate the efficiency of the proposed method in comparison with state-of-the-art methods and to exhibit its wide application prospects in 3D reconstruction.",
                "year": 2022,
                "publisher": "Information Fusion"
            }
        }
    },
    "An automatic active contour method for sea cucumber segmentation in natural underwater environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169916306068/pdfft?md5=a5441eb4c7cc148ae902acade652ddf3&pid=1-s2.0-S0168169916306068-main.pdf",
                "title": "An automatic active contour method for sea cucumber segmentation in natural underwater environments",
                "abstract": "Sea cucumbers have become an important sector of the marine industry in northern China, with a culture area exceeding one million acres and a production value over one hundred and twenty million dollars. However, sea cucumber culture and fishing are mainly dependent on manual work. To promote the development of sea cucumber culture automation, it is necessary to research sea cucumber automatic segmentation based on machine vision in natural underwater environments. Sea cucumbers usually live in an environment where lighting, visibility and stability are generally not controllable, which cause underwater images of sea cucumbers to be distorted, blurred, and severely attenuated. Moreover, sea cucumbers are flexible and colored much like sandy sediments. Therefore, it is difficult to fully separate a cucumber from the background in an underwater image. For fast and accurate automatic segmenting of sea cucumbers, an improved method based on active contour is presented in this paper. Image fusion based on the RGB color space and the contrast limited adaptive histogram equalization (CLAHE) method are used to increase the contrast of the sea cucumber thorns and body, respectively. Then, an edge detection algorithm is proposed to extract the edge of the sea cucumber thorns as an initial contour for the thorn segmentation, and a rectangular contour based on the edge information is built as the initial contour for the body segmentation. Finally, the results of the thorn and body are fused. All the procedures are automatically completed without human intervention. Qualitative and quantitative analysis indicates that the proposed method outperformed the other two compared methods in sea cucumber segmentation. A test with 120 samples showed that for the proposed method, the mean values of Euclidean distance, sensitivity, specificity, and accuracy were 12.7, 84.51, 96.97, and 96.54, respectively. The average time to run the algorithm for all images is 4.27 s. Thus, the proposed method could work for sea cucumber monitoring and fishing in real time.",
                "year": 2017,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Real-time stereo on GPGPU using progressive multi-resolution adaptive windows": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S026288561100014X/pdfft?md5=2aa699a20468dbba5a0d23667c7cc081&pid=1-s2.0-S026288561100014X-main.pdf",
                "title": "Real-time stereo on GPGPU using progressive multi-resolution adaptive windows",
                "abstract": "We introduce a new GPGPU-based real-time dense stereo matching algorithm. The algorithm is based on a progressive multi-resolution pipeline which includes background modeling and dense matching with adaptive windows. For applications in which only moving objects are of interest, this approach effectively reduces the overall computation cost quite significantly, and preserves the high definition details. Running on an off-the-shelf commodity graphics card, our implementation achieves a 36 fps stereo matching on 1024 × 768 stereo video with a fine 256 pixel disparity range. This is effectively same as 7200 M disparity evaluations per second. For scenes where the static background assumption holds, our approach outperforms all published alternative algorithms in terms of the speed performance, by a large margin. We envision a number of potential applications such as real-time motion capture, as well as tracking, recognition and identification of moving objects in multi-camera networks.",
                "year": 2011,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Genetic algorithm for analysis of mutations in Parkinson's disease": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0933365705000801/pdfft?md5=a09c1970b0a8c76e66ff2fed26f82152&pid=1-s2.0-S0933365705000801-main.pdf",
                "title": "Genetic algorithm for analysis of mutations in Parkinson's disease",
                "abstract": "Objective\n\nMitochondrial genetics has unique features that impede analysis of the biological significance of mitochondrial mutations. Simple searches for differences in total mutational load between normal and pathological samples have been frequently unrewarding, raising the possibility that more complex patterns of mutations may be responsible for some conditions. We explore this possibility in the context of Parkinson's disease (PD).",
                "year": 2005,
                "publisher": "Artificial Intelligence in Medicine"
            }
        }
    },
    "Machine learning-based optimal design of an acoustic black hole metaplate for enhanced bandgap and load-bearing capacity": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0888327024003340/pdfft?md5=4e7d646c18779900a6473f87d6f983f0&pid=1-s2.0-S0888327024003340-main.pdf",
                "title": "Machine learning-based optimal design of an acoustic black hole metaplate for enhanced bandgap and load-bearing capacity",
                "abstract": "This paper introduces a novel machine learning-based optimization strategy for multi-functional acoustic black hole (ABH) metaplates. The primary objective is to achieve a multi-functional metaplate with excellent performance in elastic wave attenuation and load-bearing capacity simultaneously. The paper begins by describing the design of nanocomposite ABH metaplates, presenting a new pathway to realize multi-functional metaplates. Then, a semi-analytical method, based on plate theory and the Bloch–Floquet theorem, is introduced to consider the band structure of the nanocomposite metaplates. Through systematic analysis, the impacts of the ABH effect, nanocomposite reinforcements, and the viscoelastic damping layer on the bandgaps and strain energy compliance are highlighted. Meanwhile, two optimization objectives representing bandgap characteristics and in-plane stiffness are derived respectively. Subsequently, a deep learning surrogate model is employed to establish a relationship involving significant parameters with the optimization objectives. The performance evaluation confirms accuracy and computational speed of the surrogate model. Finally, an optimization strategy based on deep reinforcement learning is proposed to obtain multi-functional metaplates with superior bandgaps, enhanced in-plane stiffness, or both. The robustness and efficiency of the strategy are demonstrated under different tests. The results show that the proposed strategy can achieve identical results as the genetic algorithm and nondominated sorting genetic algorithm-II, while surpassing them in computational efficiency and balancing multiple objectives. The findings of this study serve as valuable references for the future development and application of multi-functional advanced metamaterials.",
                "year": 2024,
                "publisher": "Mechanical Systems and Signal Processing"
            }
        }
    },
    "CBRA: Cardiac biomarkers release analyzer": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260721001127/pdfft?md5=49a4543e84020954a32b2968939b2193&pid=1-s2.0-S0169260721001127-main.pdf",
                "title": "CBRA: Cardiac biomarkers release analyzer",
                "abstract": "Background and objectives\n\nThe most advanced technologies and continuous innovations in the medical field require a necessary interaction between the clinical and the engineering world. In this context, software applications are proposed as a bridge between the two scientific fields and, therefore, as powerful tools, easy to use, and with great analytical skills. In this work, we propose CBRA as an innovative software platform, moving towards personalized medicine, which aims to simplify and speed up the triage of patients and support doctors in the diagnostic and prognostic phase.",
                "year": 2021,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Knowledge and reasoning for question answering: Research perspectives": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457311000434/pdfft?md5=4a5d3a64651d46e233e02a9c29346258&pid=1-s2.0-S0306457311000434-main.pdf",
                "title": "Knowledge and reasoning for question answering: Research perspectives",
                "abstract": "This paper presents a roadmap of current promising research tracks in question answering with a focus on knowledge acquisition and reasoning. We show that many current techniques developed in the frame of text mining and natural language processing are ready to be integrated in question answering search systems. Their integration opens new avenues of research for factual answer finding and for advanced question answering. Advanced question answering refers to a situation where an understanding of the meaning of the question and the information source together with techniques for answer fusion and generation are needed.",
                "year": 2011,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Visual motion ambiguities of a plane in 2-D FS sonar motion sequences": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314212000379/pdfft?md5=1552b2dc3cfb613b2824c8b90f2ea447&pid=1-s2.0-S1077314212000379-main.pdf",
                "title": "Visual motion ambiguities of a plane in 2-D FS sonar motion sequences",
                "abstract": "Sonar is the most common imaging modality in underwater, and high-resolution high data rate 2-D video systems have been emerging in recent years. As for visually guided terrestrial robot navigation and target-based positioning, the estimation of 3-D motion by tracking features in recorded 2-D sonar images is also a highly desirable capability for submersible platforms. Additionally, theoretical results dealing with robustness and multiplicity of solution constitute important fundamental findings due to nature of sonar data, namely, high noise level, narrow field of view coverage, scarcity of robust features, and incorrect matches.\n\nThis paper explores the inherent ambiguities of 3-D motion and scene structure interpretation from 2-D forward-scan sonar image sequences. Analyzing the sonar image motion transformation model, which depends on the affine components of the projective transformation (or homography) of two plane views, we show that two interpretations are commonly inferred. The true and spurious planes form mirror images relative to the zero-elevation plane of the sonar reference frame. Even under each of pure rotation or translation, a spurious motion exists comprising both translational and rotational components. In some cases, the two solutions share certain motion components, where the imaged surface becomes parallel to a plane defined by two of the sonar coordinate axes. A unique solution exists under the very special condition where the sonar motion aligns the imaged plane with the zero-elevation planes. We also derive the relationship between the two interpretations, thus allowing closed-form computation of both solutions.",
                "year": 2012,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "The Situation Awareness Weighted Network (SAWN) model and method: Theory and application": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687017300273/pdfft?md5=0bebc74128c81204108a273188e4f577&pid=1-s2.0-S0003687017300273-main.pdf",
                "title": "The Situation Awareness Weighted Network (SAWN) model and method: Theory and application",
                "abstract": "We introduce a novel model and associated data collection method to examine how a distributed organisation of military staff who feed a Common Operating Picture (COP) generates Situation Awareness (SA), a critical component in organisational performance. The proposed empirically derived Situation Awareness Weighted Network (SAWN) model draws on two scientific models of SA, by Endsley involving perception, comprehension and projection, and by Stanton et al. positing that SA exists across a social and semantic network of people and information objects in activities connected across a set of tasks. The output of SAWN is a representation as a weighted semi-bipartite network of the interaction between people (‘human nodes’) and information artefacts such as documents and system displays (‘product nodes’); link weights represent the Endsley levels of SA that individuals acquire from or provide to information objects and other individuals. The SAWN method is illustrated with aggregated empirical data from a case study of Australian military staff undertaking their work during two very different scenarios, during steady-state operations and in a crisis threat context. A key outcome of analysis of the weighted networks is that we are able to quantify flow of SA through an organisation as staff seek to “value-add” in the conduct of their work.",
                "year": 2017,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "How to improve user engagement and retention in mobile payment: A gamification affordance perspective": {
        "accordingTo": {
            "scienceDirect": {
                "title": "How to improve user engagement and retention in mobile payment: A gamification affordance perspective",
                "abstract": "Given the cut-throat competition in the Chinese mobile payment market, user retention is essential for guaranteeing the long-term profitability of mobile payment platforms. Meanwhile, gamification (i.e., the use of game artifacts applying game elements and principles) has been proposed as an effective way to foster mobile payment platform retention (i.e., users' deep-rooted commitment to a certain mobile payment platform). This study aims to provide a holistic understanding of gamification effects on mobile payment platform retention. Using a survey dataset from 447 Alipay users and an online scenario experiment of 1022 participants, we find that four gamified artifactual affordances (rewards, competition, feedback, and cooperation) positively affect user retention through the mediation effect of user engagement. Moreover, narrative affordance moderates the relationship between the four gamified artifactual affordances and user engagement. This study extends the affordances theory by theoretically conceptualizing gamified artifactual and situational affordances in the mobile payment context. This study also provides guidelines for mobile payment platform administrators and proposes avenues for future IS research.",
                "year": 2023,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Pipelined architecture for real-time cost-optimized extraction of visual primitives based on FPGAs": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1051200412002369/pdfft?md5=fcfbe34a8e2cd10d5ba02108000a053c&pid=1-s2.0-S1051200412002369-main.pdf",
                "title": "Pipelined architecture for real-time cost-optimized extraction of visual primitives based on FPGAs",
                "abstract": "This paper presents an architecture for the extraction of visual primitives on chip: energy, orientation, disparity, and optical flow. This cost-optimized architecture processes in real time high-resolution images for real-life applications. In fact, we present a versatile architecture that may be customized for different performance requirements depending on the target application. In this case, dedicated hardware and its potential on-chip implementation on FPGA devices become an efficient solution. We have developed a multi-scale approach for the computation of the gradient-based primitives. Gradient-based methods are very popular in the literature because they provide a very competitive accuracy vs. efficiency trade-off. The hardware implementation of the system is performed using superscalar fine-grain pipelines to exploit the maximum degree of parallelism provided by the FPGA. The system reaches 350 and 270 VGA frames per second (fps) for the disparity and optical flow computations respectively in their mono-scale version and up to 32 fps for the multi-scale scheme extracting all the described features in parallel. In this work we also analyze the performance in accuracy and hardware resources of the proposed implementation.",
                "year": 2013,
                "publisher": "Digital Signal Processing"
            }
        }
    },
    "Rethinking formal models of partially observable multiagent decision making": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Rethinking formal models of partially observable multiagent decision making",
                "abstract": "Multiagent decision-making in partially observable environments is usually modelled as either an extensive-form game (EFG) in game theory or a partially observable stochastic game (POSG) in multiagent reinforcement learning (MARL). One issue with the current situation is that while most practical problems can be modelled in both formalisms, the relationship of the two models is unclear, which hinders the transfer of ideas between the two communities. A second issue is that while EFGs have recently seen significant algorithmic progress, their classical formalization is unsuitable for efficient presentation of the underlying ideas, such as those around decomposition.\n\nTo solve the first issue, we introduce factored-observation stochastic games (FOSGs), a minor modification of the POSG formalism which distinguishes between private and public observation and thereby greatly simplifies decomposition. To remedy the second issue, we show that FOSGs and POSGs are naturally connected to EFGs: by “unrolling” a FOSG into its tree form, we obtain an EFG. Conversely, any perfect-recall timeable EFG corresponds to some underlying FOSG in this manner. Moreover, this relationship justifies several minor modifications to the classical EFG formalization that recently appeared as an implicit response to the model's issues with decomposition. Finally, we illustrate the transfer of ideas between EFGs and MARL by presenting three key EFG techniques – counterfactual regret minimization, sequence form, and decomposition – in the FOSG framework.",
                "year": 2022,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Applying capacity analyses to psychophysical evaluation of multisensory interactions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253509000360/pdfft?md5=31baa46c5fac73a5078709bd70d98c14&pid=1-s2.0-S1566253509000360-main.pdf",
                "title": "Applying capacity analyses to psychophysical evaluation of multisensory interactions",
                "abstract": "Determining when, if, and how information from separate sensory channels has been combined is a fundamental goal of research on multisensory processing in the brain. This can be a particular challenge in psychophysical data, as there is no direct recording of neural output. The most common way to characterize multisensory interactions in behavioral data is to compare responses to multisensory stimulation with the race model, a model of parallel, independent processing constructed from the probability of responses to the two unisensory stimuli which make up the multisensory stimulus. If observed multisensory reaction times are faster than those predicted by the model, it is inferred that information from the two channels is being combined rather than processed independently. Recently, behavioral research has been published employing capacity analyses where comparisons between two conditions are carried out at the level of the integrated hazard function. Capacity analyses seem to be particularly appealing technique for evaluating multisensory functioning, as they describe relationships between conditions across the entire distribution curve, are relatively easy and intuitive to interpret. The current paper presents capacity analysis of a behavioral data set previously analyzed using the race model. While applications of capacity analyses are still somewhat limited due to their novelty, it is hoped that this exploration of capacity and race model analyses will encourage the use of this promising new technique both in multisensory research and other applicable fields.",
                "year": 2010,
                "publisher": "Information Fusion"
            }
        }
    },
    "Explaining the interactions of humans and artifacts in insider security behaviors: The mangle of practice perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404820303370/pdfft?md5=670801f2f82ae8e370b7e7798d6ae4e1&pid=1-s2.0-S0167404820303370-main.pdf",
                "title": "Explaining the interactions of humans and artifacts in insider security behaviors: The mangle of practice perspective",
                "abstract": "Recent decades have seen numerous studies of individual information security behaviors and compliance within organizational contexts. One regular finding is that individuals continue to fail at compliance; however the explanations for why this occurs typically fail to consider the temporal, emergent and dynamic nature of security encounters between human users information security-related artifacts when they come together in the practice of security. In addition, existing research typically does not consider the inherent tension that exists between information systems (IS) use and security controls where the former is used to perform tasks and the latter may constrain the accomplishment of those tasks. This paper offers a new perspective that may better explain how the human and the security-related artifacts they create interact to both afford and constrain the enactment of information security. Our perspective builds on Pickering's mangle of practice metaphor to consider the constitutive entanglement of humans and artifacts and explain information security behaviors. This perspective is applied and discussed in the context of insider compliance with information security policies but could apply to a wide range of other information security and information systems behavioral contexts.",
                "year": 2020,
                "publisher": "Computers & Security"
            }
        }
    },
    "Using an integrated methods approach to analyse the emergent properties of military command and control": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687008001014/pdfft?md5=380798774e4315685f3cb7dabe9c13a6&pid=1-s2.0-S0003687008001014-main.pdf",
                "title": "Using an integrated methods approach to analyse the emergent properties of military command and control",
                "abstract": "This paper applies the event analysis for systemic teamwork (EAST) method to an example of military command and control. EAST offers a way to describe system level emergent properties that arise from the complex interactions of system components (human and technical). These are described using an integrated methods approach and modelled using Task, Social and Propositional networks. The current article is divided into three parts: a brief description of the military command and control context, a brief description of the EAST method, and a more in depth presentation of the analysis outcomes. The emergent properties of the military scenario relate to the degree of system reconfigurability, systems level situational awareness and the role of mediating technology. The findings are compared with similar analyses undertaken in civilian domains, in which the latest developments in command and control, under the aegis of Network Enabled Capability (NEC), are already in place.",
                "year": 2009,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Hybrid semantics-based vulnerability detection incorporating a Temporal Convolutional Network and Self-attention Mechanism": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Hybrid semantics-based vulnerability detection incorporating a Temporal Convolutional Network and Self-attention Mechanism",
                "abstract": "Context:\n\nDesirable characteristics in vulnerability-detection (VD) systems (VDSs) include both good detection capability (high accuracy, low false positive rate, low false negative rate, etc.) and low time overheads. The widely used VDSs based on models such as Recurrent Neural Networks (RNNs) have some problems, such as low time efficiency, failing to learn the vulnerability features better, and insufficient amounts of vulnerability features. Therefore, it is very important to construct an automatic detection model with high detection accuracy.",
                "year": 2024,
                "publisher": "Information and Software Technology"
            }
        }
    },
    "PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260723000652/pdfft?md5=e0b7effdac1238152ddcba9ded55522e&pid=1-s2.0-S0169260723000652-main.pdf",
                "title": "PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation",
                "abstract": "Background and Objective: Open-source deep learning toolkits are one of the driving forces for developing medical image segmentation models that are essential for computer-assisted diagnosis and treatment procedures. Existing toolkits mainly focus on fully supervised segmentation that assumes full and accurate pixel-level annotations are available. Such annotations are time-consuming and difficult to acquire for segmentation tasks, which makes learning from imperfect labels highly desired for reducing the annotation cost. We aim to develop a new deep learning toolkit to support annotation-efficient learning for medical image segmentation, which can accelerate and simplify the development of deep learning models with limited annotation budget, e.g., learning from partial, sparse or noisy annotations.\n\nMethods: Our proposed toolkit named PyMIC is a modular deep learning library for medical image segmentation tasks. In addition to basic components that support development of high-performance models for fully supervised segmentation, it contains several advanced components that are tailored for learning from imperfect annotations, such as loading annotated and unannounced images, loss functions for unannotated, partially or inaccurately annotated images, and training procedures for co-learning between multiple networks, etc. PyMIC is built on the PyTorch framework and supports development of semi-supervised, weakly supervised and noise-robust learning methods for medical image segmentation.\n\nResults: We present several illustrative medical image segmentation tasks based on PyMIC: (1) Achieving competitive performance on fully supervised learning; (2) Semi-supervised cardiac structure segmentation with only 10% training images annotated; (3) Weakly supervised segmentation using scribble annotations; and (4) Learning from noisy labels for chest radiograph segmentation.\n\nConclusions: The PyMIC toolkit is easy to use and facilitates efficient development of medical image segmentation models with imperfect annotations. It is modular and flexible, which enables researchers to develop high-performance models with low annotation cost. The source code is available at:https://github.com/HiLab-git/PyMIC.",
                "year": 2023,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Social networking in the aging context: Why older adults use or avoid Facebook": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585317300230/pdfft?md5=0886b9fb30d6f304837aabaeb587bdf4&pid=1-s2.0-S0736585317300230-main.pdf",
                "title": "Social networking in the aging context: Why older adults use or avoid Facebook",
                "abstract": "Despite a growing body of research about older adults’ use of social networking sites (SNS), scholars have not fully explored how this technology is meeting this group’s interactional and information-seeking needs. How do these older adults view this technology? What are their communication needs and expectations and why are they drawn to it? To address these questions and fill a gap in the literature, this study draws upon in-depth interviews with 46 older adults (average age: 80.4 years) about their perceptions of Facebook, which was the leading SNS at the time of writing. Analysis of interview data revealed six primary reasons for using Facebook (keeping in touch, sharing photos, social surveillance, responding to family member requests, convenient communication, curiosity) and six primary reasons for not using Facebook (privacy, need for media richness, preference for familiarity, triviality of communication, time commitment, frustration with site tools). Emergent findings hold implications for future research and SNS design.",
                "year": 2017,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Deep neural networks for neuro-oncology: Towards patient individualized design of chemo-radiation therapy for Glioblastoma patients": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046422000223/pdfft?md5=45626b6b340e8c6ad357e112c4dcd237&pid=1-s2.0-S1532046422000223-main.pdf",
                "title": "Deep neural networks for neuro-oncology: Towards patient individualized design of chemo-radiation therapy for Glioblastoma patients",
                "abstract": "Background and objectives\n\nGlioblastoma multiforme (GBM) is the most common and deadly type of primary cancers of the brain and central nervous system in adults. Despite the importance of designing a personalized treatment regimen for the patient, clinical trials prescribe a set of conventional regimens for GBM patients. We propose a computerized framework for designing chemo-radiation therapy (CRT) regimen based on patient characteristics.",
                "year": 2022,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Game-based e-retailing in GOLEM agent environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119209000492/pdfft?md5=e3750d5699fc33d7bbfc1970632e6744&pid=1-s2.0-S1574119209000492-main.pdf",
                "title": "Game-based e-retailing in GOLEM agent environments",
                "abstract": "We present a prototype multi-agent system whose goal is to support a 3D application for e-retailing. The prototype demonstrates how the use of agent environments can be amongst the most promising and flexible approaches to engineer e-retailing applications. We illustrate this point by showing how the agent environment GOLEM supports social interactions and how it combines them with semantic-web technologies to develop the e-retailing application. We also describe the features of GOLEM that allow a user to engage in e-retailing activities in order to explore the virtual social environment by searching and dynamically discovering new agents, products and services.",
                "year": 2009,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "A review of source term estimation methods for atmospheric dispersion events using static or mobile sensors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S156625351630152X/pdfft?md5=d43c83880f7cb888740eda02a57488b7&pid=1-s2.0-S156625351630152X-main.pdf",
                "title": "A review of source term estimation methods for atmospheric dispersion events using static or mobile sensors",
                "abstract": "Understanding atmospheric transport and dispersal events has an important role in a range of scenarios. Of particular importance is aiding in emergency response after an intentional or accidental chemical, biological or radiological (CBR) release. In the event of a CBR release, it is desirable to know the current and future spatial extent of the contaminant as well as its location in order to aid decision makers in emergency response. Many dispersion phenomena may be opaque or clear, thus monitoring them using visual methods will be difficult or impossible. In these scenarios, relevant concentration sensors are required to detect the substance where they can form a static network on the ground or be placed upon mobile platforms. This paper presents a review of techniques used to gain information about atmospheric dispersion events using static or mobile sensors. The review is concluded with a discussion on the current limitations of the state of the art and recommendations for future research.",
                "year": 2017,
                "publisher": "Information Fusion"
            }
        }
    },
    "Similarity-based machine learning framework for predicting safety signals of adverse drug–drug interactions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914821001830/pdfft?md5=ebaa4cab26b8b389cbbfa6f9305516a9&pid=1-s2.0-S2352914821001830-main.pdf",
                "title": "Similarity-based machine learning framework for predicting safety signals of adverse drug–drug interactions",
                "abstract": "Drug–drug interaction (DDI) is a major public health problem contributing to 30% of the unexpected clinical adverse drug events. Informatics-based studies for DDI signal detection have been evolving in the last decade. We aim at providing a boosted machine learning (ML) framework to predict novel DDI safety signals with high precision. We propose a similarity-based machine learning framework called “SMDIP” using DrugBank as one of the most reliable pharmaceutical knowledge bases. For this study, DrugBank provides the latest drug information in terms of DDIs, targets, enzymes, transporters, and carriers. We computed drug–drug similarities using a Russell–Rao measure for the available biological and structural information on DrugBank for representing the sparse feature space. Logistic regression is adopted to conduct DDI classification with a focus on searching for key similarity predictors. Six types of ML models are deployed on the selected DDI key features. Our study reveals that SMDIP has yielded favourable predictive performance compared to relevant studies with results as follows: AUC 76%, precision 82%, accuracy 79%, recall 62%, specificity 90%, and F-measure 78%. To further confirm the reliability and reproducibility of SMDIP, we investigate SMDIP on an unseen subset of direct-acting-antiviral (DAA) drugs for treating hepatitis C infections. Forty novel DAA DDIs are predicted that show consistency with the pharmacokinetic and pharmacodynamic profiles of these drugs. Furthermore, several reports from the pharmacovigilance literature corroborate our framework results. Those evaluations show that SMDIP is a promising framework for uncovering DDIs, which can be multifariously feasible in drug development, postmarketing surveillance, and public health fields.",
                "year": 2021,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Maintenance in aeronautics in an Industry 4.0 context: The role of Augmented Reality and Additive Manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2288430018302781/pdfft?md5=d27547370c4a98ab1b0c51fd098d51c3&pid=1-s2.0-S2288430018302781-main.pdf",
                "title": "Maintenance in aeronautics in an Industry 4.0 context: The role of Augmented Reality and Additive Manufacturing",
                "abstract": "The paper broadly addresses how Industry 4.0 program drivers will impact maintenance in aviation. Specifically, Industry 4.0 practices most suitable to aeronautical maintenance are selected, and a detailed exposure is provided. Advantages and open issues are widely discussed and case studies dealing with realistic scenarios are illustrated to support what has been proposed by authors. The attention has been oriented towards Augmented Reality and Additive Manufacturing technologies, which can support maintenance tasks and spare parts production, respectively. The intention is to demonstrate that Augmented Reality and Additive Manufacturing are viable tools in aviation maintenance, and while a strong effort is necessary to develop an appropriate regulatory framework, mandatory before the wide-spread introduction of these technologies in the aerospace systems maintenance process, there has been a great interest and pull from the industry sector.",
                "year": 2019,
                "publisher": "Journal of Computational Design and Engineering"
            }
        }
    },
    "The effect of context-dependent information and sentence constructions on perceived humanness of an agent in a Turing test": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705118304921/pdfft?md5=52708d4a486469561a5b93be6b0eaa5d&pid=1-s2.0-S0950705118304921-main.pdf",
                "title": "The effect of context-dependent information and sentence constructions on perceived humanness of an agent in a Turing test",
                "abstract": "In a Turing test, a judge decides whether their conversation partner is either a machine or human. What cues does the judge use to determine this? In particular, are presumably unique features of human language actually perceived as humanlike? Participants rated the humanness of a set of sentences that were manipulated for grammatical construction: linear right-branching or hierarchical center-embedded and their plausibility with regard to world knowledge.\n\nWe found that center-embedded sentences are perceived as less humanlike than right-branching sentences and more plausible sentences are regarded as more humanlike. However, the effect of plausibility of the sentence on perceived humanness is smaller for center-embedded sentences than for right-branching sentences.\n\nParticipants also rated a conversation with either correct or incorrect use of the context by the agent. No effect of context use was found. Also, participants rated a full transcript of either a real human or a real chatbot, and we found that chatbots were reliably perceived as less humanlike than real humans, in line with our expectation. We did, however, find individual differences between chatbots and humans.",
                "year": 2019,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Distributed resolution of feature interactions for internet applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128606002131/pdfft?md5=6f371e83944371da2c50bd9cfded20a0&pid=1-s2.0-S1389128606002131-main.pdf",
                "title": "Distributed resolution of feature interactions for internet applications",
                "abstract": "Internet applications, such as Email, VoIP and WWW, have been enhanced with many features. However, the introduction and modification of features may result in undesired behaviors, and this effect is known as feature interaction—FI.\n\nAfter a brief review of FI detection principles, we propose an interaction resolution adviser, consisting of two phases. The first phase implements an initial selection, by filtering the features that satisfy a set of formulas. We describe several strategies according to the nodes that participate in the FI resolution. The second phase selects the feature for execution, and adapts parameters, according to user policies.\n\nThe interaction resolution adviser is distributed, scalable and independent of the applications and their features.",
                "year": 2007,
                "publisher": "Computer Networks"
            }
        }
    },
    "Opportunities to practice historical thinking and reasoning in a made-for-school history-oriented videogame": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Opportunities to practice historical thinking and reasoning in a made-for-school history-oriented videogame",
                "abstract": "Building on the education community’s longstanding interest in videogames for learning, and in response to continued calls for more engaging pedagogy in history classrooms, some developers have introduced history-oriented videogames designed specifically to fit within the institutional and resource constraints of traditional classrooms. Nevertheless, little empirical evidence exists concerning such made-for-school history-oriented videogames. Leveraging a view of game mechanics as a form of language, and guided by the assumption from discourse analysis that language operates as a tool offering affordances and constraints for doing work in the world, we conducted a content analysis to identify opportunities to practice historical thinking and reasoning in the made-for-school history-oriented videogame Mission US. We found several in-game moments that invited the use of disciplinary skills, though few that required them. Confirming previous research on videogames for learning, our findings suggest the game may be more appropriately leveraged not on its own but as one part of a broader teaching and learning ecology. We highlight two mechanics, those pertaining to in-game, map-based navigation, and in-game, historically relevant trading, as holding promise for the design of future made-for-school history-oriented videogames. We suggest these findings are valuable to teachers, teacher educators, developers, and researchers.",
                "year": 2022,
                "publisher": "International Journal of Child-Computer Interaction"
            }
        }
    },
    "Deep learning approaches for anomaly-based intrusion detection systems: A survey, taxonomy, and open issues": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705119304897/pdfft?md5=8511d6674a8651cbb6c62e623bfd1c2b&pid=1-s2.0-S0950705119304897-main.pdf",
                "title": "Deep learning approaches for anomaly-based intrusion detection systems: A survey, taxonomy, and open issues",
                "abstract": "The massive growth of data that are transmitted through a variety of devices and communication protocols have raised serious security concerns, which have increased the importance of developing advanced intrusion detection systems (IDSs). Deep learning is an advanced branch of machine learning, composed of multiple layers of neurons that represent the learning process. Deep learning can cope with large-scale data and has shown success in different fields. Therefore, researchers have paid more attention to investigating deep learning for intrusion detection. This survey comprehensively reviews and compares the key previous deep learning-focused cybersecurity surveys. Through an extensive review, this survey provides a novel fine-grained taxonomy that categorizes the current state-of-the-art deep learning-based IDSs with respect to different facets, including input data, detection, deployment, and evaluation strategies. Each facet is further classified according to different criteria. This survey also compares and discusses the related experimental solutions proposed as deep learning-based IDSs.\n\nBy analysing the experimental studies, this survey discusses the role of deep learning in intrusion detection, the impact of intrusion detection datasets, and the efficiency and effectiveness of the proposed approaches. The findings demonstrate that further effort is required to improve the current state-of-the art. Finally, open research challenges are identified, and future research directions for deep learning-based IDSs are recommended.",
                "year": 2020,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Oral cancer detection model in distributed cloud environment via optimized ensemble technique": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809422007650/pdfft?md5=9b0af58c8769620ade389f0c10580ab3&pid=1-s2.0-S1746809422007650-main.pdf",
                "title": "Oral cancer detection model in distributed cloud environment via optimized ensemble technique",
                "abstract": "A new Two-tier m-healthcare oral cancer detection framework is constructed in a distributed cloud environment in this study. The following is the detection model: The supplied data is pre-processed to reduce noise as well as any undesired artifacts. The region of interest is separated from the backdrop via image segmentation. The segmentation in this research is done utilizing the Region Growing Technique. Textural features, Graph features, and Morphological features are also retrieved. A suggested Improved Linear Discriminant Analysis (ILDA) is used to pick the features in this study. For cancer classification, the selected features are exposed to ensemble classifiers (EC). In EC, stage 1 includes the Support Vector Machine (SVM) and Multi-layer Perceptron (MLP)) modeled for disease classification. The stage 2 phase includes the optimized CNN, which makes the final decisions regarding the presence/ absence of oral cancer. The optimized CNN is trained with outcomes acquired from SVM and the input for optimal CNN is MLP, which will provide the final detected outcomes. Since, the CNN is the final decision makes, its weight of it is fine-tuned using a new hybrid optimization model- Aquila Exploration Updated with Local Movement (AEULM), this ensures enhanced detection accuracy. The traditional Aquila Optimizer (AO) and the proposed hybrid optimization method are conceptually combined to form Wildebeest Herd Optimization (WHO). To confirm the effectiveness of the predicted model, a comparative assessment is completed.",
                "year": 2023,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "An ontology-based framework to formalize and represent 4D printing knowledge in design": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An ontology-based framework to formalize and represent 4D printing knowledge in design",
                "abstract": "Over the last decade, 4D printing paradigm has received intensive research efforts, whether from researchers in additive manufacturing (AM) or in smart materials (SMs) development. Related research works have thereby generated a large number of ad-hoc solutions with relevant disparate and scattered knowledge. This lack of common core knowledge is mainly due to the multiple involved expertise for fabricating stimulus-reactive structures. The scientific issue of federating and reconciling knowledge is also reinforced especially if such technology must be integrated into the product design process, falling under the field of design for 4D printing. To tackle this challenge, it becomes crucial to formalize and represent knowledge relating AM processes/techniques, SMs behaviours, stimuli and transformation functions with the variety of design objects. In such a context, the paper aims at developing an ontology-based framework for the semantic and logical description of transformable objects in the era of 4D printing for product-process design related purposes. This framework – which is built upon a foundational ontology associated with mereotopology for describing dynamical phenomena called basic formal ontology – consists in introducing a domain ontology equipped with reasoning capabilities supported by description logics for SMs selection and distribution, transformation sequence planning and AM process planning purposes.",
                "year": 2021,
                "publisher": "Computers in Industry"
            }
        }
    },
    "Asynchronous spiking neural P systems with rules on synapses and coupled neurons": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705122009893/pdfft?md5=633c15e792e5dd2036009a57d3dc192f&pid=1-s2.0-S0950705122009893-main.pdf",
                "title": "Asynchronous spiking neural P systems with rules on synapses and coupled neurons",
                "abstract": "The asynchronous spiking neural P system with rules on synapses (ASNPR system) is a type of distributive and non-deterministic computing model inspired by neural activities in biology. In this work, the synchronous excitation/inhibition phenomenon of coupled neurons is abstracted as the all-or-none behavior of coupled neurons: spikes in the coupled neurons are supplemented/consumed if and only if all the coupled neurons in the same set synchronously receive/consume spikes. Based on all-or-none behaviors, the ASNPR system with coupled neurons (ASNPRC system) is introduced, where coupled neurons are grouped into several sets, and the changes in the content of coupled neurons in the same set are positively correlated. The computational power of the ASNPRC systems is investigated. It has been proven that ASNPRC systems using standard rules are Turing universal function-computing devices. Moreover, a universal ASNPRC system consisting of four neurons is constructed to compute functions. The results show that “coupled neurons” is an efficient ingredient for the computation power of ASNPR systems in the sense that ASNPR systems using relatively few neurons achieve their universality with the help of “couple neurons”.",
                "year": 2022,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Inspection and grading of agricultural and food products by computer vision systems—a review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169902001011/pdfft?md5=6f98d1fee8f566eed859c7a1978dc792&pid=1-s2.0-S0168169902001011-main.pdf",
                "title": "Inspection and grading of agricultural and food products by computer vision systems—a review",
                "abstract": "Computer vision is a rapid, economic, consistent and objective inspection technique, which has expanded into many diverse industries. Its speed and accuracy satisfy ever-increasing production and quality requirements, hence aiding in the development of totally automated processes. This non-destructive method of inspection has found applications in the agricultural and food industry, including the inspection and grading of fruit and vegetable. It has also been used successfully in the analysis of grain characteristics and in the evaluation of foods such as meats, cheese and pizza. This paper reviews the progress of computer vision in the agricultural and food industry, then identifies areas for further research and wider application the technique.",
                "year": 2002,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "A comparative analysis of programming games, looking through the lens of an instructional design model and a game attributes taxonomy": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A comparative analysis of programming games, looking through the lens of an instructional design model and a game attributes taxonomy",
                "abstract": "The emerging trend of teaching computer programming to more and younger people has led to the development of game-based learning and teaching approaches. In this context, educational games are considered as a promising learning platform. However, research in the field of programming games has mainly focused on what is being taught in these games. Less is known, however, about how programming games afford learning and playing by design. To address this, we performed a qualitative, comparative analysis of 19 programming games from an instructional and game attributes perspective. The findings showed that a majority of programming games presents only a moderate alignment with established instructional principles. Furthermore, significant differences in the presence and prevalence of their game attributes were found. Our analysis resulted in a systematic classification of programming games according to their alignment with instructional principles and their positioning on a playing-versus-programming spectrum. Informed by this twofold classification, we explored whether particular programming games can still be conceptualized as games, as opposed to learning or programming environments. Accordingly, we formulated opportunities and restrictions towards their potential context-of-use.",
                "year": 2018,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "Hierarchical classified storage and incentive consensus scheme for building IoT under blockchain": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157824001642/pdfft?md5=add4298d76d4f1f8ccfe23321cd101f3&pid=1-s2.0-S1319157824001642-main.pdf",
                "title": "Hierarchical classified storage and incentive consensus scheme for building IoT under blockchain",
                "abstract": "With the advancements of IoT and blockchain, a novel era has emerged in the domain of smart building systems. At the same time, it also brings some problems and challenges. Most traditional solutions typically utilize the fully-replicated storage strategy that results in high storage costs, while recent solutions like coded blockchain may compromise query efficiency. Moreover, traditional reputation-based consensus schemes do not consider dynamic situations, limiting scalability. To handle these problems, we propose a novel hierarchical message aggregation scheme and a classified storage method under Reed–Solomon (RS) coding to minimize storage overhead while ensuring data recoverability and query performance. Additionally, we introduce a dynamic incentive reputation consensus mechanism to tackle scalability challenges such as preventing node monopolies, promoting new node integration, and enhancing fault tolerance. Through theoretical analysis and experimental simulation, the proposed scheme demonstrates a high degree of decentralization and scalability. Our scheme achieves a 20% reduction in the Gini coefficient compared to other approaches. Furthermore, our scheme can save 19 of storage overhead compared to traditional solutions while maintaining high query performance.",
                "year": 2024,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "You have e-mail, what happens next? Tracking the eyes for genre": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457313000952/pdfft?md5=2777136f6930b7ef9d22c17cd9babb85&pid=1-s2.0-S0306457313000952-main.pdf",
                "title": "You have e-mail, what happens next? Tracking the eyes for genre",
                "abstract": "This paper reports on an approach to the analysis of form (layout and formatting) during genre recognition recorded using eye tracking. The researchers focused on eight different types of e-mail, such as calls for papers, newsletters and spam, which were chosen to represent different genres. The study involved the collection of oculographic behavior data based on the scanpath duration and scanpath length based metric, to highlight the ways in which people view the features of genres. We found that genre analysis based on purpose and form (layout features, etc.) was an effective means of identifying the characteristics of these e-mails.\n\nThe research, carried out on a group of 24 participants, highlighted their interaction and interpretation of the e-mail texts and the visual cues or features perceived. In addition, the ocular strategies of scanning and skimming, they employed for the processing of the texts by block, genre and representation were evaluated.",
                "year": 2014,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Environmental conditions, mobile digital culture, mobile usability, knowledge of app in COVID-19 risk mitigation: A structural equation model analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352648322000216/pdfft?md5=899dc49edd2d3496d5475b75eb34cac1&pid=1-s2.0-S2352648322000216-main.pdf",
                "title": "Environmental conditions, mobile digital culture, mobile usability, knowledge of app in COVID-19 risk mitigation: A structural equation model analysis",
                "abstract": "Introduction\n\nThe mobile digital culture (MDC) supports individual lives, communities, and real-time organizational surveillance during COVID-19 emergencies. Hence, the study examined the advancement in smart health devices evidence in smartphone apps technologies in surveillance, control, and tracking potential virus areas among high-risk populations.",
                "year": 2022,
                "publisher": "Smart Health"
            }
        }
    },
    "A contribution to vision-based autonomous helicopter flight in urban environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889004001745/pdfft?md5=ab45a30858f8215f7a03cf5a60327d4c&pid=1-s2.0-S0921889004001745-main.pdf",
                "title": "A contribution to vision-based autonomous helicopter flight in urban environments",
                "abstract": "A navigation strategy that exploits the optic flow and inertial information to continuously avoid collisions with both lateral and frontal obstacles has been used to control a simulated helicopter flying autonomously in a textured urban environment. Experimental results demonstrate that the corresponding controller generates cautious behavior, whereby the helicopter tends to stay in the middle of narrow corridors, while its forward velocity is automatically reduced when the obstacle density increases. When confronted with a frontal obstacle, the controller is also able to generate a tight U-turn that ensures the UAV’s survival. The paper provides comparisons with related work, and discusses the applicability of the approach to real platforms.",
                "year": 2005,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "An efficient and scalable deformable model for virtual reality-based medical applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0933365704000363/pdfft?md5=4e80662490d25636d047494fd0f699a3&pid=1-s2.0-S0933365704000363-main.pdf",
                "title": "An efficient and scalable deformable model for virtual reality-based medical applications",
                "abstract": "Modeling of tissue deformation is of great importance to virtual reality (VR)-based medical simulations. Considerable effort has been dedicated to the development of interactively deformable virtual tissues. In this paper, an efficient and scalable deformable model is presented for virtual-reality-based medical applications. It considers deformation as a localized force transmittal process which is governed by algorithms based on breadth-first search (BFS). The computational speed is scalable to facilitate real-time interaction by adjusting the penetration depth. Simulated annealing (SA) algorithms are developed to optimize the model parameters by using the reference data generated with the linear static finite element method (FEM). The mechanical behavior and timing performance of the model have been evaluated. The model has been applied to simulate the typical behavior of living tissues and anisotropic materials. Integration with a haptic device has also been achieved on a generic personal computer (PC) platform. The proposed technique provides a feasible solution for VR-based medical simulations and has the potential for multi-user collaborative work in virtual environment.",
                "year": 2004,
                "publisher": "Artificial Intelligence in Medicine"
            }
        }
    },
    "End-to-end multimodal image registration via reinforcement learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841520302425/pdfft?md5=968350ac316dd8607e3bda7f8f73d056&pid=1-s2.0-S1361841520302425-main.pdf",
                "title": "End-to-end multimodal image registration via reinforcement learning",
                "abstract": "Multimodal image registration is a vital initial step in several medical image applications for providing complementary information from different data modalities. Since images with different modalities do not exhibit the same characteristics, finding their accurate correspondences remains a challenge. For convolutional multimodal registration methods, two components are quite significant: descriptive image feature as well as the suited similarity metric. However, these two components are often custom-designed and are infeasible to the high diversity of tissue appearance across modalities. In this paper, we translate image registration into a decision-making problem, where registration is achieved via an artificial agent trained by asynchronous reinforcement learning. More specifically, convolutional long-short-term-memory is incorporated after stacked convolutional layers in this method to extract spatial-temporal image features and learn the similarity metric implicitly. A customized reward function driven by landmark error is advocated to guide the agent to the correct registration direction. A Monte Carlo rollout strategy is also leveraged to perform as a look-ahead inference in the testing stage, to increase registration accuracy further. Experiments on paired CT and MR images of patients diagnosed as nasopharyngeal carcinoma demonstrate that our method achieves state-of-the-art performance in medical image registration.",
                "year": 2021,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "New perspectives on optical flow": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0031320393901997/pdfft?md5=9b03079ac61ad5285832b052449aa34e&pid=1-s2.0-0031320393901997-main.pdf",
                "title": "New perspectives on optical flow",
                "abstract": "The concept of optical flow originated in the study of moving vision, as a means of describing the instantaneous motion of images in the visual field of an observer resulting from relative motion of the observer and objects in the outside world. Work in this area has hitherto been based on the assumption that light travels in straight lines from object to observer. If, however, the refractive index of the medium between object and observer is nonuniform, light rays will bend according to Snell's law of refraction; under such conditions the motion field will be distorted. Furthermore, a time-dependent refractive index profile will result in image motion even when there is no relative motion between object and observer. This suggests a need for a broader definition of what constitutes optical flow. A very different phenomenon, but one which again involves image motion, is the motion of interference fringes in interferometry. Here also, optical flow would be a potentially useful way of describing the image motion. A simple example is given of an interferometry set-up where the optical flow is directly related to a changing physical parameter. Hence it would appear that the optical flow concept can be applied to a range of phenomena which involve moving images. A general framework to encompass the various forms of optical flow is presented.",
                "year": 1993,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "IPVNet: Learning implicit point-voxel features for open-surface 3D reconstruction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002201/pdfft?md5=c02fa95f73dd2a4c36367145ba1574e3&pid=1-s2.0-S1047320323002201-main.pdf",
                "title": "IPVNet: Learning implicit point-voxel features for open-surface 3D reconstruction",
                "abstract": "Reconstruction of 3D open surfaces (e.g., non-watertight meshes) is an underexplored area of computer vision. Recent learning-based implicit techniques have removed previous barriers by enabling reconstruction in arbitrary resolutions. Yet, such approaches often rely on distinguishing between the inside and outside of a surface in order to extract a zero level set when reconstructing the target. In the case of open surfaces, this distinction often leads to artifacts such as the artificial closing of surface gaps. However, real-world data may contain intricate details defined by salient surface gaps. Implicit functions that regress an unsigned distance field have shown promise in reconstructing such open surfaces. Nonetheless, current unsigned implicit methods rely on a discretized representation of the raw data. This not only bounds the learning process to the representation’s resolution, but it also introduces outliers in the reconstruction. To enable accurate reconstruction of open surfaces without introducing outliers, we propose a learning-based implicit point-voxel model (IPVNet). IPVNet predicts the unsigned distance between a surface and a query point in 3D space by leveraging both raw point cloud data and its discretized voxel counterpart. Experiments on synthetic and real-world public datasets demonstrates that IPVNet outperforms the state of the art while producing far fewer outliers in the resulting reconstruction.",
                "year": 2023,
                "publisher": "Journal of Visual Communication and Image Representation"
            }
        }
    },
    "Context-dependent communication under environmental constraints": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Context-dependent communication under environmental constraints",
                "abstract": "There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signalling model, we explore the conditions for the emergence of context-dependent communication in an agent-based situated model. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that (a) regularities in the context are not necessary for context-dependent communication and that (b) environmental constraints on the receiver’s referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver’s end. Consistent with common assumptions, the sender’s awareness of the context appears to be required for contextual communication. Our results further demonstrate the crucial role of the environment in the seemingly multilayered phenomenon of context-dependent communication — where language is influenced not only by the distribution of objects in the context, as indicated by previous studies, but also by the very presence of environmental constraints on referent choice. The computational model developed in this work is a demonstration of how signals may be ambiguous out of context, but still allow for near-perfect communication accuracy.",
                "year": 2024,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Deformable model of the prostate for TURP surgery simulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849304000974/pdfft?md5=640c7cb750685885bcaa5e51a8694135&pid=1-s2.0-S0097849304000974-main.pdf",
                "title": "Deformable model of the prostate for TURP surgery simulation",
                "abstract": "During a prostatectomy, a surgeon removes the inner prostate tissue that obstructs the urinary flow. The standard procedure, called transurethral resection of the prostate (TURP), is a minimally invasive surgery in which a resectoscope is introduced through the urethra of the patient to remove the obstructing tissue. In this paper, we present a three-dimensional (3D) computer model of the prostate for TURP simulation. The prostate model is designed to be the basis of a computer simulator for TURP training. The model was built from a set of ultrasound images with a technique that constructs a 3D volumetric mesh of the prostate shape, which is able to closely reproduce tissue resections as they are performed during real TURP procedures. A mass-spring method is used to model tissue deformation due to surgical tool interaction. The model simulates, in real-time: resections; tissue deformations; the cavity produced by the user as the surgical procedure progresses; and the corresponding reduction of the prostate volume.",
                "year": 2004,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Recursive estimation of time-varying motion and structure parameters": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/003132039500114X/pdfft?md5=c875c4e08f9f30e62ef36331d0d6bbeb&pid=1-s2.0-003132039500114X-main.pdf",
                "title": "Recursive estimation of time-varying motion and structure parameters",
                "abstract": "We present a computational framework for recovering both first-order motion parameters (observer direction of translation and observer rotation), second-order motion parameters (observer rotational acceleration) and relative depth maps from time-varying optical flow. We recover translation speed and acceleration in units which are scaled relative to the distance to the object. Our assumption is that the observer rotational motion is no more than “second order”, in other words, observer motion is either constant or has at most constant acceleration. We examine the effect of noise on the solution of the motion and structure parameters. This ensemble of unknowns comprises a solution to the classical “structure-and-motion from optic flow” problem. Our complete framework utilizes a method for interpreting the bilinear image velocity equation by solving simple systems of linear equations. Since our noise analysis yields uncertainty measures for each parameter, a Kalman filter is employed to incrementally integrate new measurements as they become available as each additional frame in the sequence is processed. We conclude by analysing this reduction of uncertainty over time as the system converges to a stable solution for both synthetic and real image sequences.",
                "year": 1996,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "The effects of dynamic workload and experience on commercially available EEG cognitive state metrics in a high-fidelity air traffic control environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S000368701930016X/pdfft?md5=5391ab52ad4fa750f1aba67dc89d55d7&pid=1-s2.0-S000368701930016X-main.pdf",
                "title": "The effects of dynamic workload and experience on commercially available EEG cognitive state metrics in a high-fidelity air traffic control environment",
                "abstract": "The current study evaluated the validity of commercially available electroencephalography (EEG) cognitive state metrics of workload and engagement in differentially experienced air traffic control (ATC) students. EEG and pupil diameter recordings were collected from 47 ATC students (27 more experienced and 20 less experienced) during a high-fidelity, variable workload approach-control scenario. Scenario workload was manipulated by increasing the number of aircraft released and the presence of a divided attention task. Results showed that scenario performance significantly degraded with increased aircraft and the presence of the divided attention task. No scenario performance differences were found between experience groups. The EEG engagement metric significantly differed between experience groups, with less experienced controllers exhibiting higher engagement than more experienced controllers. The EEG workload metric and pupil diameter were sensitive to workload manipulations but did not differentiate experience groups. Commercially available EEG cognitive state metrics may be a viable tool for enhancing ATC training.",
                "year": 2019,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Knowledge graph construction for product designs from large CAD model repositories": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1474034622001409/pdfft?md5=64c22271f19515e95c63d0bcb2b4b375&pid=1-s2.0-S1474034622001409-main.pdf",
                "title": "Knowledge graph construction for product designs from large CAD model repositories",
                "abstract": "Product Design based Knowledge graphs (KG) aid the representation of product assemblies through heterogeneous relationships that link entities obtained from multiple structured and unstructured sources. This study describes an approach to constructing a multi-relational and multi-hierarchical knowledge graph that extracts information contained within the 3D product model data to construct Assembly-Subassembly-Part and Shape Similarity relationships. This approach builds on a combination of utilizing 3D model meta-data and structuring the graph using the Assembly-Part hierarchy alongside 3D Shape-based Clustering. To demonstrate our approach, from a dataset consisting of 110,770 CAD models, 92,715 models were organized into 7,651 groups of varying sizes containing highly similar shapes, demonstrating the varied nature of design repositories, but inevitably also containing a significant number of repetitive and unique designs. Using the Product Design Knowledge Graph, we demonstrate the effectiveness of 3D shape retrieval using Approximate Nearest Neighbor search. Finally, we illustrate the use of the KG for Design Reuse of co-occurring components, Rule-Based Inference for Assembly Similarity and Collaborative Filtering for Multi-Modal Search of manufacturing process conditions. Future work aims to expand the KG to include downstream data within product manufacturing and towards improved reasoning methods to provide actionable suggestions for design bot assistants and manufacturing automation.",
                "year": 2022,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "Adhocracy culture buffers for mindfulness outcome: A cross-level moderated mediation analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2772503023000312/pdfft?md5=9a7b15a1c7fc738d2e0ea453031e9134&pid=1-s2.0-S2772503023000312-main.pdf",
                "title": "Adhocracy culture buffers for mindfulness outcome: A cross-level moderated mediation analysis",
                "abstract": "The prolonged COVID-19 pandemic reduced the performance of the software developer team in the Indian IT industry because of numerous psychosocial challenges while working in a non-dedicated workspace. Therefore, using the lens of self-determination theory (SDT), the present study investigated the indirect effect of mindfulness on team performance via psychological safety and active remote engagement under the bounded condition of adhocracy culture. The web-based responses of 604 team members nested in 99 teams from 10 Indian IT companies were used to perform a multilevel analysis. The analyses were conducted using Mplus 8.0 version to test the hypothesis. The results showed a significant serial mediation role of psychological safety and remote engagement in the relationship between the mindfulness of software developers and team performance. The relationship between mindfulness and psychological safety becomes stronger when the adhocracy culture is high. Similarly, the relationship between mindfulness and team performance via the serial mediation of psychological safety and active remote engagement becomes stronger when the adhocracy culture is high. Similar to many psychological approaches, this study supports the postulates advocated by SDT, while emphasizing the central role played by mindfulness.",
                "year": 2023,
                "publisher": "Telematics and Informatics Reports"
            }
        }
    },
    "Semiotic engineering: bringing designers and users together at interaction time": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543805000202/pdfft?md5=9a58f2e9c9d13565a5296cc09c3c3eac&pid=1-s2.0-S0953543805000202-main.pdf",
                "title": "Semiotic engineering: bringing designers and users together at interaction time",
                "abstract": "Semiotic engineering is a semiotic theory of human–computer interaction, where interactive computer systems are viewed as one-shot messages sent from designers to users. Through the system's interface, in many direct and indirect ways, designers are telling the users how they can, should, or must interact with the system in order to achieve a particular range of goals anticipated at design time. Designers are thus active interlocutors at human–computer interaction time. Their interactive discourse is delivered implicitly and/or explicitly by the system, which constitutes the designer's deputy. The importance of bringing designers and users together at interaction time springs from the intellectual nature of software artifacts. They are the result of human reasoning, choice and decision, rather than the direct effect of universal or natural laws. An adequate understanding of interactive artifacts depends on apprehending and comprehending the human intellect in action. Hence, in addition to producing interactive artifacts, designers must also introduce them appropriately, as is the case of other intellectual products. In this paper, we show how semiotic engineering can provide substantial theoretic support for viewing and exploring design possibilities brought about by this shift in perspective. We also discuss ontological and epistemological aspects of the theory, and conclude that it can bridge some of the gaps between other fragmented HCI theories and approaches.",
                "year": 2005,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Navigating the World Wide Web: bookmark maintenance architectures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543800000412/pdfft?md5=acfaf90e41001efc020dfdd05cb06d4b&pid=1-s2.0-S0953543800000412-main.pdf",
                "title": "Navigating the World Wide Web: bookmark maintenance architectures",
                "abstract": "The World Wide Web is increasingly becoming the preferred repository of information. The strength of this information infrastructure is also its weakness. Faced with the chaos of millions of places to go and thousands of places to remember having been, the thousands of new Web users who join every day, need a helping hand. The aim of this paper is, to highlight possible components of technologies supporting web navigation and the maintenance of indexes to web resources. The BASE framework is suggested as a means of understanding the pragmatic technological choices, and six experimental prototypes are presented and discussed. The prototypes support various aspects of bookmark maintenance and information filtering.",
                "year": 2001,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Variational optimization based single image dehazing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001577/pdfft?md5=10c72d3de774a46eb79e1cb060db3669&pid=1-s2.0-S1047320321001577-main.pdf",
                "title": "Variational optimization based single image dehazing",
                "abstract": "In this paper, we present a new approach for single image dehazing based on the proposed variational optimization. A hazy image captures the information about haze in terms of the transmission map and object details present in it. We propose to estimate the initial transmission map by performing the structure-aware smoothing of the hazy image. Further, we formulated a variational optimization for the estimation of final transmission, which refines the initial transmission of a hazy image. Atmospheric light can be considered to be constant throughout the scene for practical purposes. The uniform atmospheric light is computed from the dark channel of a hazy image. The exhaustive experimentation shows that the performance of the proposed method is comparable or better.",
                "year": 2021,
                "publisher": "Journal of Visual Communication and Image Representation"
            }
        }
    },
    "Knowledge and the visual process: Content, form and use": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0031320384900323/pdfft?md5=4c0cc40dd055f5ecbf722a4e713711d2&pid=1-s2.0-0031320384900323-main.pdf",
                "title": "Knowledge and the visual process: Content, form and use",
                "abstract": "A knowledge based system for the analysis of imagery clearly requires large amounts of domain specific knowledge and also requires a recognition control scheme that will manipulate this knowledge in order to interpret the imagery that represents various scenes of the domain. Many current systems indeed satisfy this statement. In addition, however, they all contain modules that access the actual image data and process this data. Typically, the methodologies for the image specific aspects and the domain specific aspects are separate yet interact, and the representational formalisms and control schemes for these two tasks are not related.\n\nThis paper will attempt, by overviewing a current hypothesis of the kinds of knowledge required for general purpose vision and the current representational tools available, to reconcile the “low” and “high” levels of knowledge based vision systems and to propose a set of uniform representational tools. The discussion will be at the conceptual level and not at the implementational level. Pointers to current computer vision schemes that are relevant to the discussion will be given. Several good surveys and discussions of requirements of vision systems can be found in Nevatia,(1) Nagel,(2) Hanson and Riseman,(3) Barrow,(4) Weszka,(5) Reddy,(6) and Kanade.(7)",
                "year": 1984,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Exploitative and exploratory learning in transactive memory systems and project performance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720613000438/pdfft?md5=c702d6477b23088c06e922eb3a87490e&pid=1-s2.0-S0378720613000438-main.pdf",
                "title": "Exploitative and exploratory learning in transactive memory systems and project performance",
                "abstract": "Based on organizational learning theory and the dynamic capability view, this study examines the relationships between transactive memory systems, team learning, and project performance in new product teams. Regression analysis is used to test the hypotheses in a sample of 218 Taiwanese firms. The findings indicate differential effects of three dimensions of a transactive memory system on exploitative and exploratory learning. Exploitative and exploratory learning are positively associated with project performance. The results also support that the interaction between exploitative and exploratory learning has a positive effect on project performance. Managerial implications and future research directions are discussed.",
                "year": 2013,
                "publisher": "Information & Management"
            }
        }
    },
    "Community-oriented Motivational Interviewing (MI): A novel framework extending MI to address COVID-19 vaccine misinformation in online social media platforms": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222004290/pdfft?md5=f0229edae4b55eea3d2db6780a4e0fb4&pid=1-s2.0-S0747563222004290-main.pdf",
                "title": "Community-oriented Motivational Interviewing (MI): A novel framework extending MI to address COVID-19 vaccine misinformation in online social media platforms",
                "abstract": "Researchers have linked circulating misinformation in online platforms to low COVID-19 vaccine uptake. Two disparate literatures provide relevant initial guidance to address the problem. Motivational Interviewing (MI) effectively reduces vaccine hesitancy in clinical environments; meanwhile, social scientists note inoculation, rebuttal, and appeals to accuracy are persuasive in digital contexts. A tension is inherent in these approaches. MI in digital forums may induce an ‘illusory truth effect,’ wherein falsehoods appear more accurate through repetition. Yet, rebutting misinformation directly may elicit backfire or reactance effects, motivating some to amplify their presentation of misinformation. Building on Identity Process Theory, we propose a theoretical framework for conducting MI-based infodemiology interventions among digital communities that conceptualizes the community in toto (rather than one specific person) as the unit of focus. Case examples from interventions on public Facebook posts illustrate three processes unique to such interventions: 1) Navigating tension between addressing commenters and “bystanders”; 2) Activating pro-vaccine bystanders; and 3) Reframing uncertainty or information individuals might find concerning or threatening according to implied collective values. This paper suggests community-oriented MI can maximize persuasive effects on bystanders while minimizing potential reactance from those with committed beliefs, thereby guiding community-oriented public health messaging interventions enacted in digital environments.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "On the development of a haptic system for rapid product development": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448504001563/pdfft?md5=26061b3684df568635fbc1967bd3c791&pid=1-s2.0-S0010448504001563-main.pdf",
                "title": "On the development of a haptic system for rapid product development",
                "abstract": "This paper presents a system development that extends haptic modeling to a number of key aspects in product development. Since haptic modeling has been developed based on physical laws, it is anticipated that a natural link between the virtual world and practical applications can be established based on haptic interaction. In the proposed system, a haptic device is used as the central mechanism for reverse engineering, shape modeling, real time mechanical property analysis, machining tool path planning and coordinate measuring machine (CMM) tolerance inspection path planning. With all these features in a single haptic system, it is possible to construct a three dimensional part by either haptic shape modeling or reverse engineering, then performing real-time mechanical property analysis in which the stiffness of a part can be felt and intuitively evaluated by the user, or generating collision free cutter tool path and CMM tolerance inspection path. Due to the force feed back in all of the above activities, the product development process is more intuitive, efficient and user-friendly. A prototype system has been developed to demonstrate the proposed capabilities.",
                "year": 2005,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "AnswerAuth: A bimodal behavioral biometric-based user authentication scheme for smartphones": {
        "accordingTo": {
            "scienceDirect": {
                "title": "AnswerAuth: A bimodal behavioral biometric-based user authentication scheme for smartphones",
                "abstract": "In this paper, we present a behavioral biometric-based smartphone user authentication mechanism, namely, AnswerAuth, which relies on the very common users’ behavior. Behavior, here, refers to the way a user slides the lock button on the screen, to unlock the phone, and brings the phone towards her ear. The authentication mechanism works with the biometric behavior based on the extracted features from the data recorded using the built-in smartphone sensors, i.e., accelerometer, gyroscope, gravity, magnetometer and touchscreen, while the user performed sliding and phone-lifting actions. We tested AnswerAuth on a dataset of 10,200 behavioral patterns collected from 85 users while they performed the unlocking actions, in sitting, standing, and walking postures, using six state-of-the-art conceptually different machine learning classifiers in two settings, i.e., with and without simultaneous feature selection and classification. Among all the chosen classifiers, Random Forest (RF) classifier proved to be the most consistent and accurate classifier on both full and reduced features and provided a True Acceptance Rate (TAR) as high as 99.35%. We prototype proof-of-the-concept Android app, based on our findings, and evaluate it in terms of security and usability. Security analysis of AnswerAuth confirms its robustness against the possible mimicry attacks. Similarly, the usability study based on Software Usability Scale (SUS)1 questionnaire verifies the user-friendliness of the proposed scheme (SUS Score of 75.11). Experimental results prove AnswerAuth as a secure and usable authentication mechanism.",
                "year": 2019,
                "publisher": "Journal of Information Security and Applications"
            }
        }
    },
    "Interacting with virtual environments: an evaluation of a model of interaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543898000599/pdfft?md5=1dd9dbd8b95aa3415e9843c2f24b8c43&pid=1-s2.0-S0953543898000599-main.pdf",
                "title": "Interacting with virtual environments: an evaluation of a model of interaction",
                "abstract": "There is a need for interface design guidance for virtual environments, in order to avoid common usability problems. To develop such guidance an understanding of user interaction is required. Theoretical models of interaction with virtual environments are proposed, which consist of stages of interaction for task/goal oriented, exploratory and reactive modes of behaviour. The models have been evaluated through user studies and results show the models to be reasonably complete in their predictions about modes and stages of interaction. Particular stages were found to be more predominant than others. The models were shown to be less accurate about the exact flow of interaction between stages. Whilst the general organisation of stages in the models remained the same, stages were often skipped and there was backtracking to previous stages. Results have been used to refine the theoretical models for use in informing interface design guidance for virtual environments.",
                "year": 1999,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Optimum design of uniform and non-uniform infill-coated structures with discrete variables": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448524001088/pdfft?md5=818e970a6501d02a83f834dc682faba0&pid=1-s2.0-S0010448524001088-main.pdf",
                "title": "Optimum design of uniform and non-uniform infill-coated structures with discrete variables",
                "abstract": "This article introduces a novel computer-aided procedure to design optimised coated structures with precise shell thickness control using the Smallest Univalue Segment Assimilating Nucleus operator and a novel augmentation-projection technique. Structures with heterogeneous sections, or coated structures, combine two different materials for the nucleus and the shell, which are generally chosen so that the material in the infill is lighter and the material in the coating is stiffer, which in this work are supposed homogeneous. Solving the interface problem requires material properties interpolation equations that consider three material phases, accurate placement of the coating over the base material, and precise control over the coating's thickness. The formation of the coating is controlled by the Smallest Univalue Segment Assimilating Nucleus, an edge detection operator developed in Digital Image Processing. The coating's thickness is controlled by an innovative methodology consisting of the projection of an augmented contour field, which is shown to create a constant thickness coating around the material domain. The optimisation problem is solved with the Sequential Element Rejection and Admission method. The validity of the procedure has been verified by solving various numerical application examples.",
                "year": 2024,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Can we predict success from log data in VLEs? Classification of interactions for learning analytics and their relation with performance in VLE-supported F2F and online learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S074756321300188X/pdfft?md5=ce7d710881a15f037a21c2b8c80dbabe&pid=1-s2.0-S074756321300188X-main.pdf",
                "title": "Can we predict success from log data in VLEs? Classification of interactions for learning analytics and their relation with performance in VLE-supported F2F and online learning",
                "abstract": "Learning analytics is the analysis of electronic learning data which allows teachers, course designers and administrators of virtual learning environments to search for unobserved patterns and underlying information in learning processes. The main aim of learning analytics is to improve learning outcomes and the overall learning process in electronic learning virtual classrooms and computer-supported education. The most basic unit of learning data in virtual learning environments for learning analytics is the interaction, but there is no consensus yet on which interactions are relevant for effective learning. Drawing upon extant literature, this research defines three system-independent classifications of interactions and evaluates the relation of their components with academic performance across two different learning modalities: virtual learning environment (VLE) supported face-to-face (F2F) and online learning. In order to do so, we performed an empirical study with data from six online and two VLE-supported F2F courses. Data extraction and analysis required the development of an ad hoc tool based on the proposed interaction classification. The main finding from this research is that, for each classification, there is a relation between some type of interactions and academic performance in online courses, whereas this relation is non-significant in the case of VLE-supported F2F courses. Implications for theory and practice are discussed next.",
                "year": 2014,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Automatic foreground extraction from imperfect backgrounds using multi-agent consensus equilibrium": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301474/pdfft?md5=3ff9e88602c46081bc3baf5f1d7f0c6c&pid=1-s2.0-S1047320320301474-main.pdf",
                "title": "Automatic foreground extraction from imperfect backgrounds using multi-agent consensus equilibrium",
                "abstract": "Extracting accurate foreground objects from a scene is an essential step for many video applications. Traditional background subtraction algorithms can generate coarse estimates, but generating high quality masks requires professional softwares with significant human interventions, e.g., providing trimaps or labeling key frames. We propose an automatic foreground extraction method in applications where a static but imperfect background is available. Examples include filming and surveillance where the background can be captured before the objects enter the scene or after they leave the scene. Our proposed method is very robust and produces significantly better estimates than state-of-the-art background subtraction, video segmentation and alpha matting methods. The key innovation of our method is a novel information fusion technique. The fusion framework allows us to integrate the individual strengths of alpha matting, background subtraction and image denoising to produce an overall better estimate. Such integration is particularly important when handling complex scenes with imperfect background. We show how the framework is developed, and how the individual components are built. Extensive experiments and ablation studies are conducted to evaluate the proposed method.",
                "year": 2020,
                "publisher": "Journal of Visual Communication and Image Representation"
            }
        }
    },
    "Reduced parameter set descriptions for system and event identification": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0736584588900166/pdfft?md5=ee328a203d69195ed3f011cdbbf8a093&pid=1-s2.0-0736584588900166-main.pdf",
                "title": "Reduced parameter set descriptions for system and event identification",
                "abstract": "The operation of a machine generates signatures that carry information about its physical condition and operating parameters. In the case of vibration signatures, these signatures may be so complicated that system identification procedures may converge very slowly, if at all. This is due to the large number of physical degrees of freedom which the system has, which translates into a large number of parameters needed to describe the signal. Our research has focused on ways to reduce the required parameter set to that quick and accurate estimates of the source event and the structural signal path can be made.\n\nWe shall discuss two procedures which we have used for achieving these aims. Our examples are drawn from studies of combustion pressure in a diesel engine. The available signal is the casing vibration of the engine, due to combustion onset in a cylinder. The signal is smoothed by windowing the cepstrum, which reduces the number of parameters needed to describe it to N. Once smoothed, there are two ways of separating the contributions from the combustion event itself and the structural path.\n\nThe first method is Kalman filtering, which describes the “system” by an adjustable impulse response illustrated by m parameters. The input event is estimated by a small number of N-m parameters. The optimization continues to provide a best estimate of the input and system response. Without the cepstral smoothing that precedes this step however, it is essentially impossible to achieve this optimization.\n\nThe second method uses a functional expansion to describe the cepstra of the event itself and the transmission path. The functions are Hermite polynomials, which combined with a Gaussian window are called Hermite functions, and have very useful properties. Using this procedure, we want to determine the coefficients in the series expansion for the event and the system cepstra, and these become the unknown parameters to be determined.\n\nThe procedures described here have application to diagnostic monitoring of machines and structures, to security systems, and to noise control. Some examples of prospective applications will be described.",
                "year": 1988,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Multi-objective optimization for parameter selection and characterization of optical flow methods": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494616300254/pdfft?md5=e44cdff0feccc24a75f4f8b432ec7838&pid=1-s2.0-S1568494616300254-main.pdf",
                "title": "Multi-objective optimization for parameter selection and characterization of optical flow methods",
                "abstract": "Optical flow methods are among the most accurate techniques for estimating displacement and velocity fields in a number of applications that range from neuroscience to robotics. The performance of any optical flow method will naturally depend on the configuration of its parameters, and for different applications there are different trade-offs between the corresponding evaluation criteria (e.g. the accuracy and the processing speed of the estimated optical flow). Beyond the standard practice of manual selection of parameters for a specific application, in this article we propose a framework for automatic parameter setting that allows searching for an approximated Pareto-optimal set of configurations in the whole parameter space. This final Pareto-front characterizes each specific method, enabling proper method comparison and proper parameter selection. Using the proposed methodology and two open benchmark databases, we study two recent variational optical flow methods. The obtained results clearly indicate that the method to be selected is application dependent, that in general method comparison and parameter selection should not be done using a single evaluation measure, and that the proposed approach allows to successfully perform the desired method comparison and parameter selection.",
                "year": 2016,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Activity recognition on streaming sensor data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119212000776/pdfft?md5=701bcbcfa259eeb95a397a9d776ad53a&pid=1-s2.0-S1574119212000776-main.pdf",
                "title": "Activity recognition on streaming sensor data",
                "abstract": "Many real-world applications that focus on addressing needs of a human, require information about the activities being performed by the human in real-time. While advances in pervasive computing have led to the development of wireless and non-intrusive sensors that can capture the necessary activity information, current activity recognition approaches have so far experimented on either a scripted or pre-segmented sequence of sensor events related to activities. In this paper we propose and evaluate a sliding window based approach to perform activity recognition in an on line or streaming fashion; recognizing activities as and when new sensor events are recorded. To account for the fact that different activities can be best characterized by different window lengths of sensor events, we incorporate the time decay and mutual information based weighting of sensor events within a window. Additional contextual information in the form of the previous activity and the activity of the previous window is also appended to the feature describing a sensor window. The experiments conducted to evaluate these techniques on real-world smart home datasets suggests that combining mutual information based weighting of sensor events and adding past contextual information to the feature leads to best performance for streaming activity recognition.",
                "year": 2014,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "Utilizing Machine Learning Framework to Evaluate the Effect of Climate Change on Maize and Soybean Yield": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169924003739/pdfft?md5=a720b75dbb0d4863fc9fde3687f99b54&pid=1-s2.0-S0168169924003739-main.pdf",
                "title": "Utilizing Machine Learning Framework to Evaluate the Effect of Climate Change on Maize and Soybean Yield",
                "abstract": "In recent years, climate change patterns and extreme weather events have adversely affected agricultural production and have raised concerns about its effect on crop yields. These changes can affect the crop yield in many ways including the changes in the length of the growing season, planting and harvest time windows, precipitation amount and frequency, and the growing degree days, etc. So, it is important to analyze the effect of climate change on yield variability for a better understanding of its effect on the crops. This study aims to analyze the historical monthly county-level data for the state of Ohio to quantify maize (Zea mays) and soybean (Glycine max) yield temporal variability and to study the impact of proposed climate change scenarios on the yield. Machine learning algorithms were used to model the effect of monthly temperature and precipitation levels on yield variability, and to study the effect of climate change scenarios on yield. In addition, yield prediction models were integrated with proposed climate change scenarios for higher and lower emissions scenarios to predict the maize and soybean yield for the year 2100. To model the effect of weather parameters on temporal yield variability, the Random Forest model outperformed with RMSE of 0.61 Mt/ha and R2 of 0.73 for maize and with RMSE of 0.21 Mt/ha, R2 of 0.64 for soybean crop. Maximum temperature for July which has a negative correlation with yield was found to be the most dominant parameter for maize yield followed by the precipitation in August, July, and June which have a positive correlation with maize yield. Precipitation in August was found to be the most dominant weather parameter for soybean yield followed by maximum temperature in July. Based on the projected average increase in Ohio’s temperature and precipitation levels alone, maize yield was predicted to drop by 13.2% or 18.5% respectively due to lower emissions and higher emissions climate change scenario of Ohio. Similarly, a 6.64% and a 9.63% drop is expected in the soybean yield by the year 2100 for both lower emissions and higher emissions climate change scenarios. In economic terms based on current commodity values, this is an astounding loss of 254.7 to 369.7 million USD for maize and 535.9 – 751.1 million USD for soybean crop to the state of Ohio.",
                "year": 2024,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Bioelectrical impedance analysis of thermal-induced cutaneous nociception": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809423001118/pdfft?md5=6131591258e4ec9a315589fb04b26c56&pid=1-s2.0-S1746809423001118-main.pdf",
                "title": "Bioelectrical impedance analysis of thermal-induced cutaneous nociception",
                "abstract": "Objective:\n\nPeople perceive different levels of pain at the same stimulus intensity, hence overburdening objective nociception assessment. In the context of recently developed pain monitors, this work presents the analysis of the complex bioimpedance measured during cold-induced cutaneous nociception. Using the Anspec-PRO device, this analysis results in a more complex characterization of the acquired signal (i.e., 3-dimensional or image-based representations), rather than evaluations of one/multiple single-dimension signals (i.e., time-based index).",
                "year": 2023,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Mobile technology at the frontlines of patient care: Understanding fit and human drives in utilization decisions and performance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923608002017/pdfft?md5=64ecc148d8f90f8d4ddfc5a4171f6780&pid=1-s2.0-S0167923608002017-main.pdf",
                "title": "Mobile technology at the frontlines of patient care: Understanding fit and human drives in utilization decisions and performance",
                "abstract": "Mobile information communication technologies (MICTs) have considerable promise in patient care settings. But that promise can only be realized if the MICT applications are used by the medical staff. This paper reports on a study examining nurses' decisions to utilize MICTs. A mixed-methods approach is used, consisting of both qualitative and quantitative elements, that reveals and empirically tests the significance of novel constellations of fit (i.e., identification, information, patient interaction, physical, time criticality, user comfort, and workflow fit) and individual characteristics, presented as basic human drives (i.e., drive to acquire, bond, defend, and learn). Findings indicate that fit is a multi-faceted construct and that archetypical human drives have an influence on these various notions, which in turn, impact technology adoption in the healthcare context.",
                "year": 2009,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Aggregate centrality measures for IoT-based coordination": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Aggregate centrality measures for IoT-based coordination",
                "abstract": "Collecting statistics from graph-based data is an increasingly studied topic in the data mining community. We argue that they can have great value in the coordination of dynamic IoT systems as well, especially to support complex coordination strategies related to distributed situation recognition.\n\nThanks to a mapping to the field calculus, a distribution coordination model proposed for collective adaptive systems, we show that many existing “centrality measures” for graphs can be naturally turned into field computations that compute the centrality of nodes in a network. Not only this mapping gives evidence that the field coordination is well-suited to accommodate massively parallel computations over graphs, but also it provides a new basic “brick” of coordination which can be used in several contexts, there including improved leader election or network vulnerabilities detection. We validate our findings by simulation, first measuring the ability of the translated algorithm to self-adjust to network changes, then investigating an application of centrality measures for data summarisation.",
                "year": 2021,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Fundamentals of ergonomics in theory and practice": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S000368700000034X/pdfft?md5=8d74a528e783efcb6acfa52bb83e9d1b&pid=1-s2.0-S000368700000034X-main.pdf",
                "title": "Fundamentals of ergonomics in theory and practice",
                "abstract": "In this paper ergonomics is defined as a discipline in its own right, as the theoretical and fundamental understanding of human behaviour and performance in purposeful interacting socio-technical systems, and the application of that understanding to design of interactions in the context of real settings. This definition is justified in the financial, technical, legal, organisational, social, political and professional contexts in which ergonomists work. On the basis of the history of ergonomics and contemporary contributions, it is proposed that it is one of the modern sciences, drawing as much from the field as from the laboratory, and including elements of an art and a craft as well. Justification for the new definition is provided by examining the interacting systems which are prevalent in the modern world and which are the domain best understood through the holistic approach of ergonomics. Finally a number of challenges for ergonomics are identified.",
                "year": 2000,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "OO/DEVS: A platform for industry simulation and strategic modelling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/016792369500013V/pdfft?md5=8d48298c32c772de454992086f2f1f02&pid=1-s2.0-016792369500013V-main.pdf",
                "title": "OO/DEVS: A platform for industry simulation and strategic modelling",
                "abstract": "OO/DEVS is a software platform designed for industry modelling and simulation. By that we refer to the modelling of enterprises, their policies and their interaction in the marketplace with the aim to explore different industrial scenarios or alternative strategies. The framework exploits concepts from Object Oriented design and analysis, as well as the Discrete Event System Specification (DEVS) formalism, to support the development of natural, modular and reusable models. This paper provides an overview of OO/DEVS and discusses its implementation in Smalltalk. It also presents its graphical user interface that allows graphical model design and management.",
                "year": 1995,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Mobile use induces local attentional precedence and is associated with limited socio-cognitive skills in preschoolers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563221000807/pdfft?md5=a9d935e4e2209118e6231084f84ddd1f&pid=1-s2.0-S0747563221000807-main.pdf",
                "title": "Mobile use induces local attentional precedence and is associated with limited socio-cognitive skills in preschoolers",
                "abstract": "Mobile touch screen devices (MTSDs; i.e., smartphones and tablets) are now being used at an early and neuroplastic age by an ever-growing number of children, with this use likely affecting cognitive development. In a cross-sectional study, we investigated whether frequent MTSD user preschoolers exhibit different attentional and socio-cognitive skills compared to non-users. In a second, experimental study, we tested whether exposure to digital and non-digital games is associated with differences in attentional performance, and whether game pace moderates observed effects. Findings of both studies indicate pre-existing and experimentally-induced MTSD use was associated with global precedence in selective attention tasks, but an atypical, local precedence in a divided attention task. Further, playing with a fast digital game eliminated the advantage of selective attention over divided attention observed in the non-digital and slow digital game conditions. MTSD use was not associated with emotion recognition but was associated with worse theory of mind. We argue that the observed correlates and effects of MTSD use, and specifically of games, can be explained by a combination of MTSD characteristics (e.g., screens are rich in local and multiple modes of information, relatively limited social experience) and game characteristics (e.g., fast speed). Our results may be informative for the design and optimization of game structure and function, and may even call for influencing parameters of MTSD use that could affect mental functioning in this sensitive age.",
                "year": 2021,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Managing for results: An examination of professional group perceptions of organizational practices": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/037872069090001X/pdfft?md5=2ec2453884107c8f1cb35d6ca0b2b417&pid=1-s2.0-037872069090001X-main.pdf",
                "title": "Managing for results: An examination of professional group perceptions of organizational practices",
                "abstract": "The primary purpose of the study was to ascertain if organizational practices, as perceived by individuals comprising information systems (IS) professional groups (i.e., system analysts, programmer analysts, programmers, and operations personnel), are related to job satisfaction and job performance, and, if such relationships exist, to determine which perceived organizational practices (factors) are significant for each IS professional group. Job satisfaction variance is explained by people, structural, and task oriented factors. Job performance variance is explained only by people oriented factors. Significant factors vary for each IS professional group.",
                "year": 1990,
                "publisher": "Information & Management"
            }
        }
    },
    "Virtual gossip: How gossip regulates moral life in virtual worlds": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216303922/pdfft?md5=70b0845c592452efd49c1447141ad7b3&pid=1-s2.0-S0747563216303922-main.pdf",
                "title": "Virtual gossip: How gossip regulates moral life in virtual worlds",
                "abstract": "This article explores to what extent the functions of interpersonal offline gossip can be mapped on to the virtual community of Second Life and its subsequent in-world and out-world interactions. A long-term hybrid ethnographic study was conducted that involved recurrent actual and virtual meetings with informants. The main objectives are, first, to look for similarities and to explain dissimilarities and, second, to gain some much-needed insight into how moral life is structured in social virtual communities and how important the role of gossip is. Results show overlaps between online and offline gossip concerning uses and functions. Gossip is important as a means for reputation management; as a cultural learning system; as a sanctioning system; and as entertainment. Just as in traditional offline communities, gossip is a central mechanism to regulate virtual moral life that stretches out to blogs, websites, and face-to-face meetings. Yet, technology amplifies the effects by creating new possibilities such as logging the evidence in order to spot cheaters. This way, in-world gossip becomes an inflated form of traditional gossip.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Data-driven invariant modelling patterns for digital twin design": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Data-driven invariant modelling patterns for digital twin design",
                "abstract": "The Digital Twin (DT) is one of the most promising technologies in the digital transformation market. A digital twin is a virtual copy of a physical system that emulates its behaviour to predict failures and opportunities for change, prescribe actions in real-time, and optimise and/or mitigate unexpected events. Modelling the virtual copy of a physical system is a rather complex task and requires the availability of a large amount of information and a set of accurate models that adequately represent the reality to model. At present, the modelling depends on the specific use case. Hence, the need to design a modelling solution suitable for virtual reality modelling in the context of a digital twin. The paper proposes a new approach to design a DT by endeavouring the concept of \"modelling patterns\" and their invariance property. Modelling patterns are here thought of as data-driven, as they can be derived autonomously from data using a specific approach devised to reach an invariance feature, to allow these to be used (and re-used) in modelling situations and/or problems with any given degree of similarity. The potentialities of invariance modelling patterns are proved here by the grace of a real industrial application, where a dedicated DT has been built using the approach proposed here.",
                "year": 2023,
                "publisher": "Journal of Industrial Information Integration"
            }
        }
    },
    "Learning function-based object classification from 3D imagery": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314207000884/pdfft?md5=4624e8fd4487875423868b9dbe2658dc&pid=1-s2.0-S1077314207000884-main.pdf",
                "title": "Learning function-based object classification from 3D imagery",
                "abstract": "We propose a novel scheme for using supervised learning for function-based classification of objects in 3D images. During the learning process, a generic multi-level hierarchical description of object classes is constructed. The object classes are described in terms of functional components. The multi-level hierarchy is designed and constructed using a large set of signature-based reasoning and grading mechanisms. This set employs likelihood functions that are built as radial-based functions from the histograms of the object instances. During classification, a probabilistic matching measure is used to search through a finite graph to find the best assignment of geometric parts to the functional structures of each class. An object is assigned to the class that provides the highest matching value. Reuse of functional primitives in different classes enables easy expansion to new categories. We tested the proposed scheme on a database of about 1000 different 3D objects. The proposed scheme achieved high classification accuracy while using small training sets.",
                "year": 2008,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Cognitive decline with pupil constriction independent of subjective fatigue during prolonged esports across player expertise levels": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563224000876/pdfft?md5=ca0a4acfd7cae08b7a06b4c8cc4f1b54&pid=1-s2.0-S0747563224000876-main.pdf",
                "title": "Cognitive decline with pupil constriction independent of subjective fatigue during prolonged esports across player expertise levels",
                "abstract": "Cognitive fatigue, a transient cognitive decline stemming from extended mental and physical activity, is well established in conventional contexts. However, its manifestation and self-awareness during digital pursuits, as an integrated style of mental and physical activity, notably in electronic sports (esports), remain underexplored. Here we show that, using virtual football as a representative model for esports, prolonged esports cause cognitive fatigue with pupil constriction, independent of subjective fatigue across player expertise. Casual and hardcore esports players engaged in an intensive 3 h gaming session, and sense of fatigue, executive function, pupil diameter as a potential signifier, and other physiological parameters were recorded. Subjective fatigue remained unchanged for up to 2 h, but increased after 3 h, inversely linked to reported enjoyment, in both groups of players. Executive function, assessed by flanker interference, decreased after 2 h and 3 h in casual players. The accuracy of the flanker task decreased after 2 h and 3 h in hardcore players, despite an initial accuracy edge. Pupil diameter decreased after 2 h and 3 h and correlated with cognitive performance decline but diverged from subjective fatigue in both player types. Our findings clearly suggest a disparity between actual cognitive decline and subjective fatigue awareness in esports, transcending levels of player expertise. Pupil constriction is a potential neurobiological marker integrating divergent cognitive and subjective fatigue during digital engagement.",
                "year": 2024,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Versatile recurrent neural network for wide types of video restoration": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000614/pdfft?md5=1b9a6cd00c45f0dfbd16b8fe57344fae&pid=1-s2.0-S0031320323000614-main.pdf",
                "title": "Versatile recurrent neural network for wide types of video restoration",
                "abstract": "Video shooting of natural scenes often suffers from various serious degradation, such as motion blur, impact of atmospheric turbulence, random noise and resolution reduction, etc. Different from the maturity of image restoration research, video restoration is much more complicated so that it lacks effective general method. Here, we present a versatile recurrent neural network (VRNN) to handle wide types of video degradation and generate stable videos with ideal clarity. We complete the design of VRNN through deducing a general video restoration paradigm that reveals the importance of simultaneously utilizing past and future information for restoring current frame. Specifically, we propose a novel RNN cell in which hidden state flows in bidirections, enriching temporal information contained in the extracted features. Furthermore, a feature fusion module involves temporal and spatial attention processing is designed to refine features of neighbouring frames and help reconstruct current frame. Extensive experiments on well-known public datasets (including four different kinds of video restoration tasks, with a total of 35,666 videos and 515,774 frames) show that the proposed VRNN achieves 1–4 dB of PSNR increasing or several times less of computational complexity in all tasks against state-of-the-art methods, manifesting the versatile and efficient ability of proposed VRNN in wide types of video restoration.",
                "year": 2023,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "The state-of-the-art in ultrasound-guided spine interventions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S136184152030133X/pdfft?md5=63b5e885856e81d5c4e7fe5a9ae1add0&pid=1-s2.0-S136184152030133X-main.pdf",
                "title": "The state-of-the-art in ultrasound-guided spine interventions",
                "abstract": "During the last two decades, intra-operative ultrasound (iUS) imaging has been employed for various surgical procedures of the spine, including spinal fusion and needle injections. Accurate and efficient registration of pre-operative computed tomography or magnetic resonance images with iUS images are key elements in the success of iUS-based spine navigation. While widely investigated in research, iUS-based spine navigation has not yet been established in the clinic. This is due to several factors including the lack of a standard methodology for the assessment of accuracy, robustness, reliability, and usability of the registration method. To address these issues, we present a systematic review of the state-of-the-art techniques for iUS-guided registration in spinal image-guided surgery (IGS). The review follows a new taxonomy based on the four steps involved in the surgical workflow that include pre-processing, registration initialization, estimation of the required patient to image transformation, and a visualization process. We provide a detailed analysis of the measurements in terms of accuracy, robustness, reliability, and usability that need to be met during the evaluation of a spinal IGS framework. Although this review is focused on spinal navigation, we expect similar evaluation criteria to be relevant for other IGS applications.",
                "year": 2020,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Agile gravitational search algorithm for cyber-physical path-loss modelling in 5G connected autonomous vehicular network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2214209623001158/pdfft?md5=ea96b8713d61332e08466cc564c4a2e0&pid=1-s2.0-S2214209623001158-main.pdf",
                "title": "Agile gravitational search algorithm for cyber-physical path-loss modelling in 5G connected autonomous vehicular network",
                "abstract": "Based on the characteristics of the 5 G standard defined in Release 17 by 3GPP and that of the emerging Beyond 5 G (or the so-called 6 G) network, cyber-physical systems (CPSs) used in smart transport network infrastructures, such as connected autonomous vehicles (CAV), will significantly depend on the cellular networks. The 5 G and Beyond 5 G (or 6 G) will operate over millimetre-wave (mmWave) bands. These network standards require suitable path loss (PL) models to guarantee effective communication over the network standards of CAV. The existing PL models suffer heavy signal losses and interferences at mmWave bands and may not be suitable for cyber-physical (CP) signal propagation. This paper develops an Agile Gravitational Search Algorithm (AGSA) that mitigates the PL and signal interference problems in the 5G–NR network for CAV. On top of that, a modified Okumura-Hata model (OHM) suitable for deployment in CP terrestrial mobile networks is derived for the CAV-CPS application. These models are tested on the real-world 5 G infrastructure. Results from the simulated models are compared with measured data for the modified, enhanced model and four other existing models. The comparative evaluation shows that the modified OHM and AGSA performed better than existing OHM, COST, and ECC-33 models by 90%. Also, the modified OHM demonstrated reduced signal interference compared to the existing models. In terms of optimisation validation, the AGSA scheme outperforms the Genetic algorithm, Particle Swarm Optimisation, and OHM models by at least 57.43%. On top of that, the enhanced AGSA outperformed existing PL (i.e., Okumura, Egli, Ericson 999, and ECC-33 models) by at least 67%, thus presenting the potential for efficient service provisioning in 5G-NR driverless car applications.",
                "year": 2024,
                "publisher": "Vehicular Communications"
            }
        }
    },
    "Predicting Depression Risk in Patients With Cancer Using Multimodal Data: Algorithm Development Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Predicting Depression Risk in Patients With Cancer Using Multimodal Data: Algorithm Development Study",
                "abstract": "Background\n\nPatients with cancer starting systemic treatment programs, such as chemotherapy, often develop depression. A prediction model may assist physicians and health care workers in the early identification of these vulnerable patients.",
                "year": 2024,
                "publisher": "JMIR Medical Informatics"
            }
        }
    },
    "A feature interaction benchmark for the first feature interaction detection contest": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128600000074/pdfft?md5=951d557650a1f5bedad969f6c41c9fe2&pid=1-s2.0-S1389128600000074-main.pdf",
                "title": "A feature interaction benchmark for the first feature interaction detection contest",
                "abstract": "The feature interaction problem is an inherently difficult problem that affects the entire software life cycle for development of new features for reactive systems. A considerable body of work has been created over the last 10 years addressing this problem. To encourage testing and comparing different approaches to the problem, a feature interaction detection contest was held for the Fifth International Workshop on Feature Interactions in Telecommunications and Software Systems. The contest required the participants to use automated tools to address the feature interaction problem in the requirements phase of feature development. The participants were required to discover the pair-wise feature interactions inherent in the requirements for a collection of features. This special issue of COMNET contains the best papers from the participants in that contest. Although there could be only one winner of the contest, all of the approaches represented here provide useful insight into how to approach the feature interaction problem as it affects the requirements phase of feature development. We hope that subsequent contests will provide opportunities to refine these approaches and to address other phases of feature development.",
                "year": 2000,
                "publisher": "Computer Networks"
            }
        }
    },
    "Multiscale agent-based modeling of restenosis after percutaneous transluminal angioplasty: Effects of tissue damage and hemodynamics on cellular activity": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482522005273/pdfft?md5=8f89543a62072fd3588f7679f1f9cc11&pid=1-s2.0-S0010482522005273-main.pdf",
                "title": "Multiscale agent-based modeling of restenosis after percutaneous transluminal angioplasty: Effects of tissue damage and hemodynamics on cellular activity",
                "abstract": "Background\n\nRestenosis following percutaneous transluminal angioplasty (PTA) in femoral arteries is a major cause of failure of the revascularization procedure. The arterial wall response to PTA is driven by multifactorial, multiscale processes, whose complete understanding is lacking. Multiscale agent-based modeling frameworks, simulating the network of mechanobiological events at cell-tissue scale, can contribute to decipher the pathological pathways of restenosis. In this context, the present study proposes a fully-automated multiscale agent-based modeling framework simulating the arterial wall remodeling due to the wall damage provoked by PTA and to the altered hemodynamics in the post-operative months.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Trans4E: Link prediction on scholarly knowledge graphs": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231221009607/pdfft?md5=a409ec1f32f9b082304917184cc98d60&pid=1-s2.0-S0925231221009607-main.pdf",
                "title": "Trans4E: Link prediction on scholarly knowledge graphs",
                "abstract": "The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the quality of AI-based services. In the scholarly domain, KGs describing research publications typically lack important information, hindering our ability to analyse and predict research dynamics. In recent years, link prediction approaches based on Knowledge Graph Embedding models became the first aid for this issue. In this work, we present Trans4E, a novel embedding model that is particularly fit for KGs which include N to M relations with N ≫ M. This is typical for KGs that categorize a large number of entities (e.g., research articles, patents, persons) according to a relatively small set of categories. Trans4E was applied on two large-scale knowledge graphs, the Academia/Industry DynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the information about Fields of Study (e.g., ‘neural networks’, ‘machine learning’, ‘artificial intelligence’), and affiliation types (e.g., ‘education’, ‘company’, ‘government’), improving the scope and accuracy of the resulting data. We evaluated our approach against alternative solutions on AIDA, MAG, and four other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms the other models when using low embedding dimensions and obtains competitive results in high dimensions.",
                "year": 2021,
                "publisher": "Neurocomputing"
            }
        }
    },
    "From knowledge theory to management practice: towards an integrated approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457300000315/pdfft?md5=49ea13ee7b261ba35890b361b6a704ba&pid=1-s2.0-S0306457300000315-main.pdf",
                "title": "From knowledge theory to management practice: towards an integrated approach",
                "abstract": "This paper critically contrasts the three main schools of thought on knowledge – namely, those that respectively conceptualize knowledge as situated in mind, process, and object – and assesses the resulting implications for knowledge management (KM). Against the background of the existing diversity of definitions of KM an integrated and holistic view of the KM value chain is put forward. Within this theoretical framework five main research streams (culture, knowledge location, awareness, evaluation, and absorption) are identified with a view to devising a practical concept of KM applicable in a business context. With a focus on knowledge flow and detailed approaches to potential solutions, conflicts and compatibilities between existing business strategies and KM are examined. A conceptual model is devised to offer a holistic integration of the theoretical and practical themes in order to serve as a framework for developing a future research agenda for the development of theoretically grounded, yet practical, KM business tools and applications.",
                "year": 2001,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "A pedagogical model to develop teaching skills. The collaborative learning experience in the Immersive Virtual World TYMMI": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563215002022/pdfft?md5=347a49b195691c1a79f4aa9c29a2a17c&pid=1-s2.0-S0747563215002022-main.pdf",
                "title": "A pedagogical model to develop teaching skills. The collaborative learning experience in the Immersive Virtual World TYMMI",
                "abstract": "The initial training of quality teachers is seen as a key to improving the learning outcomes of students in Chile. The TYMMI project is one of the initiatives being developed to provide a space for simulation for teaching practices in immersive virtual environments in Second Life and Open Sim. Initial Teachers Training belong to the School of Education at the Universidad Católica de la Santísima Concepción, participated during 2014 in the implementation of challenges, based on a pedagogical model and teaching strategies such as role play and problem-based learning. Through direct observation and blogs, the results show that participants have an important domain in the pedagogical and technological interaction. Despite the perception of the technical difficulties of using platforms, students emphasize that the experience has been supportive along their teaching practices, and it has allowed them to reinforce subject content, which poses a very motivating intellectual and technological challenge.",
                "year": 2015,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Generating C: Heterogeneous metaprogramming system description": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Generating C: Heterogeneous metaprogramming system description",
                "abstract": "Heterogeneous metaprogramming systems use a higher-level host language to generate code in a lower-level object language. Their appeal is taking advantage of the module system, higher-order functions, data types, type system and verification tools of the host language to quicker produce high-performant lower-level code with some correctness guarantees.\n\nWe present two heterogeneous metaprogramming systems whose host language is OCaml and object language is C. The first relies on offshoring: treating a subset of (MetaOCaml-generated) OCaml as a different notation for (a subset of) C. The second embeds C in OCaml in tagless-final style. The systems have been used in several projects, including the generation of C supersets OpenCL and OpenMP.\n\nGenerating C with some correctness guarantees is far less trivial than it may appear, with pitfalls abound: e.g., local variables may only be introduced in statement context; mutable variables are not first-class. Maintenance has challenges of its own, e.g., extensibility. On many examples, we expound the pitfalls we have come across in our experience, and describe the design and implementation to address them.",
                "year": 2024,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Dimensions of commonsense knowledge": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705121006092/pdfft?md5=026dc6d697a6c1ccc57950f00cad3bea&pid=1-s2.0-S0950705121006092-main.pdf",
                "title": "Dimensions of commonsense knowledge",
                "abstract": "Commonsense knowledge is essential for many AI applications, including those in natural language processing, visual processing, and planning. Consequently, many sources that include commonsense knowledge have been designed and constructed over the past decades. Recently, the focus has been on large text-based sources, which facilitate easier integration with neural (language) models and application to textual tasks, typically at the expense of the semantics of the sources and their harmonization. Efforts to consolidate commonsense knowledge have yielded partial success, with no clear path towards a comprehensive solution. We aim to organize these sources around a common set of dimensions of commonsense knowledge. We survey a wide range of popular commonsense sources with a special focus on their relations. We consolidate these relations into 13 knowledge dimensions. This consolidation allows us to unify the separate sources and to compute indications of their coverage, overlap, and gaps with respect to the knowledge dimensions. Moreover, we analyze the impact of each dimension on downstream reasoning tasks that require commonsense knowledge, observing that the temporal and desire/goal dimensions are very beneficial for reasoning on current downstream tasks, while distinctness and lexical knowledge have little impact. These results reveal preferences for some dimensions in current evaluation, and potential neglect of others.",
                "year": 2021,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Computer games, trust, and immediacy: Role-playing as immigrants in the South": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222003910/pdfft?md5=1319608e5a33b4f43e538cb209a01f67&pid=1-s2.0-S0747563222003910-main.pdf",
                "title": "Computer games, trust, and immediacy: Role-playing as immigrants in the South",
                "abstract": "Trust in racial and ethnic diversity has been decreasing in America for the better part of a century. The present study replicated a games-based approach to establishing trust in immigrants during COVID-19. Students in an online American National Government class created a fictional persona from either Mexico, India, or China, and sought U.S. citizenship. A posttest-only control group design was analyzed during Fall 2021 and Spring 2022. Subjects who played the game had significantly higher levels of trust in immigrants. They trusted immigrants from China, India, and the Middle East at higher levels than the control group did as well. Immediacy also interacted with role-playing group, such that applying for citizenship as Chinese immigrants had the largest effect on trusting when the experience was perceived as realistic, immersive, and engaging. Pretending to be less visible immigrant groups appears to generalize trust to immigrants from everywhere at high levels of immediacy.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "HCI ‘Intraface Model’ for System Design": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/095354389090002Y/pdfft?md5=54f6f0005d070237c0652a7d47b7a382&pid=1-s2.0-095354389090002Y-main.pdf",
                "title": "HCI ‘Intraface Model’ for System Design",
                "abstract": "We outline an alternative model of the interface in HCI, the ‘intraface’, in response to design issues arising from navigational and learning problems in hypertext domains. Ours is a model of general application to computer systems. It is composed of four key elements, identifiable within a dynamic interconnected context. These are the user; his/her interests; the tools employed and the ‘ensemble’ of representations brought to bear. In this paper we sketch the present shortcomings of HCI design before outlining the background for the model which draws upon two themes in contemporary psychology, conversational analysis and ‘affordance’ realist theories in perception. This framework allows for the development of principles of cooperation, user engagement and learning in HCI environments.",
                "year": 1990,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Chapter 14 General theory of geometric estimation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Chapter 14 General theory of geometric estimation",
                "abstract": "The chapter presents a rigorous mathematical foundation for the theory of geometric correction and the theory of parametric fitting. For both, the problem is stated in general terms without assuming Gaussian noise: the role of the covariance matrix for a Gaussian distribution is played by the Fisher information matrix. A lower bound is derived that corresponds to the Cramer–Rao lower bound in traditional statistics, on the covariance matrix of the unbiased estimator of the parameter. Then, the maximum likelihood estimator is proved to attain it in the first order if the problem belongs to the exponential family. The maximum likelihood estimation process is expressed in a computationally convenient form, where the rank-constrained generalized inverse is used to discuss the ill-posedness of the problem and the numerical instability of the solution. The statistical problem closely related to the parametric fitting problem is what is known as the Neyman-Scott problem: observing multiple data, each having a distribution characterized by a common parameter, called the “structure parameter” or the “parameter of interest,” and a distinct parameter, called the “nuisance parameter,”) that depends on each observation, one has to estimate the structural parameter without knowing the nuisance parameters.",
                "year": 1996,
                "publisher": "Machine Intelligence and Pattern Recognition"
            }
        }
    },
    "Context driven text segmentation and recognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167865595000984/pdfft?md5=af0a281d37182a0180707dd6417aeea1&pid=1-s2.0-0167865595000984-main.pdf",
                "title": "Context driven text segmentation and recognition",
                "abstract": "We present an algorithm for text segmentation and recognition mainly suited for complex problems where many merged characters are present. The basic idea is to define a distance, between lines of text and strings, which allows us to postpone the final decision about text segmentation and character classification until the contextual analysis is performed. The distance takes into account both the hypotheses about segmentation generated by a text segmentation module and the hypotheses about character classification produced by a probabilistic classifier. The algorithm has been tested by reading text on books' covers; the experimental results highlight the quality of the solution proposed.",
                "year": 1996,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "Evaluation of proximal sensing technologies for mapping bovine urine patches under grazing pastures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169921003264/pdfft?md5=be40dfefca58d923c487e5a66d360734&pid=1-s2.0-S0168169921003264-main.pdf",
                "title": "Evaluation of proximal sensing technologies for mapping bovine urine patches under grazing pastures",
                "abstract": "Animal urine patches, particularly bovine, are the major source of gaseous and leaching losses of nitrogen (N) in livestock-grazed pastoral systems. One method of mitigating these losses is the application of N inhibitors to patches that slow down the N transformations, thus allowing more time for plant uptake. If inhibitors can be applied as spot-treatments, then the overall volume of chemical can be minimized without compromising inhibitor efficacy. To enable this, accurate and reliable methods of detecting patches within hours of deposition are required. In this study, we aimed to validate the output of the newly developed and modified instrument ‘Spikey-R’ for detecting and mapping urine patches via measurement of soil surface electrical conductivity with the aim of using these data for targeting in-situ spot treatment of detected patches. We compared measurements from Spikey-R against thermal imagery from a handheld camera taken during urine deposition, as well as imagery taken from a remotely piloted aircraft system (RPAS or ‘drone’) 2 weeks after deposition. Wetted areas, as a result of urine application, across all sites were 0.2–0.3 m2 (1 L urine), 0.3–0.5 m2 (2 L), and 0.3–0.8 m2 (3 L). Spikey-R data generally compared well with the reference map produced from the thermal imagery, with slightly larger mean patch areas that still fell within the bounds of uncertainty. Repeat Spikey-R runs over the same patches showed no significant difference in detected areas up to 48 h after application. The drone was successful in detecting most urine patches 14 days post-application and identified a much larger pasture response (average > 150% larger again) than the initial wetted area with sizes ranging from 0.4 to 0.7 m2 (1 L), to 0.5 to 1.1 m2 (2 L), to 0.5 to 1.5 m2 (3 L). Spikey-R proved to be an effective tool in a research setting; advantages and disadvantages of each technology are discussed in detail.",
                "year": 2021,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "An application independent review of multimodal 3D registration methods": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S009784932030114X/pdfft?md5=22df7c29350e70033de4ec1e0b1270f0&pid=1-s2.0-S009784932030114X-main.pdf",
                "title": "An application independent review of multimodal 3D registration methods",
                "abstract": "Registration is a ubiquitous operation in Visual Computing, with applications in 3D object retrieval among others. Registration is the process of overlaying two or more datasets taken from different viewpoints, at different times or by different sensors into a common reference frame. Multimodal registration is a special case where the data to be matched do not belong to the same modality and is challenging due to the diverse nature of the modalities involved which makes the creation of a distance function harder. Due to the large number of possible modality combinations and application fields, a considerable number of multimodal registration techniques have been proposed in diverse fields, including medicine and archaeology. This survey aims to unify 3D multimodal registration techniques (i.e. where at least one of the modalities is in 3D) across application domains, with the hope of providing an application-independent view and the potential for cross-fertilization. The problem of 3D multimodal registration is explicitly defined and the various methods are systematically categorized and described in terms of a number of important properties. Methods with publicly available source code have been compared on common datasets. A discussion on trends, observations and challenges for further research concludes the review.",
                "year": 2020,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Trust in and Acceptance of Artificial Intelligence Applications in Medicine: Mixed Methods Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Trust in and Acceptance of Artificial Intelligence Applications in Medicine: Mixed Methods Study",
                "abstract": "Background\n\nArtificial intelligence (AI)–powered technologies are being increasingly used in almost all fields, including medicine. However, to successfully implement medical AI applications, ensuring trust and acceptance toward such technologies is crucial for their successful spread and timely adoption worldwide. Although AI applications in medicine provide advantages to the current health care system, there are also various associated challenges regarding, for instance, data privacy, accountability, and equity and fairness, which could hinder medical AI application implementation.",
                "year": 2024,
                "publisher": "JMIR Human Factors"
            }
        }
    },
    "Unsupervised learning of optical flow with patch consistency and occlusion estimation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304911/pdfft?md5=ae1922188136c734d7caa000e397231a&pid=1-s2.0-S0031320319304911-main.pdf",
                "title": "Unsupervised learning of optical flow with patch consistency and occlusion estimation",
                "abstract": "Recent works have shown that deep networks can be trained for optical flow estimation without supervision. Based on the photometric constancy assumption, most of these methods adopt the reconstruction loss as the supervision by point-based backward warping. Inspired by the traditional patch matching based approaches, we propose a patch-based consistency to improve the vanilla unsupervised learning method Ren et al. [1]. Instead of only comparing the corresponding pixel intensity, we locate the correspondence by using the image patches with census transform, which is more robust for the illumination variation and occlusion. Moreover, a novel parallel branch is devised to estimate a soft occlusion mask jointly in an unsupervised way. The mask is adopted to weight our patch-based consistency loss to alleviate the influence of the occlusion. The plenty of experiments have been implemented on Flying Chairs, KITTI and MPI-Sintel benchmarks. The results show that our method is efficient and outperforms the peer unsupervised learning methods that are using the FlowNet-liked network.",
                "year": 2020,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "A probabilistic framework for source localization in anisotropic composite using transfer learning based multi-fidelity physics informed neural network (mfPINN)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0888327023002674/pdfft?md5=15620d4eb577958503e13600ab1e2fa9&pid=1-s2.0-S0888327023002674-main.pdf",
                "title": "A probabilistic framework for source localization in anisotropic composite using transfer learning based multi-fidelity physics informed neural network (mfPINN)",
                "abstract": "The practical application of data-driven frameworks like deep neural network in acoustic emission (AE) source localization is impeded due to the collection of significant clean data from the field. The utility of the such framework is governed by data collected from the site and/or laboratory experiment. The noise, experimental cost and time consuming in the collection of data further worsen the scenario. To address the issue, this work proposes to use a novel multi-fidelity physics-informed neural network (mfPINN). The proposed framework is best suited for the problems like AE source detection, where the governing physics is known in an approximate sense (low-fidelity model), and one has access to only sparse data measured from the experiment (high-fidelity data). This work further extends the governing equation of AE source detection to the probabilistic framework to account for the uncertainty that lies in the sensor measurement. The mfPINN fuses the data-driven and physics-informed deep learning architectures using transfer learning. The results obtained from the data-driven artificial neural network (ANN) and physics-informed neural network (PINN) are also presented to illustrate the requirement of a multi-fidelity framework using transfer learning. In the presence of measurement uncertainties, the proposed method is verified with an experimental procedure that contains the carbon-fiber-reinforced polymer (CFRP) composite panel instrumented with a sparse array of piezoelectric transducers. The results conclude that the proposed technique based on a probabilistic framework can provide a reliable estimation of AE source location with confidence intervals by taking measurement uncertainties into account.",
                "year": 2023,
                "publisher": "Mechanical Systems and Signal Processing"
            }
        }
    },
    "An H∞ loopshaping approach for bio-inspired reflexive visual navigation in three-dimensional urban environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0921889014000943/pdfft?md5=5db9ea56102eedc54499c258ae6bd7d6&pid=1-s2.0-S0921889014000943-main.pdf",
                "title": "An H∞ loopshaping approach for bio-inspired reflexive visual navigation in three-dimensional urban environments",
                "abstract": "Safe autonomous navigation of aerial microsystems in unknown, cluttered environments is contingent on developing accurate localization estimates within the power and bandwidth constraints imposed by the microsystem. The insect visuomotor system combines a lightweight, spatially distributed sensor array (compound eyes) with high bandwidth-capable processing algorithms for efficient information extraction that enables autonomous navigation in complex, obstacle laden environments. In this paper, a novel control theoretic framework is introduced that combines the H∞ loop shaping controller synthesis approach with spatial decompositions of instantaneous patterns of optic flow modeled using analogs of wide field motion sensitive interneurons of the insect visuomotor system. Local proximity and velocity estimates are obtained by the decomposition of spherical optic flow patterns that are based on parameterization of typical three-dimensional urban environments. The motion-state estimates are combined with an H∞ controller to synthesize closed loop systems that provide theoretically justified robustness guarantees in the presence of large uncertainties in the local environment structure while mitigating measurement noise and gusts. The insect-inspired visual navigation technique is used to demonstrate safe obstacle avoidance and terrain following behavior in simulation on an autonomous rotary wing microsystem in an urban-like environment subjected to gusts. The current study presents a strong theoretical contribution that demonstrates the suitability of coupling insect-like vision sensing and processing mechanism with the H∞ loop shaping controller, which precludes explicit extraction of the detailed environment depth map, in achieving safe reflexive navigation in unstructured three-dimensional environments. The computational efficiency and simplicity of the current approach offer a promising alternative to satisfying the payload, power and bandwidth constraints imposed by aerial microsystems.",
                "year": 2014,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "An initial understanding of how game users explore virtual environments": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An initial understanding of how game users explore virtual environments",
                "abstract": "Spatial exploration is a core component of play in a rich and diverse range of modern video games. However, there is insufficient research into understanding spatial exploration in order to design better gameplay experiences. In this paper, we investigate the gameplay behaviors of 25 players across three types of exploration games by collecting in-game data, think-aloud data, questionnaire responses and post-game interview data. We use thematic analysis to analyze the data and map out four player exploration archetypes (PEAs): Wanderers, Seers, Pathers and Targeters. Then, a lens analysis is conducted to investigate the behavioral traits of these four archetypes to highlight different aspects of exploration. Gender, weekly gameplay time and real-life navigation abilities are the three factors which have been found to significantly impact the archetypes. Finally, the relationships between the participants’ preferences to the terrain features and their archetypes are also investigated. These results match the participants’ traits.",
                "year": 2017,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "Global optimal constrained ICA and its application in extraction of movement related cortical potentials from single-trial EEG signals": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016926071830587X/pdfft?md5=14192efda68faa6c783c6f4a073b78fa&pid=1-s2.0-S016926071830587X-main.pdf",
                "title": "Global optimal constrained ICA and its application in extraction of movement related cortical potentials from single-trial EEG signals",
                "abstract": "Background and objective\n\nThe constrained ICA (cICA) is a recent approach which can extract the desired source signal by using prior information. cICA employs gradient-based algorithms to optimize non convex objective functions and therefore global optimum solution is not guaranteed. In this study, we propose the Global optimal constrained ICA (GocICA) algorithm for solving the conventional cICA problems. Due to the importance of movement related cortical potentials (MRCPs) for neurorehabilitation and developing a suitable mechanism for detection of movement intention, single-trial MRCP extraction is presented as an application of GocICA.",
                "year": 2018,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "GLLPA: A Graph Layout based Label Propagation Algorithm for community detection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705120305104/pdfft?md5=3dd92f976f8b6d5aa6296db67f2b79e5&pid=1-s2.0-S0950705120305104-main.pdf",
                "title": "GLLPA: A Graph Layout based Label Propagation Algorithm for community detection",
                "abstract": "Community is an important property of networks. Recently, label propagation based community detection algorithms develop rapidly, since they can discover communities with high efficiency. However, the results of most of them are inaccurate and unstable because the node order of label updating and the mechanism of label propagation are random. In this paper, a new label propagation algorithm, Graph Layout based Label Propagation Algorithm (GLLPA), is proposed to reveal communities in networks, which aims at detecting accurate communities and improving stability by exploiting multiple graph layout information. Firstly, GLLPA draws networks to compact layout based on the force-directed methods with (a,r)-energy model, then a label initialization strategy is proposed to assign the nodes locating in a position with the same label. Secondly, GLLPA begins to draw networks to uniform layout and conduct community detection simultaneously, in which we design node influence and label influence based on node attraction in the uniform layout to handle the instability problem and enhance its accuracy and efficiency. Experimental results on 16 synthetic and 15 real-world networks demonstrate that the proposed method outperforms state-of-the-art algorithms in most networks.",
                "year": 2020,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Cognitive map or medium materiality? Reading on paper and screen": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216307154/pdfft?md5=d5ba013b061e23c26ccdd512272e6f8d&pid=1-s2.0-S0747563216307154-main.pdf",
                "title": "Cognitive map or medium materiality? Reading on paper and screen",
                "abstract": "The present study examined two common mechanisms that are used to explain why reading on an electronic screen versus paper result in different reading outcomes: The Cognitive Map Mechanism and the Medium Materiality Mechanism. A laboratory experiment (N = 45), was conducted using a three-group comparison design (paper book vs. digital equivalent vs. digital disrupted view). Our hypotheses that were based on the cognitive map mechanism were largely supported. On the other hand, our hypotheses following the medium materiality mechanism were not sufficiently evidenced. Specifically, our results showed that the paper book was similar to its digital equivalent, and both were better than the digital disrupted view in terms of reading comprehension, feelings of fatigue, and psychological immersion. The findings implied that it is not the materiality of the presentation medium that influences reading outcomes, rather it is the extent to which the text presentation facilitates, or impedes, the reader's ability to construct a cognitive map that influences the reading process. Implications for future research and practice are discussed.",
                "year": 2017,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Do you prefer, Pinterest or Instagram? The role of image-sharing SNSs and self-monitoring in enhancing ad effectiveness": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563217300237/pdfft?md5=1a378ec6e084ec3f55eb8f7609ae1079&pid=1-s2.0-S0747563217300237-main.pdf",
                "title": "Do you prefer, Pinterest or Instagram? The role of image-sharing SNSs and self-monitoring in enhancing ad effectiveness",
                "abstract": "The current research applied self-monitoring theory to investigate how individuals’ dispositional self-monitoring levels differ when exposed to two image-sharing social network sites (SNSs)—Pinterest and Instagram. Two consecutive studies were conducted. Through an online survey of 153 U.S. college students (study 1) the relationship between individuals’ dispositional self-monitoring and their preference of image-sharing SNSs was investigated. And a lab experiment (study 2) involving a subset of the first sample (n = 61) was conducted to explore the impact of the image-sharing SNSs use on individuals’ self-monitoring and determine advertisement message efficacy based on exposure to image-sharing social media. Results suggest that students in the low self-monitoring group interacted more intensely with Pinterest than Instagram. Additionally, individuals’ interactions with Pinterest and Instagram influenced their dispostional self-monitoring levels, as well as their preferences toward different types of persuasive messages (image-oriented vs. product-oriented advertisement). Results indicate not only that self-monitoring can shift in the context of different image-based social media behaviors, but that self-monitoring as influenced by social media can have implications for advertising message efficacy.",
                "year": 2017,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Supporting diagnostics and therapy planning for percutaneous ablation of liver and abdominal tumors and pre-clinical evaluation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Supporting diagnostics and therapy planning for percutaneous ablation of liver and abdominal tumors and pre-clinical evaluation",
                "abstract": "Percutaneous ablation methods are used to treat primary and metastatic liver tumors. Image guided navigation support minimally invasive interventions of rigid anatomical structures. When working with the displacement and deformation of soft tissues during surgery, as in the abdomen, imaging navigation systems are in the preliminary implementation stage.\n\nIn this study a multi-stage approach has been developed to support percutaneous liver tumors ablation. It includes CT image acquisition protocol with the amplitude of respiratory motion that yields images subjected to a semi-automatic method able to deliver personalized abdominal model. Then, US probe and ablation needle calibration, as well as patient position adjustment method during the procedure for the preoperative anatomy model, have been combined. Finally, an advanced module for fusion of the preoperative CT with intraoperative US images was designed. These modules have been tested on a phantom and in the clinical environment.\n\nThe final average Spatial calibration error was 1,7 mm, the average error of matching the position of the markers was about 2 mm during the entire breathing cycle, and average markers fusion error 495 mm. The obtained results indicate the possibility of using the developed method of navigation in clinical practice.",
                "year": 2019,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "Agent based modelling as a decision support system for shadow accounting": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923617300118/pdfft?md5=a3e4eea6a82d26c3dfc5f962fcfec64b&pid=1-s2.0-S0167923617300118-main.pdf",
                "title": "Agent based modelling as a decision support system for shadow accounting",
                "abstract": "We propose the use of agent based modelling to create a shadow account, that is, a secondary account of a business which is used to audit or verify the primary account. Such a model could be used to test the claims of industries and businesses. For example, the model could determine whether a business is generating enough funds to pay minimum wage. Parameters in the model can be set by observation or a range of values can be tested to determine points at which enough revenue could be generated. We illustrate the potential of agent based modelling as a tool for shadow accounting with a case study of a car wash business.",
                "year": 2017,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Phase-field description of fracture in NiTi single crystals": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782523008009/pdfft?md5=8edc424917dd9c467d0376731fe63ce1&pid=1-s2.0-S0045782523008009-main.pdf",
                "title": "Phase-field description of fracture in NiTi single crystals",
                "abstract": "A phase-field model for thermomechanically-induced fracture in NiTi at the single crystal level, i.e., fracture under loading paths that may take advantage of either of the functional properties of NiTi–superelasticity or shape memory effect–, is presented, formulated within the kinematically linear regime. The model accounts for reversible phase transformation from austenite to martensite habit plane variants and plastic deformation in the austenite phase. Transformation-induced plastic deformation is viewed as a mechanism for accommodation of the local deformation incompatibility at the austenite–martensite interfaces and is accounted for by introducing an interaction term in the free energy derived based on the Mori–Tanaka and Kröner micromechanical assumptions and the hypothesis of martensite instantaneous growth within austenite. Based on experimental observations suggesting that NiTi fractures in a stress-controlled manner, damage is assumed to be driven by the elastic energy, i.e., phase transformation and plastic deformation are assumed to contribute in crack formation and growth indirectly through stress redistribution. The model is restricted to quasistatic mechanical loading (no latent heat effects), thermal loading sufficiently slow with respect to the time rate of heat transfer by conduction (no thermal gradients), and a temperature range below Md, which is the temperature above which the austenite phase is stable, i.e., stress-induced martensitic transformation is suppressed. The numerical implementation of the model is based on an efficient scheme of viscous regularization in both phase transformation and plastic deformation, an explicit numerical integration via a tangent modulus method, and a staggered scheme for the coupling of the unknown fields. The model is shown able to capture transformation-induced toughening, i.e., stable crack advance attributed to the shielding effect of inelastic deformation left in the wake of the growing crack under nominal isothermal loading, actuation-induced fracture under a constant bias load, and crystallographic dependence on crack pattern.",
                "year": 2024,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "The humane use of human beings?": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687018301996/pdfft?md5=3566cc13f08ae4a334c93ccc59f789d1&pid=1-s2.0-S0003687018301996-main.pdf",
                "title": "The humane use of human beings?",
                "abstract": "The closing of loops exerts magical effects. This powerful act sculpts both the structural form and the functional expression of the systems which accrue from this ultimate connection. Systems and societies are each erected upon, and composed of, such intricate webs of self-correcting and self-shaping influences. However, without appropriate feedback regulation, these loops can become, in a utilitarian sense, dysfunctional. This is as true for social architectures as it is for any intentionally designed technological system. Here, a sequence of examples are used to expose an evident divergence between what is espoused for our social systems and what is actually enacted therein. Failure of regulation and associated diminution or even disconnection of such regulatory loops leads to an evident and growing dissonance between aspiration and reality. The first two of these examples used here are rather facile and even trivial while the third example is much more serious. All examples provide insight concerning, and expose the ways in which, a fuller understanding of cybernetic principles may rectify such discordant circumstances, at least in principle if not in practice.",
                "year": 2019,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Summarizing court decisions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457307000283/pdfft?md5=425d25d06ac7a34753426c81cc9bd899&pid=1-s2.0-S0306457307000283-main.pdf",
                "title": "Summarizing court decisions",
                "abstract": "In the field of law there is an absolute need for summarizing the texts of court decisions in order to make the content of the cases easily accessible for legal professionals. During the SALOMON and MOSAIC2 projects we investigated the summarization and retrieval of legal cases. This article presents some of the main findings while integrating the research results of experiments on legal document summarization by other research groups. In addition, we propose novel avenues of research for automatic text summarization, which we currently exploit when summarizing court decisions in the ACILA3 project. Techniques for automated concept learning and argument recognition are here the most challenging.",
                "year": 2007,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "A novel framework for dynamic and quantitative mapping of damage severity due to compound Drought–Heatwave impacts on tea Plantations, integrating Sentinel-2 and UAV images": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169924010792/pdfft?md5=44ec6eee3a49395e7e3b0200c9592d1d&pid=1-s2.0-S0168169924010792-main.pdf",
                "title": "A novel framework for dynamic and quantitative mapping of damage severity due to compound Drought–Heatwave impacts on tea Plantations, integrating Sentinel-2 and UAV images",
                "abstract": "In 2022, China experienced a historically rare compound drought–heatwave (CDH) event, which had more severe impacts on vegetation compared with individual extreme events. However, quantitatively mapping the damage severity of CDH on tea tree using satellite data remains a significant challenge. Here we proposed a novel framework for dynamic and quantitative mapping of tea trees damage severity caused by CDH in 2022 using Sentinel-2 and Unmanned Aerial Vehicle (UAV) data. The Extreme Gradient Boosting (XGBoost) was selected as the optimal machine learning algorithm to extract tea plantations using Sentinel-2 data from XGBoost, Random Forest (RF), Logistic regression (LR), and Naive Bayes. The User’s Accuracy and Producer’s Accuracy for the extraction of tea plantations are 92.20 % and 93.51 %, respectively. UAV images with 2.5 cm spatial resolution were utilized to detect the tea trees damaged caused by the CDH in 2022. A new index, named the CDH damage severity index (CDH_DSI), was proposed to quantitatively evaluate the damage severity of CDH on tea trees at pixel level, with a spatial resolution of 10 m x 10 m. Based on the results of tea plantations and damaged tea trees detection, UAV-derived CDH_DSI was calculated and used as ground truth data. Then, The XGBoost was selected as the optimal CDH_DSI prediction model from XGBoost, RF, and LR with the Sentnel-2 derived vegetation indices and spectral reflectance as predictors. The coefficient of determination was 0.81 and root mean squared error was 7.61 %. Finally, dynamic and quantitative CDH_DSI maps were generated with the optimal CDH_DSI prediction model. The results show that 50 percent of tea plantations in Wuyi were damaged by the prolonged CDH event in 2022. These results can be attributed to precipitation deficits and heatwaves. Given that more severe CDH events are projected for the future, quantifying their impacts can provide decision-making support for disaster mitigation and prevention.",
                "year": 2025,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Automated detection of pancreatic cancer with segmentation and classification using fusion of UNET and CNN through spider monkey optimization": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S174680942401471X/pdfft?md5=e651e16f3ba58b04dc006bebbd730154&pid=1-s2.0-S174680942401471X-main.pdf",
                "title": "Automated detection of pancreatic cancer with segmentation and classification using fusion of UNET and CNN through spider monkey optimization",
                "abstract": "An important step in diagnosis and prognosis of pancreatic diseases like Pancreatic Cancer (PC), and pancreas Nodules/tumors is identification and segmentation of pancreas. Automated PC segmentation and classification using a Computer-Aided Diagnostic (CAD) model is quite useful in identifying, predicting, and classifying the existence of cancer. The manual detection of this illness from the Computed Tomography (CT) pictures and Magnetic Resonance Imaging (MRI) is fraught with difficulties. In this study, an automated PC segmentation and classification network is introduced with the advent of Deep Learning (DL) algorithms namely Unet and Convolutional Neural Network (CNN). The proposed pancreas segmentation and detection model has four various stages including pre-processing, segmentation, feature extraction, and classification. Initially, the raw input images undergo pre-processing using hybrid Contrast-Limited Adaptive Histogram Equalization (CLAHE) and median filter to eliminate unnecessary noises. Secondly, semantic segmentation is applied to filtered images using Adaptive Spider Monkey optimization (ASMO) aided Improved Activation Function-enabled Unet (IAF-Unet). The CNN model processes segmented pictures to radiomic feature extraction stage. In this stage, significant and activation features are taken from the CNN’s pooling layer and activations, where ASMO is used to optimize the hidden neurons. Besides, cascade-net is used to extract deep features. Finally, an Ensemble Learning-based Classifier with Cascade-net and CNN (ELC-Casnet-CNN) is used for PC/tumor detection and classification using Auto-Encoder (AE), Random Forest (RF), and AdaBoost. The competence of the proposed is verified through a comparative study over the standard model. The codes are available at https://github.com/Chaithanyadas/chaithanyadas.",
                "year": 2025,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Behavioural operational research: Towards a framework for understanding behaviour in OR interventions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221715006657/pdfft?md5=020926f8d06653eab7d38e96bab06e9b&pid=1-s2.0-S0377221715006657-main.pdf",
                "title": "Behavioural operational research: Towards a framework for understanding behaviour in OR interventions",
                "abstract": "Stimulated by the growing interest in behavioural issues in the management sciences, research scholars have begun to address the implications of behavioural insights for Operational Research (OR). This current work reviews some foundational debates on the nature of OR to serve as a theoretical backdrop to orient a discussion on a behavioural perspective and OR. The paper addresses a specific research need by outlining that there is a distinct and complementary contribution of a behavioural perspective to OR. However, there is a need to build a theoretical base in which the insights from classical behavioural research is just one of a number of convergent building blocks that together point towards a compelling basis for behavioural OR. In particular, the focus of the paper is a framework that highlights the collective nature of OR practice and provides a distinct and interesting line of enquiry for future research.",
                "year": 2016,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "A review of irregular time series data handling with gated recurrent neural networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231221003003/pdfft?md5=370d3274012e8bfb9922d5d67479fc65&pid=1-s2.0-S0925231221003003-main.pdf",
                "title": "A review of irregular time series data handling with gated recurrent neural networks",
                "abstract": "Irregular time series data is becoming increasingly prevalent with the growth of multi-sensor systems as well as the continued use of unstructured manual data recording mechanisms. Irregular data and the resulting missing values severely limit the data's ability to be analysed and modelled for classification and forecasting tasks. Often, conventional methods used for handling time series data introduce bias and make strong assumptions on the underlying data generation process, which can lead to poor model predictions. Traditional machine learning and deep learning methods, although at the forefront of data modelling, are at best compromised by irregular time series data sets and fail to model the temporal irregularity of incomplete time series. Gated recurrent neural networks (RNN), such as LSTM and GRU, have had outstanding success in sequential modelling, and have been applied in many application fields, including natural language processing. These models have become an obvious choice for time series modelling and a promising tool for handling irregular time series data. RNNs have a unique ability to be adapted to make effective use of missing value patterns, time intervals and complex temporal dependencies in irregular univariate and multivariate time series data. In this paper, we provide a systematic review of recent studies in which gated recurrent neural networks have been successfully applied to irregular time series data for prediction tasks within several fields, including medical, human activity recognition, traffic monitoring and environmental monitoring. The review highlights the two common approaches for handling irregular time series data: missing value imputation at the data pre-processing stage and modification of algorithms to directly handle missing values in the learning process. Reviewed models are confined to those that can address issues with irregular time series data and does not cover the broader range of models that deal more generally with sequences and regular time series. This paper aims to present the most effective techniques emerging within this branch of research as well as to identify remaining challenges, so that researchers may build upon this platform of work towards further novel techniques for handling irregular time series data.",
                "year": 2021,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Industrial communications architectures and their application in a garment computer integrated manufacturing cell": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0140366497000777/pdfft?md5=b3d8c78bb0c0b8335430dd7b7dc6d443&pid=1-s2.0-S0140366497000777-main.pdf",
                "title": "Industrial communications architectures and their application in a garment computer integrated manufacturing cell",
                "abstract": "The traditional structure of manufacturing has significantly restricted the effectiveness of integration which can be supported by existing computer systems. An essential requirement for factory-wide integration is reliable communication among various components and programmable devices on the shop floor; this task is far from trivial given the wide variety of incompatible equipment and proprietary networks. Research into networks for manufacturing has resulted in several systems which are compatible with the Open Systems Interconnection reference model. This paper presents a complete review of major industrial networking approaches. A comparative study on industrial communication networks is conducted. The communication requirements of an Automatic Sewing Cell are analyzed and an appropriate protocol is selected. Finally, a set of primitive services that are considered essential for CIM cells in the garment industry is proposed.",
                "year": 1997,
                "publisher": "Computer Communications"
            }
        }
    },
    "Reprint of: Multi-scale 2D tracking of articulated objects using hierarchical spring systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320311001415/pdfft?md5=6d1c53a004ef043ce8f7e994b416c8c7&pid=1-s2.0-S0031320311001415-main.pdf",
                "title": "Reprint of: Multi-scale 2D tracking of articulated objects using hierarchical spring systems",
                "abstract": "This paper presents a flexible framework to build a target-specific, part-based representation for arbitrary articulated or rigid objects. The aim is to successfully track the target object in 2D, through multiple scales and occlusions. This is realized by employing a hierarchical, iterative optimization process on the proposed representation of structure and appearance. Therefore, each rigid part of an object is described by a hierarchical spring system represented by an attributed graph pyramid. Hierarchical spring systems encode the spatial relationships of the features (attributes of the graph pyramid) describing the parts and enforce them by spring-like behavior during tracking. Articulation points connecting the parts of the object allow to transfer position information from reliable to ambiguous parts. Tracking is done in an iterative process by combining the hypotheses of simple trackers with the hypotheses extracted from the hierarchical spring systems.",
                "year": 2011,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Multi-scale 2D tracking of articulated objects using hierarchical spring systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320310005091/pdfft?md5=59d28b31920daf481c05ab6442235f41&pid=1-s2.0-S0031320310005091-main.pdf",
                "title": "Multi-scale 2D tracking of articulated objects using hierarchical spring systems",
                "abstract": "This paper presents a flexible framework to build a target-specific, part-based representation for arbitrary articulated or rigid objects. The aim is to successfully track the target object in 2D, through multiple scales and occlusions. This is realized by employing a hierarchical, iterative optimization process on the proposed representation of structure and appearance. Therefore, each rigid part of an object is described by a hierarchical spring system represented by an attributed graph pyramid. Hierarchical spring systems encode the spatial relationships of the features (attributes of the graph pyramid) describing the parts and enforce them by spring-like behavior during tracking. Articulation points connecting the parts of the object allow to transfer position information from reliable to ambiguous parts. Tracking is done in an iterative process by combining the hypotheses of simple trackers with the hypotheses extracted from the hierarchical spring systems.",
                "year": 2011,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Feature interaction: a critical review and considered forecast": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128602003523/pdfft?md5=58b2f827962ad779950c49c196e222f8&pid=1-s2.0-S1389128602003523-main.pdf",
                "title": "Feature interaction: a critical review and considered forecast",
                "abstract": "The state of the art of the field of feature interactions in telecommunications services is reviewed, concentrating on three major research trends: software engineering approaches, formal methods, and on line techniques. Then, the impact of the new, emerging architectures on the feature interaction problem is considered. A forecast is made about how research in feature interactions needs to readjust to address the new challenges posed by the emerging architectures.",
                "year": 2003,
                "publisher": "Computer Networks"
            }
        }
    },
    "Computational insights into differential interaction of mammalian angiotensin-converting enzyme 2 with the SARS-CoV-2 spike receptor binding domain": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482521008118/pdfft?md5=62f5baf17d5968e823d217edf7f5e819&pid=1-s2.0-S0010482521008118-main.pdf",
                "title": "Computational insights into differential interaction of mammalian angiotensin-converting enzyme 2 with the SARS-CoV-2 spike receptor binding domain",
                "abstract": "The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the causative agent of the COVID-19 pandemic. Angiotensin-converting enzyme 2 (ACE2) has been identified as the host cell receptor that binds to the receptor-binding domain (RBD) of the SARS-COV-2 spike protein and mediates cell entry. Because the ACE2 proteins are widely available in mammals, it is important to investigate the interactions between the RBD and the ACE2 of other mammals. Here we analyzed the sequences of ACE2 proteins from 16 mammals, predicted the structures of ACE2-RBD complexes by homology modeling, and refined the complexes using molecular dynamics simulation. Analyses on sequence, structure, and dynamics synergistically provide valuable insights into the interactions between ACE2 and RBD. The analysis outcomes suggest that the ACE2 of bovine, cat, and panda form strong binding interactions with RBD, while in the cases of rat, least horseshoe bat, horse, pig, mouse, and civet, the ACE2 proteins interact weakly with RBD.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Rapidly finding CAD features using database optimization": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448515001232/pdfft?md5=bd244f8eb7810b33545ab7a2768400d9&pid=1-s2.0-S0010448515001232-main.pdf",
                "title": "Rapidly finding CAD features using database optimization",
                "abstract": "Automatic feature recognition aids downstream processes such as engineering analysis and manufacturing planning. Not all features can be defined in advance; a declarative approach allows engineers to specify new features without having to design algorithms to find them. Naive translation of declarations leads to executable algorithms with high time complexity. Database queries are also expressed declaratively; there is a large literature on optimizing query plans for efficient execution of database queries. Our earlier work investigated applying such technology to feature recognition, using a testbed interfacing a database system (SQLite) to a CAD modeler (CADfix). Feature declarations were translated into SQL queries which are then executed.\n\nThe current paper extends this approach, using the PostgreSQL database, and provides several new insights: (i) query optimization works quite differently in these two databases, (ii) with care, an approach to query translation can be devised that works well for both databases, and (iii) when finding various simple common features, linear time performance can be achieved with respect to model size, with acceptable times for real industrial models. Further results also show how (i) lazy evaluation can be used to reduce the work performed by the CAD modeler, and (ii) estimating the time taken to compute various geometric operations can further improve the query plan. Experimental results are presented to validate our main conclusions.",
                "year": 2015,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "A novel locomotion synthesis and optimisation framework for insects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849313001623/pdfft?md5=15e501c40e62552ddbb5bd5caac795ee&pid=1-s2.0-S0097849313001623-main.pdf",
                "title": "A novel locomotion synthesis and optimisation framework for insects",
                "abstract": "The motion modelling of insects is challenging because of their small size, high frequency movements and delicate dynamics model. This paper presents a novel framework which is specially tailored to synthesise this particular type of locomotion, inspired by the biological observation of the existence of Central Pattern Generator (CPG) which regulates the cyclic motion of creatures. In our method, the CPG is organically integrated with the modelling of dynamics with assistance of the abstract actuation mechanism, Virtual Model Control, to provide physically plausible results. To achieve realistic and natural motion, it is often not practical to manually set up the control parameters and we develop an automatic strategy to optimise such non-linear non-convex problem with the Covariance Matrix Adaptation which selects suitable candidates through an evolutionary process. This framework offers high level of control of characters' motion states, speed and directions, and is flexible to adapt characters' motion to perturbations and complex environments.",
                "year": 2014,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Management science semantics for object-oriented business modelling in BPR": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950584998000512/pdfft?md5=a0ec21fd46d783baca92ec836baa52f0&pid=1-s2.0-S0950584998000512-main.pdf",
                "title": "Management science semantics for object-oriented business modelling in BPR",
                "abstract": "BPR methodologies should justify their capability for understanding an organization's strategy, business structure and processes. For this reason techniques that provide understanding of the current and future desired positions of the organization and the discrepancy between them as well as consistent business modelling techniques are required. One of the main drawbacks of existing business modelling methodologies is the lack of theoretical underpining associated with the concepts used in business modelling. This paper presents a BPR methodology called agent relationship morphism analysis (ARMA) that goes beyond the limitations of the existing BPR methodologies taking a holistic view of the organization. In ARMA the modelling of the business environment is achieved with the use of three perspectives: the structural, behavioural and process. A technique called agent relationship modelling (ARM) has been developed for modelling the structural perspective. The more dynamic organizational concepts are described in the behavioural and process perspectives. These perspectives are modelled in a technique called agent/object lifecycles (ALCs/OLCs).",
                "year": 1998,
                "publisher": "Information and Software Technology"
            }
        }
    },
    "Perspective approximations": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/026288569090064C/pdfft?md5=e5479fa94c1057aaf6366e42e0107a5f&pid=1-s2.0-026288569090064C-main.pdf",
                "title": "Perspective approximations",
                "abstract": "In recent years, researchers in computer vision working on problems such as object recognition, shape reconstruction, shape from texture, shape from contour, pose estimation, etc., have employed in their analyses approximations of the perspective projection as the image formation process. Depending on the task, these approximations often yield very good results and present the advantage of simplicity. Indeed when one relates lengths, angles or areas on the image with the respective units in the 3D world assuming perspective projection, the resulting expressions are very complex, and consequently they complicate the recovery process. However, if we assume that the image is formed with a projection which is a good approximation of the perspective, then the recovery process becomes easier. Two such approximations, are described, the paraperspective and the orthoperspective, and it is shown that for some tasks the error introduced by the use of such an approximation is negligible. Applications of these projections to the problems of shape from texture, shape from contour, and object recognition related problems (such as determining the view vector and pose estimation) are also described.",
                "year": 1990,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Sensitivity Analysis of Electromagnetic Scattering from Dielectric Targets with Polynomial Chaos Expansion and Method of Moments": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Sensitivity Analysis of Electromagnetic Scattering from Dielectric Targets with Polynomial Chaos Expansion and Method of Moments",
                "abstract": "In this paper, an adaptive polynomial chaos expansion method (PCE) based on the method of moments (MoM) is proposed to construct surrogate models for electromagnetic scattering and further sensitivity analysis. The MoM is applied to accurately solve the electric field integral equation (EFIE) of electromagnetic scattering from homogeneous dielectric targets. Within the bistatic radar cross section (RCS) as the research object, the adaptive PCE algorithm is devoted to selecting the appropriate order to construct the multivariate surrogate model. The corresponding sensitivity results are given by the further derivative operation, which is compared with those of the finite difference method (FDM). Several examples are provided to demonstrate the effectiveness of the proposed algorithm for sensitivity analysis of electromagnetic scattering from homogeneous dielectric targets.",
                "year": 2024,
                "publisher": "CMES - Computer Modeling in Engineering and Sciences"
            }
        }
    },
    "A Review of Force Control Techniques in Friction Stir Process": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050915038326/pdfft?md5=c7342ad54ea731457bb8282c4ec5c8fa&pid=1-s2.0-S1877050915038326-main.pdf",
                "title": "A Review of Force Control Techniques in Friction Stir Process",
                "abstract": "This paper reviews current force control techniques in friction stir welding for aluminium alloys. In the context of friction stir process, travel force control, axial force control and torque control are studied in accordance to the control technique, tool geometry and process parameters such as travel speed, tool rotational speed and tilt angle that significantly affect the welding process. Thus, this study highlights the potentials and shortcomings of this control strategy which eventually leads to better performance in friction stir process. It provides insights on ways to improve the quality of welding in terms of microstructure and consistency of weldment formation as well as the elimination of wormhole generation.",
                "year": 2015,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Mobile agents for information retrieval in hybrid simulation environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1084804505000548/pdfft?md5=b77054a64b7c4e8b368b7fac3f2192ec&pid=1-s2.0-S1084804505000548-main.pdf",
                "title": "Mobile agents for information retrieval in hybrid simulation environment",
                "abstract": "In this paper, we propose a hybrid simulation environment that incorporates with wired/wireless networks, IEEE standard 1516 high-level architecture (HLA), and IBM Aglets mobile agent system. Therefore, HLA simulations are not restricted to be participated solely by using desktop computers with cable connections. Users can use a wide variety of devices to join in HLA simulations and explicitly exclude from junk data in terms of a personalized data filtering policy. Based on data correlation between HLA objects and a client's data filtering policy, we employ the simulation environment manager in distributing a client to an appropriate federate server (FS). In particular, a mobile agent, namely data filtering agent, is devised to temporarily reside at the FS to perform mobile agent-based data distribution management for clients. As a result, the clients can receive the most interested information corresponding to their pre-defined data filtering policies. Once either the data transmission quality within the wireless network is degraded below a threshold or the clients abnormally modify the data filtering policies, their own mobile agents carry out migrations to provide the users with the ubiquitous and seamless services. Consequently, the users can use any mobile device as well as using a desktop computer in a stationary point to participate in the HLA simulations. The experimental results also show that the proposed mobile agent-based data distribution can raise adaptability and applicability to large-scale HLA simulations.",
                "year": 2007,
                "publisher": "Journal of Network and Computer Applications"
            }
        }
    },
    "Aligning an ERP system with enterprise requirements: An object-process based approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0166361505000771/pdfft?md5=83b849f1b833bd8f0ffdf2db6478456b&pid=1-s2.0-S0166361505000771-main.pdf",
                "title": "Aligning an ERP system with enterprise requirements: An object-process based approach",
                "abstract": "One of the main problems in ERP implementation projects is how to align an off-the-shelf software package with the business processes of the enterprise implementing it. The paper proposes a requirement-driven approach, which benefits from reusing the business process design without being restricted by predefined solutions and criteria.\n\nThe approach applies an iterative alignment process, which employs an algorithm that matches a model of the enterprise requirements with a model of the ERP system capabilities. The algorithm identifies possible matches between the two models and evaluates the gaps between them despite differences in their completeness and detail level. It provides the enterprise with a set of feasible combinations of requirements that can be satisfied by the ERP system as a basis for making implementation decisions. We use Object-Process Methodology (OPM) to model both the ERP system and the enterprise requirements, and utilize the pair of resulting OPM models as input for the matching algorithm. The alignment algorithm has been tested in an experimental study, whose encouraging results demonstrate the ability of the approach to provide a satisfactory solution to the problem of aligning an ERP software package with an enterprise business model.",
                "year": 2005,
                "publisher": "Computers in Industry"
            }
        }
    },
    "On the computation of the affine skeletons of planar curves and the detection of skew symmetry": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320300000455/pdfft?md5=2afadb37df025078907aaef753acab01&pid=1-s2.0-S0031320300000455-main.pdf",
                "title": "On the computation of the affine skeletons of planar curves and the detection of skew symmetry",
                "abstract": "In this paper we discuss a new approach to compute discrete skeletons of planar shapes which is based on affine distances, being therefore affine invariant. The method works with generic curves that may contain concave sections. A dynamical interpretation of the affine skeleton construction, based on curve evolution, is discussed as well. We propose an efficient implementation of the method and give examples. We also demonstrate how to use this method to detect affine skew symmetry in real images.",
                "year": 2001,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "On the use of the horizon of a translating planar curve": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167865589900196/pdfft?md5=885908c165da477e26595f8f64c82c2e&pid=1-s2.0-0167865589900196-main.pdf",
                "title": "On the use of the horizon of a translating planar curve",
                "abstract": "Translation of a planar curve is considered. Using the horizon, orientation of curve plane and any curve tangent may be determined from one image, optic flow and focus of expansion from two images, spatial velocity and position from three images.",
                "year": 1989,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "Towards Knowledge Ecosystems: Modelling Knowledge Dynamics in Environmental Systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050914011442/pdfft?md5=510c1e94abbfb140ce96fee1b6380678&pid=1-s2.0-S1877050914011442-main.pdf",
                "title": "Towards Knowledge Ecosystems: Modelling Knowledge Dynamics in Environmental Systems",
                "abstract": "In order to understand basic mechanisms of so-called Knowledge Ecosystems, the paper presents a study for modelling the emergence and diffusion of knowledge in regards to environmental conditions. The text gives an outline for the description of Knowledge Ecosystems by integrating 1) models for environmental dynamics based on resources and attractiveness, and 2) models for knowledge dynamics based on the behaviors and processes of knowledge agents (innovators) and agencies. As key methods, the work employs Cellular Automata for the modelling of knowledge environments, as well as agent models for the simulation of knowledge agents (innovators). The paper presents preliminary results of the ongoing study, including a first version of both scopes integrated into one descriptive system.",
                "year": 2014,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Detection of COVID-19 severity using blood gas analysis parameters and Harris hawks optimized extreme learning machine": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482521009604/pdfft?md5=b3932a12c7fb1de5cbe0864798954e04&pid=1-s2.0-S0010482521009604-main.pdf",
                "title": "Detection of COVID-19 severity using blood gas analysis parameters and Harris hawks optimized extreme learning machine",
                "abstract": "Coronavirus disease-2019 (COVID-19) has made the world more cautious about widespread viruses, and a tragic pandemic that was caused by a novel coronavirus has harmed human beings in recent years. The new coronavirus pneumonia outbreak is spreading rapidly worldwide. We collect arterial blood samples from 51 patients with a COVID-19 diagnosis. Blood gas analysis is performed using a Siemens RAPID Point 500 blood gas analyzer. To accurately determine the factors that play a decisive role in the early recognition and discrimination of COVID-19 severity, a prediction framework that is based on an improved binary Harris hawk optimization (HHO) algorithm in combination with a kernel extreme learning machine is proposed in this paper. This method uses specular reflection learning to improve the original HHO algorithm and is referred to as HHOSRL. The experimental results show that the selected indicators, such as age, partial pressure of oxygen, oxygen saturation, sodium ion concentration, and lactic acid, are essential for the early accurate assessment of COVID-19 severity by the proposed feature selection method. The simulation results show that the established methodlogy can achieve promising performance. We believe that our proposed model provides an effective strategy for accurate early assessment of COVID-19 and distinguishing disease severity. The codes of HHO will be updated in https://aliasgharheidari.com/HHO.html.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "An imposed etic approach with Schwartz polar dimensions to explore cross-cultural use of social network services": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720618310772/pdfft?md5=82f78806667c589fc157634fdb06d9e3&pid=1-s2.0-S0378720618310772-main.pdf",
                "title": "An imposed etic approach with Schwartz polar dimensions to explore cross-cultural use of social network services",
                "abstract": "Cross-cultural studies in Information Systems (IS) have experienced an overreliance on Hofstede’s theory, which has impeded researchers from exploring alternative theoretical lenses. We examine the moderating effects of Schwartz’s Polar Dimensions on users’ perception of Social Network Service, highlighting cultural issues associated with IS privacy, trust, and social capital. We found that the effect of perceived reputation is more powerful in American culture than Chinese culture, though network centrality has a stronger effect on use intention among Chinese users than American users. We also introduce the imposed etic approach, which has been rarely seen in IS research.",
                "year": 2020,
                "publisher": "Information & Management"
            }
        }
    },
    "Network anomaly detection through nonlinear analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404810000362/pdfft?md5=350ad21f1201e2da84998b9a53dd143b&pid=1-s2.0-S0167404810000362-main.pdf",
                "title": "Network anomaly detection through nonlinear analysis",
                "abstract": "Nowadays every network is susceptible on a daily basis to a significant number of different threats and attacks both from the inside and outside world. Some attacks only exploit system vulnerabilities and their traffic pattern is undistinguishable from normal behavior, but in many cases the attack mechanisms combine protocol or OS tampering activity with a specific traffic pattern having its own particular characteristics. Since these traffic anomalies are now conceived as a structural part of the overall network traffic, it is more and more important to automatically detect, classify and identify them in order to react promptly and adequately. In this work we present a novel approach to network-based anomaly detection based on the analysis of non-stationary properties and “hidden” recurrence patterns occurring in the aggregated IP traffic flows. In the observation of the above transition patterns for detecting anomalous behaviors, we adopted recurrence quantification analysis, a nonlinear technique widely used in many science fields to explore the hidden dynamics and time correlations of statistical time series. Our model demonstrated to be effective for providing a deterministic interpretation of recurrence patterns originated by the complex traffic dynamics observable during the occurrence of “noisy” network anomaly phenomena (characterized by measurable variations in the statistical properties of the traffic time series), and hence for developing qualitative and quantitative observations that can be reliably used in detecting such events.",
                "year": 2010,
                "publisher": "Computers & Security"
            }
        }
    },
    "Implementing nonlinear least squares approach to simulate the dynamic response of laminated nanocomposite arch with magnetorheological elastomer matrix": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Implementing nonlinear least squares approach to simulate the dynamic response of laminated nanocomposite arch with magnetorheological elastomer matrix",
                "abstract": "This paper presents an investigation into the dynamics of deep arch structures constructed using an innovative composite material. The composite involves a magnetorheological elastomer (MRE) matrix strengthened by graphene platelets (GPLs) arranged with functionally graded patterns across laminated layers. The study commences by thoroughly characterizing the mechanical attributes of the MRE matrix through the first-order generalized Kelvin-Voigt viscoelastic model. A nonlinear regression method, employing the nonlinear least squares approach, establishes the dependencies of storage and loss moduli on factors including magnetic field and iron particle content. Model validation ensues through comparisons with experimental findings from existing literature, utilizing robust metrics such as root mean square error and correlation coefficients. Homogenization of the composite material is realized through the Halpin-Tsai micromechanical approach, extracting equivalent Young's modulus governing the behavior of the laminated deep arch structure. Theoretical frameworks rooted in third-order shear deformation arch theory, Sanders’ kinematic relations appropriate to deep arches and linear viscoelasticity yield the dynamic equations governing the arch's vibration response, refined via Hamilton's principle. Utilizing the generalized differential quadrature (GDQ) tool, the complex vibration frequencies of the deep arch system are computed, ensuring accurate and efficient capture of the composite structure's dynamic behavior. The analysis extensively explores the impacts of varying GPL volume fractions, iron particle concentrations, and magnetic fields on the arch's vibrational response. The study's outcomes provide insightful understandings into the interactions between GPL reinforcements and the MRE matrix in the context of deep arch geometries.",
                "year": 2024,
                "publisher": "Engineering Analysis with Boundary Elements"
            }
        }
    },
    "An affordance-based approach to architectural theory, design, and practice": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X09000039/pdfft?md5=df618aae47bcefaf27f495982d47b90e&pid=1-s2.0-S0142694X09000039-main.pdf",
                "title": "An affordance-based approach to architectural theory, design, and practice",
                "abstract": "The idea of affordance, borrowed from perceptual psychology, is applied to the domain of architecture. As to architectural theory, affordances can be used as a conceptual framework to understand the relationship between environments and occupants, especially with respect to form and function. Regarding architectural design, the concept of affordance allows for a common theoretical basis to improve the design process. Concerning architectural practice, affordances can be used as a tool to explore the connection between the intentions of the design with how the artifact is actually used, leading to archived knowledge, and the potential for avoiding common design failures.",
                "year": 2009,
                "publisher": "Design Studies"
            }
        }
    },
    "Managing OAM&P requirement conflicts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157814000056/pdfft?md5=95add8702fcc83a939eb2604ae5c6969&pid=1-s2.0-S1319157814000056-main.pdf",
                "title": "Managing OAM&P requirement conflicts",
                "abstract": "Specifying consistent services at early project stages is a telecommunication service engineering challenge. Service logic inconsistencies, also known as feature interactions (FIs), can affect various types of services ranging from signaling protocol features to value-added end user services. This problem has been investigated for all those types of services. However, inconsistencies of Operation, Administration, Management and Provisioning (OAM&P) services have not been sufficiently addressed. The present paper studies the detection of OAM&P service inconsistencies at the software requirement specification stage. The aim is at the resolution of the problem before reaching the implementation step. The basic idea of the here reported approach is to consider service inconsistencies as software requirement conflicts. The contribution of the present paper consists of an OAM&P requirement modeling language and a requirement conflict detection method. A validation with a case study is reported.",
                "year": 2014,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "The hybrid constraint equation for motion extraction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0262885689900474/pdfft?md5=f8af9cb04843fd200eb4544bf9bcf987&pid=1-s2.0-0262885689900474-main.pdf",
                "title": "The hybrid constraint equation for motion extraction",
                "abstract": "A new derivation of the constraint equation for motion extraction in the image flow is presented. The derivation was obtained by applying the constraint equation to the Fourier transform of the 2D images. It resulted in a hybrid equation joining time-space and frequency notations. The hybrid equation was numerically solved for the 1D case and tested with many image sequences representing coherent, rigid and incoherent motion patterns. Outlines of the 2D case are also presented with some solved examples. The algorithm seems superior to former derivations of the constraint equation for its ability to extract the movement parameters (magnitude and direction) in non-homogeneous movement fields for displacements greater than pixel size. The relevance of the present procedures to machine and organic motion detection is discussed.",
                "year": 1989,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Machine learning approach for label-free rapid detection and identification of virus using Raman spectra": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2667102622000766/pdfft?md5=5e797ba3c24cc937fc248c61deb404d1&pid=1-s2.0-S2667102622000766-main.pdf",
                "title": "Machine learning approach for label-free rapid detection and identification of virus using Raman spectra",
                "abstract": "Objective\n\nThe objective of this study was to develop a robust method for rapid detection and identification of the virus based on Raman spectroscopy combined with machine learning approach.",
                "year": 2023,
                "publisher": "Intelligent Medicine"
            }
        }
    },
    "Ontological modelling of content management and provision": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950584908000475/pdfft?md5=ca5373f4a1c5966e83aece8c4b33b305&pid=1-s2.0-S0950584908000475-main.pdf",
                "title": "Ontological modelling of content management and provision",
                "abstract": "Information provision to address the changing requirements can be best supported by content management. The current information technology enables information to be stored and provided from various distributed sources. To identify and retrieve relevant information requires effective mechanisms for information discovery and assembly. This paper presents a method, which enables the design of such mechanisms, with a set of techniques for articulating and profiling users’ requirements, formulating information provision specifications, realising management of information content in repositories, and facilitating response to the user’s requirements dynamically during the process of knowledge construction. These functions are represented in an ontology which integrates the capability of the mechanisms. The ontological modelling in this paper has adopted semiotics principles with embedded norms to ensure coherent course of actions represented in these mechanisms.",
                "year": 2008,
                "publisher": "Information and Software Technology"
            }
        }
    },
    "Medical image registration using deep neural networks: A comprehensive review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045790620306224/pdfft?md5=0050a2064928fcd24b19ef29d2c3cf55&pid=1-s2.0-S0045790620306224-main.pdf",
                "title": "Medical image registration using deep neural networks: A comprehensive review",
                "abstract": "Image-guided interventions are saving the lives of a large number of patients where the image registration should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, a huge progress in the field of machine learning has recently made by the possibility of implementing deep neural networks on the contemporary many-core GPUs. It has opened up a promising window to challenge with many medical applications in more efficient and effective ways, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confining challenges, novelties and main contributions, key-enabling techniques, future directions, and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature.",
                "year": 2020,
                "publisher": "Computers & Electrical Engineering"
            }
        }
    },
    "The use of microworlds to study dynamic decision making": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563204000275/pdfft?md5=dd290217ca64f660564aa0866fc089d1&pid=1-s2.0-S0747563204000275-main.pdf",
                "title": "The use of microworlds to study dynamic decision making",
                "abstract": "Dynamic decision-making (DDM) research grew out of a perceived need for understanding how people control dynamic, complex, real-world systems. DDM has describable characteristics and, with some unavoidable sacrifice of realism, is suitable for study in a laboratory setting through the use of complex computer simulations commonly called `microworlds'. This paper presents a taxonomic definition of DDM, an updated review of existing microworlds and their characteristics, and a set of cognitive demands imposed by DDM tasks. Although the study of DDM has garnered little attention to date, we believe that both technological advancement and the relationships between DDM and naturalistic decision making, complex problem solving, and general systems theory have made DDM a viable process by which to study how people make decisions in dynamic, real-world environments.",
                "year": 2005,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Image segmentation by conventional and information-integrating techniques: a synopsis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0262885685900137/pdfft?md5=3fc16becf7219f51a7da30220cf20b53&pid=1-s2.0-0262885685900137-main.pdf",
                "title": "Image segmentation by conventional and information-integrating techniques: a synopsis",
                "abstract": "A synoptic discussion of image segmentation is presented. The aim is to provide an overall understanding of the general problems and issues associated with various segmentation techniques. The discussion has been organized in two parts: the first part is on those conventional techniques which use mainly information from one type of data; the second part is on those which try to combine data from several sources to obtain interpretations that cannot be obtained or would be hard to obtain from any single source of data. The paper stresses the importance, for segmentation, of data integration from several sensors and data integration over time, particularly the use of motion. It further emphasizes the importance of stating clearly the assumptions before developing or using a particular image segmentation algorithm.",
                "year": 1985,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "A scaling algorithm for the generation of jerk-limited trajectories in the operational space": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584515300636/pdfft?md5=354be3dbd74741057fe7f174fe919f81&pid=1-s2.0-S0736584515300636-main.pdf",
                "title": "A scaling algorithm for the generation of jerk-limited trajectories in the operational space",
                "abstract": "Kinematic singularities represent a relevant problem for trajectories that are defined in the operational space. In case of industrial applications characterized by non-repetitive tasks, feasibility cannot be checked in advance, so that appropriate methods have been developed for the online management of otherwise critical situations. In this paper, a scaling scheme proposed in the past for the automatic handling of possibly unfeasible trajectories is revised in order to generate jerk-limited reference signals: close to critical points, trajectories are appropriately slowed down so as to guarantee an accurate tracking of the assigned path in the operational space. The actual performances of the proposed system have been experimentally verified on a commercial manipulator by means of extensive tests.",
                "year": 2017,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Random vector functional link network: Recent developments, applications, and future directions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494623003952/pdfft?md5=0ddc10d1605ab312546a8ffcb9fcdbac&pid=1-s2.0-S1568494623003952-main.pdf",
                "title": "Random vector functional link network: Recent developments, applications, and future directions",
                "abstract": "Neural networks have been successfully employed in various domains such as classification, regression and clustering, etc. Generally, the back propagation (BP) based iterative approaches are used to train the neural networks, however, it results in the issues of local minima, sensitivity to learning rate and slow convergence. To overcome these issues, randomization based neural networks such as random vector functional link (RVFL) network have been proposed. RVFL model has several characteristics such as fast training speed, direct links, simple architecture, and universal approximation capability, that make it a viable randomized neural network. This article presents the first comprehensive review of the evolution of RVFL model, which can serve as the extensive summary for the beginners as well as practitioners. We discuss the shallow RVFLs, ensemble RVFLs, deep RVFLs and ensemble deep RVFL models. The variations, improvements and applications of RVFL models are discussed in detail. Moreover, we discuss the different hyperparameter optimization techniques followed in the literature to improve the generalization performance of the RVFL model. Finally, we present potential future research directions/opportunities that can inspire the researchers to improve the RVFL’s architecture and learning algorithm further.",
                "year": 2023,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Extraction of texture elements from images of shaded scenes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0031320393902039/pdfft?md5=a78afc8aabc9653c90a5cbe1d217f3ba&pid=1-s2.0-0031320393902039-main.pdf",
                "title": "Extraction of texture elements from images of shaded scenes",
                "abstract": "A method is proposed that can extract texture elements in spite of the presence of global shading variations. It is assumed that a texel contains at least one brightness maximum and that the boundary of the texel is a closed contour with constant brightness below the maximum. The method decomposes a grayscale image into a stack of binary images by thresholding the grayscale image with a set of appropriately chosen thresholds. As a result, the binary image stack contains many connected regions, among which the most appropriate ones are selected as final texels. This selection process examines the binary image stack from top to bottom and decides on the basis of a few rules which of the regions become final. Experimental results for both synthesized and real images of natural scenes are reported, demonstrating the suitability of the extracted texels for computing orientation fields and for segmenting multiple textures.",
                "year": 1993,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Magpie: Experiences in supporting Semantic Web browsing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826807000285/pdfft?md5=57830b29bf67d136f8e1a4f5790bd311&pid=1-s2.0-S1570826807000285-main.pdf",
                "title": "Magpie: Experiences in supporting Semantic Web browsing",
                "abstract": "Magpie has been one of the first truly effective approaches to bringing semantics into the web browsing experience. The key innovation brought by Magpie was the replacement of a manual annotation process by an automatically associated ontology-based semantic layer over web resources, which ensured added value at no cost for the user. Magpie also differs from older open hypermedia systems: its associations between entities in a web page and semantic concepts from an ontology enable link typing and subsequent interpretation of the resource. The semantic layer in Magpie also facilitates locating semantic services and making them available to the user, so that they can be manually activated by a user or opportunistically triggered when appropriate patterns are encountered during browsing. In this paper we track the evolution of Magpie as a technology for developing open and flexible Semantic Web applications. Magpie emerged from our research into user-accessible Semantic Web, and we use this viewpoint to assess the role of tools like Magpie in making semantic content useful for ordinary users. We see such tools as crucial in bootstrapping the Semantic Web through the automation of the knowledge generation process.",
                "year": 2007,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Virtual world continuance intention": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585317302952/pdfft?md5=568cecb85159bbbad26dc7fafeec232e&pid=1-s2.0-S0736585317302952-main.pdf",
                "title": "Virtual world continuance intention",
                "abstract": "Virtual worlds face stiff competition and the threat of a high user attrition rate. While considerable research has investigated factors for continued use of virtual worlds, there has been little attention on how users’ embodiment in virtual worlds (i.e., avatars) affect this important cognitive decision. We developed a research model drawing on self-awareness theory and presence, to explain how user continuance intention may be influenced by avatars, in particular self-similar ones. Through a survey with 209 virtual world users, we tested our research model using structural equation modeling. Results indicate that avatar-self similarity positively affects continuance intention through self-awareness and self-presence. The findings verify that avatar characteristics influence post-adoption behavior and emphasize the importance of user autonomy in design.",
                "year": 2017,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Development and Validation of a Functional Behavioural Assessment Ontology to Support Behavioural Health Interventions": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Development and Validation of a Functional Behavioural Assessment Ontology to Support Behavioural Health Interventions",
                "abstract": "Background\n\nIn the cognitive-behavioral approach, Functional Behavioural Assessment is one of the most effective methods to identify the variables that determine a problem behavior. In this context, the use of modern technologies can encourage the collection and sharing of behavioral patterns, effective intervention strategies, and statistical evidence about antecedents and consequences of clusters of problem behaviors, encouraging the designing of function-based interventions.",
                "year": 2018,
                "publisher": "JMIR Medical Informatics"
            }
        }
    },
    "Chapter 13: MODELLING ENZYME - LIGAND INTERACTIONS": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1380732301800145/pdfft?md5=adfd30f7f5c31ed28cb914d630f60de7&pid=1-s2.0-S1380732301800145-main.pdf",
                "title": "Chapter 13: MODELLING ENZYME - LIGAND INTERACTIONS",
                "abstract": "The purpose of this chapter is to discuss enzyme-ligand interactions. Most physiological and pharmacological responses are mediated by specific receptor-ligand interactions. These receptors are macromolecules specialized in recognizing a specific molecular pattern from the large number of surrounding molecular species with which they could interact, the term “receptor” being used to designate pharmacological receptors, enzymes, antibodies and DNA. The major aspects that determine a biomolecular recognition event, all of them involving changes in both entropy and enthalpy, are: (1) the structural and energetic complementarity between the ligand and the receptor, (2) the conformational rearrangements both structures undergo upon complexation, (3) their desolvation, and (4) the loss of rotational and translational freedom of the ligand when it binds to the active site. The difficulty lies in quantitate and scale the relative contributions of those aspects to the free energy of binding (hence, predicting the affinity).",
                "year": 2001,
                "publisher": "Theoretical and Computational Chemistry"
            }
        }
    },
    "Doctor, patient and computer—A framework for the new consultation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505608001202/pdfft?md5=f35cf26aa50069a957d3ab6b132cc861&pid=1-s2.0-S1386505608001202-main.pdf",
                "title": "Doctor, patient and computer—A framework for the new consultation",
                "abstract": "Purpose\n\nThe use of a computer during general/family practice consultations is on the rise across the world, yet little is known about the effect the use of a computer may have on the all important physician–patient relationship. This paper provides a framework for further analysis of computers influence on physician–patient interactions during general practice consultations.",
                "year": 2009,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "A semi-supervised concept-cognitive computing system for dynamic classification decision making with limited feedback information": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221723009839/pdfft?md5=84ef354f90eeb10cc565a79566285a99&pid=1-s2.0-S0377221723009839-main.pdf",
                "title": "A semi-supervised concept-cognitive computing system for dynamic classification decision making with limited feedback information",
                "abstract": "In dynamic environments, making classification decisions based on classical intelligent decision support systems is a challenge, as the classification performance of decision-making and the time-cost of learning need to be considered simultaneously. Moreover, many tasks of classification decisions lack label information because annotating data is time-consuming, labor-intensive and expensive process. This means that some standard intelligent decision support systems will perform inferior performance if they cannot dynamically make full use of the information behind abundant unlabeled data. Therefore, by incorporating knowledge representation and dynamic updating mechanisms into concept learning processes, we introduce a novel dynamic concept learning approach, namely semi-supervised concept-cognitive computing system (s2C3S), for making classification decisions by jointly utilizing some labeled data and abundant unlabeled data under dynamic environments. A theoretical analysis has shown that the proposed s2C3S can achieve significantly lower computational costs and higher classification accuracies than the existing incremental K Nearest Neighbor method (IKNN) and concept-cognitive computing system (C3S). The experimental results on various datasets further demonstrated that our system is effective for dynamic classification decision-making with limited labeled data under dynamic learning processes. Additionally, s2C3S can also be applied to computer-assisted intelligent diagnosis from the given medical images (such as chest X-ray images) dynamically and accurately.",
                "year": 2024,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Mobile social networking and salesperson maladaptive dependence behaviors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563217307069/pdfft?md5=163abd75b0b81d6c40c3128580b8f6f5&pid=1-s2.0-S0747563217307069-main.pdf",
                "title": "Mobile social networking and salesperson maladaptive dependence behaviors",
                "abstract": "This study investigates technology dependence associated with the work-related use of mobile social networking (MSN) by salespeople. A scale for maladaptive technology dependence behaviors (MTDB) is developed and empirically validated using survey data from 242 mid-level sales managers in the US. Personal and job-related antecedents, as well as consequences of MTDB for sales outcomes, are also examined. Results suggest that emotional attachment to MSN and perceptions of its greater affordances for task accomplishment may lead to maladaptive behaviors of overreliance on MSN for job completion, blind trust, cognitive absorption and dysfunctional use. These associations increase in organizations with competitive psychological climate. Findings also show that using MSN for prospecting does not lead to maladaptive dependence, as opposed to using it for customer relationship maintenance. Salespeople using MSN for relationship maintenance exhibit more maladaptive behaviors if they experience work-related role stress. Finally, salespeople who exhibit MTDB are less likely to complete their assignments and participate in teamwork. These findings provide tools for organizations to develop technology use policies, design sales training, and enhance the work environment. Future studies can examine dependencies on others types of technologies (CRM, marketing automation, etc.), and in other contexts (online retailing, social media analytics, etc.)",
                "year": 2018,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Interactive design galleries: A general approach to interacting with design alternatives": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X17300315/pdfft?md5=034d3a62a94ada8ebc822c17c9aa1e54&pid=1-s2.0-S0142694X17300315-main.pdf",
                "title": "Interactive design galleries: A general approach to interacting with design alternatives",
                "abstract": "Designers work by exploring alternatives. While extant parametric modelers theoretically define alternatives, their interfaces generally provide access to designs serially. Our goal is to change this near-universal feature of parametric interfaces to support exploration using multiple alternatives. We built a prototype gallery system on a web browser that supports saving alternatives from three graph-based parametric modeling tools. Users can retrieve alternatives from the gallery, share them with others, and combine them to generate more alternatives. We evaluated this system qualitatively in a workshop with ten expert designers working on their own design problems. We learnt that users prefer the gallery to their accustomed workarounds. The study produced several directions for new user interface designs.",
                "year": 2017,
                "publisher": "Design Studies"
            }
        }
    },
    "Aligning story and gameplay through narrative goals": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Aligning story and gameplay through narrative goals",
                "abstract": "There is consensus that the intersection of gameplay and story is complicated. Some scholars conclude that these elements are fundamentally at odds with one another in game and narrative design. In this paper, we discuss how their relationship is symbiotic—not antagonistic—by articulating a novel nexus between story and gameplay. We term this nexus a narrative goal, defined as a player interpretation of a ludological goal, which in turn is a condition a player is expected to meet to succeed at the game. We articulate this nexus as part of a novel framework centered on Goals, Feedback, and Interpretation (GFI), which is useful for characterizing uniquely narrative phenomena that arises as part of a designer’s effort to align story and gameplay. GFI complements the long-standing Mechanics, Dynamics, and Aesthetics (MDA) framework, and we evidence its utility by analyzing problems when trying to align story and gameplay, as well as offering perspectives on how to design them to achieve particular narrative effects. We believe that narrative goals and GFI have potential to clarify the narrative design process, and offer them as conceptual tools for researchers and practitioners to decompose, study, and design a broad class of games that are intended to elicit a sense of story.",
                "year": 2023,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "Fast single image dehazing based on a regression model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231217305192/pdfft?md5=e65fa6f434c1064fee17870ad23dcff2&pid=1-s2.0-S0925231217305192-main.pdf",
                "title": "Fast single image dehazing based on a regression model",
                "abstract": "Given that single image dehazing is an ill-posed problem, it can be challenging to control the enhancement of haze images. In this paper, we propose a fast and accurate dehazing algorithm based on a learning framework. Using randomly generated training samples, we tackle the difficult problem of sampling haze/clear image pairs. Seven haze-relevant features based on image quality are extracted and analyzed. A regression model is learned using support vector regression (SVR), which can estimate the transmission map accurately. Further, a new method is presented to estimate the dynamic atmospheric light, which improves the performance in the sky and shadow regions. Experimental results demonstrate that the proposed approach has a lower computational complexity, and the dehazing results are visually appealing even on extremely challenging photos, such as street views, thick fog, and sky regions. Subjective analysis and objective quality assessments demonstrate that, the proposed method generates superior results than the state-of-the-art methods.",
                "year": 2017,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Hierarchical data placement for navigational multimedia applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169023X02001246/pdfft?md5=5726002c0c22490b050d119bdb1f0c06&pid=1-s2.0-S0169023X02001246-main.pdf",
                "title": "Hierarchical data placement for navigational multimedia applications",
                "abstract": "Navigation has added interactivity in nowadays multimedia applications which support effective accessing to objects of various formats and presentation requirements. Storage issues need to be reconsidered for the new type of navigational multimedia applications, in order to improve system’s performance. This paper addresses the problem of multimedia data storage towards improving data accessibility and request servicing under navigational applications. A navigational graph-based model (for the multimedia data representation) is proposed to guide the data placement under a hierarchical storage topology. The multimedia data dependencies, access frequencies and timing constraints are used to characterize the graph nodes which correspond to multimedia objects allocated at the tertiary storage level. Based on certain defined popularity criteria, data are elevated and placed on secondary level towards improving both the request servicing and data accessibility. The proposed multimedia data elevation is a prefetching approach since it is performed “apriori” (not on demand) based on previously extracted user access patterns. Appropriate data placement policies are also employed at the secondary level, and a simulation model has been developed based on current commercial tertiary and secondary storage devices. This model is used to evaluate the proposed popularity based data elevation approach as employed under a hierarchical storage subsystem. Experimentation is performed under artificial data workloads and it is shown that the proposed hierarchical data placement approach considerably improves data accessing and request servicing in navigational multimedia applications. The iterative improvement placement is proven to outperform earlier related multimedia data placement policies with respect to commonly used performance metrics.",
                "year": 2003,
                "publisher": "Data & Knowledge Engineering"
            }
        }
    },
    "Theory and computation of electromagnetic fields and thermomechanical structure interaction for systems undergoing large deformations": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0021999119303894/pdfft?md5=d8d3171838de9446a12e1aeab4ddc3b1&pid=1-s2.0-S0021999119303894-main.pdf",
                "title": "Theory and computation of electromagnetic fields and thermomechanical structure interaction for systems undergoing large deformations",
                "abstract": "For an accurate description of electromagneto–thermomechanical systems, electromagnetic fields need to be described in a Eulerian frame, whereby the thermomechanics is solved in a Lagrangean frame. It is possible to map the Eulerian frame to the current placement of the matter and the Lagrangean frame to a reference placement. We present a rigorous and thermodynamically consistent derivation of governing equations for fully coupled electromagneto–thermomechanical systems properly handling finite deformations. A clear separation of the different frames is necessary. There are various attempts to formulate electromagnetism in the Lagrangean frame, or even to compute all fields in the current placement. Both formulations are challenging and heavily discussed in the literature. In this work, we propose another solution scheme that exploits the capabilities of advanced computational tools. Instead of amending the formulation, we can solve thermomechanics in the Lagrangean frame and electromagnetism in the Eulerian frame and manage the interaction between the fields. The approach is similar to its analog in fluid structure interaction, but more challenging because the field equations in electromagnetism must also be solved within the solid body while following their own different set of transformation rules. We additionally present a mesh-morphing algorithm necessary to accommodate finite deformations to solve the electromagnetic fields outside of the material body. We illustrate the use of the new formulation by developing an open-source implementation using the FEniCS package and applying this implementation to several engineering problems in electromagnetic structure interaction undergoing large deformations.",
                "year": 2019,
                "publisher": "Journal of Computational Physics"
            }
        }
    },
    "Active vision-based control schemes for autonomous navigation tasks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320399000588/pdfft?md5=d2f0d8fc73d7b29f3f9bbbd9e02a94f3&pid=1-s2.0-S0031320399000588-main.pdf",
                "title": "Active vision-based control schemes for autonomous navigation tasks",
                "abstract": "This paper deals with active-vision-based practical control schemes for collision avoidance as well as maintenance of clearance in a-priori unknown textured environments. These control schemes employ a visual motion cue, called the visual threat cue (VTC) as a sensory feedback signal to accomplish the desired tasks. The VTC provides some measure for a relative change in range as well as clearance between a 3D surface and a moving observer. It is a collective measure obtained directly from the raw data of gray level images, is independent of the type of 3D surface texture. It is measured in [time−1] units and needs no 3D reconstruction. The control schemes are based on a set of If–Then fuzzy rules with almost no knowledge about the vehicle dynamics, speed, heading direction, etc. They were implemented in real-time using a 486-based Personal Computer and a camera capable of undergoing 6-DOF motion.",
                "year": 2000,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "What can be seen in a noisy optical flow field projected by a moving planar patch in 3D space?": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320396001744/pdfft?md5=a105b426ea028a1c7b7d0471cb45e0b8&pid=1-s2.0-S0031320396001744-main.pdf",
                "title": "What can be seen in a noisy optical flow field projected by a moving planar patch in 3D space?",
                "abstract": "In this paper, we would like to propose a brand new interpretation to the so-called “structure-from-motion” (SFM) problem. The optical flow field projected by a moving rigid planar patch in 3D space is our main consideration. Instead of just obtaining an explicit 3D motion/pose solution like the old approaches did before, we focus our attention on analyzing its error sensitivity, uncertainty, and ambiguity from another point of view. Our new method can handle the above error analysis easily. As known well before, the optical flow field projected by a 3D moving planar patch can be completely expressed by eight coefficients (two for second-order, four for first-order, and two for zeroth-order). Based on these flow coefficients easily determined by a linear regression method or other similar approaches, the error sensitivity of 3D estimates can be analyzed quantitatively and qualitatively in a coarse-to-fine way. The concepts of camera fixation and singular value decomposition (SVD) play important roles in our analysis. There are three goals for our experiments: (1) To prove the correctness of the algorithms (simulated image). (2) To show the tendency of error sensitivity when the 3D poses of the target planar patch are varied in a controlled manner (simulated image). (3) To show that our analysis is workable in the real-world application (real-world image).",
                "year": 1997,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "On the use of time varying shading and surface rim irradiance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/016786559190033I/pdfft?md5=6d0c773108bb8defd2340d5c4734faf6&pid=1-s2.0-016786559190033I-main.pdf",
                "title": "On the use of time varying shading and surface rim irradiance",
                "abstract": "The question of using time varying image irradiance, produced by a static object and a distant moving light source, and the question of using image irradiance of the visual rim, are addressed. Motion constraint equations relating image irradiance with light source position and motion, and with surface orientation and visual surface curvature, are derived. Possible linear determination schemes are pointed out.",
                "year": 1991,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "A survey of deep learning approaches to image restoration": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231222002089/pdfft?md5=0a78d6dc9e56463ef6c26b4acc143cac&pid=1-s2.0-S0925231222002089-main.pdf",
                "title": "A survey of deep learning approaches to image restoration",
                "abstract": "In this paper, we present an extensive review on deep learning methods for image restoration tasks. Deep learning techniques, led by convolutional neural networks, have received a great deal of attention in almost all areas of image processing, especially in image classification. However, image restoration is a fundamental and challenging topic and plays significant roles in image processing, understanding and representation. It typically addresses image deblurring, denoising, dehazing and super-resolution. There are substantial differences in the approaches and mechanisms in deep learning methods for image restoration. Discriminative learning based methods are able to deal with issues of learning a restoration mapping function effectively, while optimisation models based methods can further enhance the performance with certain learning constraints. In this paper, we offer a comparative study of deep learning techniques in image denoising, deblurring, dehazing, and super-resolution, and summarise the principles involved in these tasks from various supervised deep network architectures, residual or skip connection and receptive field to unsupervised autoencoder mechanisms. Image quality criteria are also reviewed and their roles in image restoration are assessed. Based on our analysis, we further present an efficient network for deblurring and a couple of multi-objective training functions for super-resolution restoration tasks. The proposed methods are compared extensively with the state-of-the-art methods with both quantitative and qualitative analyses. Finally, we point out potential challenges and directions for future research.",
                "year": 2022,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Causal effects of affordance change on communication behavior: Empirical evidence from organizational and leadership social media use": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585320302082/pdfft?md5=ffed08b13c6616283448f595ef94e46c&pid=1-s2.0-S0736585320302082-main.pdf",
                "title": "Causal effects of affordance change on communication behavior: Empirical evidence from organizational and leadership social media use",
                "abstract": "This article provides empirical evidence for two hypotheses in the affordance literature. First, by leveraging a small affordance change — Twitter increasing its character limit from 140 to 280 on November 7, 2017, employing an instrumental variable approach, and examining 143,771 original tweets published by organizational and leadership accounts half a year before and after the intervention, we showed the direct causal relationship between affordances and communication behavior on digital media platforms. Second, by exploring what factors could explain the heterogeneity of causal effects, we showed that previous endogenous perceptions of communication constraint predicted later behavioral changes, despite the same exogenous intervention. These findings highlight the role of human agencies in the face of technological changes and provide empirical support for the affordance approach to information communication technologies (ICTs) as a reconciliation between technological determinism and social constructivism.",
                "year": 2021,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Preset-based generation and exploration of visualization designs": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1045926X15000579/pdfft?md5=9788c9f7de24f8f6e0f68aca6fe82895&pid=1-s2.0-S1045926X15000579-main.pdf",
                "title": "Preset-based generation and exploration of visualization designs",
                "abstract": "Generating the “right” visual representation for the data and task at hand remains a standing challenge in visualization research and practice. A variety of different approaches to produce visual representations have been proposed in the past, including such noteworthy instances as visualization by example and visualization by analogy. With this paper, we add a new twist to creating visual representations by proposing a way to construct new visualization designs by blending together a number of existing visual representations, called presets. We embed this novel blending approach in suitable visual interfaces, such as a gridded canvas to be used by the casual user in the style of a palette for mixing colors, or a range of sliders to be used by the expert user in the style of a studio mixer for audio tracks. These can be employed for rapid prototyping of a specific visual representation, as well as to explore the overall design space of visual representations captured by our approach. We showcase our preset-based blending and its interfaces with examples of the design of 2D tree visualizations and product plots.",
                "year": 2015,
                "publisher": "Journal of Visual Languages & Computing"
            }
        }
    },
    "Unconscious goal pursuit primes attitudes towards technology usage: A virtual reality experiment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216304721/pdfft?md5=f1de0ca4e9993fb25ccbdc1f6f7dfb3b&pid=1-s2.0-S0747563216304721-main.pdf",
                "title": "Unconscious goal pursuit primes attitudes towards technology usage: A virtual reality experiment",
                "abstract": "Several approaches in technology adoption, such as the Technology Acceptance Model (TAM), ask future users to provide evaluations of technology. Such evaluations are expected to predict actual use behavior. For example, users’ evaluations in terms of perceived usefulness and perceived ease of use are considered meaningful indicators of intention to use the technology, and future usage. However, these approaches still show limited reliability and do not consider other critical aspects, such as situated, unconscious goals and the tendency to perceive related affordances. In order to test the hypothesis that technology evaluation may be influenced by unconscious goals, forty participants were split in two groups. The experimental session included two phases. In the first phase, each group explored a virtual environment that primed a specific goal. In the second phase, participants were asked to evaluate the usefulness and the easiness of use of two versions of the same technology (a mobile devices interface). Results showed that each group evaluated as more useful the version of the technology which featured an affordance related to the respective primed goal. Discussion deals with the possible unconscious influences on attitudes towards technology adoption, and provides operative guidelines to account for them in technology adoption research.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Understanding workflow in telehealth video visits: Observations from the IDEATel project": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046409000513/pdfft?md5=dca3e20f97d296194cc301d63eb61b9c&pid=1-s2.0-S1532046409000513-main.pdf",
                "title": "Understanding workflow in telehealth video visits: Observations from the IDEATel project",
                "abstract": "Home telemedicine is an emerging healthcare paradigm that has the potential to transform the treatment of chronic illness. The purpose of this paper is to: (1) develop a theoretical and methodological framework for studying workflow in telemediated clinician–patient encounters drawing on a distributed cognition approach and (2) employ the framework in an in-depth analysis of workflow in the IDEATel project, a telemedicine program for older adults with diabetes. The methods employed in this research included (a) videotaped observations of 27 nurse–patient encounters and (b) semi-structured interviews with participants. The analyses were used to provide a descriptive analysis of video visits, understand the mediating role of different technologies and to characterize the ways in which artifacts and representations are used to understand the state of the patient. The study revealed barriers to productive use of telehealth technology as well as adaptations that circumvented such limitations. This research has design implications for: (a) improving the coordination of communication and (b) developing tools that better integrate and display information. Although home telemedicine programs will differ in important respects, there are invariant properties across such systems. Explicating these properties can serve as a needs requirement analysis to develop more effective systems and implementation plans.",
                "year": 2009,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Associative Memories and Processors: The Exact Match Paradigm": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157899800032/pdfft?md5=407f5f51bc694ebcf9f877e191e3c934&pid=1-s2.0-S1319157899800032-main.pdf",
                "title": "Associative Memories and Processors: The Exact Match Paradigm",
                "abstract": "Associative or content addressable memories (CAM) are crucial in the implementation of high performance computing architectures for applications that require intensive data management or are cognitive in nature. The basic architecture of associative memories can be based on either the exact match or neural network models. This paper focuses on exact match associative memories. The milestone achievements in the field since the first associative memory implementation four decades ago are discussed. A classification of the diverse associative computing architectures is presented. It comprises of two levels of distinction, the associative memory organization and the processing capability which heavily depends on the application domain. Recent development in associative processing applications are also discussed which include fast routing in communication networks, memory management, database management, image processing, and artificial intelligence applications.",
                "year": 1999,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "Self-recalibration of a structured light system via plane-based homography": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0031320306001348/pdfft?md5=83b099d6b3a2bbd369be8454f10b5325&pid=1-s2.0-S0031320306001348-main.pdf",
                "title": "Self-recalibration of a structured light system via plane-based homography",
                "abstract": "Self-recalibration of the relative pose in a vision system plays a very important role in many applications and much research has been conducted on this issue over the years. However, most existing methods require information of some points in general three-dimensional positions for the calibration, which is hard to be met in many practical applications. In this paper, we present a new method for the self-recalibration of a structured light system by a single image in the presence of a planar surface in the scene. Assuming that the intrinsic parameters of the camera and the projector are known from initial calibration, we show that their relative position and orientation can be determined automatically from four projection correspondences between an image and a projection plane. In this method, analytical solutions are obtained from second order equations with a single variable and the optimization process is very fast. Another advantage is the enhanced robustness in implementation via the use of over constrained systems. Computer simulations and real data experiments are carried out to validate our method.",
                "year": 2007,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "Machines should not see as people do, but must know how people see": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0734189X87800038/pdfft?md5=937e452e4cc9f3dc4fc3e187e5ed6118&pid=1-s2.0-S0734189X87800038-main.pdf",
                "title": "Machines should not see as people do, but must know how people see",
                "abstract": "Human perceptions of real objects and events, and of pictures of them, differ from physical and from depicted reality in many critical ways: in the prevalence of robust illusions of space and motion; in a tolerance of spatial inconsistencies; in being not everywhere dense. Such anomalous phenomena, and the classical pictorial depth cues as well, have been held in recent perceptual theorizing to be artifacts of static, impoverished line drawings, and it is asserted (with essentially no support) that the information about the world that is potentially available to a moving viewer in fact results in direct veridical perception of objects and events; causes the anomalies to vanish; and renders the pictorial depth cues unnecessary, irrelevant, and powerless. The constraints needed to use such motion-based information are usually given as rigidity or invariance preferences. We show that the major classes of misperception described above, and the effects of static pictorial cues, occur in force with moving object or observer, readily overcoming rigidity and invariance constraints, if such exist in humans. Machine vision, if subject to these anomalies, would be unnecessarily (so far as we now know) faulty. Machines that provide pictorial information to humans, unfiltered by human editors, should be so programmed as to avoid or compensate for the strong anomalies described. Current perceptual knowledge and theory are evaluated in relation to these phenomena.",
                "year": 1987,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Network embedding based link prediction in dynamic networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X21003708/pdfft?md5=9a072e7f32b8033365a4ad2c0793f9f0&pid=1-s2.0-S0167739X21003708-main.pdf",
                "title": "Network embedding based link prediction in dynamic networks",
                "abstract": "Link prediction is a fundamental task in network theory due to the wide variety of applications in different domains. The objective of link prediction is to find the future links that are likely to be seen in some future time. In this paper, we propose a novel embedding-based technique that utilizes the concept of the Skip-gram framework. An embedding-based method embodies the learning of feature representations of nodes or links in a network. Our method jointly exploits the Skip-gram framework and max aggregator for edge embedding tasks. To test the effectiveness of the proposed method, we have conducted experiments on large size real-world networks. In the experimental evaluation, we have compared the proposed method against both similarity-based and learning-based approaches. The experimental results indicate the effectiveness of the proposed method both in terms of time and accuracy.",
                "year": 2022,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Spin field effect transistors and their applications: A survey": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Spin field effect transistors and their applications: A survey",
                "abstract": "Among the various devices being researched as possible alternative for conventional scaled down transistor, the spin-FET (Datta-Das transistor) held some early promise. Various research groups have proposed the complementary spin-FETs viz., parallel and anti-parallel spin-FETs, later designed some basic and higher order logic circuits using them, and compared the performance with the conventional CMOS design. Various bulk materials like InAs, InAlAs etc. and 2D materials like graphene, silicene etc. have been used as channel materials in spin-FETs by many researchers. Different ferromagnetic and half-metallic electrodes like cobalt, nickel, CrO2 have also been employed in the spin-FETs. Several research groups have worked on the usage of multi-gate and multifunctional logic using spin-FET devices. In this paper, a review of the development of spin-FETs and spin-FET based design has been performed. In addition, the various applications of spin-FETs and the challenges faced for the implementation of spin-FETs are highlighted in the paper.",
                "year": 2020,
                "publisher": "Microelectronics Journal"
            }
        }
    },
    "Structural significance of Neprylysin from Streptococcus suis GZ1 in the degradation of Aβ peptides, a causative agent in Alzheimer's disease": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482521004856/pdfft?md5=340bef3e43739f69be88deebc9c8f2ac&pid=1-s2.0-S0010482521004856-main.pdf",
                "title": "Structural significance of Neprylysin from Streptococcus suis GZ1 in the degradation of Aβ peptides, a causative agent in Alzheimer's disease",
                "abstract": "Alzheimer's disease (AD) is a progressive brain disorder. The accumulation of amyloid beta (Aβ) peptides in the human brain leads to AD. The cleavage of Aβ peptides by several enzymes is being considered as an essential aspect in the treatment of AD. Neprilysin (NEP) is an important enzyme that clears the Aβ plaques in the human brain. The human NEP activity has been found reduced due to mutations in NEP and the presence of inhibitors. However, the role of NEP in the degradation of Aβ peptides in detail at the molecular level is not yet clear. Hence, in the present study, we have investigated the structural significance of NEP from the bacterial source Streptococcus suis GZ1 using various bioinformatics approaches. The homology modelling technique was used to predict the three-dimensional structure of NEP. Further, molecular dynamic (MD) simulated model of NEP was docked with Aβ peptide. Analysis of MD simulated docked complex showed that the wild-type NEP-Aβ-peptide complex is more stable as compared to mutant complex. Hydrogen bonding interactions between NEP with Zn2+and Aβ peptide confirm the degradation of the Aβ peptide. The molecular docking and MD simulation results revealed that the active site residue Glu-538 of bacterial NEP along with Zn2+ interact with His-13 of Aβ peptide. This stable interaction confirms the involvement of NEP with Glu-538 in the degradation of the Aβ peptide. The other residues such as Glu203, Ser537, Gly140, Val587, and Val536 could also play an important role in the cleavage of Aβ peptide in between Asp1-Ala2, Arg5-His6, Val18-Phe19, Gly9-Tyr10, and Arg5-His6. Hence, the predicted model of the NEP enzyme of Streptococcus suis GZ1could be useful to understand the Aβ peptide degradation in detail at the molecular level. The information obtained from this study would be helpful in designing new lead molecules for the effective treatment of AD.",
                "year": 2021,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Fundamental requirements of a machine learning operations platform for industrial metal additive manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Fundamental requirements of a machine learning operations platform for industrial metal additive manufacturing",
                "abstract": "Metal-based Additive Manufacturing (AM) can realize fully dense metallic components and thus offers an opportunity to compete with conventional manufacturing based on the unique merits possible through layer-by-layer processing. Unsurprisingly, Machine Learning (ML) applications in AM technologies have been increasingly growing in the past several years. The trend is driven by the ability of data-driven techniques to support a range of AM concerns, including in-process monitoring and predictions. However, despite numerous ML applications being reported for different AM concerns, no framework exists to systematically manage these ML models for AM operations in the industry. Moreover, no guidance exists on fundamental requirements to realize such a cross-disciplinary platform. Working with experts in ML and AM, this work identifies the fundamental requirements to realize a Machine Learning Operations (MLOps) platform to support process-based ML models for industrial metal AM (MAM). Project-level activities are identified in terms of functional roles, processes, systems, operations, and interfaces. These components are discussed in detail and are linked with their respective requirements. In this regard, peer-reviewed references to identified requirements are made available. The requirements identified can help guide small and medium enterprises looking to implement ML solutions for AM in the industry. Challenges and opportunities for such a system are highlighted. The system can be expanded to include other lifecycle phases of metallic and non-metallic AM.",
                "year": 2024,
                "publisher": "Computers in Industry"
            }
        }
    },
    "Privacy preservation in federated learning: An insightful survey from the GDPR perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404821002261/pdfft?md5=79779486e5a71c361dbe02d21e17aed8&pid=1-s2.0-S0167404821002261-main.pdf",
                "title": "Privacy preservation in federated learning: An insightful survey from the GDPR perspective",
                "abstract": "In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with difficulties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not sufficient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR.",
                "year": 2021,
                "publisher": "Computers & Security"
            }
        }
    },
    "Automatic spike sorting by unsupervised clustering with diffusion maps and silhouettes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231214015690/pdfft?md5=e0766a18c8e8c74f50d38a0058e6f9b9&pid=1-s2.0-S0925231214015690-main.pdf",
                "title": "Automatic spike sorting by unsupervised clustering with diffusion maps and silhouettes",
                "abstract": "Knowledge of the activity of single neurons is crucial for understanding neural functions. Therefore the process of attributing every single spike to a particular neuron, called spike sorting, is particularly important in electrophysiological data analysis. This task however is greatly complicated because of numerous factors. Bursts or fast changes in ion channel activation or deactivation can cause a large variability of spike waveforms. Another considerable source of uncertainties results from noise caused by firing of nearby neurons. Movement of electrodes and external electrical noise from the environment also hamper the spike sorting. This paper introduces an integrated approach of diffusion maps (DM), silhouette statistics, and k-means clustering methods for spike sorting. DM is employed to extract spike features that are highly capable of discriminating different spike shapes. The combination of k-means and silhouette statistics provides an automatic unsupervised clustering, which takes features extracted by DM as inputs. Experimental results demonstrate the noticeable superiority of the features extracted by DM compared to those selected by wavelet transformation (WT). Accordingly, the proposed integrated method significantly dominates the popular existing combination of WT and superparamagnetic clustering regarding spike sorting accuracy.",
                "year": 2015,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Investigation of 3D dynamic and quasistatic models for spinal moments during combined manual material handling tasks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687020302532/pdfft?md5=3e2b9206701086eead27b5ab3d878570&pid=1-s2.0-S0003687020302532-main.pdf",
                "title": "Investigation of 3D dynamic and quasistatic models for spinal moments during combined manual material handling tasks",
                "abstract": "Digital human modeling software uses biomechanical models to compute workers' risk of injury during industrial work processes. In many cases, the biomechanics are calculated using quasistatic models, which neglect the body's dynamics and therefore might be erroneous. This study investigated the differential effect of using a dynamic vs. a quasistatic model on spinal loading during combined manual material handling tasks that are prevalent in industry. An experiment was conducted involving nine male and nine female participants performing a total of 3402 cycles of a box-conveying task (removing, carrying and depositing) for different box masses and shelf heights. Using motion capture data, the peak and cumulative moments acting on the L5/S1 joint were calculated using 3D dynamic and quasistatic models. This revealed that neglecting the dynamic movements (i.e., using a quasistatic model) results in an on average underestimation of 19.7% in the peak spinal moment and 3.6% in the cumulative moment that in some cases exceeds the maximal limit for the compression forces acting on the lower back.",
                "year": 2021,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Modeling and simulation of Vibro-isolator Rapid Prototyping using additive manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050922010675/pdfft?md5=82f01b8fb8f9a63c9939b46717717bb7&pid=1-s2.0-S1877050922010675-main.pdf",
                "title": "Modeling and simulation of Vibro-isolator Rapid Prototyping using additive manufacturing",
                "abstract": "The emerging concept of mechanical meta-materials has gained increasing attention in recent years, partly due to advances in additive manufacturing techniques (additive manufacturing, 3D printing) that have allowed the fabrication of materials with arbitrarily complex micro / nanostructures. The designed geometry of these structures gives rise to unprecedented or rare mechanical properties that can used to create advanced elements with new, unprecedented functions. This paper presents the method of prototyping a vibro-isolator made of a material that has the properties of storing and disposing of mechanical vibration energy and that can freely be shaped by modern manufacturing techniques. An example of a vibration isolator of the proprietary concept analyzed, using the concept of stepped and adding elements. The method is based on the use of two main tools. Mechanical properties verified by analyzing the frequency and mode of free vibrations of the prototype using the finite element method. The same CAD model of the vibro-isolator prototype produced by 3D printing. The paper presents one of the production techniques, which is 3D printing in SLS (Selective Laser Sintering) technology, and the SolidWorks and Comsol Multhiphysics software used in the CAD / CAE prototyping method.",
                "year": 2022,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Using 3D printers as weapons": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Using 3D printers as weapons",
                "abstract": "Additive manufacturing, also known as 3D printing, is a transformative manufacturing technology that will play a significant role in the critical manufacturing sector. Industrial-grade 3D printers are increasingly used to produce functional parts for important systems. However, due to their reliance on computerization, 3D printers are susceptible to a broad range of attacks. More importantly, compromising a 3D printer is not a goal, but rather a staging point for launching subsequent attacks with the printer. For example, an adversary can compromise a 3D printer in order to manipulate the mechanical properties of manufactured parts. If the manufactured parts are used in jet engines or in other safety-critical systems, they could endanger human life, disrupt critical infrastructure assets and produce significant economic and societal impacts.\n\nThis paper analyzes the ability of an adversary to “weaponize” compromised additive manufacturing equipment in order to cause kinetic, nuclear/biological/chemical or cyber damage. In particular, the paper presents categories (taxonomies) of the elements in an additive manufacturing workflow that can be compromised by successful attacks, the manipulations that the compromised elements can exercise and the weapon-like effects resulting from these manipulations. The relationships between these taxonomies are discussed. Finally, the weaponization capabilities of 3D printers are characterized.",
                "year": 2016,
                "publisher": "International Journal of Critical Infrastructure Protection"
            }
        }
    },
    "Investigating Health Context Using a Spatial Data Analytical Tool: Development of a Geospatial Big Data Ecosystem": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Investigating Health Context Using a Spatial Data Analytical Tool: Development of a Geospatial Big Data Ecosystem",
                "abstract": "Background\n\nEnabling the use of spatial context is vital to understanding today’s digital health problems. Any given location is associated with many different contexts. The strategic transformation of population health, epidemiology, and eHealth studies requires vast amounts of integrated digital data. Needed is a novel analytical framework designed to leverage location to create new contextual knowledge. The Geospatial Analytical Research Knowledgebase (GeoARK), a web-based research resource has robust, locationally integrated, social, environmental, and infrastructural information to address today’s complex questions, investigate context, and spatially enable health investigations. GeoARK is different from other Geographic Information System (GIS) resources in that it has taken the layered world of the GIS and flattened it into a big data table that ties all the data and information together using location and developing its context.",
                "year": 2022,
                "publisher": "JMIR Medical Informatics"
            }
        }
    },
    "A review and evaluation of methods estimating ego-motion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314212000021/pdfft?md5=69a5143e54e03a93200e021c1ae3c7f1&pid=1-s2.0-S1077314212000021-main.pdf",
                "title": "A review and evaluation of methods estimating ego-motion",
                "abstract": "If a visual observer moves through an environment, the patterns of light that impinge its retina vary leading to changes in sensed brightness. Spatial shifts of brightness patterns in the 2D image over time are called optic flow. In contrast to optic flow visual motion fields denote the displacement of 3D scene points projected onto the camera’s sensor surface. For translational and rotational movement through a rigid scene parametric models of visual motion fields have been defined. Besides ego-motion these models provide access to relative depth, and both ego-motion and depth information is useful for visual navigation.\n\nIn the past 30 years methods for ego-motion estimation based on models of visual motion fields have been developed. In this review we identify five core optimization constraints which are used by 13 methods together with different optimization techniques.1 In the literature methods for ego-motion estimation typically have been evaluated by using an error measure which tests only a specific ego-motion. Furthermore, most simulation studies used only a Gaussian noise model. Unlike, we test multiple types and instances of ego-motion. One type is a fixating ego-motion, another type is a curve-linear ego-motion. Based on simulations we study properties like statistical bias, consistency, variability of depths, and the robustness of the methods with respect to a Gaussian or outlier noise model. In order to achieve an improvement of estimates for noisy visual motion fields, part of the 13 methods are combined with techniques for robust estimation like m-functions or RANSAC. Furthermore, a realistic scenario of a stereo image sequence has been generated and used to evaluate methods of ego-motion estimation provided by estimated optic flow and depth information.",
                "year": 2012,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Damage in a comprehensive model for shape memory alloys in logarithmic strain space": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782524000252/pdfft?md5=6276ec3068d6102730a6e24def748ab4&pid=1-s2.0-S0045782524000252-main.pdf",
                "title": "Damage in a comprehensive model for shape memory alloys in logarithmic strain space",
                "abstract": "Shape memory alloys (SMAs) possess considerable complexity in response to thermomechanical load due to their strong dependence on strain, stress, temperature and history. This high complexity is a limiting factor in the fatigue analysis of SMA structures. With this in mind, a comprehensive phenomenological model is developed to address nearly all of the features in SMAs with a particular focus on the development of damage. Due to the fact that many SMA structures experience large deformations, the model is developed in finite strains using the logarithmic strain space approach. Besides the balance of linear momentum, the model includes the heat conduction equation and two micromorphic nonlocal balance laws. The latter are introduced to regularize the softening associated with the damage and transformation. The material model incorporates a damage evolution law that depends on the integration of a simplified dissipation term. The model is applied to a few examples to observe the interaction of the features and to illustrate the applicability of the model for rupture and fatigue analysis. The interaction of the features is shown to produce a reasonable representation of the cyclic loading of wires at different loading rates. Rupture is shown to occur due to a large amount of plastic strain and can be performed due to the nonlocal approach for the damage. The fatigue analysis shows some promising accuracy but only if the dissipated area is captured properly.",
                "year": 2024,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "A simulation model for emergency evacuation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0377221795001123/pdfft?md5=ad5e3fd5ad69e7fc1e623cdda3451d27&pid=1-s2.0-0377221795001123-main.pdf",
                "title": "A simulation model for emergency evacuation",
                "abstract": "This paper describes the development of a prototype spatial decision support system for use by emergency planners in developing contingency plans for evacuations from disaster areas. It links together a geographical information system (ARC/INFO) with a specially written object-oriented micro-simulator via a windowing computer operating system. The details of the system are described, its limitations are discussed and potential enhancements are identified.",
                "year": 1996,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Policy gradient empowered LSTM with dynamic skips for irregular time series data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494623003320/pdfft?md5=0935196d367917b975f85e3d27695474&pid=1-s2.0-S1568494623003320-main.pdf",
                "title": "Policy gradient empowered LSTM with dynamic skips for irregular time series data",
                "abstract": "Time series modelling has been successfully handled by Long Short-Term Memory (LSTM) models. Yet their performance can be severely inhibited by the occurrence of missing values prevalent in many real-life datasets. Many previous studies have been dedicated to imputation methods for generating a complete time series sequence, which have their limitations in terms of imputation bias and inaccuracies. In this paper, we propose a new LSTM model incorporating policy gradient (PG) based reinforcement learning called PG-LSTM, which can mitigate the effect of missing data and capture time-based input feature patterns more effectively to improve prediction performance. Inspired by numerous sequence models’ successes in improving the efficiency of processing language data by skipping irrelevant tokens, the PG-LSTM introduces dynamic skip connections between LSTM cell states for time series data for classification and regression tasks for the first time. Specifically, the proposed model comprises a modified LSTM cell architecture that can internally call a policy-based reinforcement learning agent to generate a skipping action, allowing the model to dynamically select the optimal subset of hidden and cell states from past states to capture periodic and non-periodic patterns within a time series sequence. Moreover, the PG-LSTM also designs a lightweight imputation layer using a simple missing value imputation strategy while incorporating missing indicators and skipping segments of unimportant data to reduce the limitations associated with imputed data for handling missing values. Our experimental results on regression and classification tasks on time series data with high rates of missing values demonstrate that the PG-LSTM improves performance against current gated recurrent neural networks (RNN) and conventional non-neural network algorithms. The PG-LSTM can enhance AUC by up to 18.5% in the classification task and RMSE by up to 19.3% in the regression task over gated RNN models, respectively. Our findings are also statistically analysed using statistical significance testing with post hoc analysis.",
                "year": 2023,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Fundamentals of texture flow equations in vision calculus": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/016786559190034J/pdfft?md5=45aa32117eebce4036efdaecb951780b&pid=1-s2.0-016786559190034J-main.pdf",
                "title": "Fundamentals of texture flow equations in vision calculus",
                "abstract": "The question of texture flow equations for a textured surface moving relatively to a perspective camera is considered. Motion constraint equations involving projected image texture density and optic flow are derived and briefly compared with the classic motion constraint equation involving photometric image irradiance and optic flow. It is pointed out that these flow equations are significantly different by nature and by content.",
                "year": 1991,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "On the role of computational support for designers in action": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X15000551/pdfft?md5=13fbf7e6d62132f87268ae4ee2934264&pid=1-s2.0-S0142694X15000551-main.pdf",
                "title": "On the role of computational support for designers in action",
                "abstract": "Designers' actions are high-level mechanisms based on heuristics and assumptions learned from professional experience. Significant research has been devoted to understanding these actions as well as finding ways to aid, automate, or augment them with computational support. However, representing and manipulating such tacit knowledge in computational environments remains an open area of research. In this paper, we map designers' actions and relationships to compare them with computational approaches for the generation, evaluation, and selection of design alternatives, and attempt to integrate all of the above. The analysis provides a more thorough understanding of the role of computational approaches in supporting designer actions and identifies challenges and areas of future research.",
                "year": 2015,
                "publisher": "Design Studies"
            }
        }
    },
    "The influence of environmental features on route selection in an emergency situation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687012001925/pdfft?md5=41b70aabf155d1f896981ac828c07ab3&pid=1-s2.0-S0003687012001925-main.pdf",
                "title": "The influence of environmental features on route selection in an emergency situation",
                "abstract": "Understanding the influence of external information at a lower level of awareness during the processes of route selection could be a key factor to predict user's movements within complex buildings, avoiding wayfinding problems and improving egress in emergency situations. This study aims to verify whether corridor intersection configuration attributes, such as width and brightness, act as factors of attraction to improve the affordance of indoor hallways during an emergency egress situation, using a VR-based methodology. The main hypotheses are that users tend to move along either, wider or brighter corridors. Thirty volunteers participated in this study, moving along 57 different corridors, according to the experimental conditions of the study. The results suggest that people prefer to follow brighter pathways in “T-type” and “F-type” intersections, and wider corridors in “T-type” intersections. In situations where these variables are in conflict, there is a preference for brighter paths in both intersection configurations.",
                "year": 2013,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Finite element dynamic modeling of a translating and rotating flexible link": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/004578259500968X/pdfft?md5=8f0376cd764733168cd1adc475701fb7&pid=1-s2.0-004578259500968X-main.pdf",
                "title": "Finite element dynamic modeling of a translating and rotating flexible link",
                "abstract": "A finite element dynamic model for a translating and rotating beam is formulated. The developed model adopts a finite element mesh with a fixed number of elements, where the element length is constant. The time-dependent boundary conditions manifested by the prismatic joint constraints are considered. A transition element with variable stiffness is introduced at the interface with the joint hub. In this finite element formulation, all the inertia coupling terms between the beam reference motions and the local elastic deformations are considered. In addition, the model accounts for the dynamics of the end mass as well as the associated coupling effects. Numerical simulations and comparisons with results obtained by other methods are presented to demonstrate the validity and accuracy of the developed model.",
                "year": 1996,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Edge-finding in human vision: a multi-stage model based on the perceived structure of plaids": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885697000814/pdfft?md5=22b909d569eb2a6013beeeeaff44a7c2&pid=1-s2.0-S0262885697000814-main.pdf",
                "title": "Edge-finding in human vision: a multi-stage model based on the perceived structure of plaids",
                "abstract": "Visual experiments on the perceived structure of plaid patterns are reported and reviewed. Plaids composed of two or three sinusoidal grating components were viewed both with and without the influence of masking or adapting patterns. The results lead to the outline of a multi-stage model of the processes by which human vision derives edge-feature descriptions from the retinal image via local spatial filters. The main proposed stages are: (i) local, oriented filtering; (ii) local combination of filter outputs across orientation (or spatial frequency) to form (iii) filtered image ‘patches’; (iv) coherent combination of patches across space to form a ‘neural image’; (v) zero-crossing analysis on the neural image to localize edge features in (vi) a feature map; (vii) application of coding rules to describe feature attributes at zero-crossing locations. Evidence for the filter combination and zero-crossing stages [(ii) and (v)] comes from experiments on the perceived structure of plaids, where the perceived location and orientation of edges was predicted by zero-crossings in the output of a circular filter, and not by the Fourier component orientations. By contrast, results on the tilt aftereffect for plaids could be understood entirely as an interaction between Fourier components, but with consequential changes in the perception of ZCs that required a consideration of ‘patchwise’ analysis and the patch combination stage [(iii) and (iv)]. Theory and evidence for coding rules (vii) that describe the blur, contrast and orientation of local edge features are discussed.",
                "year": 1998,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Accessible Options for Deaf People in e-Learning Platforms: Technology Solutions for Sign Language Translation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050915031166/pdfft?md5=2498db221199a49629bb2f315a7eaad7&pid=1-s2.0-S1877050915031166-main.pdf",
                "title": "Accessible Options for Deaf People in e-Learning Platforms: Technology Solutions for Sign Language Translation",
                "abstract": "This paper presents a study on potential technology solutions for enhancing the communication process for deaf people on e-learning platforms through translation of Sign Language (SL). Considering SL in its global scope as a spatial-visual language not limited to gestures or hand/forearm movement, but also to other non-dexterity markers such as facial expressions, it is necessary to ascertain whether the existing technology solutions can be effective options for the SL integration on e-learning platforms. Thus, we aim to present a list of potential technology options for the recognition, translation and presentation of SL (and potential problems) through the analysis of assistive technologies, methods and techniques, and ultimately to contribute for the development of the state of the art and ensure digital inclusion of the deaf people in e-learning platforms. The analysis show that some interesting technology solutions are under research and development to be available for digital platforms in general, but yet some critical challenges must solved and an effective integration of these technologies in e-learning platforms in particular is still missing.",
                "year": 2015,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Early orientation selection: Tangent fields and the dimensionality of their support": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X85900039/pdfft?md5=0da332153c0e1ecb858375813707c47e&pid=1-s2.0-0734189X85900039-main.pdf",
                "title": "Early orientation selection: Tangent fields and the dimensionality of their support",
                "abstract": "Orientation selection is the inference of orientation information out of images. It is one of the foundations on which other visual structures are built, since it must precede the formation of contours out of pointillist data and surfaces out of surface markings. We take a differential geometric view in defining orientation selection, and develop algorithms for actually doing it. The goal of these algorithms is formulated in mathematical terms as the inference of a vector field of tangents (to the contours), and the algorithms are studied in both abstract and computational forms. They are formulated as matching problems, and algorithms for solving them are reduced to biologically plausible terms. We show that two different matching problems are necessary, the first for 1-dimensional contours (which we refer to as Type I processes) and second for 2-dimensional flows (or Type II processes). We conjecture that this difference is reflected in the response properties of “simple” and “complex” cells, respectively, and predict several other psychophysical phenomena.",
                "year": 1985,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Algebraic 3D Graphic Statics: Constrained Areas": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Algebraic 3D Graphic Statics: Constrained Areas",
                "abstract": "This research is a continuation of the Algebraic 3D Graphic Statics Methods that addressed the reciprocal constructions in an earlier publication (Hablicsek et al. 2019). It provides algorithms and (numerical) methods to geometrically control the magnitude of the internal and external forces in the reciprocal diagrams of 3D/Polyhedral Graphic statics. 3D graphic statics (3DGS) is a recently rediscovered method of structural form-finding based on a 150-year old proposition by Rankine and Maxwell in Philosophical Magazine. In 3DGS, the form of the structure and its equilibrium of forces are represented by two polyhedral diagrams that are geometrically and topologically related. The areas of the faces of the force diagram represent the magnitude of the internal and external forces in the members of the form diagram. The proposed method allows the user to control and constrain the areas and edge lengths of the faces of general polyhedrons that can be convex, self-intersecting, or concave in a group of aggregated polyhedral cells. In this method, a quadratic formulation is introduced to compute the area of a face based on its edge lengths only. This quadratic function is then turned into a linear formulation to facilitate the non-trivial computation of reciprocal polyhedral diagrams. The approach is applied to force diagrams, including a group of polyhedral cells, to manipulating the face geometry with a predefined area and the edge lengths. The method is implemented as a multi-step algorithm where each step includes computing the geometry of a single face with a target area and updating the polyhedral geometry. One of the remarkable results of this framework is to control the construction of the zero-area faces as proposed by McRobie (2017b). The zero-area faces represent a member with zero force in the form diagram. This research shows how self-intersecting faces, including the zero-area faces, can be constructed with additional edge constraints in a group of polyhedral cells without breaking the reciprocity of the form and force diagrams. Thus, it provides more hints on the generalization of the principle of the equilibrium of polyhedral frames. It also suggests a design approach where the boundary conditions and internal forces of compression-only systems can be manipulated to the design systems with both compression and tensile forces with no change in the geometry or the faces’ planarity of the form diagram.",
                "year": 2021,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Vertex–transformation streams": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1524070306000373/pdfft?md5=47541403a66627489ff7022c5a67399f&pid=1-s2.0-S1524070306000373-main.pdf",
                "title": "Vertex–transformation streams",
                "abstract": "Recent trends in parallel computer architecture strongly suggest the need to improve the arithmetic intensity (the compute to bandwidth ratio) for greater performance in time-critical applications, such as interactive 3D graphics. At the same time, advances in stream programming abstraction for graphics processors (GPUs) have enabled us to use parallel algorithm design methods for GPU programming. Inspired by these developments, this paper explores the interactions between multiple data streams to improve arithmetic intensity and address the input geometry bandwidth bottleneck for interactive 3D graphics applications. We introduce the idea of creating vertex and transformation streams that represent large point datasets via their interaction. We discuss how to factor such point datasets into a set of source vertices and transformation streams by identifying the most common translations amongst vertices. We accomplish this by identifying peaks in the cross-power spectrum of the dataset in the Fourier domain. We validate our approach by integrating it with a view-dependent point rendering system and show significant improvements in input geometry bandwidth requirements as well as rendering frame rates.",
                "year": 2006,
                "publisher": "Graphical Models"
            }
        }
    },
    "Tap into visual analysis of customization of grouping of activities in eLearning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563214005779/pdfft?md5=85d9e09d3f4993063deb46fa48f2ec47&pid=1-s2.0-S0747563214005779-main.pdf",
                "title": "Tap into visual analysis of customization of grouping of activities in eLearning",
                "abstract": "The use of electronic media in education has a strong impact of on student performance. Prior research has investigated the impact of forum reading and posting, as well as access to reading resources, on academic performance. However, little is known about the relationship between the temporal frequency of these activities and individual student’s performance. In addition, some studies have proposed different categorizations of student activities, and proved the usefulness of these characterizations for predictive purposes and better understanding of the learning process. One of the biggest concerns when teaching courses in a virtual learning environment is to create and develop instruction in a way that it improves the overall learning experience and results. The present study addresses this concern by drawing upon factors influencing academic performance; more specifically, the study focuses on objective factors related to the interaction between the student and the system (number of resources visited in the learning platform, and number of forum posts and views). On top of that, a visualization tool was designed and implemented to allow for further investigation of the relevance of the study variables. The results from the study show that there is a recurrent pattern in the frequency of behaviors and performance across different courses.",
                "year": 2015,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Verification and validation of a Work Domain Analysis with turing machine task analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687014001951/pdfft?md5=1223359c8429707be52da698a2747a7d&pid=1-s2.0-S0003687014001951-main.pdf",
                "title": "Verification and validation of a Work Domain Analysis with turing machine task analysis",
                "abstract": "While the use of Work Domain Analysis as a methodological framework in cognitive engineering is increasing rapidly, verification and validation of work domain models produced by this method are becoming a significant issue. In this article, we propose the use of a method based on Turing machine formalism named “Turing Machine Task Analysis” to verify and validate work domain models. The application of this method on two work domain analyses, one of car driving which is an “intentional” domain, and the other of a ship water system which is a “causal domain” showed the possibility of highlighting improvements needed by these models. More precisely, the step by step analysis of a degraded task scenario in each work domain model pointed out unsatisfactory aspects in the first modelling, like overspecification, underspecification, omission of work domain affordances, or unsuitable inclusion of objects in the work domain model.",
                "year": 2015,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Detection of a vascular wilt disease in potato (‘Blackleg’) based on UAV hyperspectral imagery: Can structural features from LiDAR or SfM improve plant-wise classification accuracy?": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169924009189/pdfft?md5=39c1fdd0e485393c6db58202439bb20b&pid=1-s2.0-S0168169924009189-main.pdf",
                "title": "Detection of a vascular wilt disease in potato (‘Blackleg’) based on UAV hyperspectral imagery: Can structural features from LiDAR or SfM improve plant-wise classification accuracy?",
                "abstract": "Ensuring plant health is a key factor to maximize crop yield. Despite that, the current field scouting and disease monitoring approaches often rely on visual evaluations and are, therefore, subjective and time demanding. New methods to assist in disease detection and severity assessment are required to allow better crop management and higher throughput in field phenotyping studies. With this objective, techniques involving the use of multi- and hyperspectral imagery for retrieval of plant traits and assessment of general crop health status are increasingly being proposed as alternatives to conventional disease monitoring approaches. Conversely, research focusing on specific pathogens are still lacking in many cases, in particular studies investigating multi-source sensing approaches, which have the potential to improve retrieval/classification accuracy. In this study, hyperspectral imagery and point clouds obtained with LiDAR or through Structure from Motion algorithm (SfM) applied to high resolution RGB images were evaluated as possible alternatives to detect Blackleg (caused by bacteria of the genera Pectobacterium and Dickeya) in potato. It was demonstrated that all the different datasets have potential to discriminate healthy from diseased plants. The combination of Vegetation Indices (VIs) derived from hyperspectral images with structural features from LiDAR resulted in the best validation results (Balanced Accuracy – BA = 0.915). Small improvements were also achieved by combining VIs with SfM features (BA = 0.876) in comparison to VIs alone (BA = 0.846). Evaluation of feature importance for classification models derived from the different datasets indicated that after structural features derived from LiDAR or RGB imagery were added as predictor variables the relative importance of VIs for the predictions decreased, in particular for VIs related to LAI or other traits describing canopy properties. Finally, analysis of false negatives and positives indicated some limitations to the predictive potential of the different datasets, with diseased and healthy plants eventually presenting atypical structural and spectral characteristics in comparison to those expected for their classes. Therefore, multi-source sensing, including additional modalities (e.g., thermal or fluorescence), might be required to further improve detection of pathogens with complex symptoms, as those affecting roots, tubers and stems.",
                "year": 2024,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Intrusion detection techniques and approaches": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0140366402000373/pdfft?md5=4ecaa6adabc0a62c7052e6198ddbd02f&pid=1-s2.0-S0140366402000373-main.pdf",
                "title": "Intrusion detection techniques and approaches",
                "abstract": "Recent security incidents and analysis have demonstrated that manual response to such attacks is no longer feasible. Intrusion detection systems (IDS) offer techniques for modelling and recognising normal and abusive system behaviour. Such methodologies include statistical models, immune system approaches, protocol verification, file and taint checking, neural networks, whitelisting, expression matching, state transition analysis, dedicated languages, genetic algorithms and burglar alarms. This paper describes these techniques including an IDS architectural outline and an analysis of IDS probe techniques finishing with a summary of associated technologies.",
                "year": 2002,
                "publisher": "Computer Communications"
            }
        }
    },
    "Federated learning with knowledge distillation for multi-organ segmentation with partially labeled datasets": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841524000811/pdfft?md5=56e69fe33ec182ef6b57f27df76069f2&pid=1-s2.0-S1361841524000811-main.pdf",
                "title": "Federated learning with knowledge distillation for multi-organ segmentation with partially labeled datasets",
                "abstract": "The state-of-the-art multi-organ CT segmentation relies on deep learning models, which only generalize when trained on large samples of carefully curated data. However, it is challenging to train a single model that can segment all organs and types of tumors since most large datasets are partially labeled or are acquired across multiple institutes that may differ in their acquisitions. A possible solution is Federated learning, which is often used to train models on multi-institutional datasets where the data is not shared across sites. However, predictions of federated learning can be unreliable after the model is locally updated at sites due to ‘catastrophic forgetting’. Here, we address this issue by using knowledge distillation (KD) so that the local training is regularized with the knowledge of a global model and pre-trained organ-specific segmentation models. We implement the models in a multi-head U-Net architecture that learns a shared embedding space for different organ segmentation, thereby obtaining multi-organ predictions without repeated processes. We evaluate the proposed method using 8 publicly available abdominal CT datasets of 7 different organs. Of those datasets, 889 CTs were used for training, 233 for internal testing, and 30 volumes for external testing. Experimental results verified that our proposed method substantially outperforms other state-of-the-art methods in terms of accuracy, inference time, and the number of parameters.",
                "year": 2024,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Developing a goal-driven data integration framework for effective data analytics": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Developing a goal-driven data integration framework for effective data analytics",
                "abstract": "Data integration plays a crucial role in business intelligence, aiding decision-makers by consolidating data from heterogeneous sources to provide deep insights into business operations and performance. In the big data era, automated data integration solutions need to process high volumes of disparate data robustly and seamlessly for various analytical needs or operational actions. Existing data integration solutions exhibit limited capabilities for capturing and modeling users' needs to execute on-demand data integration. This study, underpinned by affordance theory and the goal definition principles from the Goal-Question-Metric approach, designs and instantiates a goal-driven data integration framework for data analytics. The proposed innovative design automates data integration for non-technical data users. Specifically, it demonstrates how to elicit and ontologize users' data-analytic goals and addresses semantic heterogeneity, thereby recognizing goal-relevant datasets. In a structured evaluation using the context of counter-terrorism analytics, our design artifact shows promising performance in capturing diverse and dynamic user goals for data analytics and in generating integrated data tailored to these goals. Our research establishes a theoretical framework to guide future scholars and practitioners in building smart, goal-driven data integration.",
                "year": 2024,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "From orientation selection to optical flow": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0734189X87800026/pdfft?md5=54c8363936a6fe1e2f90ac2e63c3e7ae&pid=1-s2.0-S0734189X87800026-main.pdf",
                "title": "From orientation selection to optical flow",
                "abstract": "Optical flow can be thought of as a generalization of orientation selection to include time, and this is a first attempt to develop this generalization. This paper is informal, and begins with an overview of a computational theory of orientation selection that has been developed in our laboratory during the past few years. A theory of optical flow computations is then sketched on the basis of this background, and a number of consequences important for both computational modeling and psychophysics are discussed. We concentrate in particular on two notions—dimensionality and structure—and describe a series of experiments that demonstrate that sensitivity to discontinuities in optical flow patterns has significant similarities to those in orientation patterns. Such similarities lend strong support to the analogy.",
                "year": 1987,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Speed estimation with propagation maps": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231205002262/pdfft?md5=13f9a4d2e50c28e0bc3ec993cf5f740d&pid=1-s2.0-S0925231205002262-main.pdf",
                "title": "Speed estimation with propagation maps",
                "abstract": "We propose a neural architecture that estimates the speed of motion. The basis is a two-dimensional map made of locally connected integrate-and-fire neurons, that propagates and integrates synaptic input in a dendritic-cable-like manner, but irrespective of any direction. The propagation dynamics of such a map are tuned to filter preferred speeds: slow map dynamics filter slow speeds, fast map dynamics filter fast speeds. The propagation map is potentially simple enough for an analog hardware approach.",
                "year": 2006,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Accurate preoperative path planning with coarse-to-refine segmentation for image guided deep brain stimulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809422003810/pdfft?md5=be54f2984d420601e017f03777dd4b22&pid=1-s2.0-S1746809422003810-main.pdf",
                "title": "Accurate preoperative path planning with coarse-to-refine segmentation for image guided deep brain stimulation",
                "abstract": "Accurate preoperative path planning plays an essential role in a neurosurgical procedure of deep brain stimulation, leading to a successful procedure with significant surgical outcomes. Conventional preoperative path planning is time-consuming and uncertain, depending highly on the knowledge and experience of the clinician who has to manually plan the electrode-implant path. This work presents a new preoperative path planning strategy for neurostimulation to automatically and accurately find the optimal electrode-implant trajectory. Specifically, a coarse-to-refine neural network model is proposed to accurately segment anatomical brain structures such as the subthalamic nucleus, while the path planning is formulated as an optimization task that minimizes the surgical risk on the implantation trajectory through the segmented brain structures, as well as ensures the puncture path at the safest distance to targets of interest in the brain. We evaluate our method on retrospective neurostimulation data and compare it to the puncture path generated by experienced surgeons, with the experimental results showing that our method provides surgeons with automatic and accurate electrode implant trajectory comparable or even better than manual planning of fellow surgeons.",
                "year": 2022,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Fast swept volume approximation of complex polyhedral models": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448504000053/pdfft?md5=73ae5b02748dd416ef3eaab80eea17aa&pid=1-s2.0-S0010448504000053-main.pdf",
                "title": "Fast swept volume approximation of complex polyhedral models",
                "abstract": "We present an efficient algorithm to approximate the swept volume (SV) of a complex polyhedron along a given trajectory. Given the boundary description of the polyhedron and a path specified as a parametric curve, our algorithm enumerates a superset of the boundary surfaces of SV. The superset consists of ruled and developable surface primitives, and the SV corresponds to the outer boundary of their arrangement. We approximate this boundary by using a five-stage pipeline. This includes computing a bounded-error approximation of each surface primitive, computing unsigned distance fields on a uniform grid, classifying all grid points using fast marching front propagation, iso-surface reconstruction, and topological refinement. We also present a novel and fast algorithm for computing the signed distance of surface primitives as well as a number of techniques based on surface culling, fast marching level-set methods and rasterization hardware to improve the performance of the overall algorithm. We analyze different sources of error in our approximation algorithm and highlight its performance on complex models composed of thousands of polygons. In practice, it is able to compute a bounded-error approximation in tens of seconds for models composed of thousands of polygons sweeping along a complex trajectory.",
                "year": 2004,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Finding complete minimum driver node set with guaranteed control capacity": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231222006774/pdfft?md5=67792f3a730e45b61e6211f4eccf55c0&pid=1-s2.0-S0925231222006774-main.pdf",
                "title": "Finding complete minimum driver node set with guaranteed control capacity",
                "abstract": "A critical prerequisite for controlling complex networks is to find a driver node set with a structural controllability guarantee. This paper introduces the control capacity for a driver node set and solves the problem of finding a complete minimum driver node set that not only guarantees network structural controllability but achieves the desired level of control capacity. A novel algorithmic framework is proposed which is based on the concept of equivalent set and approximate matching replacement technique. The proposed algorithmic framework is shown to outperform the state-of-the-art approaches in the literature. The validity of the proposed algorithm is analyzed and the performance is evaluated by experiments on artificial and real-world complex networks.",
                "year": 2022,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Application of deep reinforcement learning for spike sorting under multi-class imbalance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482523007187/pdfft?md5=76386427a93491da5066b00be638c19d&pid=1-s2.0-S0010482523007187-main.pdf",
                "title": "Application of deep reinforcement learning for spike sorting under multi-class imbalance",
                "abstract": "Spike sorting is the basis for analyzing spike firing patterns encoded in high-dimensional information spaces. With the fact that high-density microelectrode arrays record multiple neurons simultaneously, the data collected often suffers from two problems: a few overlapping spikes and different neuronal firing rates, which both belong to the multi-class imbalance problem. Since deep reinforcement learning (DRL) assign targeted attention to categories through reward functions, we propose ImbSorter to implement spike sorting under multi-class imbalance. We describe spike sorting as a Markov sequence decision and construct a dynamic reward function (DRF) to improve the sensitivity of the agent to minor classes based on the inter-class imbalance ratios. The agent is eventually guided by the optimal strategy to classify spikes. We consider the Wave_Clus dataset, which contains overlapping spikes and diverse noise levels, and the macaque dataset, which has a multi-scale imbalance. ImbSorter is compared with classical DRL architectures, traditional machine learning algorithms, and advanced overlapping spike sorting techniques on these two above datasets. ImbSorter obtained improved results on the Macro_F1. The results show ImbSorter has a promising ability to resist overlapping and noise interference. It has high stability and promising performance in processing spikes with different degrees of skewed distribution.",
                "year": 2023,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Comparative study of fuzzy-AHP and BBN for spatially-explicit prediction of bark beetle predisposition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815221002759/pdfft?md5=428a2f14095d778ea561d7fb40d35ca1&pid=1-s2.0-S1364815221002759-main.pdf",
                "title": "Comparative study of fuzzy-AHP and BBN for spatially-explicit prediction of bark beetle predisposition",
                "abstract": "The European spruce bark beetle ‘Ips typographus L.’ is the most serious disturbance agent for European forests. The complex interactions of many influencing factors need to be integrated into a model-based decision-support system to reduce the potential loss of forests. This paper compares two methodological approaches for spatially-explicit prediction of the predisposition for bark beetle infestations. The fuzzy analytic hierarchy process and the Bayesian belief networks were used in combination with a geographical information system to manage uncertainties. Using available data resources, the two approaches were evaluated to produce robust results for forest practitioners and to support measures to minimize the spread of bark beetles. The findings revealed that nearly 32% of the sites investigated in a case study were moderately-high or high risk categories. It is concluded that BBN is more efficient. Both methods can easily be used to analyze environmental problems involving complex interactions among various criteria.",
                "year": 2022,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "A case for automated large-scale semantic annotation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826803000088/pdfft?md5=50a898fa3024ed39b3ed038c9be7ed89&pid=1-s2.0-S1570826803000088-main.pdf",
                "title": "A case for automated large-scale semantic annotation",
                "abstract": "This paper describes Seeker, a platform for large-scale text analytics, and SemTag, an application written on the platform to perform automated semantic tagging of large corpora. We apply SemTag to a collection of approximately 264 million web pages, and generate approximately 434 million automatically disambiguated semantic tags, published to the web as a label bureau providing metadata regarding the 434 million annotations. To our knowledge, this is the largest scale semantic tagging effort to date.\n\nWe describe the Seeker platform, discuss the architecture of the SemTag application, describe a new disambiguation algorithm specialized to support ontological disambiguation of large-scale data, evaluate the algorithm, and present our final results with information about acquiring and making use of the semantic tags. We argue that automated large-scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web.",
                "year": 2003,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Rethinking dynamic difficulty adjustment for video game design": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Rethinking dynamic difficulty adjustment for video game design",
                "abstract": "Dynamic Difficulty Adjustment (DDA) has garnered significant interest in game research fields for many years. Despite high expectations, the effectiveness of DDA in enhancing player experiences has yielded mixed results. This paper aims to rethink the theoretical foundations of DDA by addressing four crucial questions about the issues, definition, scope, value, and design of DDA. A comprehensive literature review was conducted to provide novel insights into DDA and DDA design. We revealed DDA’s unhealthy reliance on Flow theory, redefined DDA based on the concept of game difficulty, and discussed DDA’s scope and value. Additionally, we presented a goal-based DDA design framework and proposed a 6-step DDA design process as a practical approach. This work offers theoretical support for understanding and designing DDA in future game research.",
                "year": 2024,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "Voxel-level Siamese Representation Learning for Abdominal Multi-Organ Segmentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260721006210/pdfft?md5=430327052dc15e8cf0d0220c6c0a69f0&pid=1-s2.0-S0169260721006210-main.pdf",
                "title": "Voxel-level Siamese Representation Learning for Abdominal Multi-Organ Segmentation",
                "abstract": "Background and Objective: Recent works in medical image segmentation have actively explored various deep learning architectures or objective functions to encode high-level features from volumetric data owing to limited image annotations. However, most existing approaches tend to ignore cross-volume global context and define context relations in the decision space. In this work, we propose a novel voxel-level Siamese representation learning method for abdominal multi-organ segmentation to improve representation space. Methods: The proposed method enforces voxel-wise feature relations in the representation space for leveraging limited datasets more comprehensively to achieve better performance. Inspired by recent progress in contrastive learning, we suppressed voxel-wise relations from the same class to be projected to the same point without using negative samples. Moreover, we introduce a multi-resolution context aggregation method that aggregates features from multiple hidden layers, which encodes both the global and local contexts for segmentation. Results: Our experiments on the multi-organ dataset outperformed the existing approaches by 2% in Dice score coefficient. The qualitative visualizations of the representation spaces demonstrate that the improvements were gained primarily by a disentangled feature space. Conclusion: Our new representation learning method successfully encoded high-level features in the representation space by using a limited dataset, which showed superior accuracy in the medical image segmentation task compared to other contrastive loss-based methods. Moreover, our method can be easily applied to other networks without using additional parameters in the inference.",
                "year": 2022,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Underwater acoustic sensor networks: research challenges": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570870505000168/pdfft?md5=ca6370b459fd74e78cc9d8dad1ced9aa&pid=1-s2.0-S1570870505000168-main.pdf",
                "title": "Underwater acoustic sensor networks: research challenges",
                "abstract": "Underwater sensor nodes will find applications in oceanographic data collection, pollution monitoring, offshore exploration, disaster prevention, assisted navigation and tactical surveillance applications. Moreover, unmanned or autonomous underwater vehicles (UUVs, AUVs), equipped with sensors, will enable the exploration of natural undersea resources and gathering of scientific data in collaborative monitoring missions. Underwater acoustic networking is the enabling technology for these applications. Underwater networks consist of a variable number of sensors and vehicles that are deployed to perform collaborative monitoring tasks over a given area.\n\nIn this paper, several fundamental key aspects of underwater acoustic communications are investigated. Different architectures for two-dimensional and three-dimensional underwater sensor networks are discussed, and the characteristics of the underwater channel are detailed. The main challenges for the development of efficient networking solutions posed by the underwater environment are detailed and a cross-layer approach to the integration of all communication functionalities is suggested. Furthermore, open research issues are discussed and possible solution approaches are outlined.",
                "year": 2005,
                "publisher": "Ad Hoc Networks"
            }
        }
    },
    "The Concept of Virtual Reality System to Study the Media Message Effectiveness of Social Campaigns": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050918314133/pdfft?md5=4cf87ace456f5e9b2198bf1d05c1144d&pid=1-s2.0-S1877050918314133-main.pdf",
                "title": "The Concept of Virtual Reality System to Study the Media Message Effectiveness of Social Campaigns",
                "abstract": "Social campaigns are an important tool for promoting positive change in social attitudes (in ecology, health prevention, tolerance promotion, etc.). Improving their effectiveness may therefore have a very tangible effect on many aspects of life – both for individuals and for whole societies. Among the most widespread activities that are undertaken within the framework of social campaigns, one can mention advertising through different types of media – television, radio, internet and print. Assessment of this element is done mostly on the basis of questionnaires and focus groups. Research of this type relies on measures that are proximal, such as perceived effectiveness. Although it is assumed that perceived effectiveness is causally antecedent to actual effectiveness, it would be advisable to find the way of assessing actual effectiveness more directly. One of the approaches, that aim to solve this problem is application of the tools of cognitive neuroscience. The use of cognitive neuroscience techniques for pretesting media messages in social campaigns requires properly designed research. In order to do that, the pivotal stage of each experiment design is the choice of media stimuli that will be presented to the participants. Different media and contents will lead to different patterns of responses in viewers, determining a great deal about how a message is processed, including which parts of the message are attended to, and how the message is evaluated and liked. Stimuli presented during the experiment can be static, like pictures or dynamic, like video. However, such stimuli may not be representative of real-life situations. To avoid problems with picture and video stimuli, one can use virtual environments. The aim of the article is to present a concept of virtual reality system that could enable the research of media messages effectiveness in social campaigns.",
                "year": 2018,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "3D imaging system for respiratory monitoring in pediatric intensive care environment": {
        "accordingTo": {
            "scienceDirect": {
                "title": "3D imaging system for respiratory monitoring in pediatric intensive care environment",
                "abstract": "Assessment of respiratory activity in pediatric intensive care unit allows a comprehensive view of the patient’s condition. This allows the identification of high-risk cases for prompt and appropriate medical treatment. Numerous research works on respiration monitoring have been conducted in recent years. However, most of them are unsuitable for clinical environment or require physical contact with the patient, which limits their efficiency. In this paper, we present a novel system for measuring the breathing pattern based on a computer vision method and contactless design. Our 3D imaging system is specifically designed for pediatric intensive care environment, which distinguishes it from the other imaging methods. Indeed, previous works are mostly limited to the use of conventional video acquisition devices, in addition to not considering the constraints imposed by intensive care environment. The proposed system uses depth information captured by two (Red Green Blue-Depth) RGB-D cameras at different view angles, by considering the intensive care unit constraints. Depth information is then exploited to reconstruct a 3D surface of a patient's torso with high temporal and spatial resolution and large spatial coverage. Our system captures the motion information for the top of the torso surface as well as for its both lateral sides. For each reconstruction, the volume is estimated through a recursive subdivision of the 3D space into cubic unit elements. The volume change is then calculated through a subtraction technique between successive reconstructions. We tested our system in the pediatric intensive care unit of the Sainte-Justine university hospital center, where it was compared to the gold standard method currently used in pediatric intensive care units. The performed experiments showed a very high accuracy and precision of the proposed imaging system in estimating respiratory rate and tidal volume.",
                "year": 2018,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "Computational models of space: Isovists and isovist fields": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0146664X79900765/pdfft?md5=8972fb07ed0a23730118bd75fc582e7e&pid=1-s2.0-0146664X79900765-main.pdf",
                "title": "Computational models of space: Isovists and isovist fields",
                "abstract": "A new computational model for space representation, called the isovist, is defined. Given a point x in a space P, the isovist at x, Vz, is the subset of P visible from x. Procedures for computing Vx for polygonal spaces are presented. Next, isovist fields are defined by associating a scalar measure of Vx at each point x in P. The architectural and computational significance of these fields is discussed. Finally, an analysis of computing small, sufficient sets of points is given. A set of points is sufficient if the union of the isovists of the points in the set is the entire space P. Sufficient sets are related to the endpoints of branches of the skeleton in the case of polygonal spaces.",
                "year": 1979,
                "publisher": "Computer Graphics and Image Processing"
            }
        }
    },
    "Unveiling the nature's fruit basket to computationally identify Citrus sinensis csi-mir169–3p as a probable plant miRNA against Reference and Omicron SARS-CoV-2 genome": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482522002943/pdfft?md5=2dadeb25b637cdd6b24e2ea8d7cde4a5&pid=1-s2.0-S0010482522002943-main.pdf",
                "title": "Unveiling the nature's fruit basket to computationally identify Citrus sinensis csi-mir169–3p as a probable plant miRNA against Reference and Omicron SARS-CoV-2 genome",
                "abstract": "The fundamental role of microRNAs (miRNAs) has long been associated with regulation of gene expression during transcription and post transcription of mRNA's 3′UTR by the RNA interference mechanism. Also, the process of how miRNAs tend to induce mRNA degradation has been predominantly studied in many infectious diseases. In this article, we would like to discuss the interaction of dietary plant miRNAs derived from fresh fruits against the viral genome of the causative agent of COVID-19, specifically targeting the 3′UTR of SARS-CoV-2 (Severe acute respiratory syndrome coronavirus 2) genome. Expanding the analysis, we have also identified plant miRNAs that interact against the Omicron (B.1.1.529) variant of SARS-CoV-2 across 37 countries/territories throughout the world. This cross-species virus-plant interaction led us to identify the alignment of dietary plant miRNAs found in fruits like Citrus sinensis (Orange), Prunus persica (Peaches), Vitis vinifera (Grapes) and Malus domestica (Apple) onto the viral genomes. In particular, the interaction of C. sinensis miRNA - csi-miR169–3p and SARS-CoV-2 is noteworthy, as the targeted 3′UTR region “CTGCCT” is found conserved amongst all curated 772 Omicron variants across the globe. Hence this site “CTGCCT” and miRNA csi-miR169–3p may become promising therapeutic candidates to induce viral genome silencing. Thereby, this study reveals the mechanistic way of how fruits tend to enact a fight against viruses like SARS-CoV-2 and aid in maintaining a strong immune system of an individual.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Eras of business computing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Eras of business computing",
                "abstract": "The past half-century has seen amazing progress in the use of information technology and computer systems in business. Computerization and communication technologies have truly revolutionized the business organization of today. This chapter presents a structured overview of the evolution of business computing systems through six distinct eras:u\n\n• Era of Calculation\n\n• Era of Automation\n\n• Era of Integration and Innovation\n\n• Era of Decentralization\n\n• Era of Reengineering and Alignment\n\n• Era of the Internet and Ubiquitous Computing\n\nAdvances in each of the major computing technologies—Computational Platform, Communications, Software, and System Architecture—are surveyed and placed in the context of the computing eras. The focus is on how technologies have enabled innovative strategic business directions based on new business system architectures. A key observation is that around 1975 the principal role of the computer in business systems changed from a computation engine to a digital communications platform. We close the chapter by presenting a set of major conclusions drawn from this survey. Within each conclusion we identify key future directions in business computing that we believe will have a profound impact into the 21st century.",
                "year": 2000,
                "publisher": "Advances in Computers"
            }
        }
    },
    "Metaverse application, flow experience, and Gen-Zers’ participation intention of intangible cultural heritage communication": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2666764923000619/pdfft?md5=a4682285b334aaec5904fc1c4ff07288&pid=1-s2.0-S2666764923000619-main.pdf",
                "title": "Metaverse application, flow experience, and Gen-Zers’ participation intention of intangible cultural heritage communication",
                "abstract": "With outstanding advantages in virtuality, immersion, connectivity, and openness, the application of the metaverse in the development of intangible cultural heritage has demonstrated great potential to enhance Gen-Zers’ participation intention, but the effect and its mechanism remain unclear. This study constructs a theoretical model based on the stimuli-organism-response (SOR) theory, the DeLone and McLean model of information system (IS) success (D&M) model, and flow theory, and conducts an empirical study using a structural equation model and regression analysis based on questionnaire survey data in China to uncover whether and how metaverse application exerts its impact. Results show that metaverse application can enhance Gen-Zers’ participation intention in the communication of intangible cultural heritage, and their mechanism follows a chain path of “stimulus-state-response” under the joint action of “technology-individual-environment” in which metaverse application is the key stimulus factor, flow experience is the mediator, and self-efficacy and subjective norm are moderators. The findings can offer new insights for research on metaverse application from the perspectives of consequences and effects and can also provide practical implications for metaverse application as well as the development and communication of intangible cultural heritage.",
                "year": 2024,
                "publisher": "Data Science and Management"
            }
        }
    },
    "Spatial spillovers of sport industry clusters and community resilience: Bridging a spatial lens to building a smart tourism city": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457323000031/pdfft?md5=f4ef7e3670fded9b474ab01f27fb9a39&pid=1-s2.0-S0306457323000031-main.pdf",
                "title": "Spatial spillovers of sport industry clusters and community resilience: Bridging a spatial lens to building a smart tourism city",
                "abstract": "Given the impact of COVID-19 on socioeconomic conditions, community resilience (the adaptive capability in response to adverse events through possessing or using community resources) has become an increasingly critical agenda in communities. In the context of resilience, a smart tourism city could be a solution for enhancing community resilience. Thus, it is important to identify available community resources to help people cope with adversity, which can contribute to the overriding goal of a smart tourism city. To do this, we aim to provide empirical evidence on the spatial spillover effects of sport industry clusters as community resources on community resilience to advance the building of a resilient smart tourism city. Using spatial econometric modeling with spatial data, we measure the spatial spillover effects of sport industry clusters on community resilience in the United States. The findings reveal the significant spatial spillover effects of sport industry clusters on community resilience within a specific region and across surrounding regions. The findings of this study provide policymakers and practitioners with spatial information on regional sport industry clusters as community resources that are locally bound and specialize in smart tourism and planning to enhance community resilience.",
                "year": 2023,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Transcriptomic analysis of Multiple Sclerosis patient-derived monocytes by RNA-Sequencing for candidate gene discovery": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914821000538/pdfft?md5=183207237ce02ec8ea99e6a7e0b99d6a&pid=1-s2.0-S2352914821000538-main.pdf",
                "title": "Transcriptomic analysis of Multiple Sclerosis patient-derived monocytes by RNA-Sequencing for candidate gene discovery",
                "abstract": "Multiple Sclerosis (MS) is an inflammatory disorder associated with immune abnormalities in the Central Nervous System (CNS), including an abundance of monocytes in MS lesions. The molecular basis for this cellular response is still poorly understood. This study examined changes in the monocyte transcriptome occurring in patients with MS compared to healthy controls (HC) to identify genes that may improve diagnosis and help guide therapies. Unlike previous studies, which mainly utilized microarray technology, we used RNA-Sequencing (RNA-Seq) profiles to determine transcriptome changes, to identify affected pathways and functions, and to reveal the protein-protein interaction (PPI) networks. 4869 genes were significantly different between the two groups (16% upregulated and 17% downregulated). “Chemokine signaling” was the most significantly upregulated pathway. “Jak-STAT signaling,” “Toll-like receptor signaling,” “NOD-like receptor signaling,” “B cell receptor signaling,” “Apoptosis,” “Ubiquitin mediated proteolysis,” and “MAPK signaling” pathways were also significantly upregulated in MS. We also discovered novel candidate genes (RPS4Y1, XIST, DDX3Y, KDM5D, KDM6A, EIF1AY, UTY, TXLNGY, and PRKY) and pathways or pathways change (“Cell cycle,” “Osteoclast differentiation,” “ABC transporters,” “Complement and coagulation cascades,” “Fc gamma R-mediated phagocytosis,” and “Ribosome”) with potential MS implications. Finally, we proposed recommendations for future computational and experimental studies.",
                "year": 2021,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Tackling rapid technology changes by applying enterprise engineering theories": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Tackling rapid technology changes by applying enterprise engineering theories",
                "abstract": "Moore's law states that the number of transistors on a chip will double every two years. A similar force appears to drive the progress of information technology (IT). IT companies tend to struggle to keep up with the latest technological developments, and software solutions are becoming increasingly outdated. The ability for software to change easily is defined as evolvability.\n\nOne of the major fields researching evolvability is enterprise engineering (EE). The EE research paradigm applies theories from other fields to the evolvability of organisations. We argue that such theories can be applied to software engineering (SE) as well, which can contribute to the construction of software with a clear separation of dynamically changing technologies based on a relatively stable description of functions required for a specific user.\n\nEE theories introduce notions of function, construction, and affordance. We reify these concepts in terms of SE. Based on this reification, we propose affordance-driven assembling (ADA) as a software design approach that can aid in the construction of more evolvable software solutions. We exemplify the implementation of ADA in a case study on a commercial system and measure its effectiveness in terms of the impact of changes, as defined by the normalised systems theory.",
                "year": 2022,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Understanding the majority opinion formation process in online environments: An exploratory approach to Facebook": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457317307367/pdfft?md5=69b3897d59b18d4ddc24156493389775&pid=1-s2.0-S0306457317307367-main.pdf",
                "title": "Understanding the majority opinion formation process in online environments: An exploratory approach to Facebook",
                "abstract": "Majority opinions are often observed in the process of social interaction in online communities, but few studies have addressed this issue with empirical data. To identify an appropriate theoretical lens for explaining majority opinions in online environments, this study investigates the skewness statistic, which indicates how many “Likes” are skewed to major comments on a Facebook post; 3489 posts are gathered from the New York Times Facebook page for 100 days. Results show that time is not an influential factor for skewness increase, but the number of comments has a logarithmic relation to skewness increase. Regression models and Chow tests show that this relationship differs depending on topic contents, but majority opinions are significant in overall. These results suggest that the bandwagon effect due to social affordance can be a suitable mechanism for explaining majority opinion formation in an online environment and that majority opinions in online communities can be misperceived due to overestimation.",
                "year": 2018,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Understanding of computers and procrastination: A philosophical approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563213003749/pdfft?md5=aa9d415b3e3a453f78d2d6b6df63fd80&pid=1-s2.0-S0747563213003749-main.pdf",
                "title": "Understanding of computers and procrastination: A philosophical approach",
                "abstract": "Computer procrastination is a complex problem that is under-researched. After identifying a number of key characteristics of it, we survey five existing fields of research that may contribute insights into this interdisciplinary problem, and demonstrate that none of these areas can provide satisfactory insight on their own. A philosophical framework for understanding computer use is introduced, and applied to a case study to demonstrate its potential in understanding the richness of computer procrastination. We then show how this framework can reveal the ways in which each of the existing fields is limited in its ability. The result is both an understanding of why existing research has not directly addressed this issue, and suggestions for a way forward for further research into computer procrastination.",
                "year": 2014,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "An Ecological Approach to Smart Homes for Health Care Services: Conceptual Framework of a Smart Servicescape Wheel": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An Ecological Approach to Smart Homes for Health Care Services: Conceptual Framework of a Smart Servicescape Wheel",
                "abstract": "Background\n\nSmart homes are considered effective solutions for home health care for the elderly, as smart home technologies can reduce care costs and improve elderly residents’ independence. To develop a greater understanding of smart homes for health care services (SHHSs), this study accentuated the necessity of ecological approaches with an emphasis on environmental constraints. This study was based on 2 rationales: (1) users are inclined to perceive the service quality and service experience from environments (ie, servicescape) owing to the intangibility of health care and the pervasiveness of smart home technologies, and (2) both service domains are complex adaptive systems in which diversified and undefined service experiences—not only a few intended service flows—can be generated by complex combinations of servicescape elements.",
                "year": 2019,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Concepts for improved visualization of Web link attributes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128600000724/pdfft?md5=f70315e46a2342e3be31566b962822f6&pid=1-s2.0-S1389128600000724-main.pdf",
                "title": "Concepts for improved visualization of Web link attributes",
                "abstract": "This paper discusses methods to generate and display automatically additional hyperlink information to the users of the World Wide Web. Current Web browsers make it hard to predict what will happen if a link is followed: users get different information than they expect, a new window may be opened, a download starts, or the destination object is just not available. Instead of giving an appropriate notification in advance, users have to follow a link, check whether the document contains the expected information, get back, try another link etc. However, usually it is possible to obtain additional hyperlink information from several sources like link anchor tags, the user’s history and Web servers. Furthermore, with little enhancements, Web servers may include even more additional information to the hyperlinks in Web documents. These can be displayed before users select a link to improve navigation and reduce the cognitive overhead. In this paper several types of Web hyperlink information are listed, potential methods to present these facts are compared, the prototype implementation of the proposed concept — called by us HyperScout — is presented, and further developments are discussed.",
                "year": 2000,
                "publisher": "Computer Networks"
            }
        }
    },
    "Generic semantics of feature diagrams": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128606002179/pdfft?md5=60eecddda6958d41f8067b79f3a4b813&pid=1-s2.0-S1389128606002179-main.pdf",
                "title": "Generic semantics of feature diagrams",
                "abstract": "Feature Diagrams (FDs) are a family of popular modelling languages used to address the feature interaction problem, particularly in software product lines, FDs were first introduced by Kang as part of the FODA (Feature-Oriented Domain Analysis) method back in 1990. Afterwards, various extensions of FODA FDs were introduced to compensate for a purported ambiguity and lack of precision and expressiveness. However, they never received a formal semantics, which is the hallmark of precision and unambiguity and a prerequisite for efficient and safe tool automation.\n\nThe reported work is intended to contribute a more rigorous approach to the definition, understanding, evaluation, selection and implementation of FD languages. First, we provide a survey of FD variants. Then, we give them a formal semantics, thanks to a generic construction that we call Free Feature Diagrams (FFDs). This demonstrates that FDs can be precise and unambiguous. This also defines their expressiveness. Many variants are expressively complete, and thus the endless quest for extensions actually cannot be justified by expressiveness. A finer notion is thus needed to compare these expressively complete languages. Two solutions are well-established: succinctness and embeddability, that express the naturalness of a language. We show that the expressively complete FDs fall into two succinctness classes, of which we of course recommend the most succinct. Among the succinct expressively complete languages, we suggest a new, simple one that is not harmfully redundant: Varied FD (VFD). Finally, we study the execution time that tools will need to solve useful problems in these languages.",
                "year": 2007,
                "publisher": "Computer Networks"
            }
        }
    },
    "A novel one-to-multiple unsupervised domain adaptation framework for abdominal organ segmentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841523001330/pdfft?md5=f42d31fcd64db2e4333872b4c78896eb&pid=1-s2.0-S1361841523001330-main.pdf",
                "title": "A novel one-to-multiple unsupervised domain adaptation framework for abdominal organ segmentation",
                "abstract": "Abdominal multi-organ segmentation in multi-sequence magnetic resonance images (MRI) is of great significance in many clinical scenarios, e.g., MRI-oriented pre-operative treatment planning. Labeling multiple organs on a single MR sequence is a time-consuming and labor-intensive task, let alone manual labeling on multiple MR sequences. Training a model by one sequence and generalizing it to other domains is one way to reduce the burden of manual annotation, but the existence of domain gap often leads to poor generalization performance of such methods. Image translation-based unsupervised domain adaptation (UDA) is a common way to address this domain gap issue. However, existing methods focus less on keeping anatomical consistency and are limited by one-to-one domain adaptation, leading to low efficiency for adapting a model to multiple target domains. This work proposes a unified framework called OMUDA for one-to-multiple unsupervised domain-adaptive segmentation, where disentanglement between content and style is used to efficiently translate a source domain image into multiple target domains. Moreover, generator refactoring and style constraint are conducted in OMUDA for better maintaining cross-modality structural consistency and reducing domain aliasing. The average Dice Similarity Coefficients (DSCs) of OMUDA for multiple sequences and organs on the in-house test set, the AMOS22 dataset and the CHAOS dataset are 85.51%, 82.66% and 91.38%, respectively, which are slightly lower than those of CycleGAN(85.66% and 83.40%) in the first two data sets and slightly higher than CycleGAN(91.36%) in the last dataset. But compared with CycleGAN, OMUDA reduces floating-point calculations by about 87 percent in the training phase and about 30 percent in the inference stage respectively. The quantitative results in both segmentation performance and training efficiency demonstrate the usability of OMUDA in some practical scenes, such as the initial phase of product development.",
                "year": 2023,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Cape: extending Clips for the internet": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705100000563/pdfft?md5=2985dd5ad4260f3dd38a1c371a7e3600&pid=1-s2.0-S0950705100000563-main.pdf",
                "title": "Cape: extending Clips for the internet",
                "abstract": "This paper describes Cape, a programming environment combining Clips And Perl with Extensions. Clips is an efficient and expressive forward-chaining rule-based system with a flexible object system. Perl is a popular procedural language with extremely powerful regular expression matching facilities, and a huge library of freely available software. Cape closely integrates these languages, and provides extensions to facilitate building systems with an intimate mixture of the two. The paper describes the facilities Cape offers programmers and the demonstration systems and “component applications” distributed with it. The use of the system is then discussed with reference to dime (Distributed Information Manipulation Environment), a toolkit being developed to support identifying and coordinating the use of external knowledge sources. Finally, planned developments of the system are indicated.",
                "year": 2000,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Defining the virtual self: Personality, behavior, and the psychology of embodiment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563211002974/pdfft?md5=299cb79bd275176ed077afba1c33a38c&pid=1-s2.0-S0747563211002974-main.pdf",
                "title": "Defining the virtual self: Personality, behavior, and the psychology of embodiment",
                "abstract": "Although researchers have discussed the existence of a virtual self, or embodiment of human characteristics within an avatar, little known about how the virtual self influences a player’s behavior within a virtual environment. To better understand this relationship, World of Warcraft game players were asked to complete personality-rating scales for both themselves and their avatars. In addition, in-world behavior was recorded and then analyzed using a behavioral assessment checklist. Results suggested a relationship between personality and behavior within the domain of agreeableness. Based on these findings, the researchers discuss implications for the construct known as the virtual self, as well as the inclusion of psychological systems design into the overall game design process.",
                "year": 2012,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Trajectory estimation of ultrasound images based on convolutional neural network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809422004645/pdfft?md5=1d2f695ee835d66ed7fda4b4b4adfd3b&pid=1-s2.0-S1746809422004645-main.pdf",
                "title": "Trajectory estimation of ultrasound images based on convolutional neural network",
                "abstract": "Estimating ultrasound probe position and consequently its image position is challenging in image registration and reconstruction concept. On the other hand, in the last few years, the convolutional neural network has had a huge impact on the image processing concept. In this study, we proposed a new network modality. The proposed model attempts to employ the benefits of both the densely connected network and FlowNet for estimating the position information of the ultrasound images. Furthermore, for reducing implementation costs, the inertial measurement unit was utilized. Definition of the images to the networks’ input relies on stack manner. Evaluation of the network’s results was completed in three stages: in the first stage, the comparison is made between proposed network performance with two different image sequences, whereby the proposed model has better performance with a stack of three sequences; in the second stage, the network performance was compared with the conventional method, whereby results indicate better performance, especially in rotation angels; and finally, in the third stage, we attempted to answer how is that the network performance if instead of inertial measurement unit, transformation matrix computed with conventional feature extracting methods. According to the acquired results—utilizing conventional methods with three and five sequence networks performance reduces the amount of absolute mean square error in comparison to stage two results. Especially the amount of this reduction is significant in Euler angels’ estimation. However, the network has better performance while the transformation matrix is computed with IMU's information.",
                "year": 2022,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Nonlinear observer-based recurrent wavelet neuro-controller in disturbance rejection control of flexible structures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197617303044/pdfft?md5=c416ad7cda2c3b1ad57420105fc58e6e&pid=1-s2.0-S0952197617303044-main.pdf",
                "title": "Nonlinear observer-based recurrent wavelet neuro-controller in disturbance rejection control of flexible structures",
                "abstract": "In this paper, a model-based output feedback recurrent wavelet neural network (RWNN) controller is proposed for a class of nonlinear MIMO systems with time-varying matched/mismatched uncertainties. The proposed RWNN emulator adaptively trains to follow an ideal state-feedback controller which is designed on the underlying linear model (ULM) of the plant. Simultaneously, the control system employs an adaptive neural network (NN) mechanism to estimate the mismatch between the RWNN controller and this ideal control law. As a result, the conservatism associated with the classical robust control methods where the controller is synthesized based on worst-case bounds is addressed. Moreover, in order to generalize the subjected class of the investigatable plants, the echo-state feature of adaptive RWNN is used to contribute to the performance of nonminimum phase systems. Accordingly, in the context of flexible smart structures with non-collocated sensor/actuator configuration, a delayed feedback is added in the network which brings about a better match between the model output and the measured output. As a result, even for systems with an unknown Lipschitz constant of lumped uncertainty, the controller can be trained online to compensate with an additional revision of the control law following some Lyapunov-based adaptive stabilizing rules. Additionally, the current approach is proposed as an alternative to the hot topic of nonlinear system identification-based control synthesis where the exact structure of the nonlinearity is required.",
                "year": 2018,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Learning in a neural network model in real time using real world stimuli": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231201003575/pdfft?md5=856b689698bd1b3e4676f0b3643377f7&pid=1-s2.0-S0925231201003575-main.pdf",
                "title": "Learning in a neural network model in real time using real world stimuli",
                "abstract": "In this paper we present a model of the auditory system that is trained using real-world stimuli and running in real-time. The system consists of different sound sources, a microphone, an A/D board, a peripheral auditory system implemented in software and a central network of spiking neurons. The synapses formed by peripheral neurons on the central ones are subject to synaptic plasticity. We implemented a learning rule that depends on the precise temporal relation of pre- and post-synaptic action potentials. We demonstrate that this mechanism allows the development of receptive fields combining learning in real-time, learning with few stimulus presentations and robust learning in the presence of large imbalances in the probability of occurrence of individual stimuli.",
                "year": 2001,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Cyber threat intelligence using PCA-DNN model to detect abnormal network behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1110866521000785/pdfft?md5=26378ce08e2bcb1380928cbcf2be7d13&pid=1-s2.0-S1110866521000785-main.pdf",
                "title": "Cyber threat intelligence using PCA-DNN model to detect abnormal network behavior",
                "abstract": "Security issues are the most critical challenges facing new technologies associated with the internet of things (IoT), big data, and cloud computing. A secure and efficient intrusion detection system (IDS) is crucial to detect security threats. Existing IDSs are known to suffer from many problems, most notably the high rate of false positive alerts, the long time required to detect attacks, and the inability to detect zero-day attacks, which can ruin companies. The weakness of IDS backend engines costs companies time in the investigation process. This paper proposes and enhances IDS detection mechanisms via two processes: using a deep neural network (DNN) model with new features for threat detection based on two assumptions related to handling zero-day attacks, with low computing power and resources, and a comprehensive solution for detection by merging the DNN model and principle component analysis (PCA) to increase security and performance. The proposed detection mechanism combines DNN, PCA, statistical, and knowledge-based approaches to offer significantly greater efficiency than existing IDS, as indicated by analytical and software results. A simulation model is used with up-to-date web attacks, distributed denial of service (DDoS), denial of service (DoS), brute force, insider infiltration, Botnet, and Heartbleed attacks. The proposed detection techniques for large networks are analyzed and complexity in the design is avoided by reducing the number of DNN model layers, thus minimizing detection time delay and false positives, while increasing security against network attacks. Integrating the proposed DNN with PCA, an innovative contribution, introduces robust IDS to significantly improve the detection time delay and security performance. The proposed model showed a 98% accuracy rate. To best of our knowledge, the highest accuracy rate stated based on a large number of attacks is 97%, which makes our model state of art.",
                "year": 2022,
                "publisher": "Egyptian Informatics Journal"
            }
        }
    },
    "Neural and self-reported responses to antisocial news stories: Entertaining versus traditional news introduction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563221003174/pdfft?md5=6d9edf080e4093f1e45e7ff0159b226f&pid=1-s2.0-S0747563221003174-main.pdf",
                "title": "Neural and self-reported responses to antisocial news stories: Entertaining versus traditional news introduction",
                "abstract": "Recently, as entertainment programs have become more prevalent in the news media, there is a growing need to understand how entertainment appeal influences viewers' sociomoral evaluation of news content. We investigated neural and self-reported moral evaluation of antisocial news content in college students who viewed real-life news reports featuring moral violations inside a functional magnetic resonance imaging (fMRI) brain scanner. Each news report was preceded by a traditional or entertaining style of introduction. The behavioral results showed that antisocial severity was reduced for antisocial news content that was presented following entertaining news introductions versus traditional news introductions. The fMRI results showed that entertainment news introductions tax more cognitive control resources than traditional news introductions during news processing, as indicated by greater activation in the dorsolateral prefrontal region; however, they diminish moral saliency, as suggested by the reduced activation in the medial prefrontal region. We also found that greater dorsolateral prefrontal activation during the early phase of news reports was associated with more lenient moral acceptability ratings in the entertainment condition. Furthermore, reduced functional connectivity of the mentalizing brain network was observed in the entertainment condition when compared with the traditional condition. These results suggest that entertainment appeal may increase resource depletion, diminish moral sensitivity, and reduce functional integration of relevant social information during news processing, thus hindering viewers' moral scrutiny. This novel study contributes to a better understanding of how entertainment features influence viewers’ sociomoral evaluation of news stories.",
                "year": 2022,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "On Mobile Agents Resistance to Traffic Analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1571066105052187/pdfft?md5=5d29d8bd8711b778a11f1253d4913592&pid=1-s2.0-S1571066105052187-main.pdf",
                "title": "On Mobile Agents Resistance to Traffic Analysis",
                "abstract": "This paper will concern itself with a formulation of a traffic analysis problem for mobile agents. It is an interesting theoretical problem as well as a critical feature when using agents on a massive scale in decision making systems. The decision making systems are applied to demanding and complex environments such as stock markets. The mobile agents used are natural targets for attacks because they provide information for decision making. The resulting information can have a value measured in millions of dollars and information of such a high value attracts potential attacks. An efficient way to attack the user of decision making systems is to learn her strategy and respond in kind. In this respect even passive observation of agents can provide useful data, namely what information they are gathering. A common first defense is to provide anonymity for mobile agents. However, what happens when anonymity is gone? What information then becomes available and what steps will the user take? Yet, the problem has not been previously formulated for such a framework. We formulate it in terms of various factors used for traffic analysis. These factors originate from different side channels that provide information on the operating agents. At the end we state a paradox, which links an excessive use of countermeasures against traffic analysis with weakening system security.",
                "year": 2006,
                "publisher": "Electronic Notes in Theoretical Computer Science"
            }
        }
    },
    "Targeting CD84 protein on Myeloid-derived Suppressor Cells as a Novel Immunotherapy in Solid Tumors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260725000240/pdfft?md5=cc1fc99fd2c1f72db65e3b652005cf7e&pid=1-s2.0-S0169260725000240-main.pdf",
                "title": "Targeting CD84 protein on Myeloid-derived Suppressor Cells as a Novel Immunotherapy in Solid Tumors",
                "abstract": "Background and Objective\n\n: Myeloid-derived suppressor cells (MDSCs) are a crucial and diverse group of cells found in the tumor microenvironment (TME) that facilitate progression, invasion, and metastasis within solid tumors. CD84, a homophilic adhesion molecule expressed on MDSCs, plays a critical role in their accumulation and function within the TME. This study aims to investigate the protein-protein interactions of CD84 using molecular dynamics simulations and to explore potential therapeutic strategies targeting these interactions.",
                "year": 2025,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Hybrid feature recognition method for setup planning from STEP AP-203": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584508000082/pdfft?md5=911affb3c1e54a8d73582f55a08d7a50&pid=1-s2.0-S0736584508000082-main.pdf",
                "title": "Hybrid feature recognition method for setup planning from STEP AP-203",
                "abstract": "For seamless automation, computer-aided design and manufacturing activities have to be linked by computer-aided process planning (CAPP). An important subtask in CAPP is setup planning, in which a setup plan must be generated ideally from a given 3-D model of the component. In this paper, a hybrid approach that effectively uses volume subtraction and face adjacency graph is proposed to recognize manufacturing features from 3-D model data in STEP AP-203 format. The proposed feature recognition is generic in nature and is capable of recognizing intersecting features also with relative ease. The manufacturing features are clustered based on preferential base for machining and a setup sequence is obtained by alternative rating and ranking. Finally, locating and clamping for each setup are determined considering intermediate shapes of the workpiece. This setup planning method reduces the number of alternatives for evaluation and thereby the computational effort.",
                "year": 2009,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Local deadlock analysis of Simulink models based on timed behavioural patterns and theorem proving": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Local deadlock analysis of Simulink models based on timed behavioural patterns and theorem proving",
                "abstract": "Compositional deadlock analysis of process networks is a well-known challenge. We propose a compositional deadlock analysis strategy for timed process networks, more specifically, those obtained from Simulink multi-rate block diagrams. We handle models with both acyclic and cyclic communication graphs. Particularly, the latter naturally happens in Simulink models with feedback, among other kinds of cycles. Since there is no general solution to analyse cyclic models in a compositional way, we explore the use of behavioural patterns that allow the verification to be carried out in a compositional fashion. We represent process networks in tock-CSP, a dialect of CSP that allows modelling time aspects using a special tock event. The verification approach is implemented as a new package in CSP-Prover, a theorem prover for CSP which is itself implemented in Isabelle/HOL. To illustrate the overall approach and, particularly, how it can scale, we consider several variations of an actuation system with increasing complexity. We show that the examples are instances of the client/server and the asynchronous dynamic timed behaviour patterns. These patterns and all verification steps are formalised using CSP-Prover.",
                "year": 2024,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Site-shifting as the source of ambidexterity: Empirical insights from the field of ticketing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S096386871400002X/pdfft?md5=b5d3da32f9e0b13353f58b3fdfd94acf&pid=1-s2.0-S096386871400002X-main.pdf",
                "title": "Site-shifting as the source of ambidexterity: Empirical insights from the field of ticketing",
                "abstract": "Ambidexterity, defined as the capability to simultaneously explore knowledge to identify new market opportunities and exploit knowledge to capitalise on a firm’s existing niches, is considered to be crucial in today’s competitive marketplace. However, there is relatively limited research on how such a capability can be developed, and even less on the role of IT-enabled practices in promoting this. Drawing on the strategy-as-practice perspective, we investigate how interrelationships amongst practitioners, IT-enabled practices and praxis create a particular site of practice. More importantly, we consider how a site gets shifted over time through the emergence of changes in the interrelationships between IT-enabled practices and practitioners, stimulated by on-going praxis. Building on the findings derived from a case study of DaM,1 the leading ticketing company in China, we explain how the phenomenon of site-shifting can provide a useful conceptual lens for explaining ambidexterity. In doing this, we bring to the fore the importance of IT in achieving an ambidexterity capability.",
                "year": 2014,
                "publisher": "The Journal of Strategic Information Systems"
            }
        }
    },
    "Predicting expert–novice performance as serious games analytics with objective-oriented and navigational action sequences": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563215001740/pdfft?md5=fb78e560e84b4331f60a73d76e01a8dc&pid=1-s2.0-S0747563215001740-main.pdf",
                "title": "Predicting expert–novice performance as serious games analytics with objective-oriented and navigational action sequences",
                "abstract": "Previous research differentiated expert vs. novice performances based on how (dis)similar novices’ action sequences were from that of the expert’s by way of similarity measures. Action sequences were coded using an ‘objective-oriented’ (or task-based) approach based on the sequence of objectives/tasks completed in-game. Findings from these studies suggest that the task-based similarity measures is a better predictor than (a) distance traversed, and (b) time (of completion).\n\nIn this study, we suggested an alternative method to code action sequences of experts and novices by way of a ‘navigational’ (or tile-based) approach. We divided a game-map into grids/tiles of different sizes to facilitate tracing of the path traversed by players in game and proceeded to test the effect of grid sizes on differentiating between experts and novices. We further compared the two different action sequence coding approaches and their abilities to measure players’ competency improvement in serious games. The results of the study showed that the size of game grids does matter, and that both task-based and tile-based action sequence coding approaches are useful for serious games analytics.",
                "year": 2015,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Observing and modeling cognitive events through event-related potentials and ACT-R": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041710000161/pdfft?md5=3ce04da2b2d578789edd1aa91fe652e5&pid=1-s2.0-S1389041710000161-main.pdf",
                "title": "Observing and modeling cognitive events through event-related potentials and ACT-R",
                "abstract": "The study of cognition is generally thought to rely on techniques for inferring cognitive processes that are unobservable. One approach to cognitive science is to leverage an understanding of structure and function of the nervous system based on observable neurological events to determine mental processing. Event-related potential (ERP) research offers one technique to objectively measure cortical responses that are believed to be associated with perceptual and cognitive processes. Here, two ACT-R (Adaptive Control of Thought – Rational) models of mental processing are adapted based on the results of two ERP experiments. The models provide both a sequence of mental steps required to complete each task and a greater specificity of time course of mental events than traditional ACT-R models. We conclude with implications of this research for cognitive theory and ACT-R as well as future work to be conducted.",
                "year": 2011,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "End-to-end prostate cancer detection in bpMRI via 3D CNNs: Effects of attention mechanisms, clinical priori and decoupled false positive reduction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841521002012/pdfft?md5=9e6a125632be0e10af5d2986a455816b&pid=1-s2.0-S1361841521002012-main.pdf",
                "title": "End-to-end prostate cancer detection in bpMRI via 3D CNNs: Effects of attention mechanisms, clinical priori and decoupled false positive reduction",
                "abstract": "We present a multi-stage 3D computer-aided detection and diagnosis (CAD) model2 for automated localization of clinically significant prostate cancer (csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive its detection network, targeting salient structures and highly discriminative feature dimensions across multiple resolutions. Its goal is to accurately identify csPCa lesions from indolent cancer and the wide range of benign pathology that can afflict the prostate gland. Simultaneously, a decoupled residual classifier is used to achieve consistent false positive reduction, without sacrificing high sensitivity or computational efficiency. In order to guide model generalization with domain-specific clinical knowledge, a probabilistic anatomical prior is used to encode the spatial prevalence and zonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired with radiologically-estimated annotations, we hypothesize that such CNN-based models can be trained to detect biopsy-confirmed malignancies in an independent cohort.\n\nFor 486 institutional testing scans, the 3D CAD system achieves 83.69±5.22% and 93.19±2.96% detection sensitivity at 0.50 and 1.46 false positive(s) per patient, respectively, with 0.882±0.030 AUROC in patient-based diagnosis –significantly outperforming four state-of-the-art baseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from recent literature. For 296 external biopsy-confirmed testing scans, the ensembled CAD system shares moderate agreement with a consensus of expert radiologists (76.69%; kappa = 0.51±0.04) and independent pathologists (81.08%; kappa = 0.56±0.06); demonstrating strong generalization to histologically-confirmed csPCa diagnosis.",
                "year": 2021,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "On the information in optical flows": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X83900671/pdfft?md5=ff81c2a2024566a4f058f0455c3f3ff6&pid=1-s2.0-0734189X83900671-main.pdf",
                "title": "On the information in optical flows",
                "abstract": "This paper outlines the structure of optical flows and their relation to relative depth, local surface orientation, relative motion, and the source of optokinetic information, the retinal velocities. Some possibilities and limitations of optical flows as a source of information about the three-dimensional environment are also discussed. It is found that optical flows cannot directly specify egomotion or relative motion of objects with respect to the observer, and that the relative depth and local surface orientation is unambiguously contained in optical flows.",
                "year": 1983,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Structure and motion from optical flow under perspective projection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0734189X87801330/pdfft?md5=ecae23562a725bdfe1d00cdea1ca71de&pid=1-s2.0-S0734189X87801330-main.pdf",
                "title": "Structure and motion from optical flow under perspective projection",
                "abstract": "The 3D structure and motion of an object are determined from its optical flow under perspective projection. The solution is given in explicit analytical form in terms of the parameters characterizing the flow of planar motion. The solution is not unique, but the spurious solution disappears if two or more planar parts of the same object are observed, and the adjacency condition of optical flows is explicitly obtained. Unique interpretation also becomes possible by considering the transition to the “pseudo-orthographic approximation,” since no spurious solution arises in this approximation. The choice of the coordinate system and parameterization of rigid motion are discussed in relation to robustness of computation.",
                "year": 1987,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Knowledge is power: Open-world knowledge representation learning for knowledge-based visual reasoning": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Knowledge is power: Open-world knowledge representation learning for knowledge-based visual reasoning",
                "abstract": "Knowledge-based visual reasoning requires the ability to associate outside knowledge that is not present in a given image for cross-modal visual understanding. Two deficiencies of the existing approaches are that (1) they only employ or construct elementary and explicit but superficial knowledge graphs while lacking complex and implicit but indispensable cross-modal knowledge for visual reasoning, and (2) they also cannot reason new/unseen images or questions in open environments and are often violated in real-world applications. How to represent and leverage tacit multimodal knowledge for open-world visual reasoning scenarios has been less studied. In this paper, we propose a novel open-world knowledge representation learning method to not only construct implicit knowledge representations from the given images and their questions but also enable knowledge transfer from a known given scene to an unknown scene for answer prediction. Extensive experiments conducted on six benchmarks demonstrate the superiority of our approach over other state-of-the-art methods. We apply our approach to other visual reasoning tasks, and the experimental results show that our approach, with its good performance, can support related reasoning applications.",
                "year": 2024,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "A model of pilot-job resource provisioning on production grids": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167819111000366/pdfft?md5=a23e3bf5ca3b462c7f6e7ef2b3b996c4&pid=1-s2.0-S0167819111000366-main.pdf",
                "title": "A model of pilot-job resource provisioning on production grids",
                "abstract": "Pilot-job systems emerged as a computation paradigm to cope with the heterogeneity of large-scale production grids, greatly reducing fault ratios and middleware overheads. They are now widely adopted to sustain the computation of scientific applications on such platforms. However, a model of pilot-job systems is still lacking, making it difficult to build realistic experimental setups for their study (e.g. simulators or controlled platforms). The variability of production conditions, background loads and resource characteristics further complicate this issue. This paper presents a model of pilot-job resource provisioning. Based on a probabilistic modeling of pilot submission and registration, the number of pilots registered to the application host and the makespan of a divisible-load application are derived. The model takes into account job failures and it does not make any assumption on the characteristics of the computing resources, on the scheduling algorithm or on the background load. Only a minimally invasive monitoring of the grid is required. The model is evaluated in production conditions, using logs acquired on a pilot-job server deployed in the biomed virtual organization of the European Grid Infrastructure. Experimental results show that the model is able to accurately describe the number of registered pilots along time periods ranging from a few hours to a few days and in different pilot submission conditions.",
                "year": 2011,
                "publisher": "Parallel Computing"
            }
        }
    },
    "SANDS: A service-oriented architecture for clinical decision support in a National Health Information Network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046408000373/pdfft?md5=cb9dc80b4a331158a92345cd1becfbb7&pid=1-s2.0-S1532046408000373-main.pdf",
                "title": "SANDS: A service-oriented architecture for clinical decision support in a National Health Information Network",
                "abstract": "In this paper, we describe and evaluate a new distributed architecture for clinical decision support called SANDS (Service-oriented Architecture for NHIN Decision Support), which leverages current health information exchange efforts and is based on the principles of a service-oriented architecture. The architecture allows disparate clinical information systems and clinical decision support systems to be seamlessly integrated over a network according to a set of interfaces and protocols described in this paper. The architecture described is fully defined and developed, and six use cases have been developed and tested using a prototype electronic health record which links to one of the existing prototype National Health Information Networks (NHIN): drug interaction checking, syndromic surveillance, diagnostic decision support, inappropriate prescribing in older adults, information at the point of care and a simple personal health record. Some of these use cases utilize existing decision support systems, which are either commercially or freely available at present, and developed outside of the SANDS project, while other use cases are based on decision support systems developed specifically for the project. Open source code for many of these components is available, and an open source reference parser is also available for comparison and testing of other clinical information systems and clinical decision support systems that wish to implement the SANDS architecture.\n\nThe SANDS architecture for decision support has several significant advantages over other architectures for clinical decision support. The most salient of these are:\n\n1.\n\nGreater modularity than other architectures, allowing for work to be distributed.\n\n2.\n\nThe potential for creating and sustaining a commercial market for clinical decision support.\n\n3.\n\nReduced cost and risk of trying new decision support systems because of its ability to easily integrate a variety of decision support services, and to easily remove them, if desired, as well.\n\n4.\n\nSignificant freedom for developers of clinical decision support systems to choose the way they represent knowledge and internally implement their system, in comparison to other approaches which constrain such developers to a particular knowledge representation formalism.\n\n5.\n\nUnification of the direction and agenda of decision support research and development with promising near-term efforts to improve interoperability of clinical systems.",
                "year": 2008,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "The concept of virtual motion in vision calculus": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167865590900292/pdfft?md5=d02ee32629c07930db98820ff2139a8a&pid=1-s2.0-0167865590900292-main.pdf",
                "title": "The concept of virtual motion in vision calculus",
                "abstract": "A novel concept for analysis of one perspective image is presented. The calculus involved exhibits analogies with motion analysis of an image sequence. Schemes for determination of orientation and shape of textured surfaces and of direction to a distant point light source are shortly suggested as illustration of the concept.",
                "year": 1990,
                "publisher": "Pattern Recognition Letters"
            }
        }
    },
    "The role of cultural fitness in user resistance to information technology tools": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0953543897000313/pdfft?md5=8d84d88d68e7af3235aece4ecb3dbc8b&pid=1-s2.0-S0953543897000313-main.pdf",
                "title": "The role of cultural fitness in user resistance to information technology tools",
                "abstract": "Human interactions with IT tools reproduce organizational cultural patterns in evolutionary terms which are similar to those seen in the evolution of human tools and language. This paper proposes that user adoption or rejection of new IT tools is derived from the cultural fitness of the tools in the organizational context rather than being close to the user's operational adaptation. The hypothesis proposed here requires an understanding of the correlation between language and tool use and an analysis of recent multi-disciplinary research in tool-mediated activity, language and cognition. Concepts of tool-mediated activity in a cultural context and their theoretical implications for HCI are examined by using the fields of anthropology, cognitive sciences and information technology. A comparative analysis of empirical data using cultural parameters is performed showing the effects of cultural fitness on the discretionary use of a new collaborative IT tool in an organizational context.",
                "year": 1998,
                "publisher": "Interacting with Computers"
            }
        }
    },
    "Toolpath generation along directions of maximum kinematic performance; a first cut at machine-optimal paths": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448501001166/pdfft?md5=788a3cc3bcc294f76f414dec0229f6ec&pid=1-s2.0-S0010448501001166-main.pdf",
                "title": "Toolpath generation along directions of maximum kinematic performance; a first cut at machine-optimal paths",
                "abstract": "In earlier work, we introduced the concept of time-optimal toolpaths, modeled the behavior and constraints of machining, and formulated the optimization problem mathematically. The question was by what toolpath it would be possible to machine a surface in minimum time—while considering the kinematic performance of a machine, the speed limits of the motors and the surface finish requirements. The time-optimal problem is a difficult one, and does not generally yield a closed-form solution. Here we present a heuristics-based approach to the problem, which we refer to as the greedy approach. The performance envelope of the machine at a point on the surface is very anisotropic, and material can be removed much more rapidly in some directions than in other directions. The greedy approach seeks the directions of the best performance. We describe algorithms to first find such advantageous directions. We then show how they can be fitted by a continuous vector field. We also show how toolpaths with the proper side-steps can be generated from this field. We end with results showing the improvement of performance that can be derived from greedy toolpaths.",
                "year": 2002,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "An automated reasoning framework for translational research": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046409001518/pdfft?md5=6209938dcece9362fef44ca90a3d5f5b&pid=1-s2.0-S1532046409001518-main.pdf",
                "title": "An automated reasoning framework for translational research",
                "abstract": "In this paper we propose a novel approach to the design and implementation of knowledge-based decision support systems for translational research, specifically tailored to the analysis and interpretation of data from high-throughput experiments. Our approach is based on a general epistemological model of the scientific discovery process that provides a well-founded framework for integrating experimental data with preexisting knowledge and with automated inference tools.\n\nIn order to demonstrate the usefulness and power of the proposed framework, we present its application to Genome-Wide Association Studies, and we use it to reproduce a portion of the initial analysis performed on the well-known WTCCC dataset. Finally, we describe a computational system we are developing, aimed at assisting translational research. The system, based on the proposed model, will be able to automatically plan and perform knowledge discovery steps, to keep track of the inferences performed, and to explain the obtained results.",
                "year": 2010,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "From keyloggers to touchloggers: Take the rough with the smooth": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404812001654/pdfft?md5=4e742c0e55f282d00e8308b6493bc17b&pid=1-s2.0-S0167404812001654-main.pdf",
                "title": "From keyloggers to touchloggers: Take the rough with the smooth",
                "abstract": "The proliferation of touchscreen devices brings along several interesting research challenges. One of them is whether touchstroke-based analysis (similar to keylogging) can be a reliable means of profiling the user of a mobile device. Of course, in such a setting, the coin has two sides. First, one can employ the output produced by such a system to feed machine learning classifiers and later on intrusion detection engines. Second, aggressors can install touchloggers to harvest user's private data. This malicious option has been also extensively exploited in the past by legacy keyloggers under various settings, but has been scarcely assessed for soft keyboards. Compelled by these separate but interdependent aspects, we implement the first-known native and fully operational touchlogger for ultramodern smartphones and especially for those employing the proprietary iOS platform. The results we obtained for the first objective are very promising showing an accuracy in identifying misuses, and thus post-authenticating the user, in an amount that exceeds 99%. The virulent personality of such software when used maliciously is also demonstrated through real-use cases.",
                "year": 2013,
                "publisher": "Computers & Security"
            }
        }
    },
    "Gradient design and fabrication methodology for interleaved self-locking kirigami panels": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Gradient design and fabrication methodology for interleaved self-locking kirigami panels",
                "abstract": "Sandwich panels with excellent mechanical properties are widely used in the aerospace, architecture, and automobile industries. Kirigami-inspired structural designs are receiving increasing attention owing to the shape-induced functions and novel properties imparted by their folds and cuts. In this study, novel graded self-locking kirigami panels based on a tucked-interleaved pattern are developed and analyzed under quasi-static loading. The proposed tucked-interleaved pattern can be assembled to form freely supported self-locking polyhedral structures. The self-locking property is ensured by the interleaved flaps, which create in-plane compression to hold the structure in place. In particular, we analyze the effects of geometric variations in kirigami panels fabricated using a CO2 laser machining system. The experimental data under quasi-static compression and simulation results both indicate that the proposed kirigami panels have outstanding load-to-weight ratios on the order of 105. It appears that the introduction of a graded design can generate graded stiffness as well as superior specific energy absorption with an appropriate introduction of geometric gradients. These results show that the proposed kirigami panels combining self-locking and programmable non-uniform stiffness have great potential for non-uniform engineering applications.",
                "year": 2024,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Enhancing understanding of asphalt mixture dynamic modulus prediction through interpretable machine learning method": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1474034625000047/pdfft?md5=e09a7feabcf54737ad2835ef72c2ce90&pid=1-s2.0-S1474034625000047-main.pdf",
                "title": "Enhancing understanding of asphalt mixture dynamic modulus prediction through interpretable machine learning method",
                "abstract": "Dynamic modulus is a key parameter in pavement design and pavement mechanics analysis. It is essential to accurately predict dynamic modulus and study the relationships between influencing factors and dynamic modulus. In this study, a hybrid prediction model is developed based on Extreme Gradient Boosting (XGBoost) and Whale Optimization Algorithm (WOA). Based on this model, the effects of asphalt binder properties, test condition, asphalt mixture volume parameters, and asphalt mixture gradation on dynamic modulus are analyzed. The contribution of each variable to the model predictions is quantified through Shapley Additive Explanations (SHAP), and the interaction between dynamic modulus and influencing factors is evaluated by Partial Dependence Plot (PDP). The results indicate that the WOA-XGBoost model has excellent accuracy and robustness in predicting dynamic modulus. The three most important factors affecting dynamic modulus prediction results are the complex shear modulus of binder, the test temperature and the asphalt binder viscosity. The increase in dynamic modulus can be achieved through the utilization of asphalt binders characterized by relatively large complex modulus, high viscosity, small phase angle, and high asphalt PG indexes. Reducing the effective binder volume and air voids of the mixture, optimizing the mixture gradation to a suitable level, and increasing the mineral powder content can also lead to the increase of dynamic modulus. Besides, low test temperature and high frequency generally mean a large value of dynamic modulus. This study clarifies the impact of influencing factors on the performance of asphalt mixtures based on machine learning, which lay a foundation for the intelligent design of asphalt mixtures.",
                "year": 2025,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "Automatic segmentation of mitochondria and endolysosomes in volumetric electron microscopy data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482520300792/pdfft?md5=bf9007c05c278174cf1ffafabefd1a77&pid=1-s2.0-S0010482520300792-main.pdf",
                "title": "Automatic segmentation of mitochondria and endolysosomes in volumetric electron microscopy data",
                "abstract": "Automatic segmentation of intracellular compartments is a powerful technique, which provides quantitative data about presence, spatial distribution, structure and consequently the function of cells. With the recent development of high throughput volumetric data acquisition techniques in electron microscopy (EM), manual segmentation is becoming a major bottleneck of the process. To aid the cell research, we propose a technique for automatic segmentation of mitochondria and endolysosomes obtained from urinary bladder urothelial cells by the dual beam EM technique. We present a novel publicly available volumetric EM dataset – the first of urothelial cells, evaluate several state-of-the-art segmentation methods on the new dataset and present a novel segmentation pipeline, which is based on supervised deep learning and includes mechanisms that reduce the impact of dependencies in the input data, artefacts and annotation errors. We show that our approach outperforms the compared methods on the proposed dataset.",
                "year": 2020,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Semantic annotation, indexing, and retrieval": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826804000162/pdfft?md5=faafb112c8648857cea74ac269e80f78&pid=1-s2.0-S1570826804000162-main.pdf",
                "title": "Semantic annotation, indexing, and retrieval",
                "abstract": "The Semantic Web realization depends on the availability of a critical mass of metadata for the web content, associated with the respective formal knowledge about the world. We claim that the Semantic Web, at its current stage of development, is in a state of a critical need of metadata generation and usage schemata that are specific, well-defined and easy to understand. This paper introduces our vision for a holistic architecture for semantic annotation, indexing, and retrieval of documents with regard to extensive semantic repositories. A system (called KIM), implementing this concept, is presented in brief and it is used for the purposes of evaluation and demonstration.\n\nA particular schema for semantic annotation with respect to real-world entities is proposed. The underlying philosophy is that a practical semantic annotation is impossible without some particular knowledge modelling commitments. Our understanding is that a system for such semantic annotation should be based upon a simple model of real-world entity classes, complemented with extensive instance knowledge. To ensure the efficiency, ease of sharing, and reusability of the metadata, we introduce an upper-level ontology (of about 250 classes and 100 properties), which starts with some basic philosophical distinctions and then goes down to the most common entity types (people, companies, cities, etc.). Thus it encodes many of the domain-independent commonsense concepts and allows straightforward domain-specific extensions. On the basis of the ontology, a large-scale knowledge base of entity descriptions is bootstrapped, and further extended and maintained. Currently, the knowledge bases usually scales between 105 and 106 descriptions.\n\nFinally, this paper presents a semantically enhanced information extraction system, which provides automatic semantic annotation with references to classes in the ontology and to instances. The system has been running over a continuously growing document collection (currently about 0.5 million news articles), so it has been under constant testing and evaluation for some time now. On the basis of these semantic annotations, we perform semantic based indexing and retrieval where users can mix traditional information retrieval (IR) queries and ontology-based ones. We argue that such large-scale, fully automatic methods are essential for the transformation of the current largely textual web into a Semantic Web.",
                "year": 2004,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Multilevel neuronal architecture to resolve classification problems with large training sets: Parallelization of the training process": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Multilevel neuronal architecture to resolve classification problems with large training sets: Parallelization of the training process",
                "abstract": "The value of radial base function networks (RBF) has been fully demonstrated and their application in a wide number of scientific fields is undisputed. A fundamental aspect of this tool focusses on the training process, which determines both the efficiency (success or “hit rat” in the subsequent classification) and the overall performance (runtime), since the RBF training phase is the most expensive phase in terms of time.\n\nThere is abundant literature on studies to improve these aspects, in which all the training techniques proposed are classified either as iterative techniques, with very short execution times for the training process, or as the traditional exact techniques, which excel in their high rates of accuracy in the classification.\n\nIn our field of study we require the smallest error possible in the classification process, and for this reason, our research opts for exact techniques, while we also work to improve the high latencies in the training process.\n\nIn a previous study, we proposed a pseudo-exact technique with which we improved the training process by an average of 99.1638177% using RBF-SOM architecture. In the present study we exploit one characteristic of this architecture, namely the possibility of parallelization of the training process.\n\nAccordingly, our article proposes a RBF-SOM structure which, thanks to CUDA, parallelizes the training process. This we will denote as CUDA-RBF-SOM architecture.",
                "year": 2016,
                "publisher": "Journal of Computational Science"
            }
        }
    },
    "Obtaining a 3-D orientation of projective textures using a morphological method": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0031320395001018/pdfft?md5=a3a156b862d75bc4833dbc7f3ecf2d83&pid=1-s2.0-0031320395001018-main.pdf",
                "title": "Obtaining a 3-D orientation of projective textures using a morphological method",
                "abstract": "On the premise that the texture image is a planar surface, we propose a new morphological approach to obtain a 3-D surface orientation using the variation of texture image caused by projective distortions. Centroids of texels can be obtained from the farthest texel to the nearest texel sequentially by recursive erosions, so a texture image is segmented into several sub-regions. Using major axes and sizes of structuring elements in segmented regions, we can compute a vanishing point and obtain a 3-D surface orientation. In the experiments, we have ascertained the proposed algorithm was more effective than the aggregation transform.",
                "year": 1996,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "A Digital Behavioral Activation Intervention (JuNEX) for Pregnant Women With Subclinical Depression Symptoms: Explorative Co-Design Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A Digital Behavioral Activation Intervention (JuNEX) for Pregnant Women With Subclinical Depression Symptoms: Explorative Co-Design Study",
                "abstract": "Background\n\nDigital interventions are gaining increasing interest due to their structured nature, ready availability, and self-administered capabilities. Perinatal women have expressed a desire for such interventions. In this regard, behavioral activation interventions may be particularly suitable for digital administration.",
                "year": 2024,
                "publisher": "JMIR Human Factors"
            }
        }
    },
    "Categorization of web pages – Performance enhancement to search engine": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705108001378/pdfft?md5=8cbcf8c1255d0e69a9daff887a3a7ef1&pid=1-s2.0-S0950705108001378-main.pdf",
                "title": "Categorization of web pages – Performance enhancement to search engine",
                "abstract": "With the advent of technology man is endeavoring for relevant and optimal results from the web through search engines. Retrieval performance can often be improved using several algorithms and methods. Abundance in web has impelled to exert better search systems. Categorization of the web pages abet fairly in addressing this issue. The anatomy of the web pages, links, categorization of text and their relations are empathized with time. Search engines perform critical analysis using several inputs for a keyword(s) to obtain quality results in shortest possible time. Categorization is mostly done with separating the content using the web link structure. We estimated two different page weights (a) Page Retaining Weight (PRW) and (b) Page Forwarding Weight (PFW) for a web page and grouped for categorization. Using these experimental results we classified the web pages into four different groups i.e. (A) Simple type (B) Axis shifted (c) Fluctuated and (d) Oscillating types. Implication in development of such categorization alleviates the performance of search engines and also delves into study of web modeling studies.",
                "year": 2009,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Visibility of urban activities and pedestrian routes: An experiment in a virtual environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0198971516300370/pdfft?md5=f1733b84aba4e5c6f6cbcb059c7c5dd8&pid=1-s2.0-S0198971516300370-main.pdf",
                "title": "Visibility of urban activities and pedestrian routes: An experiment in a virtual environment",
                "abstract": "Patterns of pedestrian movement in cities are influenced by visibility of urban activities, among other factors. This paper describes a behavioral experiment in an immersive virtual reality, where individuals are exposed to visual stimuli and their route decisions are recorded. To evaluate the effect of urban visibility on the pedestrian route choices, we introduce visibility scores, such as a graph theoretical centrality measure, to represent the urban environment. Participants in the experiment select routes and the chains of their navigational decisions are then associated with the visibility scores. The selected routes in the virtual reality are compared with all possible routes, which are generated by computational random walk simulations.\n\nStatistical analysis suggests that pedestrian navigation choices are purposive and that they are influenced by the specific visibility attributes of the urban environment. Therefore, the research contributes toward development of a unified framework that incorporates the interaction of urban morphology and human spatial behavior. In addition, our experimental procedure and our results highlight the potential of a virtual reality laboratory as a research environment for complex urban situations.",
                "year": 2016,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "Stress flow analysis of bio-structures using the finite element method and the flow network approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168874X1830221X/pdfft?md5=26bd8f048909da81547c4f9d9d8e3dd0&pid=1-s2.0-S0168874X1830221X-main.pdf",
                "title": "Stress flow analysis of bio-structures using the finite element method and the flow network approach",
                "abstract": "Bio-structures have been optimized by nature to possess remarkable resiliency and strength yet retain a light-weight composition. The hierarchical geometrical arrangements that are prevalent in bio-structures, coupled with heterogeneous constituents, present uncertainty as to what dictates the structural response. Although computational mechanics experiments give an overall distribution of stresses in the structural system, the underlying details of the stress flow patterns in the structure are hard to identify. This paper discusses the development of software that integrates the results from computational mechanics experiments with advanced mathematical algorithms to capture the stress distribution in a bio-structure subjected to an external load. A fundamental issue addressed in this work is the procedure used to prepare data in appropriate format for the seamless transition from finite element binary database files to an abstract mathematical domain. The robust, platform-independent procedure developed for this transdisciplinary strategy efficiently handles the large datasets produced by the high performance computational mechanics experiments.",
                "year": 2018,
                "publisher": "Finite Elements in Analysis and Design"
            }
        }
    },
    "JuiceCaster: Towards automatic juice filming attacks on smartphones": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1084804516300650/pdfft?md5=6a996e460f9a2b30a7c4a5678d2a233b&pid=1-s2.0-S1084804516300650-main.pdf",
                "title": "JuiceCaster: Towards automatic juice filming attacks on smartphones",
                "abstract": "Smartphones have become a part of our daily lives. Thus, they have become a big target for attacks such as malware. While smartphone malware is very popular in the research community, charging attacks are often ignored by the literature. As public charging stations are common, we argue that charging attacks will become a big concern and be used to compromise users’ privacy. For example, government agents and malicious merchants can invade the privacy of phone users through this kind of attacks.\n\nIn this paper, we describe a vulnerability of smartphone charging and introduce juice filming attacks that can steal sensitive information by recording screen activities during charging. We show that the display of smartphones can be leaked through a standard micro-USB connector using the Mobile High-Definition Link (MHL) standard or the iPhones' lightning connector, making our attack feasible in both Android OS and iOS. Furthermore, we implement a prototype called JuiceCaster, which can automate the whole adversary procedure including video-capturing users' inputs, dividing videos into images and extracting texts from images with OCR (Optical Character Recognition) technology. In the evaluation, experimental results from various studies demonstrate that our attack is effective in practice. Our efforts aim to stimulate more awareness in this area.",
                "year": 2016,
                "publisher": "Journal of Network and Computer Applications"
            }
        }
    },
    "A taxonomy and survey on Green Data Center Networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X13001519/pdfft?md5=51326f119df87211405dd097c3e5ac23&pid=1-s2.0-S0167739X13001519-main.pdf",
                "title": "A taxonomy and survey on Green Data Center Networks",
                "abstract": "Data centers are growing exponentially (in number and size) to accommodate the escalating user and application demands. Likewise, the concerns about the environmental impacts, energy needs, and electricity cost of data centers are also growing. Network infrastructure being the communication backbone of the data center plays a pivotal role in the data center’s scalability, performance, energy consumption, and cost. Research community is endeavoring hard to overcome the challenges faced by the legacy Data Center Networks (DCNs). Serious efforts have been made to handle the problems in various DCN areas. This survey presents significant insights to the state-of-the-art research conducted pertaining to the DCN domain along with a detailed discussion of the energy efficiency aspects of the DCNs. The authors explored: (a) DCN architectures (electrical, optical, and hybrid), (b) network traffic management and characterization, (c) DCN performance monitoring, (d) network-aware resource allocation, (e) DCN experimentation techniques, and (f) energy efficiency. The survey presents an overview of the ongoing research in the broad domain of DCNs and highlights the challenges faced by the DCN research community.",
                "year": 2014,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Enabling machine learning-ready HPC ensembles with Merlin": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X22000322/pdfft?md5=4afbd47e20e035371355678a49f9814b&pid=1-s2.0-S0167739X22000322-main.pdf",
                "title": "Enabling machine learning-ready HPC ensembles with Merlin",
                "abstract": "With the growing complexity of computational and experimental facilities, many scientific researchers are turning to machine learning (ML) techniques to analyze large scale ensemble data. With complexities such as multi-component workflows, heterogeneous machine architectures, parallel file systems, and batch scheduling, care must be taken to facilitate this analysis in a high performance computing (HPC) environment. In this paper, we present Merlin, a workflow framework to enable large ML-friendly ensembles of scientific HPC simulations. By augmenting traditional HPC with distributed compute technologies, Merlin aims to lower the barrier for scientific subject matter experts to incorporate ML into their analysis. As a producer–consumer workflow model, Merlin enables multi-machine, cross-batch job, dynamically allocated yet persistent workflows capable of utilizing surge-compute resources. Key features of Merlin are a flexible HPC-centric interface, low per-task overhead, multi-tiered fault recovery, and a hierarchical sampling algorithm that allows for O(N) task execution and O(NlnN) task queuing to ensembles of millions of tasks. In addition to Merlin’s design, we test the algorithm’s performance in an HPC center and demonstrate the ability to enqueue 40 million simulations in 100 s, with a 30 millisecond per-task overhead that is independent of ensemble size. Finally, we describe some example applications that Merlin has enabled on leadership-class HPC resources, such as the ML-augmented optimization of nuclear fusion experiments and the calibration of infectious disease models to study the progression of and possible mitigation strategies for COVID-19.",
                "year": 2022,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Big data and ergonomics methods: A new paradigm for tackling strategic transport safety risks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687015300806/pdfft?md5=4d9e1d047225459f980b341c1070201f&pid=1-s2.0-S0003687015300806-main.pdf",
                "title": "Big data and ergonomics methods: A new paradigm for tackling strategic transport safety risks",
                "abstract": "Big data collected from On-Train Data Recorders (OTDR) has the potential to address the most important strategic risks currently faced by rail operators and authorities worldwide. These risk issues are increasingly orientated around human performance and have proven resistant to existing approaches. This paper presents a number of proof of concept demonstrations to show that long standing ergonomics methods can be driven from big data, and succeed in providing insight into human performance in a novel way. Over 300 ergonomics methods were reviewed and a smaller sub-set selected for proof-of-concept development using real on-train recorder data. From this are derived nine candidate Human Factors Leading Indicators which map on to all of the psychological precursors of the identified risks. This approach has the potential to make use of a significantly underused source of data, and enable rail industry stakeholders to intervene sooner to address human performance issues that, via the methods presented in this paper, are clearly manifest in on-train data recordings. The intersection of psychological knowledge, ergonomics methods and big data creates an important new framework for driving new insights.",
                "year": 2016,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "CS2-Net: Deep learning segmentation of curvilinear structures in medical imaging": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841520302383/pdfft?md5=7ddfe81ab3a1a18e13d7428345b3f961&pid=1-s2.0-S1361841520302383-main.pdf",
                "title": "CS2-Net: Deep learning segmentation of curvilinear structures in medical imaging",
                "abstract": "Automated detection of curvilinear structures, e.g., blood vessels or nerve fibres, from medical and biomedical images is a crucial early step in automatic image interpretation associated to the management of many diseases. Precise measurement of the morphological changes of these curvilinear organ structures informs clinicians for understanding the mechanism, diagnosis, and treatment of e.g. cardiovascular, kidney, eye, lung, and neurological conditions. In this work, we propose a generic and unified convolution neural network for the segmentation of curvilinear structures and illustrate in several 2D/3D medical imaging modalities. We introduce a new curvilinear structure segmentation network (CS2-Net), which includes a self-attention mechanism in the encoder and decoder to learn rich hierarchical representations of curvilinear structures. Two types of attention modules - spatial attention and channel attention - are utilized to enhance the inter-class discrimination and intra-class responsiveness, to further integrate local features with their global dependencies and normalization, adaptively. Furthermore, to facilitate the segmentation of curvilinear structures in medical images, we employ a 1×3 and a 3×1 convolutional kernel to capture boundary features. Besides, we extend the 2D attention mechanism to 3D to enhance the network’s ability to aggregate depth information across different layers/slices. The proposed curvilinear structure segmentation network is thoroughly validated using both 2D and 3D images across six different imaging modalities. Experimental results across nine datasets show the proposed method generally outperforms other state-of-the-art algorithms in various metrics.",
                "year": 2021,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Multi-layered transcriptomic analysis reveals a pivotal role of FMR1 and other developmental genes in Alzheimer's disease-associated brain ceRNA network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482523009599/pdfft?md5=faf88660da37a9feb24af218dafdac31&pid=1-s2.0-S0010482523009599-main.pdf",
                "title": "Multi-layered transcriptomic analysis reveals a pivotal role of FMR1 and other developmental genes in Alzheimer's disease-associated brain ceRNA network",
                "abstract": "Alzheimer's disease (AD) is an increasingly neurodegenerative disorder that causes progressive cognitive decline and memory impairment. Despite extensive research, the underlying causes of late-onset AD (LOAD) are still in progress. This study aimed to establish a network of competing regulatory interactions involving circular RNAs (circRNAs), microRNAs (miRNAs), RNA-binding proteins (RBPs), and messenger RNAs (mRNAs) connected to LOAD. A systematic analysis of publicly available expression data was conducted to identify integrated differentially expressed genes (DEGs) from the hippocampus of LOAD patients. Subsequently, gene co-expression analysis identified modules comprising highly expressed DEGs that act cooperatively. The competition between co-expressed DEGs and miRNAs/RBPs and the simultaneous interactions between circRNA and miRNA/RBP revealed a complex ceRNA network responsible for post-transcriptional regulation in LOAD. Hippocampal expression data for miRNAs, circRNAs, and RBPs were used to filter relevant relationships for AD. An integrated topological score was used to identify the highly connected hub gene, from which a brain core ceRNA subnetwork was generated. The Fragile X Messenger Ribonucleoprotein 1 (FMR1) coding for the RBP FMRP emerged as the prominent driver gene in this subnetwork. FMRP has been previously related to AD but not in a ceRNA network context. Also, the substantial number of neurodevelopmental genes in the ceRNA subnetwork and their related biological pathways strengthen that AD shares common pathological mechanisms with developmental conditions. Our results enhance the current knowledge about the convergent ceRNA regulatory pathways underlying AD and provide potential targets for identifying early biomarkers and developing novel therapeutic interventions.",
                "year": 2023,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Combining factor analysis with writing analytics for the formative assessment of written reflection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563221000558/pdfft?md5=18707731adf7ff4a81d2a095db4ba8d9&pid=1-s2.0-S0747563221000558-main.pdf",
                "title": "Combining factor analysis with writing analytics for the formative assessment of written reflection",
                "abstract": "The formative assessment of written reflection provides opportunities for students to improve their practice in an iterative manner using reflective writing. However, manual formative assessment of written reflection is time consuming and subjective. While progress has been made in deploying writing analytics tools to provide automated, formative feedback, few approaches to automated assessment are grounded in a validated, theory-based, formative assessment model. To address this, we propose a five-factor model of the Capability for Written Reflection (CWRef), grounded in the scholarship of reflective writing pedagogy. This paper uses Confirmatory Factor Analysis to validate the CWRef model by examining the relative contributions of textual features, derived from writing analytics, to each factor in the model, and their contributions to CWRef. The model was evaluated with two reflective writing corpora, showing which textual features, derived using Academic Writing Analytics and Linguistic Inquiry & Word Count, were significant indicators of factors in both corpora. In addition, it was found that the reflective writing context was an important factor influencing the validity of the CWRef model. Finally, we consider how this new analytical assessment model could enable improved tracking of progression in reflective writing, providing the basis for improved formative feedback.",
                "year": 2021,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Classification of mindfulness experiences from gamma-band effective connectivity: Application of machine-learning algorithms on resting, breathing, and body scan": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260724004395/pdfft?md5=fe72b706031d07640e5c14bd4d041993&pid=1-s2.0-S0169260724004395-main.pdf",
                "title": "Classification of mindfulness experiences from gamma-band effective connectivity: Application of machine-learning algorithms on resting, breathing, and body scan",
                "abstract": "Background and Objective\n\nPracticing mindfulness is a mental process toward interoceptive awareness, achieving stress reduction and emotion regulation through brain-function alteration. Literature has shown that electroencephalography (EEG)-derived connectivity possesses the potential to differentiate brain functions between mindfulness naïve and mindfulness experienced, where such quantitative differentiation could benefit telediagnosis for mental health. However, there is no prior guidance in model selection targeting on the mindfulness-experience prediction. Here we hypothesized that the EEG effective connectivity could reach a good prediction performance in mindfulness experiences with brain interpretability.",
                "year": 2024,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Finding neighbors of equal size in linear quadtrees and octrees in constant time": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/104996609290022U/pdfft?md5=8d376de8e214ab0c0ac4b042de4f6147&pid=1-s2.0-104996609290022U-main.pdf",
                "title": "Finding neighbors of equal size in linear quadtrees and octrees in constant time",
                "abstract": "Linear quadtrees and octrees are data structures which are of interest in image processing, computer graphics, and solid modeling. Their representation involves spatial addresses called location codes. For many of the operations on objects in linear quadtree and octree representation, finding neighbors is a basic operation. By considering the components of a location code, named dilated integers, a representation and associated addition and subtraction operations may be defined which are efficient in execution. The operations form the basis for the definition of location code addition and subtraction, with which finding neighbors of equal size is accomplished in constant time. The translation of pixels is a related operation. The results for linear quadtrees can be generalized without difficulty to linear octrees.",
                "year": 1992,
                "publisher": "CVGIP: Image Understanding"
            }
        }
    },
    "High accuracy NC milling simulation using composite adaptively sampled distance fields": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448512000255/pdfft?md5=a615eeac8e949e2d537d8172d936c66e&pid=1-s2.0-S0010448512000255-main.pdf",
                "title": "High accuracy NC milling simulation using composite adaptively sampled distance fields",
                "abstract": "We describe a new approach to shape representation called a composite adaptively sampled distance field (composite ADF) and describe its application to NC milling simulation. In a composite ADF each shape is represented by an analytic or procedural signed Euclidean distance field and the milled workpiece is given as the Boolean difference between distance fields representing the original workpiece volume and distance fields representing the volumes of the milling tool swept along the prescribed milling path. The computation of distance field of the swept volume of a milling tool is handled by an inverted trajectory approach where the problem is solved in tool coordinate frame instead of a world coordinate frame. An octree bounding volume hierarchy is used to sample the distance functions and provides spatial localization of geometric operations thereby dramatically increasing the speed of the system. The new method enables very fast simulation, especially of free-form surfaces, with accuracy better than 1 micron, and low memory requirements. We describe an implementation of 3 and 5-axis milling simulation.",
                "year": 2012,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Optimizing temporary work and overtime in the Time Cost Quality Trade-off Problem": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221720300345/pdfft?md5=9423647bf805d414cc1e34807c2fa80f&pid=1-s2.0-S0377221720300345-main.pdf",
                "title": "Optimizing temporary work and overtime in the Time Cost Quality Trade-off Problem",
                "abstract": "In spite of its significant contribution to project success, quality has been scarcely addressed in the literature on deterministic project scheduling problems. Although it is recognized that higher qualities are associated with longer processing times, no relationship between quality and resource consumption has been analytically derived to support this statement. As manufacturing projects can be accelerated using additional manpower such as overtime and temporary workers, we derive an analytical relationship between quality and manpower since overtime and overmanning negate quality. We also take into account productivity losses due to overmanning. Contrary to most previous contributions that focus on the project overall quality as an aggregation of quality levels attained at the individual activities, we impose each activity to reach a minimum quality threshold, which is consistent with project management practices. Consequently, we develop a mixed integer linear programming (MILP) to optimize temporary work and overtime so as to accelerate a project with quality and productivity considerations. The objective is to simultaneously determine for each activity the number of permanent, temporary and overtime workers over the processing periods in order to minimize the makespan, the total cost and the overall quality losses subject to individual quality constraints, precedence relationships, nonpreemption and availability of resources. Our approach is successfully applied on numerous instances based on a real project of a high speed locomotive as well as on other projects taken from the literature.",
                "year": 2020,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Mapping the condition of macadamia tree crops using multi-spectral UAV and WorldView-3 imagery": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S092427162030112X/pdfft?md5=382d5edc378ad46289107b4f74a1368d&pid=1-s2.0-S092427162030112X-main.pdf",
                "title": "Mapping the condition of macadamia tree crops using multi-spectral UAV and WorldView-3 imagery",
                "abstract": "Australia is one of the world’s largest producers of macadamia nuts. As macadamia trees can take up to 15 years to mature and produce maximum yield, it is important to optimize tree condition. Field based assessment of macadamia tree condition is time-consuming and often inconsistent. Using remotely sensed imagery may allow for faster, more extensive, and more consistent assessment of macadamia tree condition. To identify individual macadamia tree crowns, high spatial resolution imagery is required. Hence, the objective of this work was to develop and test an approach to map the condition of individual macadamia tree crowns using both multi-spectral Unmanned Aerial Vehicle (UAV) and WorldView-3 imagery for different macadamia varieties and three different sites located near Bundaberg, Australia. A random forest classifier, based on all available spectral bands and selected vegetation indices was used to predict five condition categories, ranging from excellent (category 1) to poor (category 5). Various combinations of the developed models were tested between the three sites and over time. The results showed that the multi-spectral WorldView-3 imagery produced the lowest out of bag (OOB) classification errors in most cases. However, for both the UAV and the WorldView-3 imagery, more than 98.5% of predicted macadamia condition categories were either correctly mapped or offset by a single category out of the five condition categories (excellent, good, moderate, fair and poor) for trees of the same variety and at one point in time. Multi-temporally, the WorldView-3 imagery performed better than the UAV data for predicting the condition of the same macadamia tree variety. Applying a model from one site to another site with the same macadamia tree variety produced OOB classification between 31.20 and 42.74%, but with >98.63% of trees predicted within a single condition category. Importantly, models trained based on one type of macadamia tree variety could not be successfully applied to a site with another variety. The developed classification models may be used as a decision and management support tool for the macadamia industry to inform management practices and improve on-demand irrigation, fertilization, and pest inspection at the individual tree level.",
                "year": 2020,
                "publisher": "ISPRS Journal of Photogrammetry and Remote Sensing"
            }
        }
    },
    "Application of GRNN and multivariate hybrid approach to predict and optimize WEDM responses for Ni-Ti shape memory alloy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494618303594/pdfft?md5=fa4514c20c3519509c006756d4bfeffb&pid=1-s2.0-S1568494618303594-main.pdf",
                "title": "Application of GRNN and multivariate hybrid approach to predict and optimize WEDM responses for Ni-Ti shape memory alloy",
                "abstract": "The current investigation aims at highlighting the application potential of a smart approximation tool, which is general regression neural network (GRNN) during machining of nickel-titanium (Ni-Ti) shape memory alloy using WEDM. A series of experiments were accomplished based on Taguchi’s orthogonal layout design. Pulse on time, discharge current, wire tension, wire speed and flushing pressure were considered as five distinct WEDM parameters, whereas arithmetic mean roughness, maximum peak to valley height, root mean square roughness, and micro-hardness were selected as the major responses to be investigated. The aforementioned WEDM responses were predicted with the help of the projected GRNN model and compared with the experimental results. The investigation was further extended to ascertain the optimum combination of input parameters using a hybrid approach. This was done by combining VIKOR method with the Fuzzy logic system. The prediction error of the GRNN model was noted as ±5% within the studied range of machining parameters. Finally, the adequacy of the multivariate VIKOR-Fuzzy approach was verified by performing confirmation test which exhibited improvement in WEDM responses.",
                "year": 2018,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Human inferior parietal cortex ‘programs’ the action class of grasping": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S138904179900011X/pdfft?md5=ee552bfc3777e3b5bd4c08383a74ff9f&pid=1-s2.0-S138904179900011X-main.pdf",
                "title": "Human inferior parietal cortex ‘programs’ the action class of grasping",
                "abstract": "If one writes with a pen grasped between the toes, or a pencil held in the mouth, the handwriting style may be of poor quality but can be identified as belonging to a particular individual. Like other actions, such as grasping or pointing, different body parts can be used to produce the movement. These findings, of reasonably consistent spatial and temporal productions by different effectors, have been used to argue for the concept of motor equivalence and the existence of motor programs abstracted from particular effectors. In this study subjects were required to perform an action (grasping a sweet) with different effectors (the mouth or the hand) while the brain was scanned. Activation of the inferior parietal lobe during real and imagined mouth grasping, and during real hand grasping actions was demonstrated. Primate neurophysiological research has implicated this region in a movement-planning role. Our results confirm the importance of the inferior parietal lobe in integrating converging multimodal sensory information for coding of general action patterns in humans.",
                "year": 2000,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Fractal dimension analysis of states of consciousness and unconsciousness using transcranial magnetic stimulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260718314263/pdfft?md5=a50a47795a5d864ed98776ba3688608b&pid=1-s2.0-S0169260718314263-main.pdf",
                "title": "Fractal dimension analysis of states of consciousness and unconsciousness using transcranial magnetic stimulation",
                "abstract": "Background and objective\n\nKnowing whether a subject is conscious or not is a current challenge with a deep potential clinical impact. Recent theoretical considerations suggest that consciousness is linked to the complexity of distributed interactions within the corticothalamic system. The fractal dimension (FD) is a quantitative parameter that has been extensively used to analyse the complexity of structural and functional patterns of the human brain. In this study we investigate FD to assess whether it can discriminate between consciousness and different states of unconsciousness in healthy individuals.",
                "year": 2019,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "A deep learning approach for real time prostate segmentation in freehand ultrasound guided biopsy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841518303499/pdfft?md5=adc144d524e45596833ac3f285de5942&pid=1-s2.0-S1361841518303499-main.pdf",
                "title": "A deep learning approach for real time prostate segmentation in freehand ultrasound guided biopsy",
                "abstract": "Targeted prostate biopsy, incorporating multi-parametric magnetic resonance imaging (mp-MRI) and its registration with ultrasound, is currently the state-of-the-art in prostate cancer diagnosis. The registration process in most targeted biopsy systems today relies heavily on accurate segmentation of ultrasound images. Automatic or semi-automatic segmentation is typically performed offline prior to the start of the biopsy procedure. In this paper, we present a deep neural network based real-time prostate segmentation technique during the biopsy procedure, hence paving the way for dynamic registration of mp-MRI and ultrasound data. In addition to using convolutional networks for extracting spatial features, the proposed approach employs recurrent networks to exploit the temporal information among a series of ultrasound images. One of the key contributions in the architecture is to use residual convolution in the recurrent networks to improve optimization. We also exploit recurrent connections within and across different layers of the deep networks to maximize the utilization of the temporal information. Furthermore, we perform dense and sparse sampling of the input ultrasound sequence to make the network robust to ultrasound artifacts. Our architecture is trained on 2,238 labeled transrectal ultrasound images, with an additional 637 and 1,017 unseen images used for validation and testing, respectively. We obtain a mean Dice similarity coefficient of 93%, a mean surface distance error of 1.10 mm and a mean Hausdorff distance error of 3.0 mm. A comparison of the reported results with those of a state-of-the-art technique indicates statistically significant improvement achieved by the proposed approach.",
                "year": 2018,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Towards effective teaching assistants: From intent-based chatbots to LLM-powered teaching assistants": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2949719124000499/pdfft?md5=7de3208cd4d6adf93098711dcb0bb283&pid=1-s2.0-S2949719124000499-main.pdf",
                "title": "Towards effective teaching assistants: From intent-based chatbots to LLM-powered teaching assistants",
                "abstract": "As chatbot technology undergoes a transformative phase in the era of artificial intelligence (AI), the integration of advanced AI models emerges as a focal point for reshaping conversational agents within the education sector. This paper explores the evolution of educational chatbot development, specifically focusing on building a teaching assistant for Data Mining and Text Analytics courses at the University of Leeds. The primary objective is to investigate and compare traditional intent-based chatbot approaches with the advanced retrieval-augmented generation (RAG) method, aiming to improve the efficiency and adaptability of teaching assistants in higher education. The study begins with the development of an Amazon Alexa teaching skill, assessing the efficacy of traditional chatbot development in higher education. To enrich the chatbot knowledge base, the research then employs an automated question–answer generation (QAG) approach using the QG Lumos Learning tool to extract contextually grounded question–answer datasets from course materials. Subsequently, the RAG-based system is proposed, leveraging LangChain with the OpenAI GPT-3.5 Turbo model. Findings highlight limitations in intent-based approaches, emphasising the need for more adaptive solutions. The proposed RAG-based teaching assistant demonstrates significant improvements in efficiently handling diverse queries, representing a paradigm shift in educational chatbot capabilities. These findings provide an in-depth understanding of the development phase, specifically illustrating the impact on chatbot performance by contrasting traditional methods with large language model-based approaches. The study contributes valuable perspectives on enhancing adaptability and effectiveness in AI-powered educational tools, providing essential considerations for future developments in the field.",
                "year": 2024,
                "publisher": "Natural Language Processing Journal"
            }
        }
    },
    "Test case generation, selection and coverage from natural language": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642319300024/pdfft?md5=0ddba173cb259c1aa0baa7738e1155dd&pid=1-s2.0-S0167642319300024-main.pdf",
                "title": "Test case generation, selection and coverage from natural language",
                "abstract": "In Model-based Testing (MBT), test cases are automatically generated from a formal model of the system. A disadvantage of MBT is that developers must deal with formal notations. This limitation is addressed in this paper, where use cases are used to model the system. In previous work, we have proposed an automatic strategy for generating test cases from use cases written in a Controlled Natural Language (CNL), which is an English textual notation with a well-defined grammar. Due to its precise syntax, it can be processed and translated into a formal representation for the purpose of automatic test case generation. This paper extends our previous work by proposing a state-based CNL for describing use case control flows enriched with state and data operations. We translate state-based use case descriptions into CSP processes from which test cases can be automatically generated. In addition, we show how a similar notation can be used to specify test selection via the definition of state-based test purposes, which are also translated into CSP processes. Test generation and selection are mechanised by refinement checking using the CSP tool FDR. Despite the fact that we work at a purely process algebraic level to define a test generation strategy, we are able to address model coverage criteria. Particularly, by using FDR, it is possible to have access to the underlying LTS models; we then implemented algorithms for covering events or transitions, possibly combined with selection using test purposes. We also discuss several ways of improving the efficiency of the test generation strategy. As far as we are aware, this integration between an algebraic approach to test case generation with an operational approach for coverage criteria is an original and promising insight. All steps of the strategy are integrated into a tool that provides a GUI for authoring use cases and test purposes described in the proposed CNL, so the formal CSP notation is completely hidden from the test designer. We illustrate our tool and techniques with a running example and a more elaborate case study taken from an industrial setting.",
                "year": 2019,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Modeling the growth of dendritic electroless silver colonies using hexagonal cellular automata": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050919310464/pdfft?md5=5ecdc2059390f91f5d7da89d7b35f170&pid=1-s2.0-S1877050919310464-main.pdf",
                "title": "Modeling the growth of dendritic electroless silver colonies using hexagonal cellular automata",
                "abstract": "In this paper, we present results of in silico simulation of dendritic electroless silver colony growth model on a hexagonal cellular automata lattice. A cellular automaton, based on a NetLogo framework, was used to replicate the behavior of a colony. Experiments showed that a Hex Cell Aggregation model from NetLogo library, which we used as a base, can demonstrate the patterns similar to those in experimental data. However, several features of the colony growth cannot be explained within the existing model and require its modifications.",
                "year": 2019,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Illumination-robust variational optical flow using cross-correlation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1077314210001530/pdfft?md5=834eeac3f3e131662e650b8cff516f32&pid=1-s2.0-S1077314210001530-main.pdf",
                "title": "Illumination-robust variational optical flow using cross-correlation",
                "abstract": "We address the problem of variational optical flow for video processing applications that need fast operation and robustness to drastic variations in illumination. Recently, a solution [1] has been proposed based on the photometric invariants of the dichromatic reflection model [2]. However, this solution is only applicable to colour videos with brightness variations. Greyscale videos, or colour videos with colour illumination changes cannot be adequately handled. We propose two illumination-robust variational methods based on cross-correlation that are applicable to colour and grey-level sequences and robust to brightness and colour illumination changes. First, we present a general implicit nonlinear scheme that assumes no particular analytical form of energy functional and can accommodate different image components and data metrics, including cross-correlation. We test the nonlinear scheme on standard synthetic data with artificial brightness and colour effects added and conclude that cross-correlation is robust to both kinds of illumination changes. Then we derive a fast linearised numerical scheme for cross-correlation based variational optical flow. We test the linearised algorithm on challenging data and compare it to a number of state-of-the-art variational flow methods.",
                "year": 2010,
                "publisher": "Computer Vision and Image Understanding"
            }
        }
    },
    "Managing organizational identity in the e-commerce industry: An ambidexterity perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720613000426/pdfft?md5=5ff65b7f16b19c58bdfd0a6876462349&pid=1-s2.0-S0378720613000426-main.pdf",
                "title": "Managing organizational identity in the e-commerce industry: An ambidexterity perspective",
                "abstract": "In the e-commerce industry, organizations should maintain an enduring and stable organizational identity to gain long-term success while also adapting quickly to the increasingly volatile environment in order to ensure profitability and survival. These ongoing paradoxical challenges concerning the management of organizational identity have been left unaddressed in existing literature. Drawing upon the literature related to organizational ambidexterity and organizational identity, this paper proposes two theoretical frameworks for systematically examining the ways in which organizations, especially those in the e-commerce industry, should manage their organizational identities by leveraging four balancing forms of organizational ambidexterity. Based on these two models, we comprehensively analyzed the case of D.com, which is China's most successful online ticket vendor. Our study not only contributes to the field of knowledge surrounding organizational identity and ambidexterity but also provides a detailed means for practitioners to manage organizational identities at both the strategic and operational levels within the e-commerce industry.",
                "year": 2013,
                "publisher": "Information & Management"
            }
        }
    },
    "The evolution of information resource management": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0378720689900037/pdfft?md5=42928e81a6c859bf8c2241058dc769e9&pid=1-s2.0-0378720689900037-main.pdf",
                "title": "The evolution of information resource management",
                "abstract": "From the beginning of its use, the term information resource management (IRM) has had a wide range of meanings. This is due, in part, to its independent development in three different sectors of the information processing community: database management, records management and data processing management. This paper traces the origins and evolution of the concept of information resource management through a review of the IRM literature over the past fifteen years. The methodology involved examining definitions of IRM along the following dimensions: disciplinary perspective, management scope, societal sector, vocabulary and goals. It was found that while the IRM concept evolved in three different arenas with little interaction occuring among them, the current view of IRM represents a convergence of perspectives. IRM, today, has three goals: to maintain a global view of corporate data, to position the chief information officer at a high level in the corporate hierarchy, and to integrate both information and the information technologies. The future success of IRM will depend upon its ability to incorporate end users into the information management framework.",
                "year": 1989,
                "publisher": "Information & Management"
            }
        }
    },
    "Design and implementation of a video on-demand system": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169755298001664/pdfft?md5=0af2b6dde83a82b0744ed7cdda1ce471&pid=1-s2.0-S0169755298001664-main.pdf",
                "title": "Design and implementation of a video on-demand system",
                "abstract": "Recent technological advances made multimedia on-demand servers feasible. Two challenging tasks in such systems are satisfying the real-time requirement for continuous delivery of objects at specified bandwidths and efficiently servicing multiple clients simultaneously. Our project is aimed at prototype development of such a large scale server. This paper jointly addresses the issues of load balancing, responsiveness, streaming capacity and cost effectiveness of high-performance storage servers and delivery systems for data streaming applications such as video-on-demand or news-on-demand. We propose a relatively simple, flexible and robust video-server architecture.",
                "year": 1998,
                "publisher": "Computer Networks and ISDN Systems"
            }
        }
    },
    "Validation of the Borg CR10 Scale for the evaluation of shoulder perceived fatigue during work-related tasks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687023002387/pdfft?md5=324a725b17a4048cae43bc13ae567209&pid=1-s2.0-S0003687023002387-main.pdf",
                "title": "Validation of the Borg CR10 Scale for the evaluation of shoulder perceived fatigue during work-related tasks",
                "abstract": "Work-related upper extremity disorders (WRUEDs) are highly prevalent and costly. Development of fatigue is thought to be one of the causes of WRUEDs. Perceived fatigue can be assessed with the Borg CR Scale® (Borg CR10). The objective was to validate the Borg CR10 for the evaluation of shoulder perceived fatigue during lifting tasks. Seventy adults in working age performed three rhythmic lifting tasks with two loads (15% and 30% of maximal voluntary contraction). Using generalized repeated-measures ANOVA (Generalized Estimating Equations), statistically significant Task and Load effects (p < 0.001), as well as Task × Load interaction effects (p < 0.0001) were observed on Borg CR10, without any influence of sex. The Borg CR10 is a valid tool to assess shoulder perceived fatigue as it can discriminate between tasks of different difficulty levels in term of complexity, height, and resistance, regardless of sex.",
                "year": 2024,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Interpreting code – Adapting the methodology to analyze the normative contents of law for the analysis of technology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0267364915000096/pdfft?md5=2f5acca1dde1246b3a931703f441644e&pid=1-s2.0-S0267364915000096-main.pdf",
                "title": "Interpreting code – Adapting the methodology to analyze the normative contents of law for the analysis of technology",
                "abstract": "In 1999, Lawrence Lessig published a since famous analogy, framed in the catchphrase “Code is Law”. By this he meant the normative dimensions that technology and legal norms both inherited. But how can the normative content of code be unveiled? And if code really is law, can they not both be treated and interpreted in just the same way? Legal science in civil law systems has come up with a profound analytic methodology to interpret legal norms hermeneutically and thus to find the normative meaning of law. Scholars working on the relation between technology and human behavior at this point struggle with the lack of a shared language, making it difficult to articulate their findings and opinions on normative content of technology. They can benefit from this discursive basis in an ensuing discourse by strengthening their results of code interpretation by applying the tools provided in this paper to rationalize their argumentation and make discrepancies with the interpretations of others visible. Through the proposed methodology of code analysis they can also find relevant questions for further empirical investigation.",
                "year": 2015,
                "publisher": "Computer Law & Security Review"
            }
        }
    },
    "Algorithms for computing strategies in two-player simultaneous move games": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0004370216300285/pdfft?md5=52b74a6aa501a3f37a039391b0fd98a7&pid=1-s2.0-S0004370216300285-main.pdf",
                "title": "Algorithms for computing strategies in two-player simultaneous move games",
                "abstract": "Simultaneous move games model discrete, multistage interactions where at each stage players simultaneously choose their actions. At each stage, a player does not know what action the other player will take, but otherwise knows the full state of the game. This formalism has been used to express games in general game playing and can also model many discrete approximations of real-world scenarios. In this paper, we describe both novel and existing algorithms that compute strategies for the class of two-player zero-sum simultaneous move games. The algorithms include exact backward induction methods with efficient pruning, as well as Monte Carlo sampling algorithms. We evaluate the algorithms in two different settings: the offline case, where computational resources are abundant and closely approximating the optimal strategy is a priority, and the online search case, where computational resources are limited and acting quickly is necessary. We perform a thorough experimental evaluation on six substantially different games for both settings. For the exact algorithms, the results show that our pruning techniques for backward induction dramatically improve the computation time required by the previous exact algorithms. For the sampling algorithms, the results provide unique insights into their performance and identify favorable settings and domains for different sampling algorithms.",
                "year": 2016,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Using instability to reconfigure smart structures in a spring-mass model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0888327016305039/pdfft?md5=070c75057162662ca09c3c5fab826c37&pid=1-s2.0-S0888327016305039-main.pdf",
                "title": "Using instability to reconfigure smart structures in a spring-mass model",
                "abstract": "Multistable phenomenon have long been used in mechanism design. In this paper a subset of unstable configurations of a smart structure model will be used to develop energy-efficient schemes to reconfigure the structure. This new concept for reconfiguration uses heteroclinic connections to transition the structure between different unstable equal-energy states. In an ideal structure model zero net energy input is required for the reconfiguration, compared to transitions between stable equilibria across a potential barrier. A simple smart structure model is firstly used to identify sets of equal-energy unstable configurations using dynamical systems theory. Dissipation is then added to be more representative of a practical structure. A range of strategies are then used to reconfigure the smart structure using heteroclinic connections with different approaches to handle dissipation.",
                "year": 2017,
                "publisher": "Mechanical Systems and Signal Processing"
            }
        }
    },
    "A Responsibility-based Transport Control for Named Data Networking": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X19305230/pdfft?md5=f2417b4a5e6c339ac66b0ee6eaf21ac2&pid=1-s2.0-S0167739X19305230-main.pdf",
                "title": "A Responsibility-based Transport Control for Named Data Networking",
                "abstract": "Transport Control in Named Data Networking (NDN) architecture is a challenging task. The lack of end-to-end communications in this architecture makes traditional, timeout-driven transport control schemes inefficient and wasteful. Hop-by-hop transport control is an alternative solution to tackle this problem that because of the stateful forwarding plan of NDN can be applied more easily than the IP networks. Most existing solutions in this direction assume known link bandwidths and Data packet sizes or require a loop-free multipath forwarding strategy to work well, but these assumptions do not always hold true and there exist no a loop-free multipath forwarding strategy among the existing forwarding strategies for NDN. In this paper, a Responsibility-based Transport Control (RTC) protocol for NDN is proposed. This protocol does not make strong assumptions about the network and avoids looping paths by applying a window-based rate control mechanism and a capacity-aware, multipath forwarding strategy in each face. In RTC, routers maintain a congestion window in each face and decide on accepting or refusing to take responsibility for forwarding of a newly received Interest packet through exchanging three new control packets. These control packets provide reliable information for managing the congestion windows and capacity-aware traffic splitting in routers. They also enable diverse deployment scenarios for NDN such as IP-overly and wireless links. The RTC is implemented in ndnSIM and its capability in managing congestion, achieving high throughput and providing flow fairness are demonstrated through extensive simulations.",
                "year": 2020,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "It's a circular argument: Examining how a novel configuration impacts information flow in submarine control rooms": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687021001812/pdfft?md5=6353103509fc0ced5c90fe67e3f5db60&pid=1-s2.0-S0003687021001812-main.pdf",
                "title": "It's a circular argument: Examining how a novel configuration impacts information flow in submarine control rooms",
                "abstract": "The continuing advancement of technology means that sociotechnical systems are primed for revolutionary changes to ways of working that can increase capability. It is critical to consider the unintended impact technology can have on human operators particularly regarding information flow and interactions within teams. Previous research revealed that the co-location of operator's dependent on each other for task relevant information can optimise information flow previously constrained by engineering considerations. The current work compared a novel circular configuration to that of a contemporary submarine control room. In the circular configuration, consoles faced inwards, permitting eye contact between operators, and three large screen displays were introduced to provide all operators with the same information. Ten teams participated in low and high demand dived tracking scenarios in a simulated submarine control room. All communications between operators were recorded in order to generate social, information, and task networks. These were statistically compared to networks generated from a baseline study of contemporary operation. Overall, the volume of verbal communications significantly reduced, information exchange was more structured, and the volume of tasks completed by operators significantly increased when operating in an inward facing circle configuration. The current work provides support for a data driven evidence-based approach to design that is information centric but endorsed by the end user to optimise performance and increase productivity. Implications of the work and future research ideas are discussed.",
                "year": 2021,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Automating fault tolerance in high-performance computational biological jobs using multi-agent approaches": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482514000365/pdfft?md5=18c4cc914b692972f089a5096364e0b1&pid=1-s2.0-S0010482514000365-main.pdf",
                "title": "Automating fault tolerance in high-performance computational biological jobs using multi-agent approaches",
                "abstract": "Background: Large-scale biological jobs on high-performance computing systems require manual intervention if one or more computing cores on which they execute fail. This places not only a cost on the maintenance of the job, but also a cost on the time taken for reinstating the job and the risk of losing data and execution accomplished by the job before it failed. Approaches which can proactively detect computing core failures and take action to relocate the computing core׳s job onto reliable cores can make a significant step towards automating fault tolerance.\n\nMethod: This paper describes an experimental investigation into the use of multi-agent approaches for fault tolerance. Two approaches are studied, the first at the job level and the second at the core level. The approaches are investigated for single core failure scenarios that can occur in the execution of parallel reduction algorithms on computer clusters. A third approach is proposed that incorporates multi-agent technology both at the job and core level. Experiments are pursued in the context of genome searching, a popular computational biology application.\n\nResult: The key conclusion is that the approaches proposed are feasible for automating fault tolerance in high-performance computing systems with minimal human intervention. In a typical experiment in which the fault tolerance is studied, centralised and decentralised checkpointing approaches on an average add 90% to the actual time for executing the job. On the other hand, in the same experiment the multi-agent approaches add only 10% to the overall execution time.",
                "year": 2014,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Automatic transfer function design for medical visualization using visibility distributions and projective color mapping": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0895611113001419/pdfft?md5=405b4b1d4eef8d13bdac390304130ba7&pid=1-s2.0-S0895611113001419-main.pdf",
                "title": "Automatic transfer function design for medical visualization using visibility distributions and projective color mapping",
                "abstract": "Transfer functions play a key role in volume rendering of medical data, but transfer function manipulation is unintuitive and can be time-consuming; achieving an optimal visualization of patient anatomy or pathology is difficult. To overcome this problem, we present a system for automatic transfer function design based on visibility distribution and projective color mapping. Instead of assigning opacity directly based on voxel intensity and gradient magnitude, the opacity transfer function is automatically derived by matching the observed visibility distribution to a target visibility distribution. An automatic color assignment scheme based on projective mapping is proposed to assign colors that allow for the visual discrimination of different structures, while also reflecting the degree of similarity between them. When our method was tested on several medical volumetric datasets, the key structures within the volume were clearly visualized with minimal user intervention.",
                "year": 2013,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "A novel all-neighbor fuzzy association approach for multitarget tracking in a cluttered environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016516841100082X/pdfft?md5=53dd215b274df699a99de04073d3d0c5&pid=1-s2.0-S016516841100082X-main.pdf",
                "title": "A novel all-neighbor fuzzy association approach for multitarget tracking in a cluttered environment",
                "abstract": "This paper proposes a novel all-neighbor fuzzy association approach for multitarget tracking in a cluttered environment. It performs data association with a little prior knowledge and updates the predicted target state estimate using a fuzzy weighted sum of innovations. Unlike the joint probabilistic data association filter, in which the similarity measures are determined in terms of the conditional probability for all feasible data association hypothesis, the proposed fuzzy association approach determines the similarity measures between measurements and tracks in terms of possibility weights based on a partition matrix. The possibility weights are determined according to the fuzzy clustering algorithm. The proposed approach is able to perform all-neighbor association with a lower computational complexity in the expense of a little lower performance compared to the standard joint probabilistic data association filter. Computer simulation shows the feasibility and the efficiency of the proposed all-neighbor fuzzy association approach.",
                "year": 2011,
                "publisher": "Signal Processing"
            }
        }
    },
    "Finding groups in data: Cluster analysis with ants": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494608000331/pdfft?md5=9c1b2efeac3761d48df9812c1d00b303&pid=1-s2.0-S1568494608000331-main.pdf",
                "title": "Finding groups in data: Cluster analysis with ants",
                "abstract": "We present in this paper a modification of Lumer and Faieta’s algorithm for data clustering. This approach mimics the clustering behavior observed in real ant colonies. This algorithm discovers automatically clusters in numerical data without prior knowledge of possible number of clusters. In this paper we focus on ant-based clustering algorithms, a particular kind of a swarm intelligent system, and on the effects on the final clustering by using during the classification different metrics of dissimilarity: Euclidean, Cosine, and Gower measures. Clustering with swarm-based algorithms is emerging as an alternative to more conventional clustering methods, such as e.g. k-means, etc. Among the many bio-inspired techniques, ant clustering algorithms have received special attention, especially because they still require much investigation to improve performance, stability and other key features that would make such algorithms mature tools for data mining.\n\nAs a case study, this paper focus on the behavior of clustering procedures in those new approaches. The proposed algorithm and its modifications are evaluated in a number of well-known benchmark datasets. Empirical results clearly show that ant-based clustering algorithms performs well when compared to another techniques.",
                "year": 2009,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "A systematic review on the status and progress of homomorphic encryption technologies": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A systematic review on the status and progress of homomorphic encryption technologies",
                "abstract": "With the emergence of big data and the continued growth in cloud computing applications, serious security and privacy concerns emerged. Consequently, several researchers and cybersecurity experts have embarked on a quest to extend data encryption to big data systems and cloud computing applications. As most cloud users turn to using public cloud services, confidentiality becomes and even more complicated issue. Cloud clients storing their data on a public cloud always seek solutions to confidentiality problem. Homomorphic encryption emerged as a possible solution where client’s data is encrypted on the cloud in a way that allows some search and manipulation operations without proper decryption. In this paper, we present a systematic review of research paper published in the field of homomorphic encryption. This paper uses PRISMA checklist alongside some items of Cochrane’s Quality Assessment to review studies retrieved from various resources. It was highly noticeable in the reviewed papers that security in big data and cloud computing has received most attention. Most papers suggested the use of homomorphic encryption although the thematic analysis has identified other potential concerns. Regarding the quality of the articles, 38% of the articles failed to meet three checklist items, including explicit statement of research objectives, procedure recognition and sources of funding used in the study. The review also presented compendium textual analysis of different homomorphic encryption algorithms, application areas, and areas of future developments. Results of the evaluation through PRISMA and the Cochrane tool showed that a majority of research articles discussed the potential use and application of Homomorphic Encryption as a solution to the growing demands of big data and absence of security and privacy mechanisms therein. This was evident from 26 of the total 59 articles that met the inclusion criteria. The term Homomorphic Encryption appeared 1802 times in the word cloud derived from the selected articles, which speaks of its potential to ensure security and privacy, while also preserving the CIA triad in the context of big data and cloud computing.",
                "year": 2019,
                "publisher": "Journal of Information Security and Applications"
            }
        }
    },
    "Lions and contamination: Monotone clearings": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925772122001043/pdfft?md5=3b228bd72a9b43f0182306629b6b3b98&pid=1-s2.0-S0925772122001043-main.pdf",
                "title": "Lions and contamination: Monotone clearings",
                "abstract": "We consider a special variant of a pursuit-evasion game called lions and contamination. In a graph whose vertices are originally contaminated, a set of lions walks around the graph and each lion clears the contamination from every vertex it visits. The contamination, however, simultaneously spreads to any adjacent vertex not occupied by a lion. We study the relationship between different types of clearings of graphs, such as clearings which do not allow recontamination, clearings where at most one lion moves at each time step and clearings where lions are forbidden to be stacked on the same vertex. We answer several questions raised by Adams et al. [1].",
                "year": 2023,
                "publisher": "Computational Geometry"
            }
        }
    },
    "Fuzzy expert system approach for coronary artery disease screening using clinical parameters": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705112001840/pdfft?md5=aa1f21d5755766f9ea46f2ec1c06611a&pid=1-s2.0-S0950705112001840-main.pdf",
                "title": "Fuzzy expert system approach for coronary artery disease screening using clinical parameters",
                "abstract": "Coronary artery disease (CAD) affects millions of people all over the world including a major portion in India every year. Although much progress has been done in medical science, but the early detection of this disease is still a challenge for prevention. The objective of this paper is to describe developing of a screening expert system that will help to detect CAD at an early stage. Rules were formulated from the doctors and fuzzy expert system approach was taken to cope with uncertainty present in medical domain. This work describes the risk factors responsible for CAD, knowledge acquisition and knowledge representation techniques, method of rule organisation, fuzzification of clinical parameters and defuzzification of fuzzy output to crisp value. The system implementation is done using object oriented analysis and design. The proposed methodology is developed to assist the medical practitioners in predicting the patient’s risk status of CAD from rules provided by medical experts. The present paper focuses on rule organisation using the concept of modules, meta-rule base, rule address storage in tree representation and rule consistency checking for efficient search of large number of rules in rule base. The developed system leads to 95.85% sensitivity and 83.33% specificity in CAD risk computation.",
                "year": 2012,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Technology affordance, national polycontextuality, and customer loyalty in the cross-border e-commerce platform: A comparative study between China and South Korea": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585324000030/pdfft?md5=59f067aaba9861e9079e405362007c12&pid=1-s2.0-S0736585324000030-main.pdf",
                "title": "Technology affordance, national polycontextuality, and customer loyalty in the cross-border e-commerce platform: A comparative study between China and South Korea",
                "abstract": "Given the intense competition in the cross-border e-commerce (CBEC) market, researchers have been exploring information technology (IT) functionalities to promote perceived value and customer loyalty. Building upon technology affordance theory, we develop a two-dimensional taxonomy of CBEC affordance with four archetypes: cross-culture information guidance, cross-culture shopping guidance, triggered attending, and social comparison. We refer to the polycontextual lens and propose a research model to explain how CBEC affordances and national polycontextuality jointly influence perceived value and customer loyalty. We empirically validate the conceptual model with two samples: 232 customers from China and 215 customers from South Korea. Our findings suggest that CBEC affordances significantly strengthen consumer loyalty by enhancing their perceived value. Specifically, cross-culture information guidance and cross-culture shopping guidance have more substantial effects on Chinese customers’ perceived value than South Korean customers’. Conversely, triggered attending and social comparison exert stronger influences on perceived value for South Korean customers.",
                "year": 2024,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "Using disparity functionals for stereo correspondence and surface reconstruction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0734189X87802037/pdfft?md5=15416419813117fef5296e182737b34a&pid=1-s2.0-S0734189X87802037-main.pdf",
                "title": "Using disparity functionals for stereo correspondence and surface reconstruction",
                "abstract": "In this paper, we investigate stereo constraints that derive from an analytic model of surface depth. Analyticity is the mathematical tool by which we model smoothness of object surfaces, and therefore the disparity field, as piecewise analytic functions of visual direction. Our model of analytic coherence mathematically formulates the principle of coherence stated by Prazdny (Biol. Cybern.52, 1985, 93–99). In using this property, we follow the work in stereo of Koenderink and van Doorn (Biol. Cybern.21, 1976, 29–35) and our own work on motion. We formulate stereo as a single stage process in which potential feature point or contour matches interact to provide support for local estimates of a polynomial model of disparity (the disparity functional), not just estimates of disparity at isolated points. This refines the notion of local support defined by Marr and Poggio (Proc. R. Soc. London B204, 1979, 301–328). We present an algorithm that integrates the disparity functional with multiresolution matching of zero-crossings to derive depth to surface patches. The analyticity of the disparity field is thereby exploited early in the matching process, and yields surface reconstruction as a direct byproduct of correspondence.",
                "year": 1987,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Academic agility in digital innovation research: The case of mobile ICT publications within information systems 2000–2014": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Academic agility in digital innovation research: The case of mobile ICT publications within information systems 2000–2014",
                "abstract": "The Information Systems (IS) field has never been more relevant as digital innovations are emerging at a rapid pace fuelled by recombinant innovations based on digital infrastructures, advanced middle-ware layers, and mobile and ubiquitous technologies. This paper argues, based on a bibliometric study of the representation of a mobile ICT discourse within the AIS Senior Scholars’ ‘basket’ of eight IS journals over the past 15 years, that the field needs to become much more academically agile. The study showed that a mere 3.2% of all papers published during this period had any relationship to one of the key technological phenomena in the late 20th and early 21st centuries. The paper formulates the hypothesis that the relative shift in impact between European- and US-based journals within the ‘basket of 8’ could have been influenced by editorial strategising to further encourage academic agility exploring new horizons rather than emphasis on further exploitation of existing ground. The paper, further argues that the IS field seems to more readily engage in a debate of phenomena involving centralised and organisationally-bound technological innovations whereas distributed, decentralised and infrastructural discourses find it much more difficult to gain a foothold. So, whilst the IS field may still be stuck in the mainframe age, it needs to move beyond in order to fully engage with the world we live in.",
                "year": 2015,
                "publisher": "The Journal of Strategic Information Systems"
            }
        }
    },
    "Automatic topics discovery from hyperlinked documents": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457303000396/pdfft?md5=3a8c66f8918b39f766a4253dfe10f8be&pid=1-s2.0-S0306457303000396-main.pdf",
                "title": "Automatic topics discovery from hyperlinked documents",
                "abstract": "Topic discovery is an important means for marketing, e-Business and social science studies. As well, it can be applied to various purposes, such as identifying a group with certain properties and observing the emergence and diminishment of a certain cyber community. Previous topic discovery work (J.M. Kleinberg, Proceedings of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms, San Francisco, California, p. 668) requires manual judgment of usefulness of outcomes and is thus incapable of handling the explosive growth of the Internet. In this paper, we propose the Automatic Topic Discovery (ATD) method, which combines a method of base set construction, a clustering algorithm and an iterative principal eigenvector computation method to discover the topics relevant to a given query without using manual examination. Given a query, ATD returns with topics associated with the query and top representative pages for each topic. Our experiments show that the ATD method performs better than the traditional eigenvector method in terms of computation time and topic discovery quality.",
                "year": 2004,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Network-based analysis between SARS-CoV-2 receptor ACE2 and common host factors in COVID-19 and asthma: Potential mechanistic insights": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809423009357/pdfft?md5=aa726719f2f072fda19f3f2b6783b2d9&pid=1-s2.0-S1746809423009357-main.pdf",
                "title": "Network-based analysis between SARS-CoV-2 receptor ACE2 and common host factors in COVID-19 and asthma: Potential mechanistic insights",
                "abstract": "ACE2 as a functional receptor for SARS coronavirus plays an important role in COVID-19 infection of the host. Thus, we aimed to explore interaction networks between ACE2 and common host factors in COVID-19/Asthma comorbidity. We identified 1,191 protein targets closely related to ACE2, and integrated the GEO database and multiple public databases to identify 305 host factors related to ACE2 between COVID-19/Asthma comorbidity. Further enrichment analyses showed that metabolic processes, Th1 and Th2 cell differentiation and PPAR signaling pathways had vital significance in the comorbidity of asthma and COIVD-19. HRAS, IFNG, CAT, CDH1, FASN, ACLY, CCL5, VCAM1, SCD and HMGCR were the key host factors related to ACE2. Tissue-specific enrichment and correlation analysis with ACE2 showed that SCD, CAT and FASN were highly expressed in the lung; and these 10 molecules were closely related to ACE2 and specifically expressed in multiple tissues. We also identified a series of drugs, such as estradiol, tetrachlorodibenzodioxin, resveratrol, cyclosporine, perfluorooctanoic acid and so on. Finally, the expression levels and diagnostic performance of HRAS and SCD showed statistical significance in external databases. This study explored the interaction networks of ACE2-related host factors in COVID-19 and asthma and identified several potential drugs for COVID-19/Asthma comorbidity. Although our research needs further verification, it still informs the mechanisms and treatment of COVID-19/Asthma comorbidity.",
                "year": 2024,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Wall shear stress and near-wall convective transport: Comparisons with vascular remodelling in a peripheral graft anastomosis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0021999110001488/pdfft?md5=4fd6a1d83a3d5ec4391cccc6c4f21789&pid=1-s2.0-S0021999110001488-main.pdf",
                "title": "Wall shear stress and near-wall convective transport: Comparisons with vascular remodelling in a peripheral graft anastomosis",
                "abstract": "Fluid dynamic properties of blood flow are implicated in cardiovascular diseases. The interaction between the blood flow and the wall occurs through the direct transmission of forces, and through the dominating influence of the flow on convective transport processes. Controlled, in vitro testing in simple geometric configurations has provided much data on the cellular-level responses of the vascular walls to flow, but a complete, mechanistic explanation of the pathogenic process is lacking. In the interim, mapping the association between local haemodynamics and the vascular response is important to improve understanding of the disease process and may be of use for prognosis. Moreover, establishing the haemodynamic environment in the regions of disease provides data on flow conditions to guide investigations of cellular-level responses.\n\nThis work describes techniques to facilitate comparison between the temporal alteration in the geometry of the vascular conduit, as determined by in vivo imaging, with local flow parameters. Procedures to reconstruct virtual models from images by means of a partition-of-unity implicit function formulation, and to align virtual models of follow-up scans to a common coordinate system, are outlined. A simple Taylor series expansion of the Lagrangian dynamics of the near-wall flow is shown to provide both a physical meaning to the directional components of the flow, as well as demonstrating the relation between near-wall convection in the wall normal direction and spatial gradients of the wall shear stress.\n\nA series of post-operative follow-up MRI scans of two patient cases with bypass grafts in the peripheral vasculature are presented. These are used to assess how local haemodynamic parameters relate to vascular remodelling at the location of the distal end-to-side anastomosis, i.e. where the graft rejoins the host artery. Results indicate that regions of both low wall shear stress and convective transport towards the wall tend to be more associated with regions of inward remodelling. A strong point-wise correlation was not found to exist however between local changes in wall location and either quantity.",
                "year": 2010,
                "publisher": "Journal of Computational Physics"
            }
        }
    },
    "Neural network modelling of soft tissue deformation for surgical simulation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Neural network modelling of soft tissue deformation for surgical simulation",
                "abstract": "This paper presents a new neural network methodology for modelling of soft tissue deformation for surgical simulation. The proposed methodology formulates soft tissue deformation and its dynamics as the neural propagation and dynamics of cellular neural networks for real-time, realistic, and stable simulation of soft tissue deformation. It develops two cellular neural network models; based on the bioelectric propagation of biological tissues and principles of continuum mechanics, one cellular neural network model is developed for propagation and distribution of mechanical load in soft tissues; based on non-rigid mechanics of motion in continuum mechanics, the other cellular neural network model is developed for governing model dynamics of soft tissue deformation. The proposed methodology not only has computational advantage due to the collective and simultaneous activities of neural cells to satisfy the real-time computational requirement of surgical simulation, but also it achieves physical realism of soft tissue deformation according to the bioelectric propagation manner of mechanical load via dynamic neural activities. Furthermore, the proposed methodology also provides stable model dynamics for soft tissue deformation via the nonlinear property of the cellular neural network. Interactive soft tissue deformation with haptic feedback is achieved via a haptic device. Simulations and experimental results show the proposed methodology exhibits the nonlinear force-displacement relationship and associated nonlinear deformation of soft tissues. Furthermore, not only isotropic and homogeneous but also anisotropic and heterogeneous materials can be modelled via a simple modification of electrical conductivity values of mass points.",
                "year": 2019,
                "publisher": "Artificial Intelligence in Medicine"
            }
        }
    },
    "The visual landscape of rock art in Qeydu Valley in Teymare in the Central Iranian Plateau": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2212054824000432/pdfft?md5=6bf49451f27334395fa6cc41f9426a53&pid=1-s2.0-S2212054824000432-main.pdf",
                "title": "The visual landscape of rock art in Qeydu Valley in Teymare in the Central Iranian Plateau",
                "abstract": "Qeydu Valley is a part of the Teymare region, which contains one the largest clusters of rock art in the central Iranian Plateau. Analyzing about 495 panels in more than 100 sites, this paper is intended to provide an insight into the landscape of rock art in Qeydu Valley. The paper explores the intersection of rock art positioning, movement patterns, and visual landscape in the study area. Total and cumulative viewshed, cost distance, and least-cost path analysis have been used to reconstruct the visibility dimension of the rock art locations in movement across the region. The total and cumulative viewshed analysis suggest that the panels showing a hunting theme benefit from a higher degree of visibility compared to the other petroglyphs.",
                "year": 2024,
                "publisher": "Digital Applications in Archaeology and Cultural Heritage"
            }
        }
    },
    "Human upper limb 1dynamics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0921889089900079/pdfft?md5=ab7dce31514ab6b9da6566a1a6779960&pid=1-s2.0-0921889089900079-main.pdf",
                "title": "Human upper limb 1dynamics",
                "abstract": "A dynamical model is developed for studying the motion and control of a pair of human upper limbs, which are coupled mechanically through an articulated torso. The model is particularly applicable to the design and development of arm powered vehicles as well as the simulation of any one- or two-arm task. An equivalent open link chain model of the relevant upper body skeletal members is presented. Kane's equations are then used to derive the three dimensional dynamical equations of motion for the system, subject to constraint forces and torques on the end effectors (hands).",
                "year": 1989,
                "publisher": "Robotics and Autonomous Systems"
            }
        }
    },
    "Efficient privacy-preserving Gaussian process via secure multi-party computation": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Efficient privacy-preserving Gaussian process via secure multi-party computation",
                "abstract": "Gaussian processes (GPs), known for their flexibility as non-parametric models, have been widely used in practice involving sensitive data (e.g., healthcare, finance) from multiple sources. With the challenge of data isolation and the need for high-performance models, how to jointly develop privacy-preserving GP for multiple parties has emerged as a crucial topic. In this paper, we propose a new privacy-preserving GP algorithm, namely PP-GP, which employs secret sharing (SS) techniques. Specifically, we introduce a new SS-based exponentiation operation (PP-Exp) through confusion correction and an SS-based matrix inversion operation (PP-MI) based on Cholesky decomposition. However, the advantages of the GP come with a great computational burden and space cost. To further enhance the efficiency, we propose an efficient split learning framework for privacy-preserving GP, named Split-GP, which demonstrably improves performance on large-scale data. We leave the private data-related and SMPC-hostile computations (i.e., random features) on data holders, and delegate the rest of SMPC-friendly computations (i.e., low-rank approximation, model construction, and prediction) to semi-honest servers. The resulting algorithm significantly reduces computational and communication costs compared to PP-GP, making it well-suited for application to large-scale datasets. We provide a theoretical analysis in terms of the correctness and security of the proposed SS-based operations. Extensive experiments show that our methods can achieve competitive performance and efficiency under the premise of preserving privacy.",
                "year": 2024,
                "publisher": "Journal of Systems Architecture"
            }
        }
    },
    "Latent Transition Analysis (LTA) : A Method for Identifying Differences in Longitudinal Change Among Unobserved Groups": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050920304968/pdfft?md5=fda5f96c3386bfe6b488138b0eaf74a1&pid=1-s2.0-S1877050920304968-main.pdf",
                "title": "Latent Transition Analysis (LTA) : A Method for Identifying Differences in Longitudinal Change Among Unobserved Groups",
                "abstract": "The latent transition analysis (LTA) model is a version of Latent Class Analysis (LCA) which is used in longitudinal data analysis. The goal of LTA is to examine the variation over time and to identify the association of repeated measures. LTA gives an elegant solution to study heterogeneous changes in longitudinal data. As a classic LCA, it assumes that the data consists of several unknown groups that have homogeneous choices. This paper aims to present a review of assess the performance of LTA to identify the differences in longitudinal differences among unobserved classes. An example of LTA application in educational assessment was developed to illustrate the process and to explore the change among the time in reading comprehension.",
                "year": 2020,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Simulating behaviors of human situation awareness under high workloads": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0954181001000255/pdfft?md5=388a7e926eb97b0835a796e319e65dc6&pid=1-s2.0-S0954181001000255-main.pdf",
                "title": "Simulating behaviors of human situation awareness under high workloads",
                "abstract": "Recent, highly advanced artifact systems make it more complex and unpredictable to understand how they are used by human under a particular situation. Disastrous errors would occur when the methods of interactions deviate from what are expected at the design stage by the designer, who is different from the user. This paper presents a method by which human dynamic interactions with the externally provided tasks can be simulated. We concentrate on the resource-bounded human situation awareness, which is a complex phenomenon occurring at the interface between the human internal cognitive processes and the external environment. Finally, this is applied to the analysis of the high-tech aircraft accident that occurred in Columbia in 1995, which is a well-known accident caused by the discoordination between the human and the automated aircraft.",
                "year": 2001,
                "publisher": "Artificial Intelligence in Engineering"
            }
        }
    },
    "R.I.P.: Remain in perpetuity. Facebook memorial pages": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736585312000263/pdfft?md5=b3e09a24a01b296cdc179523580065f5&pid=1-s2.0-S0736585312000263-main.pdf",
                "title": "R.I.P.: Remain in perpetuity. Facebook memorial pages",
                "abstract": "Facebook is not only a virtual space to commune with the living, it is also a place to honor, memorialize, and engage in dialogs with the deceased. This study examines 550 memorial pages on Facebook for age, gender, race, and cause of death of the memorialized, as well as to whom the communication is addressed. Where ritualistic memorials and mourning practices usually occur in cemeteries or at the sites of accidents, memorial sites on Facebook offer an alternative space to mourn that is public, collective, and with archival capabilities. Individual dialogs and memories in this alternative space are not private, and often involve direct communications with the deceased. In this way, the dead never really die; rather perpetually remain in a digital state of dialogic limbo.",
                "year": 2013,
                "publisher": "Telematics and Informatics"
            }
        }
    },
    "A derived information framework for a dynamic knowledge graph and its application to smart cities": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X23003825/pdfft?md5=89ed68700979dddd5d04a89a88418d24&pid=1-s2.0-S0167739X23003825-main.pdf",
                "title": "A derived information framework for a dynamic knowledge graph and its application to smart cities",
                "abstract": "In this work, we develop a derived information framework to semantically annotate how a piece of information can be obtained from others in a dynamic knowledge graph. We encode this using the notion of a “derivation” and capture its metadata with a lightweight ontology. We provide an agent template designed to monitor derivations and to standardise agents performing this and related operations. We implement both synchronous and asynchronous communication modes for agents interacting with the knowledge graph. When occurring in conjunction, directed acyclic graphs of derivations can arise, with changing data propagating through the knowledge graph by means of agents’ actions. While the framework itself is domain-agnostic, we apply it in the context of smart cities as part of the World Avatar project and demonstrate that it is capable of handling sequential events across different timescales. Starting from source information, the framework automatically populates derived data and ensures they remain up to date upon access for a potential flood impact assessment use case.",
                "year": 2024,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Health Professionals’ Perspectives on Electronic Medical Record Infusion and Individual Performance: Model Development and Questionnaire Survey Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Health Professionals’ Perspectives on Electronic Medical Record Infusion and Individual Performance: Model Development and Questionnaire Survey Study",
                "abstract": "Background\n\nElectronic medical records (EMRs) are integrated information sources generated by health care professionals (HCPs) from various health care information systems. EMRs play crucial roles in improving the quality of care and medical decision-making and in facilitating cross-hospital health information exchange. Although many hospitals have invested considerable resources and efforts to develop EMRs for several years, the factors affecting the long-term success of EMRs, particularly in the EMR infusion stage, remain unclear.",
                "year": 2021,
                "publisher": "JMIR Medical Informatics"
            }
        }
    },
    "A saliency-based hierarchy for local symmetries": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0262885601000804/pdfft?md5=23f4294daa8c24cd9b45d47d1f4f512d&pid=1-s2.0-S0262885601000804-main.pdf",
                "title": "A saliency-based hierarchy for local symmetries",
                "abstract": "Shape representation is fundamental to many areas of computer vision. Local symmetries provide a tool on which to base flexible representations of complex shapes, however, ‘multiple participation’ symmetries such as SLS generate many more symmetries, than required. This clutters the representation with mostly irrelevant symmetries, hiding perceptually salient information.\n\nIn this paper, a salience hierarchy is proposed that allows compact, perceptually pleasing representations to be extracted from the highly redundant full SLS. Both individual and competitive saliency is used to create the hierarchy which partitions the set of symmetries into subsets, each of which is an independent shape representation.",
                "year": 2002,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Processing translational motion sequences": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0734189X83900981/pdfft?md5=710c54c5d92118007e6d3a859695509f&pid=1-s2.0-0734189X83900981-main.pdf",
                "title": "Processing translational motion sequences",
                "abstract": "A procedure for processing real world image sequences produced by relative translational motion between a sensor and environmental objects is presented. In this procedure, the determination of the direction of sensor translation is effectively combined with the determination of the displacement of image features and environmental depth. It requires no restrictions on the direction of motion, nor the location and shape of environmental objects. It has been applied successfully to real-world image sequences from several different task domains. The processing consists of two basic steps: feature extraction and search. The feature extraction process picks out small image areas which may correspond to distinguishing parts of environmental objects. The direction of translational motion is then found by a search which determines the image displacement paths along which a measure of feature mismatch is minimized for a set of features. The correct direction of translation will minimize this error measure and also determine the corresponding image displacement paths for which the extracted features match well.",
                "year": 1983,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Design and empirical evaluation of search software for legal professionals on the WWW": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457399000576/pdfft?md5=920ac68d28e1562deeed5d5627f8880e&pid=1-s2.0-S0306457399000576-main.pdf",
                "title": "Design and empirical evaluation of search software for legal professionals on the WWW",
                "abstract": "Our research focuses on designing effective search aids for legal researchers interested in law-related information on the world wide web. In this paper we report on the design and evaluation of two software systems developed to explore models for browsing and searching across a user-selected set of WWW sites. A directory services tool, LIBClient, provides a hierarchical index of legal information resources in an interface emphasizing ease-of-use by Internet novices and management of multiple-site searching. To study the relative effectiveness of LIBClient in the hands of legal professionals, nineteen law students were observed using LIBClient and, in separate trials, the popular general-purpose search services to perform known-item searches within a fixed time limit. The experiment indicates the value of LIBClient for focused searching, most properly as a supplement to general-purpose search engines. Motivated by observations from the LIBClient study, a second retrieval experiment explores the effectiveness of a radically different LIBClient design in which the LIBClient interface is combined with a crawler-enhanced search engine, IRISWeb. The LIBClient–IRISWeb system enables full-text searching using natural language queries across a set of WWW pages collected by the IRISWeb crawler. The page harvesting process relies on a cascading set of filters to define the final set of WWW pages to be collected, including user selections in LIBClient, search results from site-specific search engines, and the hyperlink structure at target sites. To evaluate the LIBClient–IRISWeb method, the queries used in the user study are submitted to the system, with excellent retrieval results. In conclusion, our research points to the promise of WWW search tool designs that tightly couple directed browsing with query-based search capabilities using new forms of search automation.",
                "year": 2000,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Value alignment's role in mitigating resistance to IT use: The case of physicians'resistance to electronic health record systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720622001112/pdfft?md5=c0d4a3baa04f9264aa8bf5de481e1c37&pid=1-s2.0-S0378720622001112-main.pdf",
                "title": "Value alignment's role in mitigating resistance to IT use: The case of physicians'resistance to electronic health record systems",
                "abstract": "This research investigates the impact of value alignment on physician resistance to electronic health record systems (EHRS) through the reduction of perceived threat to professional autonomy and perceived changes in work processes. Analyses of data collected from 104 US physicians using SmartPLS reveal that value alignment (i) directly impacts resistance to EHRS, (ii) influences resistance to EHRS use by reducing the perceived threat to professional autonomy, and (iii) influences perceived changes in work processes. Overall, our research holds significant promise for theory and practice and has the potential to become foundational for future research on mitigating resistance to IT use.",
                "year": 2022,
                "publisher": "Information & Management"
            }
        }
    },
    "Dense Indoor Sensor Networks: Towards passively sensing human presence with LoRaWAN": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119222000700/pdfft?md5=116640067f86484daa15edc53dc21cfe&pid=1-s2.0-S1574119222000700-main.pdf",
                "title": "Dense Indoor Sensor Networks: Towards passively sensing human presence with LoRaWAN",
                "abstract": "Sensors have become ubiquitous in buildings but are rarely connected to a network, and their potential to analyse the performance, use, and interaction with a building is not yet fully realised. In the coming years, we expect sensors in buildings to become part of the Internet of Things (IoT) and grow in numbers to form a Dense Indoor Sensor Network (DISN) that allows for unprecedented analysis of the performance, use, and interaction with buildings. Multiple technologies vie for leading this transformation. We explore Long Range Wide Area Network (LoRaWAN) as an alternative for creating indoor sensor networks that extends beyond its original long-distance communication purpose. For the present paper, we developed a DISN with 390 sensor nodes and four gateways and empirically evaluated its performance for two years. Our analysis of more than 86 million transmissions revealed that DISNs achieve a much lower distance coverage compared to estimations from previous research indicating that more gateways are required. In addition, the deployment of multiple gateways decreased the loss of transmissions due to environmental and network factors. Given the complexity of our system, we received few colliding concurrent messages, which demonstrates a gap between the projected requirements of LoRaWAN systems and the actual requirements of real-world applications given sufficient gateways. We also contribute to the modelling of transmissions with our comparison of attenuation models derived from multiple methodologies. Across all models, we find that robust coverage in an indoor environment can be maintained by placing a gateway every 30 m and every 5 floors. Finally, we also investigate the application of DISNs for the passive sensing and visualisation of human presence using a Digital Twin (DT) and a Fused Twins (FT) representation in Augmented Reality (AR). A passive sensing approach allows us to gather relevant data on human use of a building while still preserving privacy via the aggregation process. Immersive in situ visualisations in FT allow for new interactions and new forms of participation. We conclude that DISNs are already technologically feasible today and basing them on Low Power Wide Area Network (LPWAN) offers intriguing possibilities to reduce energy consumption, maintenance cost, and bandwidth use while also enabling new forms of human-building interaction.",
                "year": 2022,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "Passing crisis and emergency risk communications: The effects of communication channel, information type, and repetition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687014002993/pdfft?md5=3e34667ef03e95f95123e35f2d4ae190&pid=1-s2.0-S0003687014002993-main.pdf",
                "title": "Passing crisis and emergency risk communications: The effects of communication channel, information type, and repetition",
                "abstract": "Three experiments explore several factors which influence information transmission when warning messages are passed from person to person. In Experiment 1, messages were passed down chains of participants using five different modes of communication. Written communication channels resulted in more accurate message transmission than verbal. In addition, some elements of the message endured further down the chain than others. Experiment 2 largely replicated these effects and also demonstrated that simple repetition of a message eliminated differences between written and spoken communication. In a final field experiment, chains of participants passed information however they wanted to, with the proviso that half of the chains could not use telephones. Here, the lack of ability to use a telephone did not affect accuracy, but did slow down the speed of transmission from the recipient of the message to the last person in the chain. Implications of the findings for crisis and emergency risk communication are discussed.",
                "year": 2015,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Towards a Social Psychology-based Microscopic Model of Driver Behavior and Decision-making: Modifying Lewin's Field Theory": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050914006966/pdfft?md5=2258d33445c832cdae7a1b11cdcb952c&pid=1-s2.0-S1877050914006966-main.pdf",
                "title": "Towards a Social Psychology-based Microscopic Model of Driver Behavior and Decision-making: Modifying Lewin's Field Theory",
                "abstract": "Central to effective roadway design is the ability to understand how drivers behave as they traverse a segment of roadway. While simple and complex microscopic models have been used over the years to analyse driver behaviour, most models: 1.) incorporate separate car-following and lane-changing algorithms, and thus do not capture the interdependencies between lane-changing and car-following vehicle; 2.) do not capture differences in the drivers’ cognitive and physical characteristics; and 3.) are constructed from observed vehicle movements and make no attempt to model the discrete differences between how each roadway element alters each driver's behaviour.\n\nThis paper employs field theory to construct a conceptual framework for a new microscopic model. In field theory, an agent (e.g. the driver) views a field (i.e. the area surrounding the vehicle) filled with stimuli and perceives forces associated with each stimuli once these stimuli are internalized. Based on this theory, the resulting model would be designed to directly incorporate drivers’ perceptions to roadway stimuli along with vehicle movements for drivers of different cognitive and physical abilities. It is postulated that such a model would more effectively reflect reality, and if this model were accurately calibrated, could potentially model the effects of external stimuli such as innovative geometric configurations, lane closures, and technology applications such as variable message boards. A modified field theory could potentially capture and model “hot topics” in traffic engineering, such as the distracted drivers, road rage, the incorporation of ITS elements, and driver behaviour through a work zone.",
                "year": 2014,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "The effects of anonymity on computer-mediated communication: The case of independent versus interdependent self-construal influence": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563215301576/pdfft?md5=3c9280a4b431bd572e23e1f303ca794d&pid=1-s2.0-S0747563215301576-main.pdf",
                "title": "The effects of anonymity on computer-mediated communication: The case of independent versus interdependent self-construal influence",
                "abstract": "This experiment investigated how anonymity influenced group identification, inter-group antagonism, and attitude change in computer-mediated communication in samples of both Korean and American participants. This study also examined how self-construal moderated the effect of anonymity on inter-group antagonism. Consistent with the social identity model of deindividuation effects (SIDE), findings from an analysis of variance (ANOVA) test showed that anonymity fostered group identification among the discussion partners and created greater attitude change following a group discussion. Anonymity correlated negatively with the exhibition of critical comments in both Korean and American participants. Although Korean participants showed a greater interdependent self-construal than the American participants did, the effects of self-construal on group identification and inter-group antagonism did not differ from those of American participants. Implications are discussed in light of the social identity theory, SIDE, and self-categorization theory.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Variable frequency selective structure (CFRP/Cu/CFRP) micro blind hole laser high-quality drilling method guided by acoustic emission technology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0888327024008707/pdfft?md5=fb42acabbd0138b8a709ddc98a17840e&pid=1-s2.0-S0888327024008707-main.pdf",
                "title": "Variable frequency selective structure (CFRP/Cu/CFRP) micro blind hole laser high-quality drilling method guided by acoustic emission technology",
                "abstract": "The laminated structure of carbon fiber reinforced plastic (CFRP)-Cu-CFRP is an important structure for aerospace radome. The blind holes manufacturing is core process, but there are technological challenges in achieving the exposure of a 10 μm thick copper foil at the bottom of the blind hole. The exact position of the 10 μm thick copper foil within the material’s internal space is unknown, and the uneven heat accumulation and conduction can easily cause burning and damage to the ultra-thin copper foil. To date, there is no publicly accessible literature that documents the execution of this specific process. In this paper, novel variable inner diameter spiral scanning guided by Cu-CFRP interface accurately extracted using cosine similarity (CS) algorithm is proposed to obtain a blind hole with near-zero ablation damage. The similarity of acoustic emission signals generated by laser interaction with Cu and CFRP, as well as thermal accumulation and conduction behavior under different scanning radius, are described. The high-quality blind holes have been successfully drilled, with a large area copper foil exposed at the bottom, while the surface of the copper foil remaining intact without any damage or heat affected zone. Compared with traditional drilling method, the average exposed copper foil area at the bottom of the blind holes is increased by 238.4 % (from 0.464 mm2 to 1.570 mm2). And the average taper of blind hole is reduced by 41.7 % (from 0.470 to 0.274). This method can effectively increase the area of exposed copper foil and reduces the taper of blind holes, improve the welding strength and conductivity of wires in aerospace radomes, thereby enhancing the stealth performance and operational reliability of fighter jets.",
                "year": 2025,
                "publisher": "Mechanical Systems and Signal Processing"
            }
        }
    },
    "SMILE: Siamese Multi-scale Interactive-representation LEarning for Hierarchical Diffeomorphic Deformable image registration": {
        "accordingTo": {
            "scienceDirect": {
                "title": "SMILE: Siamese Multi-scale Interactive-representation LEarning for Hierarchical Diffeomorphic Deformable image registration",
                "abstract": "Deformable medical image registration plays an important role in many clinical applications. It aims to find a dense deformation field to establish point-wise correspondences between a pair of fixed and moving images. Recently, unsupervised deep learning-based registration methods have drawn more and more attention because of fast inference at testing stage. Despite remarkable progress, existing deep learning-based methods suffer from several limitations including: (a) they often overlook the explicit modeling of feature correspondences due to limited receptive fields; (b) the performance on image pairs with large spatial displacements is still limited since the dense deformation field is regressed from features learned by local convolutions; and (c) desirable properties, including topology-preservation and the invertibility of transformation, are often ignored. To address above limitations, we propose a novel Convolutional Neural Network (CNN) consisting of a Siamese Multi-scale Interactive-representation LEarning (SMILE) encoder and a Hierarchical Diffeomorphic Deformation (HDD) decoder. Specifically, the SMILE encoder aims for effective feature representation learning and spatial correspondence establishing while the HDD decoder seeks to regress the dense deformation field in a coarse-to-fine manner. We additionally propose a novel Local Invertible Loss (LIL) to encourage topology-preservation and local invertibility of the regressed transformation while keeping high registration accuracy. Extensive experiments conducted on two publicly available brain image datasets demonstrate the superiority of our method over the state-of-the-art (SOTA) approaches. Specifically, on the Neurite-OASIS dataset, our method achieved an average DSC of 0.815 and an average ASSD of 0.633 mm.",
                "year": 2024,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "Overlay striping and optimal parallel I/O for modern applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167819197001154/pdfft?md5=646eccaf227260f6e9e49f6dbae909ad&pid=1-s2.0-S0167819197001154-main.pdf",
                "title": "Overlay striping and optimal parallel I/O for modern applications",
                "abstract": "Disk array systems are rapidly becoming the secondary-storage media of choice for many emerging applications with large storage and high bandwidth requirements. Striping data across the disks of a disk array introduces significant performance benefits mainly because the effective transfer rate of the secondary storage is increased by a factor equal to the stripe width. However, the choice of the optimal stripe width is an open problem: no general formal analysis has been reported and intuition alone fails to provide good guidelines. As a result one may find occasionally contradictory recommendations in the literature. With this work we first contribute an analytical calculation of the optimal stripe width. Second, we recognize that the optimal stripe width is sensitive to the multiprogramming level, which is not known a priori and fluctuates with time. Thus, calculations of the optimal stripe width are, by themselves only, of little practical use. For this reason we propose a novel striping technique, called overlay striping, which allows objects to be retrieved using a number of alternative stripe widths. We provide the detailed algorithms for our overlay striping method and we study the associated storage overhead and performance improvements and we show that we can achieve near optimal performance for very wide ranges of the possible multiprogramming levels, while incurring small storage overheads.",
                "year": 1998,
                "publisher": "Parallel Computing"
            }
        }
    },
    "Computational selection of distinct class- and subclass-specific gene expression signatures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046402005257/pdfft?md5=7e4ed77510882947e71ab613ed1f5152&pid=1-s2.0-S1532046402005257-main.pdf",
                "title": "Computational selection of distinct class- and subclass-specific gene expression signatures",
                "abstract": "In this investigation we used statistical methods to select genes with expression profiles that partition classes and subclasses of biological samples. Gene expression data corresponding to liver samples from rats treated for 24 h with an enzyme inducer (phenobarbital) or a peroxisome proliferator (clofibrate, gemfibrozil or Wyeth 14,643) were subjected to a modified Z-score test to identify gene outliers and a binomial distribution to reduce the probability of detecting genes as differentially expressed by chance. Hierarchical clustering of 238 statistically valid differentially expressed genes partitioned class-specific gene expression signatures into groups that clustered samples exposed to the enzyme inducer or to peroxisome proliferators. Using analysis of variance (ANOVA) and linear discriminant analysis methods we identified single genes as well as coupled gene expression profiles that separated the phenobarbital from the peroxisome proliferator treated samples and discerned the fibrate (gemfibrozil and clofibrate) subclass of peroxisome proliferators. A comparison of genes ranked by ANOVA with genes assessed as significant by mixed linear models analysis [J. Comput. Biol. 8 (2001) 625] or ranked by information gain revealed good congruence with the top 10 genes from each statistical method in the contrast between phenobarbital and peroxisome proliferators expression profiles. We propose building upon a classification regimen comprised of analysis of replicate data, outlier diagnostics and gene selection procedures to utilize cDNA microarray data to categorize subclasses of samples exposed to pharmacologic agents.",
                "year": 2002,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Deployment of systems development methodologies: Perceptual congruence between IS managers and systems developers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720605000182/pdfft?md5=c7e9faecf58b48be716b01d6f3e0ba15&pid=1-s2.0-S0378720605000182-main.pdf",
                "title": "Deployment of systems development methodologies: Perceptual congruence between IS managers and systems developers",
                "abstract": "We studied the differences in perception between IS managers and developers about the deployment of systems development methodologies. The results indicated that IS managers were generally more positive about systems development methodologies than were developers. IS managers perceived methodology support for organizational alignment, and methodology impact on the productivity and quality of the development process to be significantly more important than did system developers, who, in turn, perceived methodology support for verification and validation significantly higher than did IS managers. These differences can be explained by the relevance and importance of the support to the task that the stakeholders perform.",
                "year": 2006,
                "publisher": "Information & Management"
            }
        }
    },
    "Consumer responses to promoted tweets sent by brands and political parties": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216305982/pdfft?md5=d221112edd2b3207a022b10ff5e73d20&pid=1-s2.0-S0747563216305982-main.pdf",
                "title": "Consumer responses to promoted tweets sent by brands and political parties",
                "abstract": "This study aims to understand how consumers respond to social media advertising (SMA) by focusing on promoted tweets sent by brands and political parties, and examines persuasion knowledge as underlying mechanism of these responses. Two online experiments with between-subjects designs, comparing the effects of SMA (promoted vs. non-promoted tweet) and the source of the tweet (political party vs. brand), were conducted. Study 1 showed that consumers rarely notice it when a tweet is promoted. Study 2 demonstrated that when a promoted tweet was sent by a political party, the recipient's recognition that the tweet was a form of advertisement (i.e., activated persuasion knowledge) reduced online behavioral intention, increased skepticism, and negatively affected source trustworthiness and attitudes. This effect was not present for brands. Although research has shown that social media can be an important platform to engage audiences, this study is the first to study the mechanisms underlying the effects of SMA, and whether there are any boundary conditions to these effects. These findings suggest that political parties should be cautious in their use of social media advertising as it can evoke negative responses.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Marginal loss and exclusion loss for partially supervised multi-organ segmentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841521000256/pdfft?md5=42c9fe472bd9075e1711b0546b0373a2&pid=1-s2.0-S1361841521000256-main.pdf",
                "title": "Marginal loss and exclusion loss for partially supervised multi-organ segmentation",
                "abstract": "Annotating multiple organs in medical images is both costly and time-consuming; therefore, existing multi-organ datasets with labels are often low in sample size and mostly partially labeled, that is, a dataset has a few organs labeled but not all organs. In this paper, we investigate how to learn a single multi-organ segmentation network from a union of such datasets. To this end, we propose two types of novel loss function, particularly designed for this scenario: (i) marginal loss and (ii) exclusion loss. Because the background label for a partially labeled image is, in fact, a ‘merged’ label of all unlabelled organs and ‘true’ background (in the sense of full labels), the probability of this ‘merged’ background label is a marginal probability, summing the relevant probabilities before merging. This marginal probability can be plugged into any existing loss function (such as cross entropy loss, Dice loss, etc.) to form a marginal loss. Leveraging the fact that the organs are non-overlapping, we propose the exclusion loss to gauge the dissimilarity between labeled organs and the estimated segmentation of unlabelled organs. Experiments on a union of five benchmark datasets in multi-organ segmentation of liver, spleen, left and right kidneys, and pancreas demonstrate that using our newly proposed loss functions brings a conspicuous performance improvement for state-of-the-art methods without introducing any extra computation.",
                "year": 2021,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "The Grid Workloads Archive": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X08000125/pdfft?md5=eb42597b9197e00444f686fe6cd570e6&pid=1-s2.0-S0167739X08000125-main.pdf",
                "title": "The Grid Workloads Archive",
                "abstract": "While large grids are currently supporting the work of thousands of scientists, very little is known about their actual use. Due to strict organizational permissions, there are few or no traces of grid workloads available to the grid researcher and practitioner. To address this problem, in this work we present the Grid Workloads Archive (GWA), which is at the same time a workload data exchange and a meeting point for the grid community. We define the requirements for building a workload archive, and describe the approach taken to meet these requirements with the GWA. We introduce a format for sharing grid workload information, and tools associated with this format. Using these tools, we collect and analyze data from nine well-known grid environments, with a total content of more than 2000 users submitting more than 7 million jobs over a period of over 13 operational years, and with working environments spanning over 130 sites comprising 10 000 resources. We show evidence that grid workloads are very different from those encountered in other large-scale environments, and in particular from the workloads of parallel production environments: they comprise almost exclusively single-node jobs, and jobs arrive in “bags-of-tasks”. Finally, we present the immediate applications of the GWA and of its content in several critical grid research and practical areas: research in grid resource management, and grid design, operation, and maintenance.",
                "year": 2008,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Microcomputer controlled arc oscillator for automated TIG welding": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0745713890900345/pdfft?md5=36d59cb07da166e725abbedd21612d65&pid=1-s2.0-0745713890900345-main.pdf",
                "title": "Microcomputer controlled arc oscillator for automated TIG welding",
                "abstract": "A magnetic arc oscillation system has been developed to control the fusion characteristic of arc welding. A single-pole electromagnet generates a transverse magnetic field parallel to the weld line. The weave pattern of the magnetic field, hence the bead geometry, is controlled by a custom-built computer. The applied magnetic field has a marked influence on the bead width, but less on the bead depth. Experiments also show that bead appearance is improved by magnetically oscillating the arc.",
                "year": 1990,
                "publisher": "Journal of Microcomputer Applications"
            }
        }
    },
    "Scene analysis using regions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0004370270900081/pdfft?md5=40ef7abcf714f4d227583457a5f70b28&pid=1-s2.0-0004370270900081-main.pdf",
                "title": "Scene analysis using regions",
                "abstract": "One of the vision projects of the Stanford Research Institute Artificial Intelligence Group is described. The method employed uses regions as basic data and progresses by successive partitioning of the picture toward an interpretable “goal partition”, which is then explored by a heuristic decision tree. A general structure is discussed and an example problem is shown in detail.",
                "year": 1970,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Polycyclic aromatic hydrocarbon carcinogenicity: Theoretical modelling and experimental facts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1380732398800162/pdfft?md5=5f25e5cc1def8249ded0e5de7024e58a&pid=1-s2.0-S1380732398800162-main.pdf",
                "title": "Polycyclic aromatic hydrocarbon carcinogenicity: Theoretical modelling and experimental facts",
                "abstract": "This chapter discusses the modeling of polycyclic aromatic hydrocarbons (PAH) and their activated metabolites in the light of the accumulated experimental evidence for their modes of genotoxic action. PAH's form a large class of molecules that is ubiquitous in human environment and encompass a variety of structural types. The parent PAH's are largely inactive and do not cause DNA damage directly. They are made excretable by metabolism, which in principle can either activate or detoxify chemical carcinogens. The metabolism displays common characteristics for the activation of the PAH themselves and their N-heteroaromatic relatives of the benzacridine and dibenzacridine family. The initial activating step is an epoxidation of the M-region by the microsomal NADPH-dependent monoxygenase system (MOS), containing the enzyme cytochrome P-450c. The MOS activity from different tissues or cells toward different regions of a PAH varies greatly. This “regioselectivity” is a factor that determines the susceptibility of animals and humans to carcinogenic effects. The MOS oxidizes compounds with different structures; thus it is a flexible metabolic system and possesses overlapping substrate selectivities. The chapter discusses the bay-region theory, the MCS model, metabolic factor, carbocation formation, size factor, and the performance and limitations of the MCS model. There is a synergy or complimentary combination of various factors that affect the multiple parallel and/or consecutive steps of chemical carcinogenesis. Overview of DNA-carcinogen binding and DNA-mediated hydrolysis of PAHDE's is presented in the chapter. The chapter describes DNA binding of carcinogenic hydrocarbon metabolites, hydrolysis and PAH carcinogenicity, and molecular modeling of intercalated PAH triol carbocations. Diverse physico-chemical and biological parameters representing a range of possible mechanisms of interactions between DNA and several possible important metabolites modulate the carcinogenic potency of a particular PAH.",
                "year": 1998,
                "publisher": "Theoretical and Computational Chemistry"
            }
        }
    },
    "Fuzzy track-to-track association and track fusion approach in distributed multisensor–multitarget multiple-attribute environment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0165168407000060/pdfft?md5=080cd8b28268b221caab729d6720354d&pid=1-s2.0-S0165168407000060-main.pdf",
                "title": "Fuzzy track-to-track association and track fusion approach in distributed multisensor–multitarget multiple-attribute environment",
                "abstract": "A great deal of attention is currently focused on multisensor data fusion. Multisensor data fusion combines data from multiple sensor systems to achieve improved performance and provide more inferences than could be achieved using a single sensor system. One of the most important aspects of it is track-to-track-association. This paper develops a fuzzy data fusion approach to solve the problem of track-to-track association and track fusion in distributed multisensor–multitarget multiple-attribute environments in overlapping coverage scenarios. The proposed approach uses the fuzzy clustering means algorithm to reduce the number of target tracks and associate duplicate tracks by determining the degree of membership for each target track. It uses current sensor data and the known sensor resolutions for track-to-track association, track fusion, and the selection of the most accurate sensor for tracking fused targets. Numerical results based on Monte Carlo simulations are presented. The results show that the proposed approach significantly reduces the computational complexity and achieves considerable performance improvement compared to Euclidean clustering. We also show that the performance of the proposed approach is reasonable close to the performance of the Bayesian minimum mean square error criterion.",
                "year": 2007,
                "publisher": "Signal Processing"
            }
        }
    },
    "Key organizational factors in data warehouse architecture selection": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923610000436/pdfft?md5=d41fe36b5b523381e30a5cbfa1a3f6f1&pid=1-s2.0-S0167923610000436-main.pdf",
                "title": "Key organizational factors in data warehouse architecture selection",
                "abstract": "Even though data warehousing has been in existence for over a decade, companies are still uncertain about a critical decision — which data warehouse architecture to implement? Based on the existing literature, theory, and interviews with experts, a research model was created that identifies the various contextual factors that affect the selection decision. The results from the field survey and multinomial logistic regression suggest that various combinations of organizational factors influence data warehouse architecture selection. The strategic view of the data warehouse prior to implementation emerged as a key determinant. The research suggests an overall model for predicting the data warehouse architecture selection decision.",
                "year": 2010,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Nested structures of control: An intuitive view": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0734189X87800130/pdfft?md5=d34dbc2878d6bfb5212ed132a2b91f85&pid=1-s2.0-S0734189X87800130-main.pdf",
                "title": "Nested structures of control: An intuitive view",
                "abstract": "An intuitive exposition is provided of a theory (rigorously elaborated elsewhere) that perceptual organization is the description of phenomena as nested structures of control. The theory is used to explain a number of perceptual-organizational phenomena such a complex shape prototypification, generalized-cone analysis, the importance of curvature singularities, grouping phenomena, motion phenomena, etc. The nested-control theory maintains that any perceptual organization is given a stratification where each level of the stratification is operated on (1) asymmetrically, and (2) as a whole, by the level in which it is embedded. The levels-within-levels hierarchy is divided into two subhierarchies, one describing the stimulus set as generated via a sequence of deformations, and the other describing the initial prototype as generated from a subset of itself. The structure of the latter sub-hierarchy yields the grouping structure of the stimulus set. Principles are proposed that determine the way these two sub-hierarchies are interrelated: Symmetry axes in the grouping structure are converted into lines of flexibility in the prototype modifications. Using these concepts, solutions are offered to some classical problems in perceptual organization, and a theory of complex shape is developed.",
                "year": 1987,
                "publisher": "Computer Vision, Graphics, and Image Processing"
            }
        }
    },
    "Network-based exploratory data analysis and explainable three-stage deep clustering for financial customer profiling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197623015622/pdfft?md5=31096ed9a3b1c24ab77cb8c81c1a6939&pid=1-s2.0-S0952197623015622-main.pdf",
                "title": "Network-based exploratory data analysis and explainable three-stage deep clustering for financial customer profiling",
                "abstract": "Effective customer segmentation and communication of these findings to non-experts is a pressing task in the financial services sector, with the potential for widespread applications. This study employs a three-stage dimension reduction and clustering technique to segment a large, high-dimensional dataset, emphasizing explainability and intuitive visualization. We present the high-dimensional data and feature set using novel network-based visualization methods and identify the multi-stage process's optimal configuration. The approach segments 14,837 potential customers, each with 163 categorical and 143 numerical features. The first stage of the dimension reduction process employs deep neural network-based autoencoders. The second and third stage uses a non-neural network-based dimension reduction algorithm and clustering algorithm contingent on clustering performance. Subsequently, game theory-inspired Shapley values are computed for each feature to enhance explainability. The optimal approach involves an autoencoder, isometric mapping to three dimensions, and K-means clustering. Lastly, we derive investment portfolios for each segment to demonstrate an expert system application in financial investment advisory to underscore the importance of explainable segmentations.",
                "year": 2024,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Benchmarking big data systems: A survey": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Benchmarking big data systems: A survey",
                "abstract": "With the enormous growth on the availability and usage of Big Data storage and processing systems, it has become essential to assess the various performance aspects of these systems so that we can carefully understand their strong and weak aspects. In practice, currently, when an individual/enterprise aims to develop a Big Data storage and processing solution for harnessing the knowledge inside their data, they will get challenged by the availability of several frameworks from which they need to select. This is a challenging task which needs to directed by with good knowledge about various perspectives of such systems. Additionally, the choice normally vary from one scenario to another according to the essential needs of the application. In practice, there is no single benchmark study which can cover the different types of big data processing requirements, systems, application scenarios and metrics. Several benchmarks and benchmarking studies have been developed where each study focuses on some representative type of frameworks and only consider some aspects to cover. In this article, we provide a comprehensive survey and analysis of the state-of-the-art of benchmarking the different types of big data systems (e.g., NoSQL databases, Big SQL engines, Big Streaming engines, Big Graph Processing engines, Big Machine/Deep Learning engines). Additionally, we highlight some of the significant open challenges and missing requirements of current benchmarks of big data systems with suggestions of directions for future extensions and improvements.",
                "year": 2020,
                "publisher": "Computer Communications"
            }
        }
    },
    "Momentum-driven adaptive synchronization model for distributed DNN training on HPC clusters": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S074373152100191X/pdfft?md5=c27c6fc6d27603b7eaea36623f4f5365&pid=1-s2.0-S074373152100191X-main.pdf",
                "title": "Momentum-driven adaptive synchronization model for distributed DNN training on HPC clusters",
                "abstract": "Building a distributed deep learning (DDL) system on HPC clusters that guarantees convergence speed and scalability for the training of DNNs is challenging. The HPC cluster, which consists of multiple high-density multi-GPU servers connected by the Infiniband network (HDGib), compresses the computing and communication time for distributed DNNs' training but brings new challenges. The convergence time is far from linear scalability (with respect to the number of workers) for parallel DNNs training. We thus analyze the optimization process and identify three key issues that cause scalability degradation. First, the high-frequency update for parameters due to the compression of the computing and communication times exacerbates the stale gradient problem, which slows down the convergence. Second, the previous methods used to constrain the gradient noise (stochastic error) of the SGD are outdated, as HDGib clusters can support more strict constraints due to the Infiniband network connections, which can further constrain the stochastic error. Third, the same learning rate for all workers is inefficient due to the different training stages of each worker. We thus propose a momentum-driven adaptive synchronization model that focuses on solving the above issues and accelerating the training procedure on HDGib clusters. Our adaptive k-synchronization algorithm uses the momentum term to absorb the stale gradients and adaptively bind the stochastic error to provide an approximate optimal descent direction for the distributed SGD. Our model also includes an individual dynamic learning rate search method for each worker to further improve training performance. Compared with previous linear and exponent decay methods, it can provide a more precise descent distance for distributed SGD based on different training stages. Extensive experimental results indicate that the proposed model effectively improves the training performance of CNNs, which retains high accuracy with a speed-up of up to 57.76% and 125.3% on the CPU-based and GPU-based clusters, respectively.",
                "year": 2022,
                "publisher": "Journal of Parallel and Distributed Computing"
            }
        }
    },
    "Architecture of a wireless Personal Assistant for telemedical diabetes care": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505608002086/pdfft?md5=8235adabbec153d642226eefd5f8c9c8&pid=1-s2.0-S1386505608002086-main.pdf",
                "title": "Architecture of a wireless Personal Assistant for telemedical diabetes care",
                "abstract": "Purpose\n\nAdvanced information technologies joined to the increasing use of continuous medical devices for monitoring and treatment, have made possible the definition of a new telemedical diabetes care scenario based on a hand-held Personal Assistant (PA). This paper describes the architecture, functionality and implementation of the PA, which communicates different medical devices in a personal wireless network.",
                "year": 2009,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "A compressed-domain approach for shot boundary detection on H.264/AVC bit streams": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0923596508000489/pdfft?md5=19abc23f08167327fa4fec1de08b5004&pid=1-s2.0-S0923596508000489-main.pdf",
                "title": "A compressed-domain approach for shot boundary detection on H.264/AVC bit streams",
                "abstract": "The amount of digital video content has grown extensively during recent years, resulting in a rising need for the development of systems for automatic indexing, summarization, and semantic analysis. A prerequisite for video content analysis is the ability to discover the temporal structure of a video sequence. In this paper, a novel shot boundary detection technique is introduced that operates completely in the compressed domain using the H.264/AVC video standard. As this specification contains a number of new coding tools, the characteristics of a compressed bit stream are different from prior video specifications. Furthermore, the H.264/AVC specification introduces new coding structures such as hierarchical coding patterns, which can have a major influence on video analysis algorithms. First, a shot boundary detection algorithm is proposed which can be used to segment H.264/AVC bit streams based on temporal dependencies and spatial dissimilarities. This algorithm is further enhanced to exploit hierarchical coding patterns. As these sequences are characterized by a pyramidal structure, only a subset of frames needs to be considered during analysis, allowing the reduction of the computational complexity. Besides the increased efficiency, experimental results also show that the proposed shot boundary detection algorithm achieves a high accuracy.",
                "year": 2008,
                "publisher": "Signal Processing: Image Communication"
            }
        }
    },
    "An FPGA implementation of the matching pursuit algorithm for a compressed sensing enabled e-Health monitoring platform": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0141933118304630/pdfft?md5=d396b9d6e26e5c2aa418f42f075a7630&pid=1-s2.0-S0141933118304630-main.pdf",
                "title": "An FPGA implementation of the matching pursuit algorithm for a compressed sensing enabled e-Health monitoring platform",
                "abstract": "Wireless monitoring of physiological signals is an evolving direction in personalized medicine and home-based e-Health systems. There are several constraints in designing such systems, with two of the most important being energy consumption and data compression. Compressed Sensing (CS) is an emerging data compression technique that can be used to overcome those constraints. This work presents a low-complexity CS hardware implementation on a Field-Programmable Gate Array (FPGA) for the reconstruction of compressively sensed signals using the matching pursuit (MP) algorithm, targeting health-care applications. The proposed hardware design is based on pipeline optimization of the Programmable Logic (PL) implementation performed on the Zynq FPGA, which provides a significant performance enhancement, namely an increased processing speed and a reduced computational time since it is 115x faster than the Matlab implementation and 75x faster than the Processing System (PS) implementation carried out on the same Zynq FPGA device, while achieving alternative a high-quality signal recovery with a Peak Signal to Noise Ratio (PSNR) of 23.8 dB. Comparisons against other state-of-the-art methods showed that the low complexity of the MP algorithm can be exploited for providing almost similar results to more complex algorithms using 87–583 less Digital Signal Processor (DSP) cores, 28–540 less Block RAMs and 10,300 to 84,700 less Look-Up Table (LUT) slices.",
                "year": 2019,
                "publisher": "Microprocessors and Microsystems"
            }
        }
    },
    "Three-dimensional meso-scale modeling of asphalt concrete": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045794924002645/pdfft?md5=8e71369e88ebf5de1ea95712443db967&pid=1-s2.0-S0045794924002645-main.pdf",
                "title": "Three-dimensional meso-scale modeling of asphalt concrete",
                "abstract": "An efficient method to address the three-dimensional modeling of the visco-elasto-plastic material behavior, specifically of bituminous conglomerates used in asphalt concrete production, is proposed. The method resorts to one of the most recent formulations for asphalt creep modeling, represented by the modified Huet-Sayegh fractional rheological model. The Grünwald-Letnikov representation of the fractional operator is adopted to treat the operator numerically in an efficient manner. Further, a coupling scheme between the creep model and elasto-plasticity is proposed by adopting the additive decomposition of the total strain tensor. This enables the numerical assessment of the mechanical behavior for bituminous materials under short- to long-term loading. In this context, both constant strain rate tests, and creep recovery tests are numerically simulated.\n\nNumerical analyses are conducted at the meso-scale with the aim to evaluate the development of inelastic strains in the binder during creep, due to the local interaction between the different material components.",
                "year": 2024,
                "publisher": "Computers & Structures"
            }
        }
    },
    "Deep learning of first-order nonlinear hyperbolic conservation law solvers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0021999124003632/pdfft?md5=73c1165e1e31c4b3746cd57b2c0ef51a&pid=1-s2.0-S0021999124003632-main.pdf",
                "title": "Deep learning of first-order nonlinear hyperbolic conservation law solvers",
                "abstract": "In this contribution, we study the numerical approximation of scalar conservation laws by computational optimization of the numerical flux function in a first-order finite volume method. The cell-averaging inherent to finite volume schemes presents a challenge in the design of such numerical flux functions toward achieving high solution accuracy. Dense neural networks, as well as (piecewise) polynomials, serve as function classes for the numerical flux. Using a parametrization of such a function class, we optimize the numerical flux with respect to its solution accuracy on a set of Riemann problems for which we have closed-form solutions. By design, our resulting first-order finite volume schemes are conservative.\n\nWe show empirically that the proposed schemes can achieve higher accuracy than the Godunov and Enquist-Osher method on a wide range of discretizations.\n\nThe proposed method can be applied to the inverse problem, i.e., the data-driven synthesis of macroscopic physical laws for real-world flux phenomena.",
                "year": 2024,
                "publisher": "Journal of Computational Physics"
            }
        }
    },
    "Segmentation of coronary arteries from X-ray angiographic images using density based spatial clustering of applications with noise (DBSCAN)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809424012333/pdfft?md5=78a204fd90197b4e4fb606d1b0f62a50&pid=1-s2.0-S1746809424012333-main.pdf",
                "title": "Segmentation of coronary arteries from X-ray angiographic images using density based spatial clustering of applications with noise (DBSCAN)",
                "abstract": "Coronary heart disease is one of the most fatal illnesses. Its critical role in the prevention and diagnosis of cardiovascular conditions has made the segmentation of cardiovascular images a growing area of research in the medical field. This study introduces a new method for segmenting coronary arteries in X-ray angiographic images using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm. This algorithm is capable of restoring the vessel tree pixels incrementally by adjusting two parameters, from a single pixel to values where the thin vessels are detected. In this approach, thick and thin veins are initially separated into two distinct spaces and later merged. The algorithm uses morphological techniques such as reconstruction, skeletonization, pruning, and dilation to remove noise and achieve the final extraction of the vessel tree. The segmented vessel tree reveals several new, thin vessels that exhibit low light intensity in the original image. These vessels align with the ground truth field images, demonstrating the efficacy of the algorithm. The evaluation metrics, including an accuracy of 0.9729, specificity of 0.9768, and sensitivity of 0.9027, indicate highly favorable results. Ultimately, various segmented images illustrate the nature of vessel obstructions.",
                "year": 2025,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "A fast recursive algorithm for molecular dynamics simulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S002199918371106X/pdfft?md5=b16041ecad1adce6c059ccc2436c62c5&pid=1-s2.0-S002199918371106X-main.pdf",
                "title": "A fast recursive algorithm for molecular dynamics simulation",
                "abstract": "In this paper, we develop a recursive algorithm for solving the dynamical equations of motion for molecular systems. We make use of internal variable models which have been shown to reduce the computation times of molecular dynamics simulations by an order of magnitude when compared with Cartesian models. The O(N) algorithm described in this paper for solving the equations of motion provides additional significant improvements in computational speed. We make extensive use of the spatial operator methods which have been developed recently for the analysis and simulation of the dynamics of multibody systems. The spatial operators are used to derive the equations of motion and obtain an operator expression for the system mass matrix. An alternative square factorization of the mass matrix leads to a closed form expression for its inverse. From this follows the recursive algorithm for computing the generalized accelerations. The computational cost of this algorithm grows only linearly with the number of degrees of freedom. This is in contrast to conventional constrained dynamics algorithms whose cost is a cubic function of the number of degrees of freedom. For the case of a polypeptide molecule with 400 residues, the O(N) algorithm provides computational speedup by a factor of 450 over the conventional O(N3) algorithm. We also describe a simplified method for computing and handling the potential function gradients within the dynamics computations.",
                "year": 1993,
                "publisher": "Journal of Computational Physics"
            }
        }
    },
    "Evaluating single event upsets in deep neural networks for semantic segmentation: An embedded system perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1383762124001796/pdfft?md5=7c38696062f91b81f83a171baa075ab6&pid=1-s2.0-S1383762124001796-main.pdf",
                "title": "Evaluating single event upsets in deep neural networks for semantic segmentation: An embedded system perspective",
                "abstract": "As the deployment of artificial intelligence (AI) algorithms at edge devices becomes increasingly prevalent, enhancing the robustness and reliability of autonomous AI-based perception and decision systems is becoming as relevant as precision and performance, especially in applications areas considered safety-critical such as autonomous driving and aerospace. This paper delves into the robustness assessment in embedded Deep Neural Networks (DNNs), particularly focusing on the impact of parameter perturbations produced by single event upsets (SEUs) on convolutional neural networks (CNN) for image semantic segmentation. By scrutinizing the layer-by-layer and bit-by-bit sensitivity of various encoder–decoder models to soft errors, this study thoroughly investigates the vulnerability of segmentation DNNs to SEUs and evaluates the consequences of techniques like model pruning and parameter quantization on the robustness of compressed models aimed at embedded implementations. The findings offer valuable insights into the mechanisms underlying SEU-induced failures that allow for evaluating the robustness of DNNs once trained in advance. Moreover, based on the collected data, we propose a set of practical lightweight error mitigation techniques with no memory or computational cost suitable for resource-constrained deployments. The code used to perform the fault injection (FI) campaign is available at https://github.com/jonGuti13/TensorFI2, while the code to implement proposed techniques is available at https://github.com/jonGuti13/parameterProtection.",
                "year": 2024,
                "publisher": "Journal of Systems Architecture"
            }
        }
    },
    "Immunoinformatics guided modeling of CCHF_GN728, an mRNA-based universal vaccine against Crimean-Congo hemorrhagic fever virus": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482521008921/pdfft?md5=03f340f421f1c03761826bc7c02fdde0&pid=1-s2.0-S0010482521008921-main.pdf",
                "title": "Immunoinformatics guided modeling of CCHF_GN728, an mRNA-based universal vaccine against Crimean-Congo hemorrhagic fever virus",
                "abstract": "The Crimean-Congo hemorrhagic fever virus (CCHFV) is a lethal human pathogen belonging to the Nairoviridae family that causes Crimean-Congo hemorrhagic fever (CCHF), a tick-borne infection with an alarming mortality rate of up to 80%. CCHFV is the most widespread tick-borne virus with the potential to trigger a pandemic. To date, no vaccines or therapeutics for CCHF have been authorized. In this study, we implemented immunoinformatics approach for developing CCHF_GN728, a universal mRNA-based multi-epitope vaccine against CCHFV. Glycoprotein precursor (GPC) and nucleoprotein (NP) from the virus were selected and screened for potential immunogenic T- and B-cell epitopes. Our developed antigen exhibited the potential to generate 99.95% population coverage worldwide. Stable epitope-allele interaction was confirmed using molecular docking and dynamics simulation. In silico immune simulation corroborated immune cell response to antigen clearance rate. Optimized codons ensured efficient expression of the mRNA in the host cell. The vaccine exhibited stable and strong interactions with the Toll-like receptors. Our findings suggest that the CCHF_GN728 vaccine will trigger specific anti-CCHFV immune responses. Our model is ready for wet-lab experimentation to assess the efficacy of this putative vaccine candidate.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "A general model of primitive consciousness": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389041701000523/pdfft?md5=f953a50166ebcafadebc61dcf2516287&pid=1-s2.0-S1389041701000523-main.pdf",
                "title": "A general model of primitive consciousness",
                "abstract": "We present a simple model of consciousness as it may exist in animals and can exist in man-made artifacts. The minimum unit of consciousness is a brain/body in interaction with a world. No parts of that system are themselves, conscious. Emphasis is placed on structures that could have evolved from earlier structures by small steps each of which conferred advantage to its possessors. The model is functional, so it becomes possible to build such conscious systems. Indeed, we show why conscious systems should be built as well as how humans should interact with them.",
                "year": 2001,
                "publisher": "Cognitive Systems Research"
            }
        }
    },
    "Style mixup enhanced disentanglement learning for unsupervised domain adaptation in medical image segmentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841524003657/pdfft?md5=c9eb01fe4e6bdbc3134f6e6d1e79a9ca&pid=1-s2.0-S1361841524003657-main.pdf",
                "title": "Style mixup enhanced disentanglement learning for unsupervised domain adaptation in medical image segmentation",
                "abstract": "Unsupervised domain adaptation (UDA) has shown impressive performance by improving the generalizability of the model to tackle the domain shift problem for cross-modality medical segmentation. However, most of the existing UDA approaches depend on high-quality image translation with diversity constraints to explicitly augment the potential data diversity, which is hard to ensure semantic consistency and capture domain-invariant representation. In this paper, free of image translation and diversity constraints, we propose a novel Style Mixup Enhanced Disentanglement Learning (SMEDL) for UDA medical image segmentation to further improve domain generalization and enhance domain-invariant learning ability. Firstly, our method adopts disentangled style mixup to implicitly generate style-mixed domains with diverse styles in the feature space through a convex combination of disentangled style factors, which can effectively improve the model generalization. Meanwhile, we further introduce pixel-wise consistency regularization to ensure the effectiveness of style-mixed domains and provide domain consistency guidance. Secondly, we introduce dual-level domain-invariant learning, including intra-domain contrastive learning and inter-domain adversarial learning to mine the underlying domain-invariant representation under both intra- and inter-domain variations. We have conducted comprehensive experiments to evaluate our method on two public cardiac datasets and one brain dataset. Experimental results demonstrate that our proposed method achieves superior performance compared to the state-of-the-art methods for UDA medical image segmentation.",
                "year": 2025,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "A review of affective computing: From unimodal analysis to multimodal fusion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253517300738/pdfft?md5=822281861315600452fb583096333978&pid=1-s2.0-S1566253517300738-main.pdf",
                "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
                "abstract": "Affective computing is an emerging interdisciplinary research field bringing together researchers and practitioners from various fields, ranging from artificial intelligence, natural language processing, to cognitive and social sciences. With the proliferation of videos posted online (e.g., on YouTube, Facebook, Twitter) for product reviews, movie reviews, political views, and more, affective computing research has increasingly evolved from conventional unimodal analysis to more complex forms of multimodal analysis. This is the primary motivation behind our first of its kind, comprehensive literature review of the diverse field of affective computing. Furthermore, existing literature surveys lack a detailed discussion of state of the art in multimodal affect analysis frameworks, which this review aims to address. Multimodality is defined by the presence of more than one modality or channel, e.g., visual, audio, text, gestures, and eye gage. In this paper, we focus mainly on the use of audio, visual and text information for multimodal affect analysis, since around 90% of the relevant literature appears to cover these three modalities. Following an overview of different techniques for unimodal affect analysis, we outline existing methods for fusing information from different modalities. As part of this review, we carry out an extensive study of different categories of state-of-the-art fusion techniques, followed by a critical analysis of potential performance improvements with multimodal analysis compared to unimodal analysis. A comprehensive overview of these two complementary fields aims to form the building blocks for readers, to better understand this challenging and exciting research field.",
                "year": 2017,
                "publisher": "Information Fusion"
            }
        }
    },
    "Investigating the use of Spatialized Audio Augmented Reality to enhance the outdoor running experience": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Investigating the use of Spatialized Audio Augmented Reality to enhance the outdoor running experience",
                "abstract": "Recent years have seen increasing interest in the use of Virtual Reality technology to enhance the experience of physical exercise activities. When such technologies are used, they are traditionally employed to an indoor setting. In this study, we investigate the use of Spatialized Audio Augmented Reality (AAR) to allow users to compete against their past running records in real time. An experiment study was carried out where 84 participants used the AAR system to compete against a virtual opponent generated using (1) their previous best record, (2) an enhanced version of their previous best record, (3) the previous best record of another user in comparison to (4) a control group. The results showed that while the Augmented Reality conditions did significantly enhance the running performance of users, it did not outperform the traditional training experience to a significant degree. In terms of user experience, participants competing in AAR against their past best record and an enhanced version of their previous best record reported a higher level of tension but did not feel more challenge or competence. After reflecting upon the design of our system, we then highlight several insights regarding the use and design of AAR to promote running related exercise activities.",
                "year": 2023,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "Knowledge modelling for constructing an expert system to support reforestation decisions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/095070519501019X/pdfft?md5=bf40ae3375f4c0f0852db9d81bd1ab9c&pid=1-s2.0-095070519501019X-main.pdf",
                "title": "Knowledge modelling for constructing an expert system to support reforestation decisions",
                "abstract": "The paper describes the inferential model as a domain independent template for the conceptual modelling of a domain. The model consists primarily of a classification of knowledge types expressed in the most general mathematical terms. The paper also presents a systematic technique for knowledge analysis called the inferential modelling technique, which makes use of the inferential model. This is a domain independent, top-down technique which can be used in conjunction with bottom-up techniques such as protocol analysis for analyze elicited expertise in a domain that is unfamiliar to a knowledge engineer. Some results from the application of the inferential modelling technique to the conceptual modelling of a reforestation domain are also presented.",
                "year": 1996,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "A study on traffic signal control at signalized intersections in vehicular ad hoc networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S157087051200039X/pdfft?md5=19a4497ae5de151d5ef11591604d3152&pid=1-s2.0-S157087051200039X-main.pdf",
                "title": "A study on traffic signal control at signalized intersections in vehicular ad hoc networks",
                "abstract": "The Seoul metropolitan government has been operating a traffic signal control system with the name of COSMOS (Cycle, Offset, Split MOdel for Seoul) since 2001. COSMOS analyzes the degrees of saturation and congestion which are calculated by installing loop detectors. At present, subterranean inductive loop detectors are generally used for detecting vehicles but their maintenance is inconvenient and costly. In addition, the estimated queue length might be influenced by errors in measuring speed, because the detectors only consider the speed of passing vehicles. Instead, we proposed a traffic signal control algorithm which enables smooth traffic flow at intersections. The proposed algorithm assigns vehicles to the group of each lane and calculates traffic volume and congestion degree using the traffic information of each group through inter-vehicle communication in Vehicular Ad-hoc Networks (VANETs). This does not require the installation of additional devices such as cameras, sensors or image processing units. In this paper, the algorithm we suggest is verified for AJWT (Average Junction Waiting Time) and TQL (Total Queue Length) under a single intersection model based on the GLD (Green Light District) simulator. The results are better than random control method and best-first control method. For a generalization of the real-time control method with VANETs, this research suggests that the technology of traffic control in signalized intersections using wireless communication will be highly useful.",
                "year": 2013,
                "publisher": "Ad Hoc Networks"
            }
        }
    },
    "Communication for a sustainable society": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0308596180900476/pdfft?md5=a9da5f4b687b6fb454b8b1bb3ce3f9ce&pid=1-s2.0-0308596180900476-main.pdf",
                "title": "Communication for a sustainable society",
                "abstract": "What would be a sustainable society in the year 2000? What communication facilities would it require? Alternatively, what changes in communications could produce a sustainable society? The prevailing scenarios of an ‘information society’ and a ‘wired world’ seem to have followed the second question. They have not defined a sustainable society, but assumed, like the post-industrial theorists, Bell and Kahn, that it would have to be a projection of the main features of post-war social developments. Making this assumption, they have little else to explore but a sort of technical determinism: how new means of communication will transform the factory, the office, the home, etc. The question they have posed is; which developments in communications technology will best enable us to realize, in the shortest time and at least cost, the developmental trends inherent in our societies? The trends most commonly noted are those of maximizing income, maximizing leisure time, maximizing individual choice, and maximizing individual security.",
                "year": 1980,
                "publisher": "Telecommunications Policy"
            }
        }
    },
    "MM-CCNB: Essential protein prediction using MAX-MIN strategies and compartment of common neighboring approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260722006289/pdfft?md5=a814e1396504a163346969d8d0fd45e6&pid=1-s2.0-S0169260722006289-main.pdf",
                "title": "MM-CCNB: Essential protein prediction using MAX-MIN strategies and compartment of common neighboring approach",
                "abstract": "Background and objective\n\nProteins are indispensable for the flow of the life of living organisms. Protein pairs in interaction exhibit more functional activities than individuals. These activities have been considered an essential measure in predicting their essentiality. Neighborhood approaches have been used frequently in the prediction of essentiality scores. All paired neighbors of the essential proteins are nominated for the suitable candidate seeds for prediction. Still now Jaccard's coefficient is limited to predicting functions, homologous groups, sequence analysis, etc. It really motivate us to predict essential proteins efficiently using different computational approaches.",
                "year": 2023,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Exploring feasible design spaces for heterogeneous constraints": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S001044851830397X/pdfft?md5=07f9abc6d32379c5bcb5c547a3d5520a&pid=1-s2.0-S001044851830397X-main.pdf",
                "title": "Exploring feasible design spaces for heterogeneous constraints",
                "abstract": "We demonstrate an approach of exploring design spaces to simultaneously satisfy kinematics- and physics-based requirements. We present a classification of constraints and solvers to enable postponing optimization as far down the design workflow as possible. The solvers are organized into two broad classes of design space ‘pruning’ and ‘exploration’ by considering the types of constraints they can satisfy. We show that pointwise constraints define feasible design subspaces that can be represented and computed as first-class entities by their maximal feasible elements. The design space is pruned upfront by intersecting maximal elements, without premature optimization. To solve for other constraints, we apply topology optimization (TO), starting from the pruned feasible space. The optimization is steered by a topological sensitivity field (TSF) that measures the global changes in violation of constraints with respect to local topological punctures. The TSF for global objective functions is augmented with TSF for global constraints, and penalized/filtered to incorporate local constraints, including set constraints converted to differentiable (in)equality constraints. We demonstrate application of the proposed workflow to nontrivial examples in design and manufacturing. Among other examples, we show how to explore pruned design spaces via TO to simultaneously satisfy physics-based constraints (e.g., minimize compliance and mass) as well as kinematics-based constraints (e.g., maximize accessibility for machining).",
                "year": 2019,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Dynamic scene analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0146664X78800033/pdfft?md5=376f717b137a675656ec6eb2e90a10ec&pid=1-s2.0-S0146664X78800033-main.pdf",
                "title": "Dynamic scene analysis",
                "year": 1978,
                "publisher": "Computer Graphics and Image Processing"
            }
        }
    },
    "Ensuring liveness properties of distributed systems: Open problems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352220817302006/pdfft?md5=d614fadcde57b8cdc18c336d9a2426e0&pid=1-s2.0-S2352220817302006-main.pdf",
                "title": "Ensuring liveness properties of distributed systems: Open problems",
                "abstract": "Often fairness assumptions need to be made in order to establish liveness properties of distributed systems, but in many situations they lead to false conclusions.\n\nThis document presents a research agenda aiming at laying the foundations of a theory of concurrency that is equipped to ensure liveness properties of distributed systems without making fairness assumptions. This theory will encompass process algebra, temporal logic and semantic models. The agenda also includes the development of a methodology and tools that allow successful application of this theory to the specification, analysis and verification of realistic distributed systems.\n\nContemporary process algebras and temporal logics fail to make distinctions between systems of which one has a crucial liveness property and the other does not, at least when assuming justness, a strong progress property, but not assuming fairness. Setting up an alternative framework involves giving up on identifying strongly bisimilar systems, inventing new induction principles, developing new axiomatic bases for process algebras and new congruence formats for operational semantics, and creating matching treatments of time and probability.\n\nEven simple systems like fair schedulers or mutual exclusion protocols cannot be accurately specified in standard process algebras (or Petri nets) in the absence of fairness assumptions. Hence the work involves the study of adequate language or model extensions, and their expressive power.",
                "year": 2019,
                "publisher": "Journal of Logical and Algebraic Methods in Programming"
            }
        }
    },
    "A solution to the facility layout problem using simulated annealing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0166361597001061/pdfft?md5=45f4a3d39c0d5d145d46f8e8104efd60&pid=1-s2.0-S0166361597001061-main.pdf",
                "title": "A solution to the facility layout problem using simulated annealing",
                "abstract": "In this paper a solution in the continual plane to the Facility Layout Problem (FLP) is presented. It is based on Simulated Annealing (SA), a relatively recent algorithm for solving hard combinatorial optimization problems, like FLP. This approach may be applied either in General Facility Layout Problems (GFLP) considering facilities areas, shapes and orientations or in Machine Layout problems (MLP) considering machine's pick-up and drop-off points. It has been applied to real-life situations with useful results, indicating the effectiveness of this approach.",
                "year": 1998,
                "publisher": "Computers in Industry"
            }
        }
    },
    "Implicit slicing for functionally tailored additive manufacturing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S001044851630015X/pdfft?md5=e00d75c20cdd3f8a7332113c4abef6f0&pid=1-s2.0-S001044851630015X-main.pdf",
                "title": "Implicit slicing for functionally tailored additive manufacturing",
                "abstract": "One crucial component of the additive manufacturing software toolchain is a class of geometric algorithms known as “slicers.” The purpose of the slicer is to compute a parametric toolpath and associated commands, which direct an additive manufacturing system to produce a physical realization of a three-dimensional input model. Existing slicing algorithms operate by application of geometric transformations upon the input geometry in order to produce the toolpath. In this paper we introduce a new implicit slicing algorithm based on the computation of toolpaths derived from the level sets of arbitrary heuristics-based or physics-based fields defined over the input geometry. This enables computationally efficient slicing of arbitrarily complex geometries in a straight forward fashion. Additionally, the calculation of component “infill” (as a process control parameter) is explored due to its crucial effect on functional performance fields of interest such as strain and stress distributions. Several examples of the application of the proposed implicit slicer are presented. Finally, an example demonstrating improved structural performance during physical testing is presented. We conclude with remarks regarding the strengths of the implicit approach relative to existing explicit approaches, and discuss future work required in order to extend the methodology.",
                "year": 2016,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Image registration using multi-scale texture moments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/026288569599721C/pdfft?md5=73b4fdb1daa0ca95c6a9941b7f4cd50a&pid=1-s2.0-026288569599721C-main.pdf",
                "title": "Image registration using multi-scale texture moments",
                "abstract": "In this paper we propose a novel, efficient and geometrically intuitive method to compute the four components of an affine transformation from the change in simple statistics of images of texture. In particular, we show how the changes in second circular moments of edge orientation are directly related to the rotation (curl), scale (divergence) and deformation components of an affine transformation, and how these components can be computed from multi-scale texture moments. A simple implementation is described which does not require point, edge or contour correspondences to be established. It is tested on repetitive and non-repetitive visual textures which are neither isotropic nor homogeneous. The theoretical accuracy and the noise sensitivity of this method are compared with other linear moment and circular moment methods.",
                "year": 1995,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Designing utilization-based spatial healthcare accessibility decision support systems: A case of a regional health plan": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923617300891/pdfft?md5=854c42ac01d510ca97a0b3f53a416510&pid=1-s2.0-S0167923617300891-main.pdf",
                "title": "Designing utilization-based spatial healthcare accessibility decision support systems: A case of a regional health plan",
                "abstract": "In the U.S., myriad healthcare reforms have begun to show some positive effects on enabling “potential access”. One facet of healthcare access, “having access”, which is the availability and accessibility of health services for the surrounding populations, has not been adequately addressed. Research regarding “having access” is presently championed by a family of methods called Floating Catchment Area (FCA). However, existing scholarship is limited in integrating non-spatial factors within the FCA methods. In this research, we propose a novel utilization-based framework as the first attempt to adopt the Behavioral Model of Health Services Use as a theoretical lens to integrate non-spatial factors in spatial healthcare accessibility research. The framework employs a unique approach to derive categorical and factor weights for different population subgroup's healthcare needs using predictive analytics. The proposed framework is evaluated using a case study of a regional health plan. A Spatial Decision Support System (SDSS) instantiates the framework and enables decision makers to explore physician shortage areas. The SDSS validates the practicality of the proposed utilization-based framework and subsequently allows other FCA methods to be implemented in real-world applications.",
                "year": 2017,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Impact of atrial fibrillation on left atrium haemodynamics: A computational fluid dynamics study": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482522008514/pdfft?md5=3d16728e042d79d6506a147e738b34d8&pid=1-s2.0-S0010482522008514-main.pdf",
                "title": "Impact of atrial fibrillation on left atrium haemodynamics: A computational fluid dynamics study",
                "abstract": "We analyse the haemodynamics of the left atrium, highlighting differences between healthy individuals and patients affected by atrial fibrillation. The computational study is based on patient-specific geometries of the left atria to simulate blood flow dynamics. We design a novel procedure to compute the boundary data for the 3D haemodynamic simulations, which are particularly useful in absence of data from clinical measurements. With this aim, we introduce a parametric definition of atrial displacement, and we use a closed-loop lumped parameter model of the whole cardiovascular circulation conveniently tuned on the basis of the patient’s characteristics. We evaluate several fluid dynamics indicators for atrial haemodynamics, validating our numerical results in terms of clinical measurements; we investigate the impact of geometric and clinical characteristics on the risk of thrombosis. To highlight the correlation of thrombus formation with atrial fibrillation, according to medical evidence, we propose a novel indicator: age stasis. It arises from the combination of Eulerian and Lagrangian quantities. This indicator identifies regions where slow flow cannot properly rinse the chamber, accumulating stale blood particles, and creating optimal conditions for clots formation.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Estimation of aircraft component production cost using knowledge based engineering techniques": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1474034615000580/pdfft?md5=6310b3563b351830305c83891567ce4c&pid=1-s2.0-S1474034615000580-main.pdf",
                "title": "Estimation of aircraft component production cost using knowledge based engineering techniques",
                "abstract": "A comprehensive method is presented to estimate aircraft component production costs using Knowledge Based Engineering (KBE) techniques. A suite of parametrical cost estimation blocks are treated as Cost Primitives (CPs), which contain attributes such as cost types, cost driving parameters, and cost estimation relationships. A CP is associated with a parameterized geometry and a set of specific design parameters including part/assembly types, materials and production methods. Production cost is estimated by aggregating the cost of different CPs within a tree structure integrating both product breakdown and cost breakdown structures. The cost analysis tool is integrated into a KBE application by building Capability Modules (CMs), which provide manufacturable geometric representations for cost estimation and can be used to summarize output reports for further optimization. Case studies concerning stiffened panels are carried out, verifying the accuracy of the cost estimation method and illustrating the applicability of this method together with the integrated KBE application for various aircraft components. The main contribution of this research is automating the cost integration in the design process to improve the fidelity, repeatability and traceability of cost analysis.",
                "year": 2015,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "Micropower fully integrated CMOS readout interface for neural recording application": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0026271409003631/pdfft?md5=b2c24080e796c8e434b6ac574e64268e&pid=1-s2.0-S0026271409003631-main.pdf",
                "title": "Micropower fully integrated CMOS readout interface for neural recording application",
                "abstract": "In this paper, we presented a micropower, small-size fully integrated CMOS readout interface for neural recording system. A crucial and important module of this system is the amplifier circuit with low-power low-noise. We describe a micropower low-noise readout circuit using an active feedback fully differential structure to reject the 1/f noise and large DC-offsets, the substrate-bias technology to further decrease the noise and power of the neural recording amplifier. Therefore, the neural amplifier with micropower low-noise and high input impedance is presented. The readout interface core, fully differential amplifier is implemented in 0.35-μm CMOS process, passes neural signals from 10 Hz to 9 kHz with an input-referred noise of 4.3 μVrms. The power consumption of single amplifier is 5.6 μW while consuming 0.03 mm2 of die area. The low cutoff frequencies of the circuit can adjusted from 10 Hz to 400 Hz, and the high cutoff frequencies form 4 kHz to 9 kHz.",
                "year": 2010,
                "publisher": "Microelectronics Reliability"
            }
        }
    },
    "ivga: A fast force-directed method for interactive visualization of complex networks": {
        "accordingTo": {
            "scienceDirect": {
                "title": "ivga: A fast force-directed method for interactive visualization of complex networks",
                "abstract": "Complex networks play a very important role in various fields of science as data structures, which aggregate information about mutual relationships between numerous objects. The structural properties of these large graphs can be scrutinized throughout their interactive visualization. However, visual analysis of complex networks consisting of |V| ∼ 106+ vertices represents a great challenge for nowadays computer systems both from computational and storage perspective. Therefore, the existing graph drawing methods involving greater than O(|V|) time and space complexity cannot be regarded as promising tools in the advent of the Big Data era. We present here a new and very fast graph drawing method with O(|V|) time and space complexity – ivga (interactive visualization of graphs). We evaluate its usefulness and performance by testing ivga on the large complex networks from the Stanford Large Network Dataset Collection. We demonstrate that ivga allows for very fast interactive visualization of large graphs consisting of up to a few million vertices on a regular laptop what makes it very competitive to other state-of-art graph drawing methods. Particularly, we recommend ivga method for interactive visualization of large non-planar complex networks such as small-world and scale-free social networks. The main concept of ivga can be seriously considered in developing tools for visualization and analysis of really huge networks, with billions of vertices and edges, on Big Data systems.",
                "year": 2017,
                "publisher": "Journal of Computational Science"
            }
        }
    },
    "SECDA-TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0743731522002301/pdfft?md5=444fdc7e73724f5d9881d162bed2a735&pid=1-s2.0-S0743731522002301-main.pdf",
                "title": "SECDA-TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference",
                "abstract": "In this paper we propose SECDA-TFLite, a new open source toolkit for developing DNN hardware accelerators integrated within the TFLite framework. The toolkit leverages the principles of SECDA, a hardware/software co-design methodology, to reduce the design time of optimized DNN inference accelerators on edge devices with FPGAs. With SECDA-TFLite, we reduce the initial setup costs associated with integrating a new accelerator design within a target DNN framework, allowing developers to focus on the design. SECDA-TFLite also includes modules for cost-effective SystemC simulation, profiling, and AXI-based data communication. As a case study, we use SECDA-TFLite to develop and evaluate three accelerator designs across seven common CNN models and two BERT-based models against an ARM A9 CPU-only baseline, achieving an average performance speedup across models of up to 3.4× for the CNN models and of up to 2.5× for the BERT-based models. Our code is available at https://github.com/gicLAB/SECDA-TFLite.",
                "year": 2023,
                "publisher": "Journal of Parallel and Distributed Computing"
            }
        }
    },
    "Deriving and verifying statistical distribution of a hyperlink-based Web page quality metric": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169023X0300034X/pdfft?md5=e3b391a651050cfa6a64a96177b9fd66&pid=1-s2.0-S0169023X0300034X-main.pdf",
                "title": "Deriving and verifying statistical distribution of a hyperlink-based Web page quality metric",
                "abstract": "The significance of modeling and measuring various attributes of the Web in part or as a whole is undeniable. Modeling information phenomena on the Web constitutes fundamental research towards an understanding that will contribute to the goal of increasing its utility. Although Web related metrics have become increasingly sophisticated, few employ models to explain their measurements. In this paper, we discuss issues related to metrics for Web page significance. These metrics are used for ranking the quality and relevance of Web pages in response to user needs. We focus on the problem of ascertaining the statistical distribution of some well-known hyperlink-based Web page quality metrics. Based on empirical distributions of Web page degrees, we derived analytically the probability distribution for the PageRank metric. We found out that it follows the familiar inverse polynomial law reported for Web page degrees. We verified the theoretical exercise with experimental results that suggest a highly concentrated distribution of the metric.",
                "year": 2003,
                "publisher": "Data & Knowledge Engineering"
            }
        }
    },
    "Reinforcement learning for optimal scheduling of Glioblastoma treatment with Temozolomide": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260719316013/pdfft?md5=a74ebd23cd4b87caa7e759f3a09af817&pid=1-s2.0-S0169260719316013-main.pdf",
                "title": "Reinforcement learning for optimal scheduling of Glioblastoma treatment with Temozolomide",
                "abstract": "Background\n\n: Glioblastoma multiforme (GBM) is the most frequent primary brain tumor in adults and Temozolomide (TMZ) is an effective chemotherapeutic agent for its treatment. In Silico models of GBM growth provide an appropriate foundation for analysis and comparison of different regimens. We propose a mathematical frame for patient specific design of optimal chemotherapy regimens for GBM patients.",
                "year": 2020,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "An extensible software platform for interdisciplinary cardiovascular imaging research": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016926071930104X/pdfft?md5=9e526fbf81551b98b9146c4171f2ff01&pid=1-s2.0-S016926071930104X-main.pdf",
                "title": "An extensible software platform for interdisciplinary cardiovascular imaging research",
                "abstract": "Background and objective\n\nCardiovascular imaging is an exponentially growing field with aspects ranging from image acquisition and analysis to disease characterization, and evaluation of therapy approaches.The transfer of innovative new technological and algorithmic solutions into clinical practice is still slow. In addition to the verification of solutions, their integration in the clinical processing workflow must be enabled for the assessment of clinical impact and risks. The goal of our software platform for cardiac image processing – CAIPI – is to support researchers from different specialties such as imaging physics, computer science, and medicine by a common extensible platform to address typical challenges and hurdles in interdisciplinary cardiovascular imaging research. It provides an integrated solution for method comparison, integrated analysis, and validation in the clinical context. The interface concept enables a combination with existing frameworks that address specific aspects of the pipeline, such as modeling (e.g., OpenCMISS, CARP) or image reconstruction (Gadgetron).",
                "year": 2020,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Sex role stereotyping is hard to kill: A field experiment measuring social responses to user characteristics and behavior in an online multiplayer first-person shooter game": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563214000867/pdfft?md5=00a45fef36acfcb50aae262237a41e82&pid=1-s2.0-S0747563214000867-main.pdf",
                "title": "Sex role stereotyping is hard to kill: A field experiment measuring social responses to user characteristics and behavior in an online multiplayer first-person shooter game",
                "abstract": "Sex role stereotyping by players in first-person shooter games and other online gaming environments may encourage a social environment that marginalizes and alienates female players. Consistent with the social identity model of deindividuation effects (SIDE), the anonymity of online games may engender endorsement of group-consistent attitudes and amplification of social stereotyping, such as the adherence to gender norms predicted by expectations states theory. A 2 × 3 × 2 virtual field experiment (N = 520) in an online first-person shooter video game examined effects of a confederate players’ sex, communication style, and skill on players’ compliance with subsequent online friend requests. We found support for the hypothesis that, in general, women would gain more compliance with friend requests than men. We also found support for the hypothesis that women making positive utterances would gain more compliance with friend requests than women making negative utterances, whereas men making negative utterances would gain more compliance with friend requests than men making positive utterances. The hypothesis that player skill (i.e., game scores) would predict compliance with friend requests was not supported. Implications for male and female game players and computer-mediated communication in online gaming environments are discussed.",
                "year": 2014,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A type-2 neuro-fuzzy system based on clustering and gradient techniques applied to system identification and channel equalization": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494610000876/pdfft?md5=0121ffe4350255a602f2c76e25796137&pid=1-s2.0-S1568494610000876-main.pdf",
                "title": "A type-2 neuro-fuzzy system based on clustering and gradient techniques applied to system identification and channel equalization",
                "abstract": "The integration of fuzzy systems and neural networks has recently become a popular approach in engineering fields for modelling and control of uncertain systems. This paper presents the development of novel type-2 neuro-fuzzy system for identification of time-varying systems and equalization of time-varying channels using clustering and gradient algorithms. It combines the advantages of type-2 fuzzy systems and neural networks. The type-2 fuzzy system allows handling the uncertainties associated with information or data in the knowledge base of the process. The structure of the proposed type-2 TSK fuzzy neural system (FNS) is given and its parameter update rule is derived, based on fuzzy clustering and gradient learning algorithm. The proposed structure is used for identification and noise equalization of time-varying systems. The effectiveness of the proposed system is evaluated by comparing the results obtained by the use of models seen in the literature.",
                "year": 2011,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Determining comprehension and quality of TV programs using eye-gaze tracking": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S003132030700444X/pdfft?md5=d8dee7af5f19943fa2c5652eae871e1b&pid=1-s2.0-S003132030700444X-main.pdf",
                "title": "Determining comprehension and quality of TV programs using eye-gaze tracking",
                "abstract": "Currently, TV programs are evaluated by using questionnaires given after previews or by using TV ratings. There are few objective criteria useful for describing technical know-how about program production. One of the TV program producers’ concerns is how to choose expression methods that convey their ideas to viewers correctly and efficiently. Research has shown that eye-gaze direction is related to the human focus and attention. Gaze-based evaluations have been proposed for image-quality evaluations and certain usability tests. Such approaches are mainly based on how often a specific region attracted the subjects’ gaze or how long their gaze was fixed on it.\n\nTo apply these approaches to TV programs, all the object regions that seem to attract a viewer's gaze need to be specified in advance. This causes several problems including the accuracy of specifying the region by using an image processing technique is not equal to the human subject's recognition ability and it is not feasible to manually specify such regions in an enormous number of frames (images) comprising the program. Further, how characteristics of well-produced TV programs appear on the viewer's gaze has not been objectively analyzed yet. There is a need to investigate the relationship between gaze and program contents which can be used as means for improving comprehension and quality of the TV programs. In this paper, we propose a new measurement and evaluating method for this purpose.\n\nThis paper focuses on the relationship between a viewer's comprehension of a program and their gaze direction in a real experimental TV educational program involving 26 elementary school children and broadcast by NHK Broadcasting Corporation of Japan. Correlation between TV program comprehension and entropy is investigated. That is, variances in the gaze direction in relation to program comprehension are based on a entropy value that represents the degree of dispersion in each frame and is calculated from a probability density function estimated from the gaze directions. The results indicate that the variances of the gaze direction for scenes that gave better comprehension tended to be lower. This tendency was further noticeable after a keyword utterance were related to the answers of corresponding questions.",
                "year": 2008,
                "publisher": "Pattern Recognition"
            }
        }
    },
    "A tutorial review on entropy-based handcrafted feature extraction for information fusion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1566253517303846/pdfft?md5=8f29dbbd4e90c4e5706d23533cc1d658&pid=1-s2.0-S1566253517303846-main.pdf",
                "title": "A tutorial review on entropy-based handcrafted feature extraction for information fusion",
                "abstract": "Entropy (H) is the main subject of this article, concisely written to serve as a tutorial introducing two feature extraction (FE) methods for usage in digital signal processing (DSP) and pattern recognition (PR). The theory, carefully exposed, is supplemented with numerical cases, augmented with C/C++ source-codes and enriched with example applications on restricted-vocabulary speech recognition and image synthesis. Complementarily and as innovatively shown, the ordinary calculation of H corresponds to the outcome of a partially pre-tuned deep neural network architecture which fuses important information, bringing a cutting-edge point-of-view for both DSP and PR communities.",
                "year": 2018,
                "publisher": "Information Fusion"
            }
        }
    },
    "Automatic segmentation and reconstruction of intracellular compartments in volumetric electron microscopy data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260722003418/pdfft?md5=a9c54332e4c4ffa5f3b5faa43867db80&pid=1-s2.0-S0169260722003418-main.pdf",
                "title": "Automatic segmentation and reconstruction of intracellular compartments in volumetric electron microscopy data",
                "abstract": "Background and objectives: In recent years, electron microscopy is enabling the acquisition of volumetric data with resolving power to directly observe the ultrastructure of intracellular compartments. New insights and knowledge about cell processes that are offered by such data require a comprehensive analysis which is limited by the time-consuming manual segmentation and reconstruction methods.\n\nMethod: We present methods for automatic segmentation, reconstruction, and analysis of intracellular compartments from volumetric data obtained by the dual-beam electron microscopy. We specifically address segmentation of fusiform vesicles and the Golgi apparatus, reconstruction of mitochondria and fusiform vesicles, and morphological analysis of the reconstructed mitochondria.\n\nResults and conclusion: Evaluation on the public UroCell dataset demonstrated high accuracy of the proposed methods for segmentation of fusiform vesicles and the Golgi apparatus, as well as for reconstruction of mitochondria and analysis of their shapes, while reconstruction of fusiform vesicles proved to be more challenging. We published an extension of the UroCell dataset with all of the data used in this work, to further contribute to research on automatic analysis of the ultrastructure of intracellular compartments.",
                "year": 2022,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Semi 3D-TENet: Semi 3D network based on temporal information extraction for coronary artery segmentation from angiography video": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809421004912/pdfft?md5=39782d6bff6fd1964d39dabdac77f9a5&pid=1-s2.0-S1746809421004912-main.pdf",
                "title": "Semi 3D-TENet: Semi 3D network based on temporal information extraction for coronary artery segmentation from angiography video",
                "abstract": "Coronary artery interventional therapy is a clinically effective minimally invasive surgery for coronary artery disease. Extracting effective coronary vascular structures from coronary angiography videos is essential for the safe navigation of coronary interventional equipment and for the doctor to observe the location of the lesion. This paper proposes a new semi 3D architecture that uses the temporal information of video to segment coronary arteries from angiography video. We combine the 3D U-Net and 2D U-Net through a dimension conversion layer and a context extracting module. The input of the 3D encoder is a set of coronary video sequences. After the extracted three-dimensional features pass through the dimension conversion layer and the context information extraction module, the valuable features are input into the 2D decoder module. Finally, a clearer and more complete coronary is extracted to help the doctor to observe the vascular status better. We tested this method and the comparison methods on the coronary angiography video data set we made before. We can see from the experimental results that even in coronary angiography video sequences with poor quality, our method can achieve better results than the other methods. The accuracy of our results can reach 98.60%, which shows that in the vessel video segmentation task, the extraction of temporal information is helpful to extract a more complete vascular structure.",
                "year": 2021,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Specifying and identifying widely used crosscutting concerns": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705117301703/pdfft?md5=f351d7e444f5dc36dd4aecb1bde474ee&pid=1-s2.0-S0950705117301703-main.pdf",
                "title": "Specifying and identifying widely used crosscutting concerns",
                "abstract": "To ensure quality of code in large software systems, refactoring operations are the norm. With this goal in mind, it is helpful to direct preventive maintenance of large object-oriented legacy systems towards the more advanced aspect-oriented paradigm. However, manually identifying crosscutting concerns, candidate to be rendered as aspects, in large legacy systems can be an overwhelming task. We propose an approach and provide a tool to identify crosscutting concerns embedded in an object-oriented system. Our approach analyses the implementation of classes and their relationships and looks for specific user-defined schemata. We discuss how to build schemata and provide some general ones, devised in accordance with current aspect-oriented programming practices.\n\nCompared to existing approaches, our solution is flexible and achieves higher precision in detecting crosscutting concerns. Moreover, it provides the developer with an accurate identification of snippets of code which can be removed from classes and embedded into aspects. The proposed solution has been validated by applying it to a case study, i.e., a large extant software system, suitably selected to assess the quality attained by the aspect-oriented version produced with the assistance of our tool, in comparison with a design performed by expert human developers.",
                "year": 2017,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Robust multi-modal COVID-19 medical image registration using dense deep learning descriptor model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809424010656/pdfft?md5=f7929d63a1e38e94b412a63781a72f4a&pid=1-s2.0-S1746809424010656-main.pdf",
                "title": "Robust multi-modal COVID-19 medical image registration using dense deep learning descriptor model",
                "abstract": "In medical image processing, multi-modal medical image registration is a challenging task due to the varied image characteristics. Because of the Non-functional strength relation and the erratic intricate deformations among images. To overcome these issues, this paper proposed an enhanced residual dense learning data descriptor for multi-modal COVID-19 image registration. In this work, input images are taken from the COVID-19 X-ray and CT Chest Images Dataset. Initially, the input images are pre-processed using the boosted switching bilateral filter (BSBF), in which the best median value is examined using a Sorted Quadrant Median Vector (SQMV). Then, the Directed Edge Enhancer (DEE) algorithm is used for the edge enhancement process. These pre-processed images are provided as the input of a deep learning based multi-scale feature extraction module to diminish the mutual interference of features and make it easier to train the network model. Data Adaptive Descriptor (DAD) is provided for structural representation, and the self-similarity metrics of the reference and floating images are examined by the Sum of Squared Differences (SSD). The goal function for image registration is made to the final deformation field based on SSD. Here, the simulation is performed by using a Python tool. The accuracy value of the proposed method in the COVID-19 X-ray and CT Chest images dataset is 96%, and the MSE value is 0.03%. Compared with other existing methods, our proposed method produces better performance. The proposed model is more efficient by using the hybrid deep learning methodology.",
                "year": 2025,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Short-term plasticity in a computational model of the tail-withdrawal circuit in Aplysia": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S092523120600422X/pdfft?md5=4b0956a63d4f4852ffce35993c3ccb10&pid=1-s2.0-S092523120600422X-main.pdf",
                "title": "Short-term plasticity in a computational model of the tail-withdrawal circuit in Aplysia",
                "abstract": "The tail-withdrawal circuit of Aplysia provides a useful model system for investigating synaptic dynamics. Sensory neurons within the circuit manifest several forms of synaptic plasticity. Here, we developed a model of the circuit and investigated the ways in which depression (DEP) and potentiation (POT) contributed to information processing. DEP limited the amount of motor neuron activity that could be elicited by the monosynaptic pathway alone. POT within the monosynaptic pathway did not compensate for DEP. There was, however, a synergistic interaction between POT and the polysynaptic pathway. This synergism extended the dynamic range of the network, and the interplay between DEP and POT made the circuit responded preferentially to long-duration, low-frequency inputs.",
                "year": 2007,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Implementing ERP through SAP R/3: A Process Change Management (PCM) Perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157802800027/pdfft?md5=ec3fc26e4d7d0638ac01a17e9134f09c&pid=1-s2.0-S1319157802800027-main.pdf",
                "title": "Implementing ERP through SAP R/3: A Process Change Management (PCM) Perspective",
                "abstract": "Enterprise resource planning (ERP) systems are increasingly considered as the technical platform for many business process change and improvement projects. Current research theories that have been adopted to understand the ERP phenomenon do not directly and comprehensively address the change issues involved in ERP implementation. This paper suggests the use of a process change management (PCM) perspective to explore the ERP phenomenon. A framework is therefore adopted to highlight the various PCM constructs in the context of SAP R/3 implementation. Evidence on how these constructs are operationalized in practice is drawn from a large collection of R/3 case studies representing various organizational experiences. The paper provides foundation and recommends several ideas for future research and investigation.",
                "year": 2002,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "The net, the web, and the children": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0065245801800267/pdfft?md5=39aa7e9c9084075163f8af4795cd267f&pid=1-s2.0-S0065245801800267-main.pdf",
                "title": "The net, the web, and the children",
                "abstract": "The Internet and the World Wide Web are explained as simply evolutionary developments of human communication and human information encoding systems. A case is made that the use of the Net and the Web is associated with, if not assisting, the accelerating gap between the rich and the poor nations of the world, and between the rich and the poor within nations. Those most severely disadvantaged by all this digital technology are the world's children. They are also the ones that could benefit most, and suggestions are made for changes that could help bring these benefits about.",
                "year": 2002,
                "publisher": "Advances in Computers"
            }
        }
    },
    "Can machine learning explain human learning?": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231216003337/pdfft?md5=833caa69fc5a41c150c984dc3da6cc38&pid=1-s2.0-S0925231216003337-main.pdf",
                "title": "Can machine learning explain human learning?",
                "abstract": "Learning Analytics (LA) has a major interest in exploring and understanding the learning process of humans and, for this purpose, benefits from both Cognitive Science, which studies how humans learn, and Machine Learning, which studies how algorithms learn from data. Usually, Machine Learning is exploited as a tool for analyzing data coming from experimental studies, but it has been recently applied to humans as if they were algorithms that learn from data. One example is the application of Rademacher Complexity, which measures the capacity of a learning machine, to human learning, which led to the formulation of Human Rademacher Complexity (HRC). In this line of research, we propose here a more powerful measure of complexity, the Human Algorithmic Stability (HAS), as a tool to better understand the learning process of humans. The experimental results from three different empirical studies, on more than 600 engineering students from the University of Genoa, showed that HAS (i) can be measured without the assumptions required by HRC, (ii) depends not only on the knowledge domain, as HRC, but also on the complexity of the problem, and (iii) can be exploited for better understanding of the human learning process.",
                "year": 2016,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Polar Hermeneutics: A Multi-faceted Decision Mechanism": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050912002785/pdfft?md5=835757c9c96347b553f36d0cb96033b0&pid=1-s2.0-S1877050912002785-main.pdf",
                "title": "Polar Hermeneutics: A Multi-faceted Decision Mechanism",
                "abstract": "Social simulation inherently involves social actors, at various levels, that make a wide range of decisions. However, the decision mechanisms used by most research programs have often been intuitive, ad hoc and/or overly simplistic. In contrast, in all cultures and times, diverse social actors are required to make decisions in complex circumstances. For simulative decision mechanisms to be credible, the complex nature of social processes needs to be recognized and represented. In order to create a more credible architecture for social simulation, the present paper introduces a rich, expressive decision process based upon a composite mechanism: polar hermeneutics.",
                "year": 2012,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Smartphone sensor data as digital evidence": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404813000527/pdfft?md5=c49e523da5dec9df564764a7602f09ad&pid=1-s2.0-S0167404813000527-main.pdf",
                "title": "Smartphone sensor data as digital evidence",
                "abstract": "The proliferation of smartphones introduces new opportunities in digital forensics. One of the reasons is that smartphones are usually equipped with sensors (e.g. accelerometer, proximity sensor, etc.), hardware which can be used to infer the user's context. This context may be useful in a digital investigation, as it can aid in the rejection or acceptance of an alibi, or even reveal a suspect's actions or activities. Nonetheless, sensor data are volatile, thus are not available in post-mortem analysis. Thus, the only way to timely acquire them, in case such a need arises during a digital investigation, is by software that collects them when they are generated by the suspect's actions. In this paper we examine the feasibility of ad-hoc data acquisition from smartphone sensors by implementing a device agent for their collection in Android, as well as a protocol for their transfer. Then, we discuss our experience regarding the data collection of smartphone sensors, as well as legal and ethical issues that arise from their collection. Finally, we describe scenarios regarding the agent's preparation and use in a digital investigation.",
                "year": 2013,
                "publisher": "Computers & Security"
            }
        }
    },
    "BG: A scalable benchmark for interactive social networking actions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X17323373/pdfft?md5=8572763e26c26276e303759729094424&pid=1-s2.0-S0167739X17323373-main.pdf",
                "title": "BG: A scalable benchmark for interactive social networking actions",
                "abstract": "BG is a benchmark that rates a data store for processing interactive social networking actions such as view a member profile and extend a friend invitation to a member. It elevates the amount of stale, inconsistent, and erroneous (termed unpredictable) data produced by a data store to a first class metric, quantifying it as a part of the benchmarking phase. It summarizes the performance of a data store in one metric, Social Action Rating (SoAR). SoAR is defined as the highest throughput provided by a data store while satisfying a pre-specified service level agreement, SLA.\n\nTo rate the fastest data stores, BG scales both vertically and horizontally, generating a higher number of requests per second as a function of additional CPU cores and nodes. This is realized using a shared-nothing architecture in combination with two multi-node execution paradigms named Integrated DataBase (IDB) and Disjoint DataBase (D2B). An evaluation of these paradigms shows the following tradeoffs. While the D2B scales superlinearly as a function of nodes, it may not evaluate data stores that employ client-side caching objectively. IDB provides two alternative execution paradigms, Retain and Delegate, that might be more appropriate. However, they fail to scale as effectively as D2B. We show elements of these two paradigms can be combined to realize a hybrid framework that scales almost as effectively as D2B while exercising the capabilities of certain classes of data stores as objectively as IDB.",
                "year": 2018,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Agent-based Approach to WEB Exploration Process": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050915010716/pdfft?md5=324629a7e90a7aa34bbb34d097263a60&pid=1-s2.0-S1877050915010716-main.pdf",
                "title": "Agent-based Approach to WEB Exploration Process",
                "abstract": "The paper contains the concept of agent-based search system and monitoring of Web pages. It is oriented at the exploration of limited problem area, covering a given sector of industry or economy. The proposal of agent-based (modular) structure of the system is due to the desire to ease the introduction of modifications or enrichment of its functionality. Commonly used search engines do not offer such a feature.\n\nThe second part of the article presents a pilot version of the WEB mining system, represent- ing a simplified implementation of the previously presented concept. Testing of the implemented application was executed by referring to the problem area of foundry industry.",
                "year": 2015,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Structure based virtual screening, molecular dynamic simulation to identify the oxadiazole derivatives as inhibitors of Enterococcus D-Ala-D-Ser ligase for combating vancomycin resistance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482523004304/pdfft?md5=88116318d0bc2fe7013fc3bd2768703c&pid=1-s2.0-S0010482523004304-main.pdf",
                "title": "Structure based virtual screening, molecular dynamic simulation to identify the oxadiazole derivatives as inhibitors of Enterococcus D-Ala-D-Ser ligase for combating vancomycin resistance",
                "abstract": "Vancomycin resistance in enterococci mainly arises due to alteration in terminal peptidoglycan dipeptide. A comprehensive structural analysis for substrate specificity of dipeptide modifying d-Alanine: d-Serine ligase (Ddls) is essential to screen its inhibitors for combating vancomycin resistance. In this study modeled 3D structure of EgDdls from E. gallinarum was used for structure based virtual screening (SBVS) of oxadiazole derivatives. Initially, fifteen oxadiazole derivatives were identified as inhibitors at the active site of EgDdls from PubChem database. Further, four EgDdls inhibitors were evaluated using pharmacokinetic profile and molecular docking. The results of molecular docking showed that oxadiazole inhibitors could bind preferentially at ATP binding pocket with the lowest binding energy. Further, molecular dynamics simulation results showed stable behavior of EgDdls in complex with screened inhibitors. The residues Phe172, Lys174, Glu217, Phe292, and Asn302 of EgDdls were mainly involved in interactions with screened inhibitors. Furthermore, MM-PBSA calculation showed electrostatic and van der Waals interactions mainly contribute to overall binding energy. The PCA analysis showed motion of central domain and omega loop of EgDdls. This is involved in the formation of native dipeptide and stabilized after binding of 2-(1-(Ethylsulfonyl) piperidin-4-yl)-5-(furan-2-yl)-1,3,4-oxadiazole, which could be reason for the inhibition of EgDdls. Hence, in this study we have screened inhibitors of EgDdls which could be useful to alleviate the vancomycin resistance problem in enterococci, involved in hospital-acquired infections, especially urinary tract infections (UTI).",
                "year": 2023,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "BITAM: An engineering-principled method for managing misalignments between business and IT architectures": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642304001881/pdfft?md5=b8d2b702bedd41545f7a278497f64965&pid=1-s2.0-S0167642304001881-main.pdf",
                "title": "BITAM: An engineering-principled method for managing misalignments between business and IT architectures",
                "abstract": "As the rates of business and technological changes accelerate, misalignments between business and IT architectures are inevitable. Existing alignment models, while important for raising awareness of alignment issues, have provided little in the way of guidance for actually correcting misalignment and thus achieving alignment. This paper introduces the BITAM (Business IT Alignment Method) which is a process that describes a set of twelve steps for managing, detecting and correcting misalignment. The methodology is an integration of two hitherto distinct analysis areas: business analysis and architecture analysis. The BITAM is illustrated via a case study conducted with a Fortune 100 company.",
                "year": 2005,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Assessing growth potential of careers with occupational mobility network and ensemble framework": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197623014902/pdfft?md5=7ae95fea3430c2067090c3eaad2a20ea&pid=1-s2.0-S0952197623014902-main.pdf",
                "title": "Assessing growth potential of careers with occupational mobility network and ensemble framework",
                "abstract": "The growth potential of a career reflects its future prospects and is an important consideration for individuals and organizations when career planning. There is still a lack of quantitative assessment tools for growth potential of careers. In this study, considering the key role of human capital in human resource management, as well as the excellent performance of complex network and machine learning in big data analysis and prediction, a career growth potential assessment model with human capital ensemble is proposed through human capital-based occupational mobility network and ensemble learning. First, an occupational mobility network is constructed based on online professional dataset to associate occupations with each other. Then, five dimensions of human capital measurements are designed to quantify human capital in terms of education, experience, social capital, occupational size, and concentration. These are then combined with the occupational mobility network to create a new network that depicts human capital flows among occupations. Finally, an ensemble framework for assessing career growth potential is constructed to integrate multidimensional human capital information in the network and obtain quantitative scores of growth potential. This study is the original attempt to adopt a data-driven idea and an intelligent approach to understand career growth potential. The experimental results show that it also makes a useful exploration for modeling human capital flows and intelligent assessment of career prospects.",
                "year": 2024,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Context granulation and subjective-information quantification": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0304397512011498/pdfft?md5=f3e42829919767e4aec3b95654e2152c&pid=1-s2.0-S0304397512011498-main.pdf",
                "title": "Context granulation and subjective-information quantification",
                "abstract": "In the course of comprehension, the human brain makes astounding inferences despite insufficient and ambiguous data—the data mostly being ‘words’ that frame sentences. Can this neural process of “abstraction of the relevant” be sufficiently modeled such that the computer may be induced to perform the same? This article is our response to this very question; a treatise on our attempt at engineering the basis of such a model, or rather, a methodology of ‘relevance cognition’. Drawing inspiration from the way children learn languages, we propose here a corpus and entropy-based methodology that follows a subjectivist approach to granulate and identify statements of consequence in a natural language sample, all within the periphery of a particular context. Besides promoting language grasping abilities of a machine–through simulation of the intuitive process of relevant-information segregation–the methodology aims at reducing overall text-processing costs. The suggested scheme considers ‘conceptual keywords’ to be the basis of sentential understanding and utilizes the principles of Bayes’ conditional probability and Shannon’s entropy theorem. Experimental results have been provided to substantiate our logic. Though this article has been formulated over the backdrop of the Z-number approach to computing with words (CWW), it nevertheless applies to research areas in natural language processing (NLP) like text summarization, semantic disambiguation and concept graph generation.",
                "year": 2013,
                "publisher": "Theoretical Computer Science"
            }
        }
    },
    "Shape from texture: Estimation, isotropy and moments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/000437029090011N/pdfft?md5=bea4920adb82b3e1fe5fd574de3199f6&pid=1-s2.0-000437029090011N-main.pdf",
                "title": "Shape from texture: Estimation, isotropy and moments",
                "abstract": "A theory is proposed for the interpretation of 3D textures with oriented elements. It builds on two previous theories: a statistical one due to Witkin, and Kanatani's “Buffon” transform. We show that the statistical estimation of orientation of a textured plane is one of a general family of optimal backprojection problems which also includes “maximal compactness”. General uniqueness and convergence results are obtained for an efficient algorithm employing “second moment feedback”. This framework greatly enhances the usefulness of surface orientation estimation for texture in two ways. First, an error distribution is derived for the estimator, and this is crucial for integration of shape information. Second, hypotheses about intrinsic texture statistics can be verified by a χ2 test which, if failed, warns that the orientation estimator is not to be believed. Finally, it is argued that these results suggest a general moment-tensor approach to analysis of 3D texture.",
                "year": 1990,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Describing the functional spatial structure of urban environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0198971513000549/pdfft?md5=839e201121f2cada54981de92ef681f5&pid=1-s2.0-S0198971513000549-main.pdf",
                "title": "Describing the functional spatial structure of urban environments",
                "abstract": "People learn the layout of cities mainly through a series of trips. Wayfinders experience the city structure differently depending on the mode of transport they use. The acquired mental representation then consists of the directly observed, physically accessible parts of the city. In this paper, we propose a computational model to construct images of cities, adapting their content depending on the wayfinder’s access constraints. First, we formally study and extend the classification of Lynch’s elements of the city form. Second, we propose a simple approach to analyzing local functional relationships between these elements, as experienced by wayfinders. The study of the functional relationships allows for the construction of the most complete image of the city that might be acquired by a wayfinder with given accessibility characteristics. These representations of urban environments can support advanced spatial assistance systems and cognitively efficient spatial interfaces.",
                "year": 2013,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "A deep semantic search method for random tweets": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2468696419300382/pdfft?md5=f6811235fffae7c5dd1b83d6586eb1cb&pid=1-s2.0-S2468696419300382-main.pdf",
                "title": "A deep semantic search method for random tweets",
                "abstract": "Contemporary social media platforms enable users to act as both producers and consumers of content, leading to the generation of enormous amounts of data. While this ability is empowering, it is also posing many challenges concerning efficient searches for relevant information. Many search approaches have been proposed in the literature. However, searching for information on Twitter is particularly challenging due to both the inconsistency in writing styles and the high generation rate of spurious and duplicate content. The quest for instant and efficient data processing to retrieve relevant information renders many existing techniques ineffective when applied to Twitter.\n\nWe present a multilevel approach based on state-of-the-art deep learning methods and a novel scalable windowing approach for pairwise-similarity search (SWAPS) to improve search efficiency. SWAPS optimises searches using a strategic balancing criterion to assess the trade-off between accuracy and search speed, thereby circumnavigating sequential search problems. Moreover, we propose a deep search strategy that establishes a relationship between the status of a tweet and its longevity measured in terms of engagement lifespan since posting. Deep search utilises a convolutional neural network for textual n-grams features extraction and meta-features from the tweet to train a fully connected network on a vast number of tweets. This approach differs from existing ones by recognising the relationship between the status of a tweet and its engagement lifespan to ensure a better understanding of the compositional semantics in tweets. The results highlight interesting symmetrical properties with respect to similarity distribution and duration. We evaluate our approach on various benchmark datasets and demonstrate the efficacy and applicability of the method. Problems of event detection, clustering and ads, among others, can utilise this approach to detect items of interest effectively.",
                "year": 2019,
                "publisher": "Online Social Networks and Media"
            }
        }
    },
    "Application of a cascade BAM neural expert system to conceptual design for facility layout": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0898122198002454/pdfft?md5=31af0d5893f0f0d71ecc6cf4155e4eb8&pid=1-s2.0-S0898122198002454-main.pdf",
                "title": "Application of a cascade BAM neural expert system to conceptual design for facility layout",
                "abstract": "The major contribution of this novel application is the pilot development and feasibility study for a bank of cascade BAM (Bidirectional Associative Memories) neural networks. This improved BAM structure functions as an expert system for conceptual facility layout or for preliminary construction layout design. This application, rather than being a better analytical algorithm or a better production expert system, builds a neural expert system with the capability of incrementally learning layout design examples for a given set of constraints. The cascade BAM incremental learning methodology, which distinguishes this system from the more frequently used Backpropagation Network (BPN) learning system, creates effective multibidirectional generalization behavior from qualitative, goal-driven layout design experience. The initial tests of learnability are presented by its applicability to conceptual layout design problems, and their solutions are assessed and compared with the learning ability of a standard BAM. Issues deserving further investigation are addressed as well.",
                "year": 1999,
                "publisher": "Computers & Mathematics with Applications"
            }
        }
    },
    "The effects of team co-location and reduced crewing on team communication characteristics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687019301073/pdfft?md5=dc81fa0c1ae075a36c818f5388e3ada9&pid=1-s2.0-S0003687019301073-main.pdf",
                "title": "The effects of team co-location and reduced crewing on team communication characteristics",
                "abstract": "The manner in which control rooms are configured can impact the flow of information between command teams. Previous research revealed bottlenecks of communications between the Sonar Controller (SOC) and the Operations Officer (OPSO) in submarine control rooms. One way to relieve such bottlenecks is to co-locate operators reliant on one another for task relevant information. The aim of the current studies was to use multiple command teams to empirically examine a novel submarine control room configuration and a reduced crew size in comparison to a baseline of contemporary operations to see if such bottlenecks could be removed. Ten teams performed high and low demand Dived Tracking (DT) scenarios in a simulated submarine control room. Activities and communications of the teams were recorded and quantified using the Event Analysis of Systemic Teamwork (EAST) method affording statistical comparisons with a baseline condition of contemporary operations. The findings showed that the co-location of operators relieved the bottleneck of communications between the SOC and the OPSO. Although overall communications increased, this was more balanced across the team and was more adaptive to scenario demand. This was coupled with a significant increase in task completion, even with a reduced crew size, suggesting greater efficiency and productivity. Future research should seek to validate the changes observed with objective measures of task performance.",
                "year": 2019,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Automatic fault tracking based on ant colony algorithms": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0098300412002804/pdfft?md5=a2428043b5e93aebf125590f9639fb99&pid=1-s2.0-S0098300412002804-main.pdf",
                "title": "Automatic fault tracking based on ant colony algorithms",
                "abstract": "The mapping of the fault network is of key importance in reservoir characterization. Interpretation of faults in seismic data is today most commonly a manual task. This is time consuming and difficult to do accurately. Automatic extraction of the fault surfaces would allow user-interaction at a higher level.\n\nIn this paper, we implemented and tested an automatic fault tracking scheme based on the ant colony algorithm. We applied the proposed method to auto-track the faults on seismic data. Its performance on real seismic data indicates that the proposed method can effectively reduce the noise level and improve the continuity of faults on seismic coherency cube. In order to assess the performance of the auto-tracking for faults, we make a comparison of the results from the auto-tracking with manual interpretations of fault geometry over the same area. The comparison demonstrates that the proposed method detects most faults accurately.",
                "year": 2013,
                "publisher": "Computers & Geosciences"
            }
        }
    },
    "Computer graphics: A keyword-indexed bibliography for the years 1976, 1977, and 1978": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0146664X80800037/pdfft?md5=645bf82807a09b2ef12f3d9a6300507c&pid=1-s2.0-S0146664X80800037-main.pdf",
                "title": "Computer graphics: A keyword-indexed bibliography for the years 1976, 1977, and 1978",
                "abstract": "References relevant to computer graphics and which appeared in the years 1976, 1977, and 1978 are presented. An extensive keyword index section to aid in locating papers of interest is included.",
                "year": 1980,
                "publisher": "Computer Graphics and Image Processing"
            }
        }
    },
    "How prior knowledge affects problem-solving performance in a medical simulation game: Using game-logs and eye-tracking": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S074756321930216X/pdfft?md5=cace66ef2d54c01aa76652a8f9914cf5&pid=1-s2.0-S074756321930216X-main.pdf",
                "title": "How prior knowledge affects problem-solving performance in a medical simulation game: Using game-logs and eye-tracking",
                "abstract": "Computer-based simulation games provide an environment to train complex problem-solving skills. Yet, it is largely unknown how the in-game performance of learners varies with different levels of prior knowledge. Based on theories of complex-skill acquisition (e.g., 4C/ID), we derive four performance aspects that prior knowledge may affect: (1) systematicity in approach, (2) accuracy in visual attention and motor reactions, (3) speed in performance, and (4) cognitive load. This study aims to empirically test whether prior knowledge affects these four aspects of performance in a medical simulation game for resuscitation skills training. Participants were 24 medical professionals (experts, with high prior knowledge) and 22 medical students (novices, with low prior knowledge). After pre-training, they all played one scenario, during which game-logs and eye-movements were collected. A cognitive-load questionnaire ensued. During game play, experts demonstrated a more systematic approach, higher accuracy in visual selection and motor reaction, and a higher performance speed than novices. Their reported levels of cognitive load were lower. These results indicate that prior knowledge has a substantial impact on performance in simulation games, opening up the possibility of using our measures for performance assessment.",
                "year": 2019,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Toward a structural textural analyzer based on statistical methods": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0146664X80900131/pdfft?md5=66c3075fc7c3c70be182c1d8c209fe1d&pid=1-s2.0-0146664X80900131-main.pdf",
                "title": "Toward a structural textural analyzer based on statistical methods",
                "abstract": "This paper reports investigations aimed at developing a feature set for the Spatial Gray Level Dependence Method (SGLDM) which measures visually perceivable qualities of textures. In particular it will be shown that the inertia measure commonly used with the SGLDM can be used to characterize the placement rules and the unit pattern of periodic textures. In this way one may formulate a structural approach to texture analysis based on the statistical SGLDM. To mathematically verify that the features used with the SGLDM can be used to characterize the unit pattern and placement rules of a periodic texture, a mathematical tiling theory model is proposed. This model allows one to develop the mathematical machinery necessary to prove the result. In a companion paper other features which measure visually perceivable qualities of patterns will be developed. The reason for concentrating on the SGLDM for developing such a feature set is predicated on perceptual psychology experiments and comparison studies of various texture algorithms. All of these studies indicate that second-order probabilities of the type measured by the spatial gray level dependence matrices are important in human texture discrimination and that these matrices contain more important texture-context information than the intermediate matrices of other statistical texture analysis algorithms.",
                "year": 1980,
                "publisher": "Computer Graphics and Image Processing"
            }
        }
    },
    "Automated construction of bounded-loss imperfect-recall abstractions in extensive-form games": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0004370220300126/pdfft?md5=f61d3d0c9217b1bd7a417981169c733f&pid=1-s2.0-S0004370220300126-main.pdf",
                "title": "Automated construction of bounded-loss imperfect-recall abstractions in extensive-form games",
                "abstract": "Extensive-form games (EFGs) model finite sequential interactions between players. The amount of memory required to represent these games is the main bottleneck of algorithms for computing optimal strategies and the size of these strategies is often impractical for real-world applications. A common approach to tackle the memory bottleneck is to use information abstraction that removes parts of information available to players thus reducing the number of decision points in the game. However, existing information-abstraction techniques are either specific for a particular domain, they do not provide any quality guarantees, or they are applicable to very small subclasses of EFGs. We present domain-independent abstraction methods for creating imperfect recall abstractions in extensive-form games that allow computing strategies that are (near) optimal in the original game. To this end, we introduce two novel algorithms, FPIRA and CFR+IRA, based on fictitious play and counterfactual regret minimization. These algorithms can start with an arbitrary domain specific, or the coarsest possible, abstraction of the original game. The algorithms iteratively detect the missing information they require for computing a strategy for the abstract game that is (near) optimal in the original game. This information is then included back into the abstract game. Moreover, our algorithms are able to exploit imperfect-recall abstractions that allow players to forget even history of their own actions. However, the algorithms require traversing the complete unabstracted game tree. We experimentally show that our algorithms can closely approximate Nash equilibrium of large games using abstraction with as little as 0.9% of information sets of the original game. Moreover, the results suggest that memory savings increase with the increasing size of the original games.",
                "year": 2020,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Interface Terminologies: Facilitating Direct Entry of Clinical Data into Electronic Health Record Systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1067502706000259/pdfft?md5=b8214bc9973a013aca8f8d2334234a30&pid=1-s2.0-S1067502706000259-main.pdf",
                "title": "Interface Terminologies: Facilitating Direct Entry of Clinical Data into Electronic Health Record Systems",
                "abstract": "Previous investigators have defined clinical interface terminology as a systematic collection of health care–related phrases (terms) that supports clinicians' entry of patient-related information into computer programs, such as clinical “note capture” and decision support tools. Interface terminologies also can facilitate display of computer-stored patient information to clinician-users. Interface terminologies “interface” between clinicians' own unfettered, colloquial conceptualizations of patient descriptors and the more structured, coded internal data elements used by specific health care application programs. The intended uses of a terminology determine its conceptual underpinnings, structure, and content. As a result, the desiderata for interface terminologies differ from desiderata for health care–related terminologies used for storage (e.g., SNOMED-CT®), information retrieval (e.g., MeSH), and classification (e.g., ICD9-CM®). Necessary but not sufficient attributes for an interface terminology include adequate synonym coverage, presence of relevant assertional knowledge, and a balance between pre- and post-coordination. To place interface terminologies in context, this article reviews historical goals and challenges of clinical terminology development in general and then focuses on the unique features of interface terminologies.",
                "year": 2006,
                "publisher": "Journal of the American Medical Informatics Association"
            }
        }
    },
    "An odyssey of virtual global team activity in the experiential learning environment of the Global Enterprise Experience (GEE)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563218305168/pdfft?md5=b5f19d8b9aa731e5d51ee963d38642d1&pid=1-s2.0-S0747563218305168-main.pdf",
                "title": "An odyssey of virtual global team activity in the experiential learning environment of the Global Enterprise Experience (GEE)",
                "abstract": "What challenges present themselves in the virtual team experiential learning environment for students and educators? More so, have these challenges morphed over time? This paper outlines a longitudinal content analysis of student experiences in an experiential learning activity (2007–2016). The Global Enterprise Experience (GEE) is designed to support integrated learning for business content and global competencies, including leadership skills, in a global virtual team (GVT) context. Students, in teams comprising eight diverse members, engage to achieve the development of a business plan. They, themselves, determine their use of communication technologies. This longitudinal study explores the challenges comprising a learner's experiences and, in so doing, implicitly explores the educator's role within a student-focussed approach. It contributes to literature focussing on the development of individuals' GVT competences, characterising the experience of this learning space. In this way, it offers pragmatic insight to educators regarding how to direct efforts to support learning. It concludes with suggestions for future research.",
                "year": 2020,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A structural model for chorismate synthase from Mycobacterium tuberculosis in complex with coenzyme and substrate": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482506000084/pdfft?md5=7dccdd72932502bf543b66a44d0ab692&pid=1-s2.0-S0010482506000084-main.pdf",
                "title": "A structural model for chorismate synthase from Mycobacterium tuberculosis in complex with coenzyme and substrate",
                "abstract": "The enzymes of the shikimate pathway constitute an excellent target for the design of new antibacterial agents; chorismate synthase (CS) catalyzes the last step of this pathway. The prediction of Mycobacterium tuberculosis (MTB) CS three-dimensional structure and the geometric docking of the coenzyme FMN and the substrate EPSP were performed using the crystal structure of CS from Streptococcus pneumoniae as template. Energy minimization of the whole complex showed, as expected, that most of the template interactions are preserved in the MTB structure, except for HIS11, ARG139 and GLN255. However, novel interactions involving ARG111, GLY113 and SER317 were also observed.",
                "year": 2007,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "How political processes shaped the IT adopted by a small make-to-order company: a case study in the insulated wire and cable industry": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720604000175/pdfft?md5=1b0a771ac8a839448a7d693a9326c938&pid=1-s2.0-S0378720604000175-main.pdf",
                "title": "How political processes shaped the IT adopted by a small make-to-order company: a case study in the insulated wire and cable industry",
                "abstract": "This paper explores how political processes shape the information technology (IT) systems implemented by small-medium sized firms. Systems designers, consultants and users often interpret technology in different ways. This may result in the implementation of inappropriate systems. Small business managers work on short time scales and may be more concerned with operational matters rather than strategic considerations. A case study of CableCo, a small make-to-order company, is used to explore the political processes that shaped the selection and implementation of the Company’s IT system. The incongruence between the ‘technological frames’ of the stakeholders is examined. Researchers, consultants and managers will learn that the implementation of IT is determined by the dominant ‘frame’.",
                "year": 2004,
                "publisher": "Information & Management"
            }
        }
    },
    "Online government advisory service innovation through Intelligent Support Systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720610000868/pdfft?md5=93e12cff472710c97001ddb8db8f60c2&pid=1-s2.0-S0378720610000868-main.pdf",
                "title": "Online government advisory service innovation through Intelligent Support Systems",
                "abstract": "Motivated by lessons learnt from countries with a high e-government readiness index, we used the lens of ‘empowerment’ to understand the ways in which online Intelligent Support Systems can be successfully developed. We found that decision transparency and decision making satisfaction played key roles providing a higher sense of empowerment to the public, including their having a greater perceived power relationship with the government agency and a better feeling of being in control. This approach has improved citizens’ positive perceptions of the government agencies as service providers. These findings make a strong case for the investment in the ISS and have provided useful information for government agencies who wish to develop or improve their online advisory services.",
                "year": 2011,
                "publisher": "Information & Management"
            }
        }
    },
    "Multi-scale modelling of arterial tissue: Linking networks of fibres to continua": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782518303281/pdfft?md5=5b5b832d1a3ab59bd674c2f60d882a73&pid=1-s2.0-S0045782518303281-main.pdf",
                "title": "Multi-scale modelling of arterial tissue: Linking networks of fibres to continua",
                "abstract": "In this work we develop a multi-scale model to characterise the large scale constitutive behaviour of a material featuring a small scale fibrous architecture. The Method of Multi-scale Virtual Power (MMVP) is employed to construct the model. At the macro-scale, a classical continuum mechanics problem is formulated in the finite strain regime. At the micro-scale, a network of fibres, modelled as one-dimensional continua, composes the representative volume element (RVE). The MMVP provides a full characterisation of the equilibrium problem at the RVE, with consistent boundary conditions, as well as the homogenisation formula which defines the first Piola–Kirchhoff stress tensor. Particular attention is given to the fact that the macro-scale continuum could be considered incompressible. Numerical experiments are presented and model consistency is verified against well-known phenomenological constitutive equations. Scenarios departing from the hypotheses of such phenomenological material models are discussed.",
                "year": 2018,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Chemotherapy-Induced Peripheral Neuropathy Detection via a Smartphone App: Cross-sectional Pilot Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Chemotherapy-Induced Peripheral Neuropathy Detection via a Smartphone App: Cross-sectional Pilot Study",
                "abstract": "Background\n\nSevere chemotherapy-induced peripheral neuropathy (CIPN) can cause long-term dysfunction of the hands and feet, interfere with activities of daily living, and diminish the quality of life. Monitoring to identify CIPN and adjust treatment before it progressing to a life-altering severity relies on patients self-reporting subjective symptoms to their clinical team. Objective assessment is not a standard component of CIPN monitoring due to the requirement for specially trained health care professionals and equipment. Smartphone apps have the potential to conveniently collect both subjective and objective CIPN data directly from patients, which could improve CIPN monitoring.",
                "year": 2021,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Temporally correlated source separation using variational Bayesian learning approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1051200407000267/pdfft?md5=3330f1455af6e0df90b4ee8558558ad3&pid=1-s2.0-S1051200407000267-main.pdf",
                "title": "Temporally correlated source separation using variational Bayesian learning approach",
                "abstract": "Basic blind source separation (BSS) algorithms did not adopt time information of signals. They assumed that each source was independent and identically distributed (i.i.d.). In the paper, we propose to use time structure and prior information of sources in order to improve separation. Modeling source by generalized autoregressive (GAR) process, we can tackle the problem of temporally correlated source separation using variational Bayesian (VB) learning approach. The advantages of our proposed algorithm are that (i) it makes full use of time structure of sources; (ii) it can separate different type of sources in noisy environment; (iii) it can avoid overfitting in separation. Experimental results demonstrate that our algorithm outperforms VB separation algorithm based on i.i.d. source model and second-order statistical decorrelation algorithm.",
                "year": 2007,
                "publisher": "Digital Signal Processing"
            }
        }
    },
    "Genetic learning of virtual team member preferences": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563213000812/pdfft?md5=8c50c0084d19a912403447242ba65864&pid=1-s2.0-S0747563213000812-main.pdf",
                "title": "Genetic learning of virtual team member preferences",
                "abstract": "Virtual team members do not have complete understanding of other team members’ preferences, which makes team coordination somewhat difficult and time consuming. Traditional approaches for team coordination require a lot of inter-agent electronic communication and often result in wasted effort. Methods that reduce inter-agent communication and conflicts are likely to increase productivity of virtual teams. In this research, we propose an evolutionary genetic algorithm (GA) based intelligent agent that learns a team member preferences from past actions, and develops a team-coordination schedule by minimizing schedule conflicts between different members serving on a virtual team. Using a discrete event simulation methodology, we test the proposed intelligent agent on different virtual teams of sizes two, four, six and eight members. The results of our experiments indicate that the GA-based intelligent agent learns individual team member preferences and generates a team-coordination schedule at a lower inter-agent communication cost.",
                "year": 2013,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Learning analytics messages: Impact of grade, sender, comparative information and message style on student affect and academic resilience": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563218303431/pdfft?md5=656e2ee9637d6a7e38700a8e5b29c744&pid=1-s2.0-S0747563218303431-main.pdf",
                "title": "Learning analytics messages: Impact of grade, sender, comparative information and message style on student affect and academic resilience",
                "abstract": "Learning analytics enable automated feedback to students through dashboards, reports and alerts. The underlying untested assumption is that providing analytics will be sufficient to improve self-regulated learning. Working within a feedback recipience framework, we begin to test this assumption by examining the impact of learning analytics messages on student affect and academic resilience. Three hundred and twenty undergraduate students completed an online survey and were exposed to three randomly assigned learning analytics alerts (High Distinction, Pass, and Fail grades). Multivariate analyses of variance indicated significant differences between grade levels (large effects), with higher positive affect and lower resilience in response to High Distinction alerts than Pass or Fail alerts. Within each hypothetical grade level, there were no differences in student affect and academic resilience. Based upon systematic changes in feedback sender, message style or whether comparative peer achievement was included or not. These findings indicate that grade level has the largest impact on both affect and academic resilience. The failure of message and sender characteristics to impact on activities that promote self-regulated learning suggests we need to look beyond these characteristics of individual messages to identify drivers of engaging students in self-regulated learning.",
                "year": 2018,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Integrated and concurrent detailed design of a mechatronic quadrotor system using a fuzzy-based particle swarm optimization": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197619300752/pdfft?md5=4ed93d1a48f8b28003b30d335aa5e087&pid=1-s2.0-S0952197619300752-main.pdf",
                "title": "Integrated and concurrent detailed design of a mechatronic quadrotor system using a fuzzy-based particle swarm optimization",
                "abstract": "Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design.",
                "year": 2019,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Enterprise resource planning: A taxonomy of critical factors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221702005544/pdfft?md5=7161fe2a2e7512c36e4629bf5020a1a9&pid=1-s2.0-S0377221702005544-main.pdf",
                "title": "Enterprise resource planning: A taxonomy of critical factors",
                "abstract": "This paper presents a novel taxonomy of the critical success factors in enterprise resource planning (ERP) implementation process. ERP benefits cannot be fully realised unless a strong alignment and reconciliation mechanism is established between technical and organisational imperatives based on the principles of process orientation. It is suggested in the taxonomy that measurement takes place in a balanced perspective, and for the purpose of providing useful information that can enable the decision making process and, which can help deliver the corporate objectives and therefore lead the business competitively forward. Upon this premise, the taxonomy is based on a comprehensive analysis of ERP literature combining research studies and organisational experiences. The taxonomy reflects the essential features of ERP systems, as being built based on the principles of business process management. Furthermore, it illustrates that ERP benefits are realised when a tight link is established between implementation approach and business process performance measures.",
                "year": 2003,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "An Overview of Procedures and Tools for Designing Nonstandard Beam-Based Compliant Mechanisms": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An Overview of Procedures and Tools for Designing Nonstandard Beam-Based Compliant Mechanisms",
                "abstract": "Beam-based Compliant Mechanisms (CMs) are increasingly studied and implemented in precision engineering. Straight beams with uniform cross section are the basic modules in several design concepts, which can be deemed as standard CMs. Their behavioral analysis can be addressed with a large variety of techniques, including the Euler–Bernoulli beam theory, the Pseudo-Rigid Body (PRB) method, the beam constraint model and the discretization-based methods. This variety is unquestionably reduced when considering nonstandard CMs, namely design problems involving special geometries, such as curve/spline beams, variable section beams, nontrivial shapes and contact pairs. The 3D Finite Element Analysis (FEA) provides accurate results but its high computational cost makes it inappropriate for optimization purposes. This work compares the potentialities of computationally efficient modeling techniques (1D FEA, PRB method and chained-beam constraint model), focusing on their applicability in nonstandard planar problems. The cross-axis flexural pivot is used as a benchmark in this research due to its high configurable behavior and wide range of applications. In parallel, as an attempt to provide an easy-to-use environment for CM analysis and design, a multi-purpose tool comprising Matlab and a set of modern Computer-Aided Design/Engineering packages is presented. The framework can implement different solvers depending on the adopted behavioral models. Summary tables are reported to guide the designers in the selection of the most appropriate technique and software framework. Lastly, efficient design procedures that allow to configure nonstandard beam-based CMs with prescribed behavior are examined with two design examples.",
                "year": 2021,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "A new attributed graph clustering by using label propagation in complex networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1319157820304511/pdfft?md5=087c39a1a349e5d7faa5f8722ebd45a8&pid=1-s2.0-S1319157820304511-main.pdf",
                "title": "A new attributed graph clustering by using label propagation in complex networks",
                "abstract": "The diffusion method is one of the main methods of community detection in complex networks. In this method, the use of the concept that diffusion within the nodes that are members of a community is faster than the diffusion of nodes that are not in the same community. In this way, the dense subgraph will detect the graph in the middle layer. The LPA algorithm, which mimics epidemic contagion by spreading labels, has attracted much attention in recent years as one of the most efficient algorithms in the subcategory of diffusion methods. This algorithm is one of the detection algorithms of most popular communities in recent years because of possessing some advantages including linear time order, the use of local information, and non-dependence on any parameter; however, due to the random behavior in LPA, there are some problems such as unstable and low quality resulting from larger monster communities. This algorithm is easily adaptable to attributed network. In this paper, it is supposed to propose a new version of the LPA algorithm for attributed graphs so that the detected communities solve the problems related to unstable and low quality in addition to possessing structural cohesiveness and attribute homogeneity. For this purpose, a weighted graph of the combination of node attributes and topological structure is produced from an attributed graph for nodes which have edges with each other. Also, the centrality of each node will be calculated equal to the influence of each node using Laplacian centrality, and the steps of selecting the node are being enhanced for updating as well as the mechanism of updating based on the influence of nodes. The proposed method has been compared to other primary and new attributed graph clustering algorithms for real and artificial datasets. In accordance with the results of the experiments on the proposed algorithm without parameter adjusting for different networks of density and entropy criteria, the normalized mutual information indicates that the proposed method is more efficient and precise than other state-of-the-art attributed graph clustering methods.",
                "year": 2022,
                "publisher": "Journal of King Saud University - Computer and Information Sciences"
            }
        }
    },
    "Curve fitting and optimal interpolation for CNC machining under confined error using quadratic B-splines": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010448515000640/pdfft?md5=e71115989aab96319030bbb7635b1bac&pid=1-s2.0-S0010448515000640-main.pdf",
                "title": "Curve fitting and optimal interpolation for CNC machining under confined error using quadratic B-splines",
                "abstract": "In CNC machining, fitting the polyline machining tool path with parametric curves can be used for smooth tool path generation and data compression. In this paper, an optimization problem is solved to find a quadratic B-spline curve whose Hausdorff distance to the given polyline tool path is within a given precision. Furthermore, adopting time parameter for the fitting curve, we combine the usual two stages of tool path generation and optimal velocity planning to derive a one-step solution for the CNC optimal interpolation problem of polyline tool paths. Compared with the traditional decoupled model of curve fitting and velocity planning, experimental results show that our method generates a smoother path with minimal machining time.",
                "year": 2015,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "A framework for evaluating economics of knowledge management systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720604000199/pdfft?md5=75e060e2313ef855b099169f1984ccde&pid=1-s2.0-S0378720604000199-main.pdf",
                "title": "A framework for evaluating economics of knowledge management systems",
                "abstract": "Organizations are implementing knowledge management (KM) systems with the assumption that the result will be an increase in organizational effectiveness, efficiency, and competitiveness. Implementing KM systems, however, may be a problem to organizations: too much or too little effort might lead to unwanted outcomes. This paper shows how the introduction of KM systems, which lead to knowledge-sharing, has a negative as well as a positive effect. Important variables from economic perspectives are identified and presented as an integrated framework to illustrate their interrelationships. This paper also explains the implications of an integrated framework for knowledge flow in organizations.",
                "year": 2004,
                "publisher": "Information & Management"
            }
        }
    },
    "Semi-automatic validation of cycle-accurate simulation infrastructures: The case for gem5-x86": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X19304972/pdfft?md5=67a376829e13c63330ecea481184d0a3&pid=1-s2.0-S0167739X19304972-main.pdf",
                "title": "Semi-automatic validation of cycle-accurate simulation infrastructures: The case for gem5-x86",
                "abstract": "Since the early 70s, simulation infrastructures have been a keystone in computer architecture research, providing a fast and reliable way to prototype and evaluate ideas for future computing systems. There are different types of simulators, from most detailed (cycle-accurate) to time-based/functional and analytical modeling. Increasing accuracy translates into several orders of magnitude in terms of simulation speed. Yet, a question remains open: are the results derived from the simulation infrastructure representative of a real machine?\n\nValidation of these infrastructures is complex and costly, usually performed upon release. However, most simulators do not provide the appropriate means to verify or validate new architectural models. In this paper, we introduce a semi-automatic validation framework based on real-hardware performance counter information. The framework provides two levels of abstraction: (a) a high level definition of the processor behavior (Top-Down model) and (b) detailed per-structure and per-pipeline-stage usage breakdown to pinpoint simulator issues. We used this framework to validate the latest available gem5-x86 simulation environment, and found several sources of error that alter the expected behavior of the simulated processor, which we were later to document and correct.",
                "year": 2020,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Gquest: Modeling patient questionnaires and administering them through a mobile platform application": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260714002983/pdfft?md5=ef5820e53809970fdfa1fb036b6479eb&pid=1-s2.0-S0169260714002983-main.pdf",
                "title": "Gquest: Modeling patient questionnaires and administering them through a mobile platform application",
                "abstract": "Background\n\nThe use of surveys is becoming popular in the health care industry for acquiring information useful to the accomplishment of several studies. Besides their exploitation on a large scale for conducting epidemiological studies, surveys are being increasingly carried out on a narrower perspective through the administration of questionnaires aimed at assessing the quality of life perceived by patients or their clinical status during mid- or long-term treatments. This is useful for managing resources or optimizing and individualizing treatments.",
                "year": 2014,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Security protocol specification and verification with AnBx": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Security protocol specification and verification with AnBx",
                "abstract": "Designing distributed protocols is complex and requires actions at very different levels: from the design of an interaction flow supporting the desired application-specific guarantees to the selection of the most appropriate network-level protection mechanisms. To tame this complexity, we propose AnBx, a formal protocol specification language based on the popular Alice & Bob notation. AnBx offers channels as the main abstraction for communication, providing different authenticity and/or confidentiality guarantees for message transmission. AnBx extends existing proposals in the literature with a novel notion of forwarding channels, enforcing specific security guarantees from the message originator to the final recipient along a number of intermediate forwarding agents. We give a formal semantics of AnBx in terms of a state transition system expressed in the AVISPA Intermediate Format. We devise an ideal channel model and a possible cryptographic implementation, and we show that, under mild restrictions, the two representations coincide, thus making AnBx amenable to automated verification with different tools. We demonstrate the benefits of the declarative specification style distinctive of AnBx by revisiting the design of two existing e-payment protocols: iKP and SET.",
                "year": 2016,
                "publisher": "Journal of Information Security and Applications"
            }
        }
    },
    "Reconstruction of optical scanned images of inhomogeneities in biological tissues by Monte Carlo simulation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482515000633/pdfft?md5=2dee809c29a9cd66ee4ae0b9ff5c9dcf&pid=1-s2.0-S0010482515000633-main.pdf",
                "title": "Reconstruction of optical scanned images of inhomogeneities in biological tissues by Monte Carlo simulation",
                "abstract": "The optical imaging of inhomogeneities located in phantoms of biological tissues, prepared from goat׳s isolated heart as control tissue and embedded with spleen and adipose tissues representing tumors, by Monte Carlo simulation, is carried out. The proposed scanning probe consists of nine units. Each unit is equipped with one photon injection port and three ports arranged in a straight line to collect backscattered photons emerging from various depths, and one port, placed coaxially to the source on the opposite side of the phantom, to collect the transmitted component. At each position of the grid, superposed on the tissue phantom, photons are introduced through source port into the phantoms and backscattered and transmitted photons are collected by respective ports. Based on the data collected from the entire grid surface the respective gray-level images are reconstructed. The inhomogeneity located at certain depth (2, 4, 6 mm) is visualized in three images formed by the backscattered data collected by three ports. Increase or decrease in normalized backscattered intensity (NBI) observed in their scans corresponds to that of high scattering (adipose) or absorbing (spleen) inhomogeneity compared to that of control tissue and also their location as determined by NBI variation as received at various ports. The images constructed from the transmitted data are associated with decrease in intensity. The scans of these images through their centers show that normalized transmitted intensity (NTI) attains its maximum value when the inhomogeneities are at depth 6 mm. These scans are of higher amplitude for spleen compared to that of adipose tissues. Thus the data received by backscattering and transmission complement each other in identifying the location and type of inhomogeneities.",
                "year": 2015,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Using a digital data analytic tool to capture dynamic change in coordination patterns: An exploratory study of the Apollo 13 mission": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687024001224/pdfft?md5=c1de7fdcefdabdd6574b0b7fb1b0d1a8&pid=1-s2.0-S0003687024001224-main.pdf",
                "title": "Using a digital data analytic tool to capture dynamic change in coordination patterns: An exploratory study of the Apollo 13 mission",
                "abstract": "The operational environment of complex sociotechnical systems is inherently uncertain, demanding constant coordination restructuring to adapt to dynamic situational demands. However, coordination changes in the Human Factors and Ergonomics Field have primarily been studied using static methods, overlooking moment-by-moment adjustments. In the current study, we address coordination restructuring by using THEME, a digital analytical tool capable of visualising and exploring coordination restructuring from a multi-layered perspective. We examine restructuring in coordination patterns during NASA's Apollo 13 Mission, revealing significant shifts from stable, long-duration ‘coordination hubs' in routine operations to shorter-duration patterns during a crisis situation. Additionally, the results highlight the importance of flexible switching between reciprocal and one-directed coordination, along with enhanced role distribution. This study underscores how exploring temporality-sensitive phenomena like coordination through digital technologies such as THEME, advances our understanding of incident analysis and resilient performance within complex systems.",
                "year": 2024,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Work for sustainability: Case studies of Brazilian companies": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687015300880/pdfft?md5=b267b3ee1e2de7265f933ddc88fe9c86&pid=1-s2.0-S0003687015300880-main.pdf",
                "title": "Work for sustainability: Case studies of Brazilian companies",
                "abstract": "The introduction of strategic corporate sustainability policies is expected to result in the improvement of several issues in companies. One of these issues is work, which should involve greater well-being for workers. Within the context of production engineering, this research connects sustainability and work-related issues, the latter seen in light of the discipline of ergonomics. Based on case studies conducted at four companies considered sustainability benchmarks, we examined how the introduction of the theme of sustainability has influenced work-related issues. The elements analyzed here were the corporate sustainability strategy, organizational practices for deploying the strategy, and the work design phase. The last element is the moment in which work is prescribed in the organization. The results show that, despite the announcement of the inclusion of changes in work, there is not any explicit evidence confirming that such changes are considered as a requirement for corporate sustainability projects.",
                "year": 2016,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Does gender matter in cyberbullying perpetration? An empirical investigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563217305952/pdfft?md5=424356e6cdf34f88918796e29e6ab6b3&pid=1-s2.0-S0747563217305952-main.pdf",
                "title": "Does gender matter in cyberbullying perpetration? An empirical investigation",
                "abstract": "Cyberbullying is a highly prevalent online misbehavior that has recently received public attention due to its potentially devastating consequences. In response to the call to understand the mechanism leading to cyberbullying perpetration, this study draws on I3 theory to understand the decisions of young adults to engage in cyberbullying. Furthermore, it examines the influence of gender on various determinants of cyberbullying perpetration. The results of an online survey involving 208 university students reveal that both cyberbullying victimization and perceived online disinhibition enhance the intention to perpetrate cyberbullying, whereas self-control is a critical buffer that represses the propensity to cyberbully others. Our findings also show that the factors influencing cyberbullying differ in strength for male and female students. We believe that these findings not only provide a theoretical explanation of cyberbullying perpetration but also offer valuable insights for combatting cyberbullying among university students.",
                "year": 2018,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Modelling one-dimensional reactive transport of toxic contaminants in natural rivers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815221000141/pdfft?md5=c33bad2260b3cf3548f121534eda649c&pid=1-s2.0-S1364815221000141-main.pdf",
                "title": "Modelling one-dimensional reactive transport of toxic contaminants in natural rivers",
                "abstract": "Solute transport models are widely employed to the predict spatio-temporal fate of contaminants in rivers. However, most previous studies have assumed that contaminants spilled are unreactive; they disregard the inherent reactivity of contaminants in water. The Reactive Solute Transport Model (RSTM) (Gooseff et al., 2005) includes lumped decay terms to reflect the decayability of the chemicals, but its applicability was still poor due to dependence on an optimization method. The purpose of this study is to perform reactive transport modelling considering chemicals’ reactivities for sorption, volatilization, and biodegradation. To this end, we manipulated the governing equations of the RSTM and suggested theoretical and empirical methods of estimating the key parameters of the reaction terms. The results showed, for example, that benzene lost 57.7% of its primary mass after being transported 4.54 km downstream due to its high volatility. Also, the arrival time of toluene was delayed by 10.4% due to adsorption.",
                "year": 2021,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Application of data augmentation techniques towards metabolomics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482522006552/pdfft?md5=e9d40e78b598d42726f8634bebea259c&pid=1-s2.0-S0010482522006552-main.pdf",
                "title": "Application of data augmentation techniques towards metabolomics",
                "abstract": "Niemann–Pick Class 1 (NPC1) disease is a rare and debilitating neurodegenerative lysosomal storage disease (LSD). Metabolomics datasets of NPC1 patients available to perform this type of analysis are often limited in the number of samples and severely unbalanced. In order to improve the predictive capability and identify new biomarkers in an NPC1 disease urinary dataset, data augmentation (DA) techniques based on computational intelligence have been employed to create synthetic samples, i.e. the addition of noise, oversampling techniques and conditional generative adversarial networks. These techniques have been used to evaluate their predictive capacities on a set of urine samples donated by 13 untreated NPC1 disease and 47 heterozygous (parental) carrier control participants. Results on the prediction have also been obtained using different machine learning classification models and the partial least squares techniques. These results provide strong evidence for the ability of DA techniques to generate good quality synthetic data. Results acquired show increases in sensitivity of 20%–50%, an F1 score of 6%–30%, and a predictive capacity of 0.3 (out of 1). Additionally, more conventional forms of multivariate data analysis have been employed. These have allowed the detection of unusual urinary metabolite profiles, and the identification of biomarkers through the use of synthetically augmented datasets. Results indicate that urinary branched-chain amino acids such as valine, 3-aminoisobutyrate and quinolinate, may be employable as valuable biomarkers for the diagnosis and prognostic monitoring of NPC1 disease.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Efficient buffering for concurrent disk and tape I/O": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0166531696900401/pdfft?md5=0185bf0f179915bffea65469b58b6499&pid=1-s2.0-S0166531696900401-main.pdf",
                "title": "Efficient buffering for concurrent disk and tape I/O",
                "abstract": "Tertiary storage is becoming increasingly important for many organizations involved in large-scale data analysis and data mining activities. Yet database management systems (DBMS) and other data-intensive systems do not incorporate tertiary storage as a first-class citizen in the storage hierarchy. For instance, the typical solution for bringing tertiary-resident data under the control of a DBMS is to use operating system facilities to copy the data to secondary storage, and then to perform query optimization and execution as if the data had been in secondary storage all along. This approach fails to recognize the opportunities for saving execution time and storage space if the data were accessed on tertiary devices directly and in parallel with other I/Os.\n\nIn this paper we examine issues in accessing secondary and tertiary storage in parallel and suggest buffering mechanisms for increasing the throughput of applications with concurrent, intensive I/O requirements. We first identify several factors that determine the parallel I/O performance of secondary and tertiary storage devices. We discuss the performance characteristics of magnetic disks and magnetic tapes when used alone and when used concurrently, sharing the same I/O bus. We then describe alternative buffering schemes for parallel I/O and analyze their efficiency via an experimental implementation.",
                "year": 1996,
                "publisher": "Performance Evaluation"
            }
        }
    },
    "Prognostic performance of two expert systems based on Bayesian belief networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923699000597/pdfft?md5=38fceaf96d896b8739d2ac9b95386924&pid=1-s2.0-S0167923699000597-main.pdf",
                "title": "Prognostic performance of two expert systems based on Bayesian belief networks",
                "abstract": "A decision support system for the prognosis at 24 h of head-injured patients of the intensive care unit (ICU), based on Bayesian belief networks, is constructed by model selection methods applied to a database (637 cases) of seven clinical and laboratory variables. Its performance is compared to other systems, including a simpler belief network that assumes conditional independence among the findings, and a human expert. Results indicate that its performance is not significantly different than that of the neurosurgeon expert and better than the performance of the independence model. Thus, the prognostic judgment of non-neurosurgeon ICU clinicians can be aided by the use of this system.",
                "year": 2000,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Steering through the murky waters of a scientific conflict: situated and symbolic models of clinical cognition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/093336579500013V/pdfft?md5=39efff28d25107b7108666feb13d33f7&pid=1-s2.0-093336579500013V-main.pdf",
                "title": "Steering through the murky waters of a scientific conflict: situated and symbolic models of clinical cognition",
                "abstract": "The situated action perspective, which embraces a diversity of views, challenges several of the fundamental assumptions of the symbolic information-processing framework underlying cognitive science and artificial intelligence. In this paper, we consider the following issues: symbolic representations, plans and actions, distributed cognition, and the transfer of learning. We evaluate each of these issues in terms of research and theories in clinical cognition and examine the implications for education and training, and for the integration of intelligent systems in medical practice. We argue for a reconceptualization of the symbolic framework in terms of the way the role of internal representations and cognitive activities are perceived. However, symbolic representations are integral to medical cognition and should continue to be central in any theoretical framework. A re-examination of cognitive science in medicine in terms of the relationship among physicians, technology, and the workplace could prove to be constructive in bridging the gap between theory and practice.",
                "year": 1995,
                "publisher": "Artificial Intelligence in Medicine"
            }
        }
    },
    "Sizing up information and communication technologies as agents of political development in sub-Saharan Africa": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0308596111001303/pdfft?md5=fe955f0df699feb92d60fe2a9fd4867c&pid=1-s2.0-S0308596111001303-main.pdf",
                "title": "Sizing up information and communication technologies as agents of political development in sub-Saharan Africa",
                "abstract": "It is widely speculated that the emergence of modern information and communication technologies (ICTs) will boost political development in the developing world. This expectation anchors on solid foundation since, presumably, the ICT revolution would radically alter access to information, dislodge entrenched social cleavages, and unleash new patterns of citizen consciousness and civic engagement by hitherto marginalized mass publics, and orchestrate new and decisive political equilibriums. This research provides an empirical assessment of the impact of ICTs on political development in sub-Saharan Africa. The analysis suggests that speculation about the potential for ICTS to enhance political development in the sub-Sahara is not unrealistic. The levels of phone, computer, and Internet diffusion are associated with political development, although only the effect of the phone remains once other variables are specified. The phone is the most robust of all individual factors explaining variations in political development. However, the effect of ICTs on political development can neither be certified as revolutionary, nor can they be codified as panacea.",
                "year": 2011,
                "publisher": "Telecommunications Policy"
            }
        }
    },
    "Self-efficacy in information security: Its influence on end users' information security practice behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016740480900056X/pdfft?md5=8d30439e1d493864f7650c02e95ec6d4&pid=1-s2.0-S016740480900056X-main.pdf",
                "title": "Self-efficacy in information security: Its influence on end users' information security practice behavior",
                "abstract": "The ultimate success of information security depends on appropriate information security practice behaviors by the end users. Based on social cognitive theory, this study models and tests relationships among self-efficacy in information security, security practice behavior and motivation to strengthen security efforts. This study also explores antecedents to individuals' self-efficacy beliefs in information security. Results provide support for the many hypothesized relationships. This study provides an initial step toward understanding of the applicability of social cognitive theory in a new domain of information security. The results suggest that simply listing what not to do and penalties associated with a wrong doing in the users' information security policy alone will have a limited impact on effective implementation of security measures. The findings may help information security professionals design security awareness programs that more effectively increase the self-efficacy in information security.",
                "year": 2009,
                "publisher": "Computers & Security"
            }
        }
    },
    "Adjusting middleware knobs to assess scalability limits of distributed cyber-physical systems": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Adjusting middleware knobs to assess scalability limits of distributed cyber-physical systems",
                "abstract": "The traditional development paradigm for time-sensitive distributed systems (and even more for real-time domains) has typically relied on unflexible low-level schemes; these have been based on (also) low-level programming of improved medium access control protocols to obtain deterministic network schedules. Such technique does not scale well in the context of cyber-physical systems (CPS), which have a complexity of several orders of magnitude higher. In this paper, we explore the actual trend of cyber-physical systems (CPS) that are progressively integrated with Internet technologies to fulfill their requirements of being highly connected systems. The integration pillar is the communication middleware; however, communication middleware technologies are prone to introducing delays at different levels, increasing the potential uncertainty of the system execution. Therefore, individual analysis of middleware technologies is needed to assess the bounds on the type and scale of CPS that they can support. We analyse the behavior of a specific a middleware technology and its cost for handling the interaction of distributed nodes in the context of cyber-physical systems. We propose the design of a reliable middleware infrastructure for distributed embedded systems integrated in a CPS environment. Our approach considers time requirements specified by the system nodes or units, and fine tunes a few specific parameters of the middleware that we refer to as knobs. We validate our solution implementation in a experimental setting and show the system scale that can be supported in a stable way.",
                "year": 2017,
                "publisher": "Computer Standards & Interfaces"
            }
        }
    },
    "Learning in the feed-forward random neural network: A critical review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0166531610000970/pdfft?md5=0e92ddfb6f6f7884e1542bfc8107ce28&pid=1-s2.0-S0166531610000970-main.pdf",
                "title": "Learning in the feed-forward random neural network: A critical review",
                "abstract": "The Random Neural Network (RNN) has received, since its inception in 1989, considerable attention and has been successfully used in a number of applications. In this critical review paper we focus on the feed-forward RNN model and its ability to solve classification problems. In particular, we paid special attention to the RNN literature related with learning algorithms that discover the RNN interconnection weights, suggested other potential algorithms that can be used to find the RNN interconnection weights, and compared the RNN model with other neural-network based and non-neural network based classifier models. In review, the extensive literature review and experimentation with the RNN feed-forward model provided us with the necessary guidance to introduce six critical review comments that identify some gaps in the RNN’s related literature and suggest directions for future research.",
                "year": 2011,
                "publisher": "Performance Evaluation"
            }
        }
    },
    "Extension of the survival dimensionality reduction algorithm to detect epistasis in competing risks models (SDR-CR)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046412001736/pdfft?md5=a89905e357ac34bdc11bf120168ecff7&pid=1-s2.0-S1532046412001736-main.pdf",
                "title": "Extension of the survival dimensionality reduction algorithm to detect epistasis in competing risks models (SDR-CR)",
                "abstract": "Background\n\nThe discovery and the description of the genetic background of common human diseases is hampered by their complexity and dynamic behavior. Appropriate bioinformatic tools are needed to account all the facets of complex diseases and to this end we recently described the survival dimensionality reduction (SDR) algorithm in the effort to model gene–gene interactions in the context of survival analysis. When one event precludes the occurrence of another event under investigation in the ‘competing risk model’, survival algorithms require particular adjustment to avoid the risk of reporting wrong or biased conclusions.",
                "year": 2013,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "A cellular automaton model to find the risk of developing autism through gut-mediated effects": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482519301714/pdfft?md5=fc1b06eb53c39963c936b5a520a2c56e&pid=1-s2.0-S0010482519301714-main.pdf",
                "title": "A cellular automaton model to find the risk of developing autism through gut-mediated effects",
                "abstract": "Background\n\nOne of the risk factors for the development of Autism Spectrum Disorder (ASD) is hypothesized to be an imbalance in the gut microbiome. Alterations in the relative numbers of gut microbiota may contribute to such a disruption in normal bacterial diversity. It is assumed that this process may be adequately mirrored for the purpose of the current paper by modeling the dynamic shifts in the numbers of three bacterial species, namely Clostridium, Desulfovibrio, and Bifidobacterium. Such imbalances in the gut microbiome are thought to promote the development of increased gut permeability (the so-called “leaky gut”) which in turn is a potential risk factor for the development of ASD.",
                "year": 2019,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "On the Use of a Pedestrian Simulation Model with Natural Behavior Representation in Metro Stations": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050915008480/pdfft?md5=83851a20b066bda801c471babfb217e3&pid=1-s2.0-S1877050915008480-main.pdf",
                "title": "On the Use of a Pedestrian Simulation Model with Natural Behavior Representation in Metro Stations",
                "abstract": "Rapid urbanization in many large cities in China makes metro station an integral part of metropolitan people's daily life. High density of crowds in metro stations would cause serious congestion problems and pose threats to pedestrian safety. Because of the heterogeneous and complex properties of pedestrians, traditional approaches face difficulties in predicting future pedestrian flow patterns. The use of agent-based simulation approach makes it possible to naturally reproduce various pedestrian behaviors in different scenarios. This paper presented an agent-based microscopic pedestrian simulation model—CityFlow, which was proved to be flexible in revealing most important pedestrian behaviors in metro stations by several simulation cases. The model applications can provide implications in evaluation of design proposals of metro facilities.",
                "year": 2015,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Coping behaviors in short message service (SMS)-based disaster alert systems: From the lens of protection motivation theory as elaboration likelihood": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720621000288/pdfft?md5=a1923ede1e87872f373758f4c8bd7b82&pid=1-s2.0-S0378720621000288-main.pdf",
                "title": "Coping behaviors in short message service (SMS)-based disaster alert systems: From the lens of protection motivation theory as elaboration likelihood",
                "abstract": "Short message service (SMS)-based disaster alert systems are used as critical methods that inform people of natural disasters. This study investigates how two different decision-making routes influence adaptive copings based upon users’ protection motivations. The two important routes, argument quality and source credibility, are presented as focal antecedents of coping adaptiveness. And perceived threat and coping efficacy are tested as a moderator as elaboration likelihood. To achieve these goals, we propose a theoretical framework and analyze survey data to test the hypotheses. The results illustrate that two elaboration likelihood variables show significant moderating roles in the proposed research model.",
                "year": 2021,
                "publisher": "Information & Management"
            }
        }
    },
    "Adaptive recursive algorithm with logarithmic transformation for nonlinear system identification in α-stable noise": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1051200415002535/pdfft?md5=ef2f8ad1e0e1efb2bc8975c1fe02eded&pid=1-s2.0-S1051200415002535-main.pdf",
                "title": "Adaptive recursive algorithm with logarithmic transformation for nonlinear system identification in α-stable noise",
                "abstract": "Although the least mean pth power (LMP) and normalized LMP (NLMP) algorithms of adaptive Volterra filters outperform the conventional least mean square (LMS) algorithm in the presence of α-stable noise, they still exhibit slow convergence and high steady-state kernel error in nonlinear system identification. To overcome these limitations, an enhanced recursive least mean pth power algorithm with logarithmic transformation (RLogLMP) is proposed in this paper. The proposed algorithm is adjusted to minimize the new cost function with the p-norm logarithmic transformation of the error signal. The logarithmic transformation, which can diminish the significance of outliers under α-stable noise environment, increases the robustness of the proposed algorithm and reduces the steady-state kernel error. Moreover, the proposed method improves the convergence rate by the enhanced recursive scheme. Finally, simulation results demonstrate that the proposed algorithm is superior to the LMP, NLMP, normalized least mean absolute deviation (NLMAD), recursive least squares (RLS) and nonlinear iteratively reweighted least squares (NIRLS) algorithms in terms of convergence rate and steady-state kernel error.",
                "year": 2015,
                "publisher": "Digital Signal Processing"
            }
        }
    },
    "Morphological and biomechanical analyses of the human healthy and glaucomatous aqueous outflow pathway: Imaging-to-modeling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260723001517/pdfft?md5=ce36399d24be0560fae627c5fab9af8d&pid=1-s2.0-S0169260723001517-main.pdf",
                "title": "Morphological and biomechanical analyses of the human healthy and glaucomatous aqueous outflow pathway: Imaging-to-modeling",
                "abstract": "Background and objective\n\nIntraocular pressure (IOP) is maintained via a dynamic balance between the production of aqueous humor and its drainage through the trabecular meshwork (TM), juxtacanalicular connective tissue (JCT), and Schlemm's canal (SC) endothelium of the conventional outflow pathway. Primary open angle glaucoma (POAG) is often associated with IOP elevation that occurs due to an abnormally high outflow resistance across the outflow pathway. Outflow tissues are viscoelastic and actively interact with aqueous humor dynamics through a two-way fluid-structure interaction coupling. While glaucoma affects the morphology and stiffness of the outflow tissues, their biomechanics and hydrodynamics in glaucoma eyes remain largely unknown. This research aims to develop an image-to-model method allowing the biomechanics and hydrodynamics of the conventional aqueous outflow pathway to be studied.",
                "year": 2023,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "An Educational and Exercise Mobile Phone–Based Intervention to Elicit Electrophysiological Changes and to Improve Psychological Functioning in Adults With Nonspecific Chronic Low Back Pain (BackFit App): Nonrandomized Clinical Trial": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An Educational and Exercise Mobile Phone–Based Intervention to Elicit Electrophysiological Changes and to Improve Psychological Functioning in Adults With Nonspecific Chronic Low Back Pain (BackFit App): Nonrandomized Clinical Trial",
                "abstract": "Background\n\nConcomitant psychological and cognitive impairments modulate nociceptive processing and contribute to chronic low back pain (CLBP) maintenance, poorly correlated with radiological findings. Clinical practice guidelines recommend self-management and multidisciplinary educational and exercise-based interventions. However, these recommendations are based on self-reported measurements, which lack evidence of related electrophysiological changes. Furthermore, current mobile health (mHealth) tools for self-management are of low quality and scarce evidence. Thus, it is necessary to increase knowledge on mHealth and electrophysiological changes elicited by current evidence-based interventions.",
                "year": 2022,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Computational modeling and druggability assessment of Aggregatibacter actinomycetemcomitans leukotoxin": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260722003340/pdfft?md5=d07a9f40c5be378a9e71f7072314359a&pid=1-s2.0-S0169260722003340-main.pdf",
                "title": "Computational modeling and druggability assessment of Aggregatibacter actinomycetemcomitans leukotoxin",
                "abstract": "The leukotoxin (LtxA) of Aggregatibacter actinomycetemcomitans (A. actinomycetemcomitans) is a protein exotoxin belonging to the repeat-in-toxin family (RTX). Numerous studies have demonstrated that LtxA may play a critical role in the pathogenicity of A. actinomycetemcomitans since hyper-leukotoxic strains have been associated with severe disease. Accordingly, considerable effort has been made to elucidate the mechanisms by which LtxA interacts with host cells and induce their death. However, these attempts have been hampered by the unavailability of a tertiary structure of the toxin, which limits the understanding of its molecular properties and mechanisms. In this paper, we used homology and template free modeling algorithms to build the complete tertiary model of LtxA at atomic level in its calcium-bound Holo-state. The resulting model was refined by energy minimization, validated by Molprobity and ProSA tools, and subsequently subjected to a cumulative 600ns of all-atom classical molecular dynamics simulation to evaluate its structural aspects. The druggability of the proposed model was assessed using Fpocket and FTMap tools, resulting in the identification of four putative cavities and fifteen binding hotspots that could be targeted by rational drug design tools to find new ligands to inhibit LtxA activity.",
                "year": 2022,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Towards semantic comparison of multi-granularity process traces": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705113002116/pdfft?md5=17742b99ff3373860a5ae6b0f97cadbd&pid=1-s2.0-S0950705113002116-main.pdf",
                "title": "Towards semantic comparison of multi-granularity process traces",
                "abstract": "A process trace describes the steps taken in a workflow to generate a particular result. Understanding a process trace is critical to be able to verify data, enable its re-use and to make appropriate decisions. Given many process traces, each with a large amount of very low level information, it is a challenge to make process traces meaningful to different users. It is more challenging to compare two complex process traces generated by heterogeneous systems and having different levels of granularity. In this paper, we present a novel notion of multi-granularity process trace that attempts to capture the conceptual abstraction of large process traces at different levels of granularity by leveraging ontology information. Based on this notion, graph matching based algorithms with semantic filtering are developed to efficiently and effectively compute the similarity between two process traces by considering both structural similarity and semantic similarity. Our experiment using both real world and synthetic datasets demonstrates that our techniques provide a practical approach for process trace similarity measurement.",
                "year": 2013,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "A generic dynamic control task for behavioral research and education": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563211000811/pdfft?md5=d502609a4756276104cb2c97d7704524&pid=1-s2.0-S0747563211000811-main.pdf",
                "title": "A generic dynamic control task for behavioral research and education",
                "abstract": "Recent research in behavioral sciences presents strong evidence of poor human understanding for dynamic systems. Computer-based dynamic control tasks have an important potential for helping behavioral scientists advance research that investigates reasons for poor understanding and for helping students understand how dynamic systems work. In this paper, we introduce a simulation called Dynamic Stocks and Flows (DSF) that portrays the basic building blocks of dynamic systems: an accumulation; an inflow and outflow determined by an environment; and an inflow and outflow determined by a decision maker. In DSF, decision makers control the accumulation to a goal level by making repeated inflow and outflow decisions. We provide details of an experiment conducted with DSF that highlight some problems people face in controlling a dynamic system with different kinds of environmental inflow and outflow functions. DSF is flexible enough to represent dynamic systems with continuous or discrete accumulations, and with real-time or event-driven decision-making. We suggest that these and other features in DSF make it a good research and educational tool.",
                "year": 2011,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Consultant engagement for computer system selection: A pro-active client role in small businesses": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0378720691900465/pdfft?md5=cdb0905288af4eb3b3265b743b8797d6&pid=1-s2.0-0378720691900465-main.pdf",
                "title": "Consultant engagement for computer system selection: A pro-active client role in small businesses",
                "abstract": "Because of their limited number of senior positions and fewer alternative career paths, small businesses have a more difficult time attracting and retaining skilled information systems (IS) staff and are thus dependent upon external expertise. Small businesses are particularly dependent on outside expertise when first computerizing. Because small businesses suffer from severe financial constraints, it is often difficult to justify the cost of custom software. Hence, for many small businesses, engaging a consultant to help with identifying suitable packaged software and related hardware, is their first critical step toward computerization. This study explores the importance of pro-active client involvement when engaging a consultant to assist with computer system selection in small businesses. Client involvement throughout consultant engagement is found to be integral to project success and frequently lacking due to mis-conceptions of small businesses regarding their role. Small businesses often overestimate the impact of consultant and vendor support in achieving succesful computer system selection and implementation. For consultant engagement to be successful, the process must be viewed as being directed toward the achievement of specific organizational results where the client accepts responsibility for direction of the process.",
                "year": 1991,
                "publisher": "Information & Management"
            }
        }
    },
    "Mobile telephony and democracy in Ghana: Interrogating the changing ecology of citizen engagement and political communication": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Mobile telephony and democracy in Ghana: Interrogating the changing ecology of citizen engagement and political communication",
                "abstract": "This paper explores how far mobile telephony, in concert with other media platforms, has borne out the optimism of the mobile phonedemocracy nexus in the context of Ghana's politics and democratic practice. It examines the relationship between mobile phone access, citizen engagement, and appropriation of the technology by the governing elite for political communication and electoral campaigns. It concludes that Ghana has witnessed a rich convergence of mobile telephony and broadcast media which, in tandem with an open political environment, has significantly transformed the ecology of political communication. This development has helped to deepen democratic engagement among citizens, and between citizens and the political class, by fostering civic vigilance and accountability and facilitating multivocal expression of views from a more diverse constituency of political participants. However, while mobile phones and related platforms have allowed contra- and counter-hegemonic voices in Ghanaian politics to find expression across the electromagnetic spectrum, this access should not be construed to necessarily mean that political discourse has shifted in significant ways as to alter the fundamental structures of political power. Overall, the fundamental structures of political power and the levers of control remain unassailed by ordinary citizens, despite some of the progress made possible by mobile telephony.",
                "year": 2017,
                "publisher": "Telecommunications Policy"
            }
        }
    },
    "Enhanced Floating Isogeometric Analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S004578252300470X/pdfft?md5=cce68d0d3b43e3b090572cec9f631495&pid=1-s2.0-S004578252300470X-main.pdf",
                "title": "Enhanced Floating Isogeometric Analysis",
                "abstract": "The numerical simulation of additive manufacturing techniques promises the acceleration of costly experimental procedures to identify suitable process parameters. We recently proposed Floating Isogeometric Analysis (FLIGA), a new computational solid mechanics approach, which is mesh distortion-free in one characteristic spatial direction. FLIGA emanates from Isogeometric Analysis and its key novel aspect is the concept of deformation-dependent “floating” of individual B-spline basis functions along one parametric axis of the mesh. Our previous work showed that FLIGA not only overcomes the problem of mesh distortion associated to this direction, but is also ideally compatible with material point integration and enjoys a stability similar to that of conventional Lagrangian mesh-based methods. These features make the method applicable to the simulation of large deformation problems with history-dependent constitutive behavior, such as additive manufacturing based on polymer extrusion. In this work, we enhance the first version of FLIGA by (i) a novel quadrature scheme which further improves the robustness against mesh distortion, (ii) a procedure to automatically regulate floating of the basis functions (as opposed to the manual procedure of the first version), and (iii) an adaptive refinement strategy. We demonstrate the performance of enhanced FLIGA on relevant numerical examples including a selection of viscoelastic extrusion problems.",
                "year": 2023,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841520303145/pdfft?md5=14bd0da7ca851128b4261cfc21c15162&pid=1-s2.0-S1361841520303145-main.pdf",
                "title": "CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation",
                "abstract": "Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.",
                "year": 2021,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Privacy calculus and its utility for personalization services in e-commerce: An analysis of consumer decision-making": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720616301999/pdfft?md5=7da9c22bbcdfc80912141b7e22460039&pid=1-s2.0-S0378720616301999-main.pdf",
                "title": "Privacy calculus and its utility for personalization services in e-commerce: An analysis of consumer decision-making",
                "abstract": "Modern consumers increasingly embrace the personalization of services. Whether to disclose private information to companies for the sake of receiving personalized service is largely contingent to relative valuations and the utility of private information. Unfortunately, there is a lack of balanced research that analyzes and reconciles the contradiction between privacy and personalization service. In this study, based on the multi-attribute utility theory (MAUT), we introduce a utility model of privacy in personalization. Our simulation results validate our white-box utility model by demonstrating significant distinctions of calculating benefits and costs among three groups of consumers.",
                "year": 2017,
                "publisher": "Information & Management"
            }
        }
    },
    "Exploring blended group interventions for depression: Randomised controlled feasibility study of a blended computer- and multimedia-supported psychoeducational group intervention for adults with depressive symptoms": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2214782916300689/pdfft?md5=cb231987ad9c6a032ada6f56d2143f9e&pid=1-s2.0-S2214782916300689-main.pdf",
                "title": "Exploring blended group interventions for depression: Randomised controlled feasibility study of a blended computer- and multimedia-supported psychoeducational group intervention for adults with depressive symptoms",
                "abstract": "Background\n\nBlended interventions aim to capitalise on the strengths of both computer-based and face-to-face therapy. Studies on this innovative treatment format remain scare. This especially accounts for the group treatment of depression.",
                "year": 2017,
                "publisher": "Internet Interventions"
            }
        }
    },
    "Application Monitoring for bug reproduction in web-based applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0164121223002297/pdfft?md5=648e91b287a8ad4f80fee4be14d939e4&pid=1-s2.0-S0164121223002297-main.pdf",
                "title": "Application Monitoring for bug reproduction in web-based applications",
                "abstract": "Web applications are often built as Single Page Applications (SPA), for example applications offered by Google, Facebook, Twitter or Netflix. Users interact with SPAs through a single HTML page that is dynamically rewritten with new data from the web server (instead of a web browser that loads entire new HTML pages). Just like with any type of software system, debugging is a common activity during the development and maintenance of SPAs. In order to fix bugs observed during runtime, developers often try to reproduce the bug first to better understand it. However, research has shown that reproducing bugs is not always possible. In this paper we (i) develop a technique for Application Monitoring (AM) to collect data to support bug reproduction; and (ii) apply the monitoring technique in a SPA test bed as well as a real-world SPA application to show its feasibility. As part of our research we developed an initial version of the AM technique and implemented it in a prototype. Our evaluation using this prototype showed that it not only improves the efficiency of the bug reproduction process but also reduces information gaps caused by incomplete bug reports submitted by users. Additionally, compared to the information provided by users, data provided by AM is more accurate and detailed and covers a wider range of data. Future work includes deploying the AM framework in more SPAs and investigating how AM can be integrated into software developer workflows.",
                "year": 2024,
                "publisher": "Journal of Systems and Software"
            }
        }
    },
    "Background adjustment of cDNA microarray images by Maximum Entropy distributions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046410000407/pdfft?md5=de34ab9cd171f1e8136f5c92bcc1e216&pid=1-s2.0-S1532046410000407-main.pdf",
                "title": "Background adjustment of cDNA microarray images by Maximum Entropy distributions",
                "abstract": "Many empirical studies have demonstrated the exquisite sensitivity of both traditional and novel statistical and machine intelligence algorithms to the method of background adjustment used to analyze microarray datasets. In this paper we develop a statistical framework that approaches background adjustment as a classic stochastic inverse problem, whose noise characteristics are given in terms of Maximum Entropy distributions. We derive analytic closed form approximations to the combined problem of estimating the magnitude of the background in microarray images and adjusting for its presence.\n\nThe proposed method reduces standardized measures of log expression variability across replicates in situations of known differential and non-differential gene expression without increasing the bias. Additionally, it results in computationally efficient procedures for estimation and learning based on sufficient statistics and can filter out spot measures with intensities that are numerically close to the background level resulting in a noise reduction of about 7%.",
                "year": 2010,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Mapping nonlinear brain dynamics by phase space embedding with fMRI data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809422009752/pdfft?md5=633a590427833143c82674df7d2856d9&pid=1-s2.0-S1746809422009752-main.pdf",
                "title": "Mapping nonlinear brain dynamics by phase space embedding with fMRI data",
                "abstract": "The human brain is a complex neurobiological system exhibiting complex nonlinear spatiotemporal dynamics. While functional magnetic resonance imaging (fMRI) has been widely used to study brain activity, whole-brain nonlinear dynamics in fMRI data have not been extensively examined. The present study applied phase space embedding on resting-state fMRI data and characterized their phase space dynamics with the sum of lengths of portrait edges (SE) in the reconstructed phase portrait. The effects of repetition time (TR), bandpass filtering and the added noise power of BOLD signals on the optimal embedding parameters (embedding time delay τ and embedding dimension m) were examined with experimental or simulated fMRI data. Our results show that τ and m vary with the three acquisition parameters. The present method was applied to the autism spectrum disorder dataset from Autism Imaging Data Exchange I to demonstrate its capability in the characterization of abnormal brain dynamics. The resultant SE maps were statistically compared between patients and controls, and the significant differences in SE were fed into a support vector machine (SVM) for classification. A significant increase in SE in the default mode network (DMN) and salience network (SN), as well as the visual network, was found in autistic patients. With the SE features of these regions, our SVM classifier achieved superior accuracy (74.55% with 10-folds cross validation) compared with prior studies, indicating that phase space embedding and SE mapping are promising in characterizing the nonlinear dynamics of the BOLD signal and might be useful for brain biomarker discovery in clinical psychiatry.",
                "year": 2023,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Keep the driver in control: Automating automobiles of the future": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687015300247/pdfft?md5=ded5674b82bbf40c5857c7a68b0af363&pid=1-s2.0-S0003687015300247-main.pdf",
                "title": "Keep the driver in control: Automating automobiles of the future",
                "abstract": "Automated automobiles will be on our roads within the next decade but the role of the driver has not yet been formerly recognised or designed. Rather, the driver is often left in a passive monitoring role until they are required to reclaim control from the vehicle. This research aimed to test the idea of driver-initiated automation, in which the automation offers decision support that can be either accepted or ignored. The test case examined a combination of lateral and longitudinal control in addition to an auto-overtake system. Despite putting the driver in control of the automated systems by enabling them to accept or ignore behavioural suggestions (e.g. overtake), there were still issues associated with increased workload and decreased trust. These issues are likely to have arisen due to the way in which the automated system has been designed. Recommendations for improvements in systems design have been made which are likely to improve trust and make the role of the driver more transparent concerning their authority over the automated system.",
                "year": 2016,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Interdependency-induced risk with applications to healthcare": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1874548214000067/pdfft?md5=2b53ea808667a1dbe95d5d94e240079f&pid=1-s2.0-S1874548214000067-main.pdf",
                "title": "Interdependency-induced risk with applications to healthcare",
                "abstract": "Developing effective protection, mitigation and recovery measures for critical infrastructures is paramount in the wake of increasing natural and human-initiated hazards, risks and threats. Influencing these measures are interconnections (i.e., interdependencies) among infrastructure systems. Understanding the nature of system interdependencies can play an essential role in minimizing and/or reducing the probabilities and consequences of cascading failures in interdependent systems. This paper discusses the need for policy-makers, infrastructure operators and researchers to consider alternative approaches to formulating risk and enabling solutions to challenging 21st century issues related to interdependent infrastructures. Using the healthcare sector as an example, this paper suggests that identifying the risks associated with maintaining public health goes beyond traditional risk formulation to include the structural complexity brought about by infrastructure interdependencies.",
                "year": 2014,
                "publisher": "International Journal of Critical Infrastructure Protection"
            }
        }
    },
    "Translational suppression of SARS-COV-2 ORF8 protein mRNA as a Viable therapeutic target against COVID-19: Computational studies on potential roles of isolated compounds from Clerodendrum volubile leaves": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482521007587/pdfft?md5=817a90265e6a074bad9e2a7ee316e1ba&pid=1-s2.0-S0010482521007587-main.pdf",
                "title": "Translational suppression of SARS-COV-2 ORF8 protein mRNA as a Viable therapeutic target against COVID-19: Computational studies on potential roles of isolated compounds from Clerodendrum volubile leaves",
                "abstract": "The open reading frame 8 (ORF8) protein of SARS-CoV-2 has been implicated in the onset of cytokine storms, which are responsible for the pathophysiology of COVID-19 infection. The present study investigated the potential of isolated compounds from Clerodendrum volubile leaves to stall oxidative bursts in vitro and interact with ORF8 mRNA segments of the SARS-CoV-2 whole genome using computational tools. Five compounds, namely, harpagide, 1-(3-methyl-2-butenoxy)-4-(1-propenyl)benzene, ajugoside, iridoid glycoside and erucic acid, were isolated from C. volubile leaves, and their structures were elucidated using conventional spectroscopy tools. Iridoid glycoside is being reported for the first time and is thus regarded as a new compound. The ORF8 mRNA sequences of the translation initiation sites (TIS) and translation termination sites (TTSs) encoding ORF8 amino acids were retrieved from the full genome of SARS-CoV-2. Molecular docking studies revealed strong molecular interactions of the isolated compounds with the TIS and TTS of ORF8 mRNA. Harpagide showed the strongest binding affinity for TIS, while erucic acid was the strongest for TTS. The immunomodulatory potentials of the isolated compounds were investigated on neutrophil phagocytic respiratory bursts using luminol-amplified chemiluminescence technique. The compounds significantly inhibited oxidative burst, with 1-(3-methyl-2-butenoxy)-4-(1-propenyl)benzene having the best activity. Ajugoside and erucic acid showed significant inhibitory activity on T-cell proliferation. These results indicate the potential of C. volubile compounds as immunomodulators and can be utilized to curb cytokine storms implicated in COVID-19 infection. These potentials are further corroborated by the strong interactions of the compounds with the TIS and TTS of ORF8 mRNA from the SARS-CoV-2 whole genome.",
                "year": 2021,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "SecureBallot: A secure open source e-Voting system": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1084804521001776/pdfft?md5=694c6579140b5089bbd470a0b593d2d5&pid=1-s2.0-S1084804521001776-main.pdf",
                "title": "SecureBallot: A secure open source e-Voting system",
                "abstract": "Voting is one of the most important acts through which a community can make a collective decision. In recent years, many works have focused on improving traditional voting mechanisms and, as a result, a wide range of electronic voting (e-Voting) systems have been proposed. Even though some approaches have achieved a proper level of usability, the main challenges of e-Voting are essentially still open: protect the privacy of participants, guarantee secrecy, anonymity, integrity, uniqueness, and authenticity of votes, while making e-Voting as trustful as voting. In order to address this issue, we present SecureBallot, a secure open-source e-Voting system that completely decouples the voter identification and voting phases by means of proven cryptographic technologies. The effectiveness of SecureBallot is demonstrated both theoretically, by presenting a formal verification of the whole protocol and assessing the security properties of its software components, and practically, by proposing a case study of university elections that contains all the challenges of a generic voting process.",
                "year": 2021,
                "publisher": "Journal of Network and Computer Applications"
            }
        }
    },
    "Disentangling the effects on OTT platform performance of three strategies: Pricing, M&As, and content investments": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Disentangling the effects on OTT platform performance of three strategies: Pricing, M&As, and content investments",
                "abstract": "OTT streaming services have been restructuring the global multimedia industry, and there have been strategic actions and reactions among OTT players and traditional pay-TV platforms. This study found that Pricing and Content investments are key factors affecting market share and subscribership in global OTT markets, and M&As have positive effects on the two performance measures, although with less statistical confidence. OTT-specialized firms in the UK, Germany, France, Japan, and South Korea did not expand subscribers to the same extent as those in the US, but they were able to expand their market share in the UK and Germany while failing to increase it in the other three countries. Local OTT players, especially those in Europe and Asia, might need to be more strategic in coping with US-based firms’ expansion by materializing the positive interaction effects among the three strategies to increase both their subscriber base and market share.",
                "year": 2023,
                "publisher": "Telecommunications Policy"
            }
        }
    },
    "Distributed Cognition on the road: Using EAST to explore future road transportation systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687017302624/pdfft?md5=39b6382587e3285a272a39391b1ace8f&pid=1-s2.0-S0003687017302624-main.pdf",
                "title": "Distributed Cognition on the road: Using EAST to explore future road transportation systems",
                "abstract": "Connected and Autonomous Vehicles (CAV) are set to revolutionise the way in which we use our transportation system. However, we do not fully understand how the integration of wireless and autonomous technology into the road transportation network affects overall network dynamism. This paper uses the theoretical principles underlying Distributed Cognition to explore the dependencies and interdependencies that exist between system agents located within the road environment, traffic management centres and other external agencies in both non-connected and connected transportation systems. This represents a significant step forward in modelling complex sociotechnical systems as it shows that the principles underlying Distributed Cognition can be applied to macro-level systems using the visual representations afforded by the Event Analysis of Systemic Teamwork (EAST) method.",
                "year": 2018,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Applications of neural networks to digital communications – a survey": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016516840000030X/pdfft?md5=d5d9fb7ecdd7a8dd77c98c03d6839e3a&pid=1-s2.0-S016516840000030X-main.pdf",
                "title": "Applications of neural networks to digital communications – a survey",
                "abstract": "Neural networks (NNs) are able to give solutions to complex problems in digital communications due to their nonlinear processing, parallel distributed architecture, self-organization, capacity of learning and generalization, and efficient hardware implementation. The paper gives an overview of the applications of NNs to digital communications such as channel identification and equalization, coding and decoding, vector quantization, image processing, nonlinear filtering, spread spectrum applications, etc. The key issue in neural network approaches is to find an appropriate architecture that gives the best results. The paper shows, through several examples, how to choose the neural network structures and how to combine neural network algorithms with other techniques such as adaptive signal processing, fuzzy systems and genetic algorithms. Finally, the paper reviews the mathematical approaches used to understand the learning and convergence behavior of neural network algorithms.",
                "year": 2000,
                "publisher": "Signal Processing"
            }
        }
    },
    "Theoretical basis of a computer-aided design education and research laboratory": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0142694X87900366/pdfft?md5=abe50c5307886e2a7f82d000902d694d&pid=1-s2.0-0142694X87900366-main.pdf",
                "title": "Theoretical basis of a computer-aided design education and research laboratory",
                "abstract": "This paper investigates the theoretical basis of computer-aided design (CAD) education and research relevant to the establishment of a micro-CAD laboratory in Istanbul Technical University (ITU). The initial paragraphs of the paper are allocated to the CAD systems' theoretical considerations. In the second part an interactive education and research model is developed. Applicability of this model through professional practice is discussed and examples of research findings are explained.",
                "year": 1987,
                "publisher": "Design Studies"
            }
        }
    },
    "Digital urban ambience: Mediating context on mobile devices in a city": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119212000624/pdfft?md5=2667a2b76a8c75c507068a0dc725d1cb&pid=1-s2.0-S1574119212000624-main.pdf",
                "title": "Digital urban ambience: Mediating context on mobile devices in a city",
                "abstract": "We explore the term digital urban ambience to describe the user experience of mobile devices mediating context in metropolitan areas. Mediated context-awareness explores a constructivist approach to context and aims to provide resources for experiencing context rather than correctly recognizing and adapting to it. Exploring this approach we have built a prototype system, eSpective2, combining maps and augmented reality on a touch phone, and we have studied its use. We conducted a triangulated field study involving 58 participants and up to one month of use. Elaborating on the work of Thibaud (2002)  [1], the main finding is that context resources provided through mobile devices digitally contribute to the experience or feeling of urban ambience (1) by adding to the quality of the situation as a whole, (2) as a catalyst for action, and (3) as a sensory background.",
                "year": 2013,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "Large-scale computations on a scalar, vector and parallel ‘supercomputer’": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167819187900044/pdfft?md5=1226f6f9709b6adfa9ee445accd03e74&pid=1-s2.0-0167819187900044-main.pdf",
                "title": "Large-scale computations on a scalar, vector and parallel ‘supercomputer’",
                "abstract": "We discuss two experimental parallel computer systems 1CAP-1 and 1CAP-2 which can be applied to the entire spectrum of scientific and engineering applications. These systems achieve ‘supercomputer’ levels of performance by spreading large scale computations across multiple cooperating processors—several with vector capabilities. We outline system hardware and software, and discuss our programming strategy for migrating codes from a conventional sequential system to a parallel one. The performance of a variety of applications programs is analyzed to demonstrate the merits of this approach. Finally, we discuss 1CAP-3, an extension to this computing system, which has been recently assembled.",
                "year": 1987,
                "publisher": "Parallel Computing"
            }
        }
    },
    "Deformable mesh for virtual shape sculpting": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584504001127/pdfft?md5=2a277188775cd1b4210f52ccd42175a5&pid=1-s2.0-S0736584504001127-main.pdf",
                "title": "Deformable mesh for virtual shape sculpting",
                "abstract": "A novel, interactive virtual sculpting framework based upon a deformable mesh model generated by a self-organizing feature map (SOFM) is described in this paper. The three-dimensional lattice of the SOFM maintains the relative connectivity of neighbouring nodes in the hexahedral mesh as it transforms from the initial reference geometry into the desired shape. Material and dynamic properties are incorporated into the deformable mesh by treating surface and internal nodes as point masses connected by a network of springs. The initial SOFM mesh can be either retrieved from a library of primitive shapes, or created by automatically adapting the 3D mesh to fit selected surface points. Once the initial mesh has been generated, the designer reshapes the virtual object by introducing external forces to the nodal mesh. The process of virtual sculpting is analogous to hand moulding of clay in the physical world where the material mass remains constant. During sculpting, the dynamically changing mesh can be easily rendered in VRML for visualization in a virtual reality environment. The deformable mesh generator and shape-sculpting system are illustrated by reshaping solid meshes created from scanned human heads.",
                "year": 2005,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Advances in multicomputers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0956052190900034/pdfft?md5=97e8959f159c3b4cc8f5845a6f648b90&pid=1-s2.0-0956052190900034-main.pdf",
                "title": "Advances in multicomputers",
                "abstract": "A multicomputer is a distributed memory parallel computing system made of multiple homogeneous or heterogeneous processors. Parallel machines of this kind are highly flexible in their configurations and usages, and are expected to be the mainstream high-performance computing environment of the 1990s.\n\nCarnegie Mellon University, with its competitively selected industrial partners, has two research projects in this area. The iWarp™ project has developed a single-chip building block processor for homogeneous multicomputers. Consisting of approximately 650,000 transistors, the iWarp processor is one of the first high-performance microprocessors specially designed for parallel processing.\n\nThe Nectar project has developed a high-speed network backplane for heterogeneous multicomputers. The Nectar network is made of fiber-optic links, large crossbar switches, and dedicated network coprocessors. A 26-host Nectar prototype using 100 megabit per second links is supporting research in distributed computing. The next generation Nectar, using 1 gigabit per second or higher speed fiber links, is under development.\n\nThese two efforts exemplify recent advances in multicomputers. Homogeneous multicomputers can now take advantage of highly integrated processors such as iWarp that have been specially designed to support the fast and flexible interprocessor communication needed in parallel processing. Heterogeneous multicomputers, capable of simultaneously providing different types of computing resources to meet application needs, can now be based on a general high-speed network backplane such as Nectar.",
                "year": 1990,
                "publisher": "Computing Systems in Engineering"
            }
        }
    },
    "Testing the potential of Multiobjective Evolutionary Algorithms (MOEAs) with Colorado water managers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815218302925/pdfft?md5=ff35414b7f16d3bbab40c656951c3788&pid=1-s2.0-S1364815218302925-main.pdf",
                "title": "Testing the potential of Multiobjective Evolutionary Algorithms (MOEAs) with Colorado water managers",
                "abstract": "Multiobjective Evolutionary Algorithms (MOEAs) generate quantitative information about performance relationships between a system's potentially conflicting objectives (termed tradeoffs). Research applications have suggested that evaluating tradeoffs can enhance long term water utility planning, but no studies have formally engaged with practitioners to assess their perceptions of tradeoffs generated by MOEAs. This article examines how practitioners interact with MOEA tradeoffs and reports their ideas for how their agencies could use MOEA results. We hosted a group of Colorado water managers at a charrette, or structured investigatory workshop, where they directly interacted with tradeoffs, discussed how they used the information, and linked their workshop experiences to opportunities for MOEAs to enhance their agencies' planning processes. Among other interesting results, we found that managers' portfolio preferences diverged as tradeoff information increased and that structured information about the relationships between decision levers and performance would be beneficial for interpreting tradeoffs.",
                "year": 2019,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Learned load balancing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Learned load balancing",
                "abstract": "Effectively balancing traffic in datacenter networks is a crucial operational goal. Most existing load balancing approaches are handcrafted to the structure of the network and/or network workloads. Thus, new load balancing strategies are required if the underlying network conditions change, e.g., due to hard or grey failures, network topology evolution, or workload shifts. While we can theoretically derive the optimal load balancing strategy by solving an optimization problem given certain traffic and topology conditions, these problems take too much time to solve and makes the derived solution stale to deploy. In this paper, we describe a load balancing scheme Learned Load Balancing (LLB), which is a general approach to finding an optimal load balancing strategy for a given network topology and workload, and is fast enough in practice to deploy the inferred strategies. LLB uses deep supervised learning techniques to learn how to handle different traffic patterns and topology changes, and adapts to any failures in the underlying network. LLB leverages emerging trends in network telemetry, programmable switching, and “smart” NICs. Our experiments show that LLB performs well under failures and can be expanded to more complex, multi-layered network topologies. We also prototype neural network inference on smartNICs to demonstrate the workability of LLB.",
                "year": 2024,
                "publisher": "Theoretical Computer Science"
            }
        }
    },
    "The Hanabi challenge: A new frontier for AI research": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0004370219300116/pdfft?md5=0286ac54c989e32902fbfec884a48539&pid=1-s2.0-S0004370219300116-main.pdf",
                "title": "The Hanabi challenge: A new frontier for AI research",
                "abstract": "From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.",
                "year": 2020,
                "publisher": "Artificial Intelligence"
            }
        }
    },
    "Mixed methods evaluation of a computerised audit and feedback dashboard to improve patient safety through targeting acute kidney injury (AKI) in primary care": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505620311539/pdfft?md5=002ee289302bfdc03b7e76387237c9f0&pid=1-s2.0-S1386505620311539-mainext.pdf",
                "title": "Mixed methods evaluation of a computerised audit and feedback dashboard to improve patient safety through targeting acute kidney injury (AKI) in primary care",
                "abstract": "Background\n\nReducing the harms associated with acute kidney injury (AKI) requires addressing a wide range of patient safety issues, including polypharmacy and transitions of care, particularly for vulnerable patient groups. Computerised audit and feedback can transform the way healthcare organisations measure, analyse and learn from quality and safety data across different care settings, potentially improving patient safety.",
                "year": 2021,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "Identifying potential human and medicinal plant microRNAs against SARS-CoV-2 3′UTR region: A computational genomics assessment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S001048252100456X/pdfft?md5=7ade82cf221d18a3c287bcb4b4b59875&pid=1-s2.0-S001048252100456X-main.pdf",
                "title": "Identifying potential human and medicinal plant microRNAs against SARS-CoV-2 3′UTR region: A computational genomics assessment",
                "abstract": "The coronavirus disease of 2019 (COVID-19) began as an outbreak and has taken a toll on human lives. The current pandemic requires scientific attention; hence we designed a systematic computational workflow to identify the cellular microRNAs (miRNAs) from human host possessing the capability to target and silence 3′UTR of SARS-CoV-2 genome. Based on this viewpoint, we extended our miRNA search to medicinal plants like Ocimum tenuiflorum, Zingiber officinale and Piper nigrum, which are well-known to possess antiviral properties, and are often consumed raw or as herbal decoctions. Such an approach, that makes use of miRNA of one species to interact and silence genes of another species including viruses is broadly categorized as cross-kingdom interactions. As a part of our genomics study on host-virus-plant interaction, we identified one unique 3′UTR conserved site ‘GGAAGAG’ amongst 5024 globally submitted SARS-CoV-2 complete genomes, which can be targeted by the human miRNA ‘hsa-miR-1236–3p’ and by Z. officinale miRNA ‘zof-miR2673b’. Additionally, we also predicted that the members of miR477 family commonly found in these three plant genomes possess an inherent potential to silence viral genome RNA and facilitate antiviral defense against SARS-CoV-2 infection. In conclusion, this study reveals a universal site in the SARS-CoV-2 genome that may be crucial for targeted therapeutics to cure COVID-19.",
                "year": 2021,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Assessment of objective voice quality over best-effort networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S014036640400369X/pdfft?md5=ba6d57c5a4c4e9320179d47b2c0a4ac2&pid=1-s2.0-S014036640400369X-main.pdf",
                "title": "Assessment of objective voice quality over best-effort networks",
                "abstract": "VoIP calls transferred over dedicated bandwidth or QoS capable networks is a cost-effective alternative for PSTN in large enterprises. However, the calls made over the best effort network, such as the global Internet, suffer packet loss and jitter. In some VoIP-codecs, such as ITU G.723.1 and G.729a, there are built-in recovery mechanisms for concealing packet-based errors in the audio/speech stream. These recovery mechanisms can conceal up to 5% packet losses without significant quality degradation, as shown in this article. The 5% quality degradation is approximately within 0.5 MOS scale when compared to the original signal. Beyond 5%, the speech quality will drop gradually. The overall quality of MOS scale 3 can be maintained even with 14–17% packet loss rates. The influence of delay variation or jitter cannot be eliminated with the concealment algorithms unless the jitter time exceeds packet loss indication delay. The influence of jitter is not critical below 20 ms but beyond 20 ms limit its influence will decrease the speech quality very steeply. This suggests that packet losses can be recovered in normal conditions, but the influence of jitter must be eliminated somehow. The interleaving and piggybacking-based stream manipulation enhances speech quality in packet dropout situations. The guaranteed delay over the whole Internet would enhance the possibilities of VoIP to achieve success.",
                "year": 2005,
                "publisher": "Computer Communications"
            }
        }
    },
    "In situ monitoring of selective laser melting of zinc powder via infrared imaging of the process plume": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584517300583/pdfft?md5=edaf6011c6e88defb3958c35ca7e0ff1&pid=1-s2.0-S0736584517300583-main.pdf",
                "title": "In situ monitoring of selective laser melting of zinc powder via infrared imaging of the process plume",
                "abstract": "Despite continuous technological improvements in metal additive manufacturing (AM) systems, process stability is still affected by several possible sources of defects especially in the presence of challenging materials. Thus, both the research community and the major AM system developers have focused an increasing attention on in situ sensing and monitoring tools in the last years. However, there is still a lack of statistical methods to automatically detect the onset of a defect and signal an alarm during the part's layer-wise production. This study contributes to this framework with two levels of novelty. First, it presents an in situ monitoring method that integrates the acquisition of infrared images with a data mining approach for feature extraction and a statistical process monitoring technique to design a data-driven and automated alarm rule. Second, the method is aimed at monitoring powder bed fusion processes for difficult-to-process materials like zinc and its alloys, which impose several challenges to the process stability and quality because of their low melting and boiling points. To this aim, the proposed approach analyzes the byproducts generated by the interaction between the energy source and the material. In particular, it detects unstable behaviors by analyzing the salient properties of the process plume to detect unstable melting conditions. This case study entails an SLM process on zinc powder, where different sets of process parameters were tested leading either to in-control or out-of-control quality conditions. A comparison analysis highlights the effectiveness of plume-based stability monitoring.",
                "year": 2018,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563224002541/pdfft?md5=9847cca1c82d7d2f77122b9b2db1ae95&pid=1-s2.0-S0747563224002541-main.pdf",
                "title": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry",
                "abstract": "This study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students' recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students’ perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.",
                "year": 2024,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Modeling and executing cyber security exercise scenarios in cyber ranges": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404822000347/pdfft?md5=77563a9d0def2110d176e84f6c9aee9c&pid=1-s2.0-S0167404822000347-main.pdf",
                "title": "Modeling and executing cyber security exercise scenarios in cyber ranges",
                "abstract": "The skill shortage in global cybersecurity is a well-known problem; to overcome this issue, cyber ranges have been developed. These ranges provide a platform for conducting cybersecurity exercises; however, conducting such exercises is a complex process because they involve people with different skill sets for the scenario modeling, infrastructure preparation, dry run, execution, and evaluation. This process is very complex and inefficient in terms of time and resources. Moreover, the exercise infrastructure created in current cyber ranges does not reflect the dynamic environment of real-world systems and does not provide adaptability for changing requirements. To tackle these issues, we developed a system that can automate many tasks of the cybersecurity exercise life cycle. We used model-driven approaches to (1) model the roles of the different teams present in the cybersecurity exercises and (2) generate automation artifacts to execute their functions efficiently in an autonomous manner. By executing different team roles such as attackers and defenders, we can add friction in the environment, making it dynamic and realistic. We conducted case studies in the form of operational cybersecurity exercises involving national-level cybersecurity competitions and a university class setting in Norway to evaluate our developed system for its efficiency, adaptability, autonomy, and skill improvement of the exercise participants. In the right conditions, our proposed system could create a complex cybersecurity exercise infrastructure involving 400 nodes with customized vulnerabilities, emulated attackers, defenders, and traffic generators under 40 minutes. It provided a realistic environment for cybersecurity exercises and positively affected the exercise participants’ skill sets.",
                "year": 2022,
                "publisher": "Computers & Security"
            }
        }
    },
    "FedPA: An adaptively partial model aggregation strategy in Federated Learning": {
        "accordingTo": {
            "scienceDirect": {
                "title": "FedPA: An adaptively partial model aggregation strategy in Federated Learning",
                "abstract": "Federated Learning has sparked increasing interest as a promising approach to utilize large amounts of data stored on network edge devices. Federated Averaging is the most widely accepted Federated Learning framework. In Federated Averaging, the server keeps waiting for client models to compute the global model in each round unless all client models are received or a pre-configured timer expires, therefore it suffers seriously from participant devices with weak computation and/or communication capability, which is a kind of straggler problem. In this paper we design FedPA, a framework based on partial model aggregation strategy, in which the server waits for only an appropriate number of device models (referred to as aggregation number) in each round. Our experiment shows that the accuracy loss of the aggregated global model in a single round is not significant if the aggregation number is decided carefully. We propose a waiting strategy to determine the aggregation number for each round dynamically and the aggregation number is adaptive to achieve a tradeoff between single-round training time and the expected number of rounds to reach the target accuracy. Stale models are also included during aggregation when they arrive, and their positive value and negative effect are carefully evaluated and reflected in the aggregation strategy. Experiments show that FedPA outperforms the baseline strategy FedAvg and other three algorithms named FedAsync, FLANP and AD-SG. It can work well in all scenarios with different distributions of data samples (characterized by non-IID ratio) and computation/communication capability (characterized by level of heterogeneity) among devices. Experiments also show that FedPA is robust when a certain amount of noise is added into the input from clients for privacy concerns.",
                "year": 2021,
                "publisher": "Computer Networks"
            }
        }
    },
    "Role of personality in computer based learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216305337/pdfft?md5=a52562703e2df0e9ab3200b280f2d943&pid=1-s2.0-S0747563216305337-main.pdf",
                "title": "Role of personality in computer based learning",
                "abstract": "With the rapid growth of technology enhanced learning, new mediums for learning have emerged. One of these mediums is computer based learning where the main concern is how to design a computer based learning system which takes into consideration the learners' differences. Personality is considered as one of the most critical sources of individual differences. This study investigates how personality differences within learners can affect computer based learning, through a comprehensive review of the literature. The highlighted results from the obtained nineteen studies are: (a) the most referred to personality model in computer based learning is MBTI; (b) personality traits affect how learners prefer learning content and learning approach like collecting information, communicating with instructor and peer, study behavior, acting and performing; (c) a new model of personality variables should be considered in computer based learning by taking all interested researchers and practitioners into accounts; and (d) the traditional questionnaire approach which is still the pre-dominant method to identify the learner's personality; and this needs to be changed with new potential of big data and learning analytics. Furthermore, this study presents a new implicit approach using learning analytics instead of questionnaire-based approach to identify the learner's personality.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A Framework for Modelling and Simulating Networks of Cells": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1571066110001568/pdfft?md5=4cd631e6f702563c115abe71a82d01b4&pid=1-s2.0-S1571066110001568-main.pdf",
                "title": "A Framework for Modelling and Simulating Networks of Cells",
                "abstract": "Several complex biological phenomena are to be modelled in terms of a large and dynamic network of compartments, where the interplay between inter-compartment and intra-compartment events plays an essential role. Key examples are embryogenesis and morphogenesis processes, where autonomous internal dynamics of cells, as well as cell-to-cell interactions through membranes, are responsible for the emergent peculiar structures of the individual phenotype.\n\nThis paper introduces a practical framework for modelling and simulating these scenarios. This is based on (i) a computational model featuring networks of compartments and an enhanced model of chemical reaction addressing molecule transfer, (ii) a logic-oriented language to flexibly specify complex simulation scenarios, and (iii) a simulation engine based on the many-species/many-channels optimised version of Gillespie's direct method. As an example of application of our framework, we model the first stages of Drosophila Melanogaster development, which generate the early spatial pattern of gene expression, and we show the correctness of our model comparing the simulation results with real data of gene expression and spatial/temporal resolution acquired in free on-line sources.",
                "year": 2010,
                "publisher": "Electronic Notes in Theoretical Computer Science"
            }
        }
    },
    "Morpheus: An efficient timing-based attestation framework for safeguarding hypervisor integrity with dynamic trust": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404824002712/pdfft?md5=1932973ca027cb7cccd5eeac5fe87b50&pid=1-s2.0-S0167404824002712-main.pdf",
                "title": "Morpheus: An efficient timing-based attestation framework for safeguarding hypervisor integrity with dynamic trust",
                "abstract": "Hypervisor, the core software in cloud computing, is susceptible to malicious intrusions, potentially jeopardizing overall platform security. Physical hosts within the cloud computing environment constantly face persistent attacks, and not all hosts are equipped with essential security hardware. The prolonged latency of previous software-based detection methods could not comprehensively address these cloud threats. Furthermore, the widespread deployment of security hardware incurs substantial costs for hardware-based detection. To address these challenges, we introduce Morpheus, an efficient framework ensuring hypervisor integrity in the current cloud. This framework rapidly employs software-based methods to detect malicious hosts, utilizing a subset of hosts equipped with security hardware as the Root of Trust. Efficiency is augmented through a Neural Network scheduling module, and an embedded exponential aging mechanism fortifies time-aging trust against consistent cloud threats. Evaluation results demonstrate that it can promptly identify threatened hosts with acceptable system overhead loss, solidifying its position as a robust cloud security solution.",
                "year": 2024,
                "publisher": "Computers & Security"
            }
        }
    },
    "Designing a personal music assistant that enhances the social, cognitive, and affective experiences of people with dementia": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216304319/pdfft?md5=24573ba316164f86e5e5d271585dec02&pid=1-s2.0-S0747563216304319-main.pdf",
                "title": "Designing a personal music assistant that enhances the social, cognitive, and affective experiences of people with dementia",
                "abstract": "Research shows that music with a strong personal meaning can enhance the social, cognitive, and affective experiences of both people with dementia (PwD) and their social environment. We applied a human-centred design method, called situated Cognitive Engineering, to develop the conceptual design and design rationale of the Music ePartner. The design rationale specifies the general knowledge-base (ontology), context (use cases), and expected effects (claims) of the ePartner support. Three functionalities were developed through rapid prototyping: (1) annotated play lists, (2) a music and picture album, and (3) a picture slide show. Accompanied by a close relative, five PwD participated in a formative evaluation of the prototype at their regular day care centres. All participants interacted with all three functionalities of the prototype as they would in their natural setting. The researchers observed participants’ responses to the prototype using observational scoring forms, and interviewed participants about their experiences using semi-structured interviews. Results showed that the music stimulated PwD to tell life stories related to the songs. Furthermore, music evoked positive individual and group experiences. Specific constraints, additional user needs, and interaction requirements for the Music ePartner resulted in a refinement of both the requirements baseline and the design rationale.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A study of IS assets, IS ambidexterity, and IS alignment: the dynamic managerial capability perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720617302604/pdfft?md5=6ed4c2d2d3c88a88d7e85d35798381da&pid=1-s2.0-S0378720617302604-main.pdf",
                "title": "A study of IS assets, IS ambidexterity, and IS alignment: the dynamic managerial capability perspective",
                "abstract": "This study aims to explore how IT-business alignment can be better achieved. Drawing on the dynamic capability view, information systems (IS) alignment and IS ambidexterity are theorized as IT departments’ ordinary capability and dynamic managerial capability, respectively. Four IS assets are identified as antecedents of both IS ambidexterity and IS alignment. A research model with 14 hypotheses is tested with a sample of 162 manufacturing firms. The PLS analysis shows that IS ambidexterity can increase IS alignment in terms of operational support and that the four IS assets can affect IS alignment directly or indirectly. Implications for research and practice are provided.",
                "year": 2019,
                "publisher": "Information & Management"
            }
        }
    },
    "Quality function deployment: A literature review": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221702001789/pdfft?md5=d6d938fb6e67d169740f65e0903e630d&pid=1-s2.0-S0377221702001789-main.pdf",
                "title": "Quality function deployment: A literature review",
                "abstract": "This paper presents a literature review of quality function deployment (QFD) based on a reference bank of about 650 QFD publications established through searching various sources. The origination and historical development of QFD, especially in Japan and the US, are briefly accounted first, followed by a partial list of QFD organizations, softwares, and online resources. Then a categorical analysis is conducted about QFD’s functional fields, applied industries and methodological development. Ten informative QFD publications are also suggested, particularly for those who are not yet familiar with QFD. It is hoped that the paper can serve the needs of researchers and practitioners for easy references of QFD studies and applications, and hence promote QFD’s future development.",
                "year": 2002,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Analogical inference from distributional structure: What recurrent neural networks can tell us about word learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2666827023000312/pdfft?md5=1ded6552152c35dd87be73cb5024f347&pid=1-s2.0-S2666827023000312-main.pdf",
                "title": "Analogical inference from distributional structure: What recurrent neural networks can tell us about word learning",
                "abstract": "One proposal that can explain the remarkable pace of word learning in young children is that they leverage the language-internal distributional similarity of familiar and novel words to make analogical inferences about possible meanings of novel words (Lany and Gómez, 2008; Lany and Saffran, 2011; Savic et al., 2022b; Unger and Fisher, 2021; Wojcik and Saffran, 2015). However, a cognitively and developmentally plausible computational account of how language-internal lexical representations are acquired to enable this kind of analogical inference has not been previously investigated. In this work, we tested the feasibility of using the SRN (Elman, 1990) as the supplier of language-internal representations for use in analogical inference. While the SRN is in many ways well suited to this task, we discuss several theoretical challenges that might limit its success. In a series of simulations with controlled artificial languages and the CHILDES corpus, we show that Recurrent Neural Networks (RNNs) are prone to acquiring ‘entangled’ lexical semantic representations, where some features of a word are partially encoded in the representations of other frequently co-occurring words. However, we also show that this problem is mitigated when RNNs are first trained on language input to young children, due to the fact that its distributional structure more reliably predicts semantic category membership of individual words. Overall, our work sheds light on the conditions under which RNNs organize their learned knowledge so that word-level information can be more easily extracted and used in downstream processes, such as word learning.",
                "year": 2023,
                "publisher": "Machine Learning with Applications"
            }
        }
    },
    "Toolbox for Visual Explorative Analysis of Complex Temporal Multiscale Contact Networks Dynamics in Healthcare": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050916310201/pdfft?md5=ab3d37f8a9b00e63c924de890f565ab7&pid=1-s2.0-S1877050916310201-main.pdf",
                "title": "Toolbox for Visual Explorative Analysis of Complex Temporal Multiscale Contact Networks Dynamics in Healthcare",
                "abstract": "Public healthcare can be cast as a complex systems and network analysis is one of the methodological approaches that are commonly used to study these types of systems. In this paper we describe a multiscale and multi-level interpretation of complex networks in public healthcare. Our contribution is to provide a toolbox for visualization and visual data-driven analysis of complex multiscale temporal contact networks that allows to simulate various dynamic processes using user-defined models. An example of explorative analysis of a dataset from real clinical data obtained from the Federal Almazov North-West Medical Research Centre in Saint Petersburg is described.",
                "year": 2016,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Cultural intelligence as an antecedent of satisfaction with the travel app and with the tourism experience": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563221003721/pdfft?md5=39138eb87413adb6176528dc8bce91b6&pid=1-s2.0-S0747563221003721-main.pdf",
                "title": "Cultural intelligence as an antecedent of satisfaction with the travel app and with the tourism experience",
                "abstract": "Smartphones and apps exert a decisive influence on the tourism industry. However, cultural differences can be a barrier to technology-transfer and they influence all aspects of individuals' behavior. In this regard, cultural intelligence (CQ) enables individuals to deal more effectively with these differences, and those with a high CQ are more adaptable and able to cope in cultural environments other than their own. The aim of the present study is to propose and validate a model in which CQ is an antecedent of satisfaction with the travel app and with the tourism experience. Based on a sample of 243 Spanish tourists who used a travel app on their trip, the study finds that a tourist's CQ influences their satisfaction both with the app and with the tourism experience. It further demonstrates the influence of satisfaction with the travel app on satisfaction with the tourism experience. This research holds a series of implications of significant interest both for scholars and professionals in the tourism industry.",
                "year": 2022,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Modeling impact induced delamination of woven fiber reinforced composites with contact/cohesive laws": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782599002224/pdfft?md5=141c97b8800f738546afce5d6160132b&pid=1-s2.0-S0045782599002224-main.pdf",
                "title": "Modeling impact induced delamination of woven fiber reinforced composites with contact/cohesive laws",
                "abstract": "The dynamic delamination in woven glass fiber reinforced plastic (GRP) composite is studied with a 3D finite deformation anisotropic viscoplastic model in conjunction with contact/cohesive laws. The large deformation of the material during impact loading is described through an anisotropic plasticity model in total Lagrangian co-ordinates whose coefficients are determined experimentally. The interaction between lamina is analyzed through a contact/interface model. The tensile and shear tractions in zero thickness interface elements, embedded between lamina, are calculated from interface cohesive law. The interface cohesive law describes the evolution of these tractions in terms of normal and tangential displacement jumps and other interface parameters. The compressive traction at the interface is calculated through the impenetrability condition employed in the contact module. Once the effective displacement jump exceeds a specified critical value, the interface elements are assumed to have failed, i.e., delamination is said to have taken place. Three interface cohesive laws are proposed to describe the delamination process. It is assumed that loading of interface takes place reversibly up to a specified value of the displacement jump followed by irreversible loading beyond this value. This feature represents a partial damage of the interface in the event of unloading. Dynamic delamination in the woven GRP composite is studied through analyses of plate-on-plate impact experiments. The heterogeneity of composite materials leading to wave dispersion and scattering is modeled by considering a layered composition of the GRP plate. Each lamina is assumed to be made of three layers of materials. The middle layer of half the thickness of lamina is considered as GRP and the two end layers of equal thicknesses are considered to be of matrix material, i.e., polyester resin. The possible delamination of the composite material under compressive shock loading is shown to occur due to local shear effects. This is modeled by considering waviness of the interface between lamina. Interfaces with flat as well as two types of wavy structures are analyzed. Analyses are carried out to establish the effect of critical displacement jump, mixed mode coupling parameter employed in interface laws, interface waviness and the effect of interface laws. The response of GRP composite during impact is characterized in terms of the free surface velocity, delamination event vs. time and interface normal and shear stresses. The interface normal and shear stresses are obtained directly from the interface cohesive laws, as well as, by extrapolating continuum stresses from integration points of the neighboring triangular elements to the interface. It is shown that the finite element model predicts the response of the material in confirmation with the available experimental results. The wave dispersion and scattering effects are obtained in the form of attenuation of shock stress and free surface velocity. The model predicts partial delamination during compressive shock loading above a certain threshold due to local shear and mode coupling.",
                "year": 2000,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Image effects on selective exposure to computer-mediated news stories": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563204001980/pdfft?md5=8e7f8995c2b109778ab449653adf4bf9&pid=1-s2.0-S0747563204001980-main.pdf",
                "title": "Image effects on selective exposure to computer-mediated news stories",
                "abstract": "Do pictures accompanying news stories influence selective exposure to the stories as compared to the same stories viewed without pictures? Further, does the emotional nature of the image – non-threatening versus threatening – impact selective exposure? This study aimed to answer these questions by examining the impact of different types of pictures on selectivity of and duration of exposure to computer-mediated news stories. Male and female participants were randomly assigned to one of four image conditions (text only control, reporter portrait, non-threatening, threatening) in which they were able to freely select and view news stories via a computer based delivery system. The findings of this study revealed that exposure to both non-threatening and threatening images resulted in significantly longer self-exposure to that portion of the news story where the image and text were presented concurrently. Furthermore, the data at hand shows that the incorporation of negatively valenced images accompanying news stories resulted in significantly longer self-exposure time to subsequent sections of the news story text devoid of an accompanying image. This latter finding provides the first evidence of selective exposure induced by a graphic, threatening image that is not confounded with the presence of the image. Some implications of these findings and avenues for future research are discussed.",
                "year": 2007,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Adolescents’ Well-being While Using a Mobile Artificial Intelligence–Powered Acceptance Commitment Therapy Tool: Evidence From a Longitudinal Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Adolescents’ Well-being While Using a Mobile Artificial Intelligence–Powered Acceptance Commitment Therapy Tool: Evidence From a Longitudinal Study",
                "abstract": "Background\n\nAdolescence is a critical developmental period to prevent and treat the emergence of mental health problems. Smartphone-based conversational agents can deliver psychologically driven intervention and support, thus increasing psychological well-being over time.",
                "year": 2022,
                "publisher": "JMIR AI"
            }
        }
    },
    "Assessing the determinants of internet banking adoption intentions: A social cognitive theory perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216306446/pdfft?md5=026cb05f994fbf078b39acae40e093bf&pid=1-s2.0-S0747563216306446-main.pdf",
                "title": "Assessing the determinants of internet banking adoption intentions: A social cognitive theory perspective",
                "abstract": "Internet banking adoption is one area that has received attention from scholars. The extant studies have mainly used technology acceptance models and behavioural theories which do not account for changes in human behaviour. This study seeks to ascertain the determinants of Internet banking adoption intentions using the social cognitive theory, which accounts for changes in human behaviour. The study selected the sample from bank customers in Ghana through an intercept approach using structured questionnaires. A two stage-approach of confirmatory factor analysis and a structural equation modelling were used in analysing the data. The findings show that websites' social feature, trust, compatibility with lifestyle and online customer services have a significant effect on customers' intentions to adopt Internet banking. However, ease of use did not have a significant relationship with customers' intentions to adopt Internet banking. The significance of the study as well as recommendations for theory, practice and future studies have been discussed.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A hybrid E-learning recommendation integrating adaptive profiling and sentiment analysis": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A hybrid E-learning recommendation integrating adaptive profiling and sentiment analysis",
                "abstract": "This research proposes a novel framework named Enhanced e-Learning Hybrid Recommender System (ELHRS) that provides an appropriate e-content with the highest predicted ratings corresponding to the learner’s particular needs. To accomplish this, a new model is developed to deduce the Semantic Learner Profile automatically. It adaptively associates the learning patterns and rules depending on the learner’s behavior and the semantic relations computed in the semantic matrix that mutually links e-learning materials and terms. Here, a semantic-based approach for term expansion is introduced using DBpedia and WordNet ontologies. Further, various sentiment analysis models are proposed and incorporated as a part of the recommender system to predict ratings of e-learning resources from posted text reviews utilizing fine-grained sentiment classification on five discrete classes. Qualitative Natural Language Processing (NLP) methods with tailored-made Convolutional Neural Network (CNN) are developed and evaluated on our customized dataset collected for a specific domain and a public dataset. Two improved language models are introduced depending on Skip-Gram (S-G) and Continuous Bag of Words (CBOW) techniques. In addition, a robust language model based on hybridization of these couple of methods is developed to derive better vocabulary representation, yielding better accuracy 89.1% for the CNN-Three-Channel-Concatenation model. The suggested recommendation methodology depends on the learner’s preferences, other similar learners’ experience and background, deriving their opinions from the reviews towards the best learning resources. This assists the learners in finding the desired e-content at the proper time.",
                "year": 2022,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Crossing the chasm between ontology engineering and application development: A survey": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826821000305/pdfft?md5=3a8563236a569af68957c17ff54e5ebc&pid=1-s2.0-S1570826821000305-main.pdf",
                "title": "Crossing the chasm between ontology engineering and application development: A survey",
                "abstract": "The adoption of Knowledge Graphs (KGs) by public and private organizations to integrate and publish data has increased in recent years. Ontologies play a crucial role in providing the structure for KGs, but are usually disregarded when designing Application Programming Interfaces (APIs) to enable browsing KGs in a developer-friendly manner. In this paper we provide a systematic review of the state of the art on existing approaches to ease access to ontology-based KG data by application developers. We propose two comparison frameworks to understand specifications, technologies and tools responsible for providing APIs for KGs. Our results reveal several limitations on existing API-based specifications, technologies and tools for KG consumption, which outline exciting research challenges including automatic API generation, API resource path prediction, ontology-based API versioning, and API validation and testing.",
                "year": 2021,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Unit commitment by an improved binary quantum GSA": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494617303940/pdfft?md5=66370c9462f48183db95ea7089de589a&pid=1-s2.0-S1568494617303940-main.pdf",
                "title": "Unit commitment by an improved binary quantum GSA",
                "abstract": "Unit commitment (UC) problem is an important optimizing task for scheduling the on/off states of generating units in power system operation over a time horizon such that the power generation cost is minimized. Since, increasing the number of generating units makes it difficult to solve in practice, many approaches have been introduced to solve the UC problem. This paper introduces an improved version of the binary quantum-inspired gravitational search algorithm (BQIGSA) and proposes a new approach to solve the UC problem based on the improved BQIGSA, called QGSA-UC. The proposed approach is applied to unit commitment problems with the number of generating units in the range of 10–120 along with 24-h scheduling horizon and is compared with nine state-of-the-art approaches. Furthermore, four different versions of gravitational approach are implemented for solving the UC problem and compared with those obtained by QGSA-UC. Comparative results clearly reveal the effectiveness of the proposed approach and show that it can be used as a reliable tool to solve UC problem.",
                "year": 2017,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Virtual assistants in the family home. Understanding parents’ motivations to use virtual assistants with their Child(dren)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222003466/pdfft?md5=ca4873965b7c408bc834d85baf29110b&pid=1-s2.0-S0747563222003466-main.pdf",
                "title": "Virtual assistants in the family home. Understanding parents’ motivations to use virtual assistants with their Child(dren)",
                "abstract": "Virtual assistants (VA) like Siri, Alexa, or Google Assistant are becoming household names - especially for families with young children. Scientific inquiry studying this user population and their intention to use VAs at home, however, remains scarce. By bridging the Technology Acceptance Model, Uses and Gratifications theory, and the first proposition of the Differential Susceptibility to Media Effects Model, this study disentangles (1) different types of families with (2) different motivations for (3) different forms of VA-usage (i.e., parent only, child only, co-use). Cross-sectional survey data (N = 305) from Dutch parents with at least one child between 3 and 8 years and a Google Assistant-powered smart speaker in their home show that (1) families mostly differ along parents' digital literacy skills, frequency of VA-use, trust in technology, and preferred degree of child media-mediation. (2) Hedonic motivation is key for parents to (3) co-use the VA together with their child(ren). New pathways for the methodological and theoretical study of technology use in families are highlighted. Developers can best anchor VA-application among families in aspects of enjoyment while scholars and policy makers might wish to consider additional meaningful intervention criteria for the future study and guidance of family VA-use practices. (197 words).",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Real-time detection of ventilatory meaneuvers by a microcomputer": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0169260786900192/pdfft?md5=78f0c942abc785dfa4d6510d69c35edf&pid=1-s2.0-0169260786900192-main.pdf",
                "title": "Real-time detection of ventilatory meaneuvers by a microcomputer",
                "abstract": "The SYTER software system was built around and Appled II microcomputer to automatically process the data yielded by three ventilatory test: analysis of lung elasticity during an expiratory maneuver, plethysmographic measurement of airway resistance during a panting maneuver, and similar measurements of lung volumes. A volume signal is displayed to the operator throughout each test to help control the maneuver. The various stages of the test are recognized in real time by routines written in assembling language, so that significant data are displayed during the maneuver. This paper describes the algorithmsused to perform waveform analysis, and presents illustrate displaysd obtained in the Lung Function Laboratory.",
                "year": 1986,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Computational analysis of drug like candidates against Neuraminidase of Human Influenza A virus subtypes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914819303016/pdfft?md5=e295575f4ee1a04c0ce34c2b0b4d959c&pid=1-s2.0-S2352914819303016-main.pdf",
                "title": "Computational analysis of drug like candidates against Neuraminidase of Human Influenza A virus subtypes",
                "abstract": "Influenza virus is an enveloped virus having segmented genome, coded by eight (negative-sense) RNA segments. The RNA segments are packed in a nucleocapsid protein. Neuraminidase (NA), an exosialidase, is the major antigen as well as vital virulence factor of the virus and is coded by the 6th segment of RNA. Eleven subtypes of NA are characterized for influenza A and the N1–N9 are divided into two phylogenic groups. NA is an active target for development of potent antivirals or Neuraminidase inhibitors (NAIs), which are reported to interact with multiple conserved residues in the active site of NA resulting in inhibition of virion release and prevention of viral spread to nearby cells. The current computational study focuses on docking of eleven different drug-like molecules including the known NAIs such as oseltamivir and zanamivir against crystal structures of NA of human Influenza A virus subtypes of both phylogenic groups. The docking results of this study indicated that zanamivir was the most active drug candidate against various NA subtypes followed by laninamivir. The study provides insights on interactions as well as cross-reactivity between lead compounds and NA proteins of various influenza subtypes. Observations on the efficacy of these NAIs assessed by the computational study have to be validated through in vitro and in vivo studies for successfully translating them as potential therapeutic agents to treat infection by various subtypes of influenza.",
                "year": 2020,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Optical and Optoelectronic Computing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Optical and Optoelectronic Computing",
                "abstract": "This chapter presents the fundamentals of optical computing and discusses the elements of optical computers—such as memories, logic arrays, and input/output devices. The different types of optical processors are also discussed. The basic mathematical operations for optical computing include addition, subtraction, and multiplication. In optical systems, the input functions (either one- or two-dimensional) are entered by spatial light modulators (SLMs). Holography is a technique for storing the whole information about an object. After the hologram is recorded and developed, it can be used to reconstruct an image of the object. Holographic memories do not use the 3-D display characteristic of holography. The hologram that is most suitable for data storage is the Fourier hologram. The hybrid processors combine the advantages of analog optics with those of digital electronics. These processors have applications in both laboratory and industry and are promising architectures for computationally intensive problems—such as solving large sets of linear or nonlinear equations.",
                "year": 1989,
                "publisher": "Advances in Computers"
            }
        }
    },
    "Gene pathways and subnetworks distinguish between major glioma subtypes and elucidate potential underlying biology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046410001322/pdfft?md5=f65bfbc4fa08236c813cadf47bbccb80&pid=1-s2.0-S1532046410001322-main.pdf",
                "title": "Gene pathways and subnetworks distinguish between major glioma subtypes and elucidate potential underlying biology",
                "abstract": "Molecular diagnostic tools are increasingly being used in an attempt to classify primary human brain tumors more accurately. While methods that are based on the analysis of individual gene expression prove to be useful for diagnostic purposes, they are devoid of biological significance since tumorgenesis is a concerted deregulation of multiple pathways rather than single genes. In a proof of concept, we utilize two large clinical data sets and show that the elucidation of enriched pathways and small differentially expressed sub-networks of protein interactions allow a reliable classification of glioblastomas and oligodendrogliomas. Applying a feature selection method, we observe that an optimized subset of pathways and subnetworks significantly improves the prediction accuracy. By determining the enrichment of altered genes in pathways and subnetworks we show that optimized subsets of genes rarely seem to be a target of genomic alteration. Our results suggest that groups of genes play a decisive role for the phenotype of the underlying tumor samples that can be utilized to reliably distinguish tumor types. In the absence of enrichment of genes that are genomically altered we assume that genetic changes largely exert an indirect rather than direct regulatory influence on a number of tumor-defining regulatory networks.",
                "year": 2010,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Active learning using adaptable task-based prioritisation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841524001063/pdfft?md5=17e14769bfbdeb3a7528934929b0cb29&pid=1-s2.0-S1361841524001063-main.pdf",
                "title": "Active learning using adaptable task-based prioritisation",
                "abstract": "Supervised machine learning-based medical image computing applications necessitate expert label curation, while unlabelled image data might be relatively abundant. Active learning methods aim to prioritise a subset of available image data for expert annotation, for label-efficient model training. We develop a controller neural network that measures priority of images in a sequence of batches, as in batch-mode active learning, for multi-class segmentation tasks. The controller is optimised by rewarding positive task-specific performance gain, within a Markov decision process (MDP) environment that also optimises the task predictor. In this work, the task predictor is a segmentation network. A meta-reinforcement learning algorithm is proposed with multiple MDPs, such that the pre-trained controller can be adapted to a new MDP that contains data from different institutes and/or requires segmentation of different organs or structures within the abdomen. We present experimental results using multiple CT datasets from more than one thousand patients, with segmentation tasks of nine different abdominal organs, to demonstrate the efficacy of the learnt prioritisation controller function and its cross-institute and cross-organ adaptability. We show that the proposed adaptable prioritisation metric yields converging segmentation accuracy for a new kidney segmentation task, unseen in training, using between approximately 40% to 60% of labels otherwise required with other heuristic or random prioritisation metrics. For clinical datasets of limited size, the proposed adaptable prioritisation offers a performance improvement of 22.6% and 10.2% in Dice score, for tasks of kidney and liver vessel segmentation, respectively, compared to random prioritisation and alternative active sampling strategies.",
                "year": 2024,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Protection, expertise and domination: Cyber masculinity in practice": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404823003188/pdfft?md5=829454e48f37c7a6814f1013240fa488&pid=1-s2.0-S0167404823003188-main.pdf",
                "title": "Protection, expertise and domination: Cyber masculinity in practice",
                "abstract": "Masculine tropes are ingrained in cyber-security discourse, as with other forms of security. Such language perpetuates concepts and paradigms that regulate the identities of those involved in cyber-security practice. Further, it may serve to exclude those for whom such identities are not desirable. Through a qualitative study of 18 commercial businesses, utilising data gained from semi-structured interviews of CISOs and senior organisational leaders as well as analysis of company documentation, this study provides empirical data that demonstrates the use of masculine language and concepts in cyber-security practice, which is discussed and deconstructed through a lens of identity work. In so doing, this paper identifies such language as being both regulatory and exclusionary. We introduce the term cyber masculinity to refer to the dominance of these masculine concepts and propose future research directions that can challenge this dominance and achieve a de-masculinization of the field.",
                "year": 2023,
                "publisher": "Computers & Security"
            }
        }
    },
    "A multiscale method for periodic structures using domain decomposition and ECM-hyperreduction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782520303777/pdfft?md5=d9561f6100669482f949b0d5912010c2&pid=1-s2.0-S0045782520303777-main.pdf",
                "title": "A multiscale method for periodic structures using domain decomposition and ECM-hyperreduction",
                "abstract": "This paper presents a nonlinear multiscale approach for periodic structures in the quasi-static, small strain regime. The approach consists in combining a domain decomposition method in which interface conditions are established through a “fictitious” frame with reduced-order modeling (ROM). We propose to approximate interface displacements, subdomain displacements and Lagrange Multipliers as linear combinations of reduced sets of dominant modes, and to assign to the coefficients of such linear combinations the role of coarse-scale displacements, strains and stresses, respectively. We discuss and propose a method based on subspace rotations that ensures that these three modal approximations lead to stable formulations. The modes of such expansions are determined in an offline “training” stage by applying the truncated Singular Value Decomposition (SVD) to the solutions obtained from Finite Element (FE) analyses. In contrast to other multiscale approaches, such as FE-ROM homogenization schemes, which uses a single unit cell with periodic conditions, here the “training” structures are formed by several unit cells. This way, we allow the SVD to extract also dominant patterns of interaction between subdomains in terms of reactive forces and deformations. This original feature confers to the proposed approach three unique advantages over FE-ROM homogenization schemes, namely: (1) It can deal with unit cells of arbitrary size (no need for scale separation). (2) It can model, not only how forces are transmitted through the structure, but also local effects. (3) It can also handle domains which are only periodic along one or two directions (beam-like or shell structures, respectively). To deal with material nonlinearities, element-wise Gauss integration of reduced internal forces is replaced by an algorithmically improved version of the hyperreduction scheme recently proposed by the author elsewhere, called the Empirical Cubature Method(ECM). Furthermore, we demonstrate that the coarse-scale Degrees of Freedom (DOFs) of a subdomain can be expressed directly in terms of the (fine-scale) stresses and strains at the ECM integration points. This feature dispenses with the need of deploying a special algorithmic infrastructure of intertwined local/global problems, as it occurs in FE-ROM schemes, since the unit cell can be treated as a special type of finite element, in which the centroids of the interfaces and the ECM points play the role of nodes and Gauss points, respectively. To illustrate all these advantages, we present 4 distinct examples: two beam-like structures of rectangular-shape and I-shaped cross-sections, a 2D hexagonal cellular material, and a cylindrical shell made of a porous composite cell. In all 4 cases the proposed method is able to produce coarse-scale models reducing the number of DOFs and integration points by over two orders of magnitude, with errors below 5 %. Interestingly, in the case of the beam-like structures, we show that the method provides 2-node beam finite elements whose kinematics are identical to that predicted by “analytical” beam theories (6 DOFs per node in the case of the rectangular beam), and totally consistent with their 3D full-order counterparts.",
                "year": 2020,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Teachers’ agency and online education in times of crisis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563221001163/pdfft?md5=7515685f4de90feff1997dc4b58fd216&pid=1-s2.0-S0747563221001163-main.pdf",
                "title": "Teachers’ agency and online education in times of crisis",
                "abstract": "This study examines academic teachers' agency and emergency responses, prompted by the physical closure of universities and university colleges due to the COVID-19 crisis. The pandemic-related lockdown accelerated the digitalization of education and forced teachers to adjust their teaching. A theoretical model is elucidated, in which teachers' agency is understood as the willingness to engage in iterational, practical-evaluative, projective, and transformative action despite the existence of practical, personal, and institutional constraints. We explored the nature and degree of this agency through a survey of university teachers in Norway in the first month of the lockdown. Teachers attempted to create learning environments that facilitated knowledge transfer and interaction and sought to solve problems through self-help and support from colleagues and network, although many struggled with insufficiently developed digital competence and institutional support. Latent profile and qualitative analyses revealed different clusters of teacher responses, from strong resistance to online teaching through to transformation of teaching practices. Qualitative analyses unveiled different expressions of teachers' agency, both ostensible and occlusive, whereby action was shaped by constraining circumstances. These findings can inform future studies of online teaching, indicate the conditions for development of teachers’ digital competence, and illustrate the challenges brought about by crises.",
                "year": 2021,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Autonomous nondestructive evaluation of resistance spot welded joints": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584521000661/pdfft?md5=a0387e2cce4384836c3181b44a9927fb&pid=1-s2.0-S0736584521000661-main.pdf",
                "title": "Autonomous nondestructive evaluation of resistance spot welded joints",
                "abstract": "The application of non-destructive evaluation approaches has attracted strong interests in modern automotive industries. This study presents an autonomous deep-computing framework to analyze raw videos from infrared systems and to predict weld nugget shape and size with unprecedented accuracy and speed. In a comprehensive training and testing experiment with 90 videos (seven sets of welding material stack-ups), a new method was developed to assemble sufficient datasets for neural network training. Our framework successfully predicts all the nugget shapes with F1 scores that range from 0.84 to 0.92. The total training time on Nvidia DGX station takes less than 10 min for each set of welding material stack-up. The real inference time of an individual dataset (with 30 video frames) takes about 0.005 s. The procedure and methods developed in the study can be applied to other image-based weld property prediction, as well as other manufacturing processes. Furthermore, our well-trained neural networks take limited memory resources (2.3 MB) and are suitable for embedded microprocessors for in-situ welding quality control as edge computing within an intelligent welding framework.",
                "year": 2021,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Modeling China's energy consumption behavior and changes in energy intensity": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815209001091/pdfft?md5=e6a6c6bbb90473c7e63ad41757434be5&pid=1-s2.0-S1364815209001091-main.pdf",
                "title": "Modeling China's energy consumption behavior and changes in energy intensity",
                "abstract": "China's demand for energy has grown to fuel its rapidly expanding industrial, commercial and consumer sectors. At the same time, China has become the second largest consumer of petroleum products having surpassed Japan for the first time in 2003. The environmental consequences of a continuation of these trends will have global implications. Government policies and consumers have become more environmentally aware, but the ability of governments to formulate policies has been hindered by the lack of data on inter-factor and inter-fuel substitution possibilities. In this paper Allen partial elasticities of factor and energy substitution, and price elasticities of energy demand are calculated for China's industrial economy using a two-stage translog cost function approach for the period 1995–2004. The results suggest that energy is substitutable with both capital and labor. Coal is significantly substitutable with electricity and slightly complementary with oil, while oil and electricity are slightly substitutable. China's energy intensity is increasing during the study period and the major driver appears to be due to the increased use of energy-intensive technology.",
                "year": 2009,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Focused crawling: a new approach to topic-specific Web resource discovery": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128699000523/pdfft?md5=b8c914a0c731ac678916cb4c8e1966ea&pid=1-s2.0-S1389128699000523-main.pdf",
                "title": "Focused crawling: a new approach to topic-specific Web resource discovery",
                "abstract": "The rapid growth of the World-Wide Web poses unprecedented scaling challenges for general-purpose crawlers and search engines. In this paper we describe a new hypertext resource discovery system called a Focused Crawler. The goal of a focused crawler is to selectively seek out pages that are relevant to a pre-defined set of topics. The topics are specified not using keywords, but using exemplary documents. Rather than collecting and indexing all accessible Web documents to be able to answer all possible ad-hoc queries, a focused crawler analyzes its crawl boundary to find the links that are likely to be most relevant for the crawl, and avoids irrelevant regions of the Web. This leads to significant savings in hardware and network resources, and helps keep the crawl more up-to-date.\n\nTo achieve such goal-directed crawling, we designed two hypertext mining programs that guide our crawler: a classifier that evaluates the relevance of a hypertext document with respect to the focus topics, and a distiller that identifies hypertext nodes that are great access points to many relevant pages within a few links. We report on extensive focused-crawling experiments using several topics at different levels of specificity. Focused crawling acquires relevant pages steadily while standard crawling quickly loses its way, even though they are started from the same root set. Focused crawling is robust against large perturbations in the starting set of URLs. It discovers largely overlapping sets of resources in spite of these perturbations. It is also capable of exploring out and discovering valuable resources that are dozens of links away from the start set, while carefully pruning the millions of pages that may lie within this same radius. Our anecdotes suggest that focused crawling is very effective for building high-quality collections of Web documents on specific topics, using modest desktop hardware.",
                "year": 1999,
                "publisher": "Computer Networks"
            }
        }
    },
    "An interview study exploring Tesla drivers' behavioural adaptation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687018300929/pdfft?md5=ea134f77c118704214fd9e96ead29d6b&pid=1-s2.0-S0003687018300929-main.pdf",
                "title": "An interview study exploring Tesla drivers' behavioural adaptation",
                "abstract": "Partially automated vehicles (PAVs) have been used in real-world environments for several years since the emergence of autonomous driving. It is important to understand the effects of partial automation systems (PAS) on the understanding of drivers and their behaviour during the first months of use. In order to adapt to new vehicle technology, drivers usually exhibit specific behaviours in this stage that are not intended by the developers, namely behavioural adaptation. The present study investigated the behavioural adaptations by early PAV adopters after short-term usage. A semi-structured interview was conducted among 20  Tesla drivers who had relatively high experience (one to five months) with Autopilot, and the interviews were synthesized to understand their behavioural adaptation, mental models, and trust during the period of use. The results showed that PAV drivers had a very positive attitude towards the PAS and drivers universally engaged in secondary tasks during automated driving. They also learned from their experiences to identify relatively safe usage conditions and they employed a safety margin to avoid exposure to excessively risky situations.",
                "year": 2018,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Design and functionalisation of shoe outsoles with antimicrobial properties using additive manufacturing technologies: industrial applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0166361520304802/pdfft?md5=6ba96c56e7b88f63bbf2d23db81b4d0f&pid=1-s2.0-S0166361520304802-main.pdf",
                "title": "Design and functionalisation of shoe outsoles with antimicrobial properties using additive manufacturing technologies: industrial applications",
                "abstract": "Cross-contamination by people movement is a serious problem in food and health environments. This is caused by the need for workers to move around different areas of the workspace, carrying bacterial agents in their shoes from one place to another. The disinfection methods commonly used do not completely eliminate the microorganisms, mainly due to the porous structure of the outsole of the footwear used. This document presents a new prototype of functional footwear for these environments, whose outsole design has been functionalised in such a way that it is presented through two independent fundamental elements: a main body made of EVA by injection; and a footprint, in direct contact with the pavement, which can be exchanged for another pathogen-free footprint when deemed necessary, avoiding the transfer of microorganisms when the user moves from one place to another. This footprint is manufactured by fused-filament 3D printing, using thermoplastic polyurethane (TPU) material additivated with silver nanoparticles (AgNPs), which give it antimicrobial properties. The industrial design of these two elements has been carried out with the aim of guaranteeing a solid attachment between them, but at the same time allowing the replacement of footprints in a simple and fast way, and without requiring the use of additional auxiliary elements. Through the experimentation phase, the antimicrobial and cytotoxic properties of the developed footprint have been analysed, and the physical-mechanical properties of the footprint as a whole have been evaluated according to various standards.",
                "year": 2020,
                "publisher": "Computers in Industry"
            }
        }
    },
    "3D functional mapping of left ventricular dynamics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0895611194000387/pdfft?md5=03d021aa534063bff740ed4e337d676e&pid=1-s2.0-0895611194000387-main.pdf",
                "title": "3D functional mapping of left ventricular dynamics",
                "abstract": "The heart is an organ which functions by a periodic change of the three dimensional (3D) spatially distributed parameters; malfunctions of the heart's operating systems are manifested by changes of the spatio-temporal heart shape dynamics. A comprehensive quantitative study of this dynamic shape-function relationship is restricted by the partial character of the available data sets obtained by conventional imaging technologies and by limitations of the image analysis tools. This paper attempts to present a set of image analysis tools aimed at a thorough study of the left ventricular (LV) shape-function relationship based on Cine CT data. Data processing methodologies aimed at analysis and interpretation of the dynamic 3D LV shape, thickening and motion are described. These include the computerized detection of the LV boundaries, dynamic reconstruction of 3D LV shape, the LV shape parameters and their spatio-temporal evolution. The procedures are demonstrated using Cine CT images of the human LV in normal and pathological cases.",
                "year": 1995,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "Genome-wide identification and structure-function studies of proteases and protease inhibitors in Cicer arietinum (chickpea)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482514002935/pdfft?md5=aa13f3ec99e2da65f04cf3e36bdec6f8&pid=1-s2.0-S0010482514002935-main.pdf",
                "title": "Genome-wide identification and structure-function studies of proteases and protease inhibitors in Cicer arietinum (chickpea)",
                "abstract": "Background\n\nProteases are a family of enzymes present in almost all living organisms. In plants they are involved in many biological processes requiring stress response in situations such as water deficiency, pathogen attack, maintaining protein content of the cell, programmed cell death, senescence, reproduction and many more. Similarly, protease inhibitors (PIs) are involved in various important functions like suppression of invasion by pathogenic nematodes, inhibition of spores-germination and mycelium growth of Alternaria alternata and response to wounding and fungal attack. As much as we know, no genome-wide study of proteases together with proteinaceous PIs is reported in any of the sequenced genomes till now.",
                "year": 2015,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Automatic detection of conserved base pairing patterns in RNA virus genomes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097848599000133/pdfft?md5=42d0516671d0ffc51ce2500e7027dc49&pid=1-s2.0-S0097848599000133-main.pdf",
                "title": "Automatic detection of conserved base pairing patterns in RNA virus genomes",
                "abstract": "Almost all RNA molecules—and consequently also almost all subsequences of a large RNA molecule—form secondary structures. The presence of secondary structure in itself therefore does not indicate any functional significance. In fact, we cannot expect a conserved secondary structure for all parts of a viral genome or a mRNA, even if there is a significant level of sequence conservation. We present a novel method for detecting conserved RNA secondary structures in a family of related RNA sequences. The method is based on combining the prediction of base pair probability matrices and comparative sequence analysis. It can be applied to small sets of long sequences and does not require a prior knowledge of conserved sequence or structure motifs. As such it can be used to scan large amounts of sequence data for regions that warrant further experimental investigation. Applications to complete genomic RNAs of some viruses show that in all cases the known secondary structure features are identified. In addition, we predict a substantial number of conserved structural elements which have not been described so far.",
                "year": 1999,
                "publisher": "Computers & Chemistry"
            }
        }
    },
    "Photo-realistic facial expression synthesis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S026288560500096X/pdfft?md5=79e0a818c751b6443e65be811a8a7208&pid=1-s2.0-S026288560500096X-main.pdf",
                "title": "Photo-realistic facial expression synthesis",
                "abstract": "This paper details a procedure for generating a function which maps an image of a neutral face to one depicting a desired expression independent of age, sex, or skin colour. Facial expression synthesis is a growing and relatively new domain within computer vision. One of the fundamental problems when trying to produce accurate expression synthesis in previous approaches is the lack of a consistent method for measuring expression. This inhibits the generation of a universal mapping function. This paper advances this domain by the introduction of the Facial Expression Shape Model (FESM) and the Facial Expression Texture Model (FETM). These are statistical models of facial expression based on anatomical analysis of expression called the Facial Action Coding System (FACS). The FESM and the FETM allow for the generation of a universal mapping function. These models provide a robust means for upholding the rules of the FACS and are flexible enough to describe subjects that are not present during the training phase. We use these models in conjunction with several Artificial Neural Networks (ANN) to generate photo-realistic images of facial expressions.",
                "year": 2005,
                "publisher": "Image and Vision Computing"
            }
        }
    },
    "Analysis of Accessibility Initiatives Applied to the Web": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2212017312004665/pdfft?md5=8ab865ac2f840ce4f612e2b66249aa57&pid=1-s2.0-S2212017312004665-main.pdf",
                "title": "Analysis of Accessibility Initiatives Applied to the Web",
                "abstract": "The objective of this study is to investigate the current accessibility initiatives applied to the Web and identify its main characteristics and trends. The method used was a literature review focused on work in the last twenty years in the United States, United Kingdom, France, Canada and Brazil, ranked according to their approaches, in six lines of study. As a result, points out the lack of studies that allow effective analysis of the accessibility of websites, and methods for continuous improvement of technical standards and recommendations. It follows, showing the application of usability metrics by Nielsen and ISO-9241 and the incorporation of research models of information behavior seeking for assessment of gaps in accessibility.",
                "year": 2012,
                "publisher": "Procedia Technology"
            }
        }
    },
    "A general non-orthogonal collocated finite volume algorithm for turbulent flow at all speeds incorporating second-moment turbulence-transport closure, Part 1: Computational implementation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0045782594901651/pdfft?md5=71034e6993896b61e9d33603072ff28e&pid=1-s2.0-0045782594901651-main.pdf",
                "title": "A general non-orthogonal collocated finite volume algorithm for turbulent flow at all speeds incorporating second-moment turbulence-transport closure, Part 1: Computational implementation",
                "abstract": "A computational procedure has been developed for predicting separated turbulent flows in complex two-dimensional and three-dimensional geometries. The procedure is based on the fully conservative, structured finite volume framework within which the volumes are non-orthogonal and collocated such that all flow variables are stored at one and the same set of nodes. To ease the task of discretization and to enhance the conservative property of the scheme, a Cartesian or datum-line-adapted decomposition of the velocity field has been used. The solution algorithm is iterative in nature, approaching the steady solution with the aid of a pressure-correction scheme. Convection is approximated with a range of schemes, among them higher-order upstream-weighted approximations and a TVD-type MUSCL form, the last applied principally to the transport equations governing turbulence properties. Effects of turbulence are represented either by two-equation eddy-viscosity models or by a full Reynolds-stress-transport closure. The former category includes both high- and low-Reynolds-number variants in two- as well as three-dimensional conditions. To achieve a stable implementation of the Reynolds-stress equations, a special interpolation practice, analogous to that of Rhie and Chow for momentum [1], has been introduced within the general framework. The procedure has been formulated so as to apply to both incompressible and compressible flows. The latter may contain shocks and highly supersonic regions. To achieve this range of applicability, the retarded-density concept has been combined with the basic pressure-based algorithm to capture shock waves. A ‘Full Approximation Multigrid’ scheme for convergence acceleration has been incorporated and applied in conjunction with all turbulence models including second-moment closure. The present first part of a twin paper focuses on numerical and turbulence-modelling issues. In Part 2, computational results are presented for six representative applications out of fifteen recently predicted with the algorithm within an extensive validation exercise.",
                "year": 1994,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Effective personalization of push-type systems — visualizing information freshness": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169755298000609/pdfft?md5=72c71d1bf6631c2491b077ee4b8b54ba&pid=1-s2.0-S0169755298000609-main.pdf",
                "title": "Effective personalization of push-type systems — visualizing information freshness",
                "abstract": "This paper discusses an effective personalization method, especially on push-type systems. Many conventional personalization systems rely strictly on personal interests during information presentation, but the “freshness” of information is often as important as the relation to personal preferences. For example, a user who accesses a WWW newspaper several times a day, expects to see fresh articles displayed in prominent positions rather than “hidden” among articles that may be more relevant but that have already been read. This paper therefore presents a novel personalization method incorporating “information freshness” and that is extremely useful for the ever-growing number of push-type systems. Information freshness is indicated by using a perspective representation which shows virtual depth on the screen: fresh articles seem “closer” to the user, while old articles seem farther away. This representation allows us to simultaneously display both the personal relevance and the freshness of the information. We have successfully implemented two applications using this technique: a personalized newspaper service and an easy-to-use scrapbook for Web pages.",
                "year": 1998,
                "publisher": "Computer Networks and ISDN Systems"
            }
        }
    },
    "Chatbot learning partners: Connecting learning experiences, interest and competence": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563218306095/pdfft?md5=d30cb235064fda814517289ee398d5d3&pid=1-s2.0-S0747563218306095-main.pdf",
                "title": "Chatbot learning partners: Connecting learning experiences, interest and competence",
                "abstract": "Conversation practice, while paramount for all language learners, can be difficult to get enough of and very expensive. In this mobile age, chatbots are an obvious means of filling this gap, but have yet to realize their potential as practice partners. The current study was undertaken to examine why chatbots are not yet a substantial instrument for language learning engagement/practice, and to provide direction for future practice and chatbot development. To this end, building on a recent experimental study examining chatbot novelty effects, students undertook a pair of conversation activities: human and human-chatbot (via speech-to-text software). Immediately following the practice conversations, students' interest in the two partners was surveyed and open-ended textual feedback was collected. With these data sources and prior standardised test results, regression and content analysis of the data was undertaken. Findings indicated: 1) prior interest in human conversation partners was the best single predictor of future interest in chatbot conversations; 2) prior language competency was more strongly linked to interest in chatbot than human conversations; 3) that the qualitative experience of having “learned more” with the chatbot was strongly connected to task interest, even when reporting communication difficulties. Implications for practicing languages with currently available chatbots, for chatbots and related educational technology as sources of student interest and directions for chatbots future development are discussed.",
                "year": 2019,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Generating parallel programs from skeleton based specifications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1383762197000866/pdfft?md5=80b7c458f5a724cc3adb39d4258fe76e&pid=1-s2.0-S1383762197000866-main.pdf",
                "title": "Generating parallel programs from skeleton based specifications",
                "abstract": "The Static Iterative Transformation Specification System (SITSS) is a portable parallel programming system. This paper describes the techniques used in SITSS to generate parallel programs from user specifications of algorithms. The paper first outlines SITSS, the algorithmic skeleton on which SITSS is based, and the user interface. The techniques for analysing the specification selecting a data partitioning strategy, and generating parallel code are described. A comparison of SITSS produced code against hand-written code is then presented, followed by a discussion of areas for future work to enhance the system.",
                "year": 1998,
                "publisher": "Journal of Systems Architecture"
            }
        }
    },
    "Exact, efficient, and complete arrangement computation for cubic curves": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925772105000957/pdfft?md5=7098397b0ef5204fc87708ef046a5b7c&pid=1-s2.0-S0925772105000957-main.pdf",
                "title": "Exact, efficient, and complete arrangement computation for cubic curves",
                "abstract": "The Bentley–Ottmann sweep-line method can compute the arrangement of planar curves, provided a number of geometric primitives operating on the curves are available. We discuss the reduction of the primitives to the analysis of curves and curve pairs, and describe efficient realizations of these analyses for planar algebraic curves of degree three or less. We obtain a complete, exact, and efficient algorithm for computing arrangements of cubic curves. Special cases of cubic curves are conics as well as implicitized cubic splines and Bézier curves.\n\nThe algorithm is complete in that it handles all possible degeneracies such as tangential intersections and singularities. It is exact in that it provides the mathematically correct result. It is efficient in that it can handle hundreds of curves with a quarter million of segments in the final arrangement. The algorithm has been implemented in C++ as an Exacus library called CubiX.",
                "year": 2006,
                "publisher": "Computational Geometry"
            }
        }
    },
    "Making the best of a bad situation: Prioritized storage management in GEMS": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X0700057X/pdfft?md5=3be090d8b7847e8d86fbdf758d29d464&pid=1-s2.0-S0167739X0700057X-main.pdf",
                "title": "Making the best of a bad situation: Prioritized storage management in GEMS",
                "abstract": "As distributed storage systems grow, the response time between the occurrence of a fault, detection, and repair becomes significant. Systems built on shared servers have additional complexity because of the high rate of service outages and revocation. Managing high replica counts in this environment becomes very costly in terms of the storage required and bandwidth consumption for file copies. The storage challenge for this situation can thus be phrased as an attempt to function inexpensively with respect to cost constraints such as: disk utilization, network bandwidth consumption, and server CPU time. The GEMS (Grid Enabled Molecular Simulation) storage system provides a replicated and shared workspace for large scale molecular dynamics simulations, and exemplifies the above issues. The framework offers a solution to this problem by prioritizing observed faults and repairing them in an intelligent manner. In this paper, we provide observations from the operation of GEMS and compare its error handling to that of like storage systems.",
                "year": 2008,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "A hybrid model to support decision making in emergency department management": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705120303968/pdfft?md5=67d245882e5d10626f0fe9adecdd165e&pid=1-s2.0-S0950705120303968-main.pdf",
                "title": "A hybrid model to support decision making in emergency department management",
                "abstract": "The Emergency Department (ED) plays a key role in restoring the health of patients. Ensuring the availability of the ED and achieving rational use of its resources is critical to avoiding ED overcrowding by patients. Given this, the critical question is how ED managers can design and select improvement actions that reduce ED overcrowding. Designing and selecting enhancement actions are viewed as a Multiple Criteria Decision Making (MCDM) problem. Thus, this work provides a hybrid MCDM model combining Decision-Making Trial and Evaluation Laboratory (DEMATEL) and Preference Ranking Organization (PROMETHEE II) methods to help ED managers design improvement actions and make decisions that reduce ED overcrowding. In the model, the role of DEMATEL method is to generate knowledge to support the design of improvement actions from the causal relationships among the criteria governing the management of the patient care and treatment process in ED units. However, as EDs have costly resources, actions need to be prioritized. Therefore, the PROMETHEE II method composes the model to prioritize improvement actions that reduce short-term ED overcrowding. The model was validated by applying it in the ED of one of the largest hospitals in the state of Paraná, Brazil, that exclusively serves patients with the Brazilian federal government’s single healthcare system (Sistema Único de Saúde — SUS). The model was easily understood by the ED managers due to its ease of use, and the integration among these managers necessitated by its development and application enriched the discussion of the overcrowding problem faced by the ED.",
                "year": 2020,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Towards a hybrid method to categorize interruptions and activities in healthcare": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505606002401/pdfft?md5=019f7d35cf4a1399ac577a811e27023f&pid=1-s2.0-S1386505606002401-main.pdf",
                "title": "Towards a hybrid method to categorize interruptions and activities in healthcare",
                "abstract": "Objective\n\nInterruptions are known to have a negative impact on activity performance. Understanding how an interruption contributes to human error is limited because there is not a standard method for analyzing and classifying interruptions. Qualitative data are typically analyzed by either a deductive or an inductive method. Both methods have limitations. In this paper, a hybrid method was developed that integrates deductive and inductive methods for the categorization of activities and interruptions recorded during an ethnographic study of physicians and registered nurses in a Level One Trauma Center. Understanding the effects of interruptions is important for designing and evaluating informatics tools in particular as well as improving healthcare quality and patient safety in general.",
                "year": 2007,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "Effects of extension ladder fly configuration on climbing safety": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687024001480/pdfft?md5=ed133444ca411769f1b62241c99a9aa0&pid=1-s2.0-S0003687024001480-main.pdf",
                "title": "Effects of extension ladder fly configuration on climbing safety",
                "abstract": "Fall injuries often occur on extension ladders. The extendable fly section of an extension ladder is typically closer to the user than the base section, though this design is minimally justified. This study investigates the effects of reversing the fly on foot placement, frictional requirements, adverse stepping events (repositioning the foot or kicking the rung), and user preferences. Participant foot placement was farther posterior (rung contacted nearer to toes) in the traditional ladder compared to the reversed fly condition during descent, with farther anterior foot placements during ascent. The reversed configuration had similar friction requirements during early/mid stance and significantly lower frictional requirements during late stance. Increased friction requirements during late stance were associated with farther anterior foot placement and further plantar flexed foot orientation. The reversed fly had 5 adverse stepping events versus 22 that occurred in the traditional configuration. Users typically preferred the reversed fly. These results suggest that a reversed extension ladder configuration offers potential benefits in reducing fall-related injuries that should motivate future research and development work.",
                "year": 2024,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Numerical simulation of magnetic nano drug targeting to atherosclerosis: Effect of plaque morphology (stenosis degree and shoulder length)": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260719312854/pdfft?md5=cc82a5da0b4663f10652acfd67f44e57&pid=1-s2.0-S0169260719312854-main.pdf",
                "title": "Numerical simulation of magnetic nano drug targeting to atherosclerosis: Effect of plaque morphology (stenosis degree and shoulder length)",
                "abstract": "Background and objective: Nanoparticle-mediated targeted drug delivery is a promising option for treatment of atherosclerosis. However, the drug targeting may be affected by multiple factors. Considerable attentions have been focused on the influences of external factors, e.g., magnetic field, drug-loaded particle, but internal factors, e.g., plaque morphology (stenosis degree and shoulder length), have not received any attention yet. Therefore, we investigate the impact of plaque morphology on magnetic nanoparticles targeting under the action of an external field. Method: Numerical simulation, based on Eulerian-Lagrangian coupled Fluid-Solid Interaction, is performed in ANSYS Workbench platform. Blood flow is solved by Navier-Stokes equation, particles are tracked by discrete phase model, and the incorporated effect is obtained by two-way method. Plaques with varying stenosis degrees and shoulder lengths are acquired by manually modifying the geometry of patient-specific. The quantified variables include targeted delivery efficiency (deposition+adhesive strength) of particles and plaque injury characterized by temporal-spatial averaged shear stress (TAWSS¯) during the process of drug transport, in which the critical deposition velocity is determined by plaques and particles, the DEFINE_DPM_BC and User Defined Memory are employed to evaluate whether the particles are deposited, and to store the total number and the adhesive strength of particles deposited on the plaque. Results: Results signify that, with an increment of plaque stenosis degree, the deposition of particle and the adhesive strength between particle and plaque decrease, while the TAWSS¯ increases. Furthermore, for the same stenosis degree, with the increase of plaque shoulder length, the deposition and the adhesive strength of particle increase, and the TAWSS¯ decreases. Conclusions: Results demonstrates that the plaque with smaller stenosis degree or longer shoulder length may achieve a better treatment effect in view of the higher targeted delivery efficiency of particles and the lighter shear damage to plaque itself during the process of drug transport.",
                "year": 2020,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Effect of neighbouring village conditions and infrastructure interdependency on economic opportunity: A case study of the Yogyakarta region, Indonesia": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0198971512000051/pdfft?md5=3bf37974901bb96a3a6c98cde8433342&pid=1-s2.0-S0198971512000051-main.pdf",
                "title": "Effect of neighbouring village conditions and infrastructure interdependency on economic opportunity: A case study of the Yogyakarta region, Indonesia",
                "abstract": "To determine whether the inclusion of conditions in neighbouring villages and infrastructure interdependency are able to improve the performance of infrastructure–economy interaction models, we compare three related and progressive concepts. The first concept defines economic opportunity in a village as a function of available infrastructure within that village. The second concept includes the effect of economic opportunity in neighbouring villages in addition to available infrastructure within the village. In the third concept, we include the interdependency of infrastructures as another factor affecting the potential level of economic development in the village. We use Ordinary Least Squares (OLS) and a Geographic Information System (GIS) to model the first concept, and we add a spatial-lag model for the second. The third model expands on the second by introducing a Sugeno Fuzzy Inference System (FIS) and a rule-based OLS to capture the nature of infrastructure interdependency. The result of the three models is validated by a known spatial distribution of poverty levels and subjective well-being that serve as proxies for economic opportunity in the Yogyakarta region in Indonesia. The results demonstrate that the third model provides a more accurate prediction of the real conditions and performs consistently better than the other two models. We therefore conclude that conditions in neighbouring villages and infrastructure interdependency influence the economic opportunity of a village and should be considered in policy making regarding resource allocation in infrastructure development.",
                "year": 2012,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "A method for diagnosis support of mild cognitive impairment through EEG rhythms source location during working memory tasks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809421000963/pdfft?md5=63cfaddae7e5cfc71b6ee1adfdb5503b&pid=1-s2.0-S1746809421000963-main.pdf",
                "title": "A method for diagnosis support of mild cognitive impairment through EEG rhythms source location during working memory tasks",
                "abstract": "Objective\n\nWe investigated group differences in current source density (CSD) patterns from EEG signals before and after a working memory (WM) task performed by mild cognitive impaired (MCI) subjects and healthy elderly (HE).",
                "year": 2021,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Selective laser melting of lattice structures: A statistical approach to manufacturability and mechanical behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584517300066/pdfft?md5=dc32b2c5020646cb68e1efcd1a4fd90f&pid=1-s2.0-S0736584517300066-main.pdf",
                "title": "Selective laser melting of lattice structures: A statistical approach to manufacturability and mechanical behavior",
                "abstract": "This paper investigates the effect of processing parameters on the dimensional accuracy and mechanical properties of cellular lattice structures fabricated by additive manufacturing, also known as 3D printing. The samples are fabricated by selective laser melting (SLM) using novel titanium-tantalum alloy. The titanium-tantalum alloy has the potential to replace commercially pure titanium and Ti6Al4V as biomedical material. In this study, the unit cell used is specially designed to carry out the analysis using regression method and analysis of variance (ANOVA). Due to the effect of the SLM process parameters, the elastic constant of the cellular lattice structures ranged from 1.36 ± 0.11 to 6.82 ± 0.15 GPa using the same unit cell design. The elastic constant range, while showing the versatility of titanium-tantalum as biomedical material, is rather wide despite using the same lattice structure designed. This shows that there is a need to carefully control the processing parameters during the lattice structures fabrication so as to obtain the desired mechanical properties. Based on the statistical analysis, it is found that the dimensional accuracy and mechanical properties such as elastic constant and yield strength of the cellular lattice structures are most sensitive to laser power as compared to other parameters such as laser scanning speed and powder layer thickness.",
                "year": 2018,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Gender-specific emotional characteristics of crisis communication on social media: Case studies of two public health crises": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457323000365/pdfft?md5=f8d25019d0a08fbad4e9a2f0d9d9ff32&pid=1-s2.0-S0306457323000365-main.pdf",
                "title": "Gender-specific emotional characteristics of crisis communication on social media: Case studies of two public health crises",
                "abstract": "Understanding the effects of gender-specific emotional responses on information sharing behaviors are of great importance for swift, clear, and accurate public health crisis communication, but remains underexplored. This study fills this gap by investigating gender-specific anxiety- and anger-related emotional responses and their effects on the virality of crisis information by creatively drawing on social role theory, integrated crisis communication modeling, and text mining. The theoretical model is tested using two datasets (Changsheng vaccine crisis with 2,423,074 textual data and COVID-19 pandemic with 893,930 textual data) collected from Weibo, a leading social media platform in China. Females express significantly high anxiety and anger levels (p value<0.001) during the Changsheng fake vaccine crisis, while express significantly higher levels of anxiety during COVID-19 than males (p value<0.001), but not anger (p value=0.13). Regression analysis suggests that the virality of crisis information is significantly strengthened when the level of anger in posts of males is high or the level of anxiety in posts of females is high for both crises. However, such gender-specific virality differences of anger/anxiety expressions are violated once females have large numbers of followers (influencers). Furthermore, the gender-specific emotional effects on crisis information are more significantly enhanced for male influencers than female influencers. This study contributes to the literature on gender-specific emotional characteristics of crisis communication on social media and provides implications for practice.",
                "year": 2023,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Evolving models of player decision making: Personas versus clones": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Evolving models of player decision making: Personas versus clones",
                "abstract": "The current paper investigates multiple approaches to modeling human decision making styles for procedural play-testing. Building on decision and persona theory we evolve game playing agents representing human decision making styles. Three kinds of agents are evolved from the same representation: procedural personas, evolved from game designer expert knowledge, clones, evolved from observations of human play and aimed at general behavioral replication, and specialized agents, also evolved from observation, but aimed at determining the maximal behavioral replication ability of the representation. These three methods are then compared on their ability to represent individual human decision makers. Comparisons are conducted using three different proposed metrics that address the problem of matching decisions at the action, tactical, and strategic levels. Results indicate that a small gallery of personas evolved from designer intuitions can capture human decision making styles equally well as clones evolved from human play-traces for the testbed game MiniDungeons.",
                "year": 2016,
                "publisher": "Entertainment Computing"
            }
        }
    },
    "Computational prediction of the bioactivity potential of proteomes based on expert knowledge": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046419300395/pdfft?md5=cadf8b8d6e01ffa1642d96ec59c69408&pid=1-s2.0-S1532046419300395-main.pdf",
                "title": "Computational prediction of the bioactivity potential of proteomes based on expert knowledge",
                "abstract": "Advances in the field of genome sequencing have enabled a comprehensive analysis and annotation of the dynamics of the protein inventory of cells. This has been proven particularly rewarding for microbial cells, for which the majority of proteins are already accessible to analysis through automatic metagenome annotation. The large-scale in silico screening of proteomes and metaproteomes is key to uncover bioactivities of translational, clinical and biotechnological interest, and to help assign functions to certain proteins, such as those predicted as hypothetical. This work introduces a new method for the prediction of the bioactivity potential of proteomes/metaproteomes, supporting the discovery of functionally relevant proteins based on prior knowledge. This methodology complements functional annotation enrichment methods by allowing the assignment of functions to proteins annotated as hypothetical/putative/uncharacterised, as well as and enabling the detection of specific bioactivities and the recovery of proteins from defined taxa.\n\nThis work shows how the new method can be applied to screen proteome and metaproteome sets to obtain predictions of clinical or biotechnological interest based on reference datasets. Notably, with this methodology, the large information files obtained after DNA sequencing or protein identification experiments can be associated for translational purposes that, in cases such as antibiotic-resistance pathogens or foodborne diseases, may represent changes in how these important and global health burdens are approached in the clinical practice.\n\nFinally, the Sequence-based Expert-driven pRoteome bioactivity Prediction EnvironmENT, a public Web service implemented in Scala functional programming style, is introduced as means to ensure broad access to the method as well as to discuss main implementation issues, such as modularity, extensibility and interoperability.",
                "year": 2019,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Computing as Social Action: The Social Dynamics of Computing in Complex Organizations": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Computing as Social Action: The Social Dynamics of Computing in Complex Organizations",
                "abstract": "This chapter examines the ways in that the behavior of people and groups in organizations influences the development, use, and consequences of computing. The chapter presents an examination of the usefulness of different social perspectives for explaining how computing developments work in complex organizations. The chapter also presents that the six perspectives are best introduced by indicating how they help explain a complex case of computer use. The six theoretical perspectives help to understand the assumptions behind the questions asked and the answers different analysts have found. The rational perspective dominates the majority of analyses of computing, particularly those that are written by practitioners and found in trade journals or the internal documents of organizations. The development, use, and impact of computing in organizations are examined in light of the six perspectives outlined. The chapter examines the development and provision of computer services through the life cycle from initiation to evaluation. The knowledge about computing is distributed throughout organizations, and this leads to systematic misperceptions of computer use and increases the likelihood of computing errors. The chapter examines the consequences of computer use for the ways decisions are made, the work lives of computer users, and the distributions of power in computer-using organizations.",
                "year": 1980,
                "publisher": "Advances in Computers"
            }
        }
    },
    "Making explicit domain knowledge in formal system development": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642315004153/pdfft?md5=25cba0f2e81fce4a7695be4cac5ad455&pid=1-s2.0-S0167642315004153-main.pdf",
                "title": "Making explicit domain knowledge in formal system development",
                "abstract": "Modeling languages are concerned with providing techniques and tool support for the design, synthesis and analysis of the models resulting from a given modeling activity, this activity being usually part of a system development model or process. These languages quite successfully focused on the analysis of the designed system exploiting the expressed semantic power of the underlying modeling language. The semantics of these modeling languages are well understood by the system designers and/or the modeling language users i.e. implicit semantics.\n\nIn general, modeling languages are not equipped with resources, concepts or entities handling explicitly domain engineering features and characteristics (domain knowledge) in which the modeled systems evolve.\n\nIndeed, the designer has to explicitly handle the knowledge issued and/or mined from an analysis of this application domain i.e. explicit semantics. Nowadays, making explicit the domain knowledge inside system design models does not obey to any methodological rule validated by the practice. The modeling languages users introduce through types, constraints, profiles, etc. these domain knowledge features.\n\nOur claim is that ontologies are good candidates for handling explicit domain knowledge. They define domain theories and provide resources for uniquely identifying domain knowledge concepts. Therefore, allowing models to make references to ontologies is a modular solution for models to explicitly handle domain knowledge.\n\nOvercoming the absence of explicit semantics expression in the modeling languages used to specify systems models will increase the robustness of the designed system models. Indeed, the axioms and theorems resulting from the ontologies, thanks to references, can be used to strengthen the properties of the designed models.\n\nThe objective of this paper is to offer rigorous mechanisms for handling domain knowledge in design models. This paper also shows how these mechanisms are set up in the cases of static, dynamic and formal models.",
                "year": 2016,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Strategizing information systems-enabled organizational transformation: A transdisciplinary review and new directions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0963868712000170/pdfft?md5=09f67585645d4ead44323db5295c28a9&pid=1-s2.0-S0963868712000170-main.pdf",
                "title": "Strategizing information systems-enabled organizational transformation: A transdisciplinary review and new directions",
                "abstract": "Twenty years after the promise of Information Systems enabling Organizational Transformation (IS-enabled OT), what have we learned? This paper reviews the literature in order to better understand this phenomenon. As specialists in IS, strategy and organizational studies, we analyze the discourse on OT found in the strategy, organizational theory and IS literature, and identify four structuring themes: organizational inertia, process, agency and performance. We apply the coding derived from these themes to a set of 62 empirical papers and discuss the results. Ten research avenues are then identified to show that IS-enabled OT is still a new frontier for strategic information systems research.",
                "year": 2012,
                "publisher": "The Journal of Strategic Information Systems"
            }
        }
    },
    "Risk factors and actionable molecular signatures in COVID-19-associated lung adenocarcinoma and lung squamous cell carcinoma patients": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482523003207/pdfft?md5=dcee7d3510b236bcca4f7caf623b0802&pid=1-s2.0-S0010482523003207-main.pdf",
                "title": "Risk factors and actionable molecular signatures in COVID-19-associated lung adenocarcinoma and lung squamous cell carcinoma patients",
                "abstract": "The molecular mechanism of the pathological impact of COVID-19 in lung cancer patients remains poorly understood to date. In this study, we used differential gene expression pattern analysis to try to figure out the possible disease mechanism of COVID-19 and its associated risk factors in patients with the two most common types of non-small-cell lung cancer, namely, lung adenocarcinoma and lung squamous cell carcinoma. We also used network-based approaches to identify potential diagnostic and molecular targets for COVID-19-infected lung cancer patients. Our study showed that lung cancer and COVID-19 patients share 36 genes that are expressed differently and in common. Most of these genes are expressed in lung tissues and are mostly involved in the pathogenesis of different respiratory tract diseases. Additionally, we also found that COVID-19 may affect the expression of several cancer-associated genes in lung cancer patients, such as the oncogenes JUN, TNC, and POU2AF1. Moreover, our findings suggest that COVID-19 may predispose lung cancer patients to other diseases like acute liver failure and respiratory distress syndrome. Additionally, our findings, in concert with published literature, suggest that molecular signatures, such as hsa-mir-93-5p, CCNB2, IRF1, CD163, and different immune cell-based approaches could help both diagnose and treat this group of patients. Altogether, the scientific findings of this study will help formulate appropriate management measures and guide the development of diagnostic and therapeutic measures for COVID-19-infected lung cancer patients.",
                "year": 2023,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "QTAR: A Q-learning-based topology-aware routing protocol for underwater wireless sensor networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128623000075/pdfft?md5=5fbd79a498af793a6c8a4cc51d7e7f28&pid=1-s2.0-S1389128623000075-main.pdf",
                "title": "QTAR: A Q-learning-based topology-aware routing protocol for underwater wireless sensor networks",
                "abstract": "In this paper, an energy-efficient Q-learning-based routing protocol, called the Q-learning-based topology-aware routing (QTAR) protocol, is proposed for underwater wireless sensor networks. Unlike existing protocols, QTAR considers the network topology to determine the next-forwarder (NF) candidates along the routing path and adopts Q-learning to aid in the optimal global decision-making of an NF from the NF candidates. In addition, QTAR utilizes implicit cut-vertex recognition to optimize NF selection, alleviating the energy wastage that arises from forwarding data packets away from the sink. In our study, we evaluated the performance of QTAR by comparing it with the Q-learning-based energy-efficient and lifetime-aware routing protocol (QELAR), energy-efficient depth-based routing protocol (EEDBR), Q-learning-based delay-aware routing (QDAR), and reinforcement learning-based opportunistic routing protocol (RLOR) in terms of the energy consumption, latency, and network lifetime. Our results revealed that QTAR demonstrated the advantages of a lower energy consumption, shorter latency, and longer network lifetime in the percentage ranges of 26.08 to 70.12, 22.2 to 50, and 37.8 to 75, respectively, than QELAR, EEDBR, QDAR, and RLOR.",
                "year": 2023,
                "publisher": "Computer Networks"
            }
        }
    },
    "Black-box model-based regression testing of fail-safe behavior in web applications": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Black-box model-based regression testing of fail-safe behavior in web applications",
                "abstract": "This paper provides an approach for selective black-box model-based regression testing for web applications, emphasizing testing proper mitigation of external failures in web applications. This approach uses an existing model of web applications, FSMWeb, and extends its test generation capabilities to include selective regression testing of fail-safe behavior. It classifies existing tests as reusable, retestable, and obsolete. The approach reduces the number of tests compared to a full retest between 3% to 81% depending on the type of changes in the example and by over 99% in the case study. Removing reusable requirements reduced test requirements between 49% to 65% in the case study. The approach also uses partial regeneration for new tests wherever possible.",
                "year": 2019,
                "publisher": "Journal of Systems and Software"
            }
        }
    },
    "The rationale of PROV": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826815000177/pdfft?md5=7b04db68aa8e15e5e5e439468a977c56&pid=1-s2.0-S1570826815000177-main.pdf",
                "title": "The rationale of PROV",
                "abstract": "The prov family of documents are the final output of the World Wide Web Consortium Provenance Working Group, chartered to specify a representation of provenance to facilitate its exchange over the Web. This article reflects upon the key requirements, guiding principles, and design decisions that influenced the prov family of documents. A broad range of requirements were found, relating to the key concepts necessary for describing provenance, such as resources, activities, agents and events, and to balancing prov’s ease of use with the facility to check its validity. By this retrospective requirement analysis, the article aims to provide some insights into how prov turned out as it did and why. Benefits of this insight include better inter-operability, a roadmap for alternate investigations and improvements, and solid foundations for future standardization activities.",
                "year": 2015,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Imitation learning with artificial neural networks for demand response with a heuristic control approach for heat pumps": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2666546824001071/pdfft?md5=b0b72a3205fd21047f86e7e894c48952&pid=1-s2.0-S2666546824001071-main.pdf",
                "title": "Imitation learning with artificial neural networks for demand response with a heuristic control approach for heat pumps",
                "abstract": "The flexibility of electrical heating devices can help address the issues arising from the growing presence of unpredictable renewable energy sources in the energy system. In particular, heat pumps offer an effective solution by employing smart control methods that adjust the heat pump’s power output in reaction to demand response signals. This paper combines imitation learning based on an artificial neural network with an intelligent control approach for heat pumps. We train the model using the output data of an optimization problem to determine the optimal operation schedule of a heat pump. The objective is to minimize the electricity cost with a time-variable electricity tariff while keeping the building temperature within acceptable boundaries. We evaluate our developed novel method, PSC-ANN, on various multi-family buildings with differing insulation levels that utilize an underfloor heating system as thermal storage. The results show that PSC-ANN outperforms a positively evaluated intelligent control approach from the literature and a conventional control approach. Further, our experiments reveal that a trained imitation learning model for a specific building is also applicable to other similar buildings without the need to train it again with new data. Our developed approach also reduces the execution time compared to optimally solving the corresponding optimization problem. PSC-ANN can be integrated into multiple buildings, enabling them to better utilize renewable energy sources by adjusting their electricity consumption in response to volatile external signals.",
                "year": 2024,
                "publisher": "Energy and AI"
            }
        }
    },
    "Identification of proinflammatory pathways and promising bioactive polyphenols for the treatment of sickle cell anemia by in silico study and network pharmacology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S235291482400090X/pdfft?md5=0b9e09e9ca664db6c02eae734387aa0a&pid=1-s2.0-S235291482400090X-main.pdf",
                "title": "Identification of proinflammatory pathways and promising bioactive polyphenols for the treatment of sickle cell anemia by in silico study and network pharmacology",
                "abstract": "Sickle cell anemia (SCA) is an autosomal recessive Mendelian trait characterized by symptoms that include acute and chronic pain, chest syndrome, pulmonary hypertension, stroke, kidney disease, and vaso-occlusive crises (VOCs), all of which worsen with age; VOCs are the leading cause of hospitalization and premature death in SCA patients. Currently, despite the existence of treatments for SCA, the negative consequences of VOCs’ chronic inflammatory state demand the exploration of alternative methods of control. For this reason, the goal of this research was to find novel pathways and promising bioactive polyphenols for the treatment of SCA using a combination of network pharmacology and in silico approaches; due to polyphenols, they have shown widely reported anti-inflammatory properties. Initially, hub genes associated with inflammatory processes in SCA were identified by extracting differentially expressed genes (DEGs) from a publicly available GEO dataset (GSE53441), followed by their validation through system biology analysis, Polyphenols with anti-inflammatory activity were selected from natural product databases; finally, molecular docking and dynamics were performed with the polyphenols and the key protein derived from the selected hub genes. As a result, 10 genes associated with the Type I interferon (IFN–I) pathway in SCA were identified (MX1, FIT1, IFIT3, STAT1, ISG15, GBP1, OAS1, OAS2, OAS3, and RSAD); among them, STAT1 was selected as a central hub gene by regulating the expression of the rest. Docking and dynamics studies showed good binding energies among STAT1 and the fifteen polyphenolic extracted compounds, with quercetin, diosmetin, and fisetin showing the lowest binding energies. Identified flavonoids have been described in the past as compounds having anti-inflammatory and antioxidant features, as well as possible alternatives for SCA treatment.",
                "year": 2024,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Customer information systems: Approaching a new field in information systems from a new perspective": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/037872069390002B/pdfft?md5=fe679a4e689570485c12c54174205e2e&pid=1-s2.0-037872069390002B-main.pdf",
                "title": "Customer information systems: Approaching a new field in information systems from a new perspective",
                "abstract": "Recent years have witnessed an increasing awareness and need for information systems that can fulfil a role on the boundary between organisations and their environments. However, these so-called Customer Information Systems (CIS) have been developed using a traditional design approach with an emphasis on information systems that ignores the role of social systems in business decision-making. Consequently, a number of well-known problems concerning the use of information technology have also risen in the CIS area. Therefore, we propose a new design approach to CIS that addresses responsibilities and norms in business decision-making and focuses on the interplay between information systems design and organisational design. We propose extensions of current design issues that - if successful - could be of interest for researchers as well as practitioners.",
                "year": 1993,
                "publisher": "Information & Management"
            }
        }
    },
    "Work experiences on MTurk: Job satisfaction, turnover, and information sharing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563215301072/pdfft?md5=1bce51baef18fb9b65eb20024d936ed2&pid=1-s2.0-S0747563215301072-main.pdf",
                "title": "Work experiences on MTurk: Job satisfaction, turnover, and information sharing",
                "abstract": "Amazon's Mechanical Turk (MTurk) is an online marketplace for work, where Requesters post Human Intelligence Tasks (HITs) for Workers to complete for varying compensation. Past research has focused on the quality and generalizability of social and behavioral science research conducted using MTurk as a source of research participants. However, MTurk and other crowdsourcing platforms also exemplify trends toward extremely short-term contract work. We apply principles of industrial–organizational (I–O) psychology to investigate MTurk Worker job satisfaction, information sharing, and turnover. We also report the top best and worst Requester behaviors (e.g., building a relationship, unfair pay) that affect Worker satisfaction. Worker satisfaction was consistently negatively related to turnover as expected, indicating that this traditional variable operates similarly in the MTurk work context. However, few of the traditional predictors of job satisfaction were significant, signifying that new operational definitions or entirely new variables may be needed in order to adequately understand the experiences of crowdsourced workers. Coworker friendships consistently predicted information sharing among Workers. The findings of this study are useful for understanding the experiences of crowdsourced workers from the perspective of I–O psychology, as well as for researchers using MTurk as a recruitment tool.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Detecting CA1 short-term plasticity variations associated with changes in stimulus intensity and extracellular medium composition": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231204003339/pdfft?md5=8f1b4095e3cd4a9b11849b0766049900&pid=1-s2.0-S0925231204003339-main.pdf",
                "title": "Detecting CA1 short-term plasticity variations associated with changes in stimulus intensity and extracellular medium composition",
                "abstract": "Changes in STP characteristics in the CA1 hippocampal region have been studied using the Volterra–Poisson modeling approach. Random impulse trains stimuli were applied to the Schaffer collaterals of the CA1 hippocampus in vitro and population spike responses were recorded at the pyramidal cell body layer. The computed Volterra–Poisson kernels captured the nonlinear dynamics associated with the mechanisms underlying STP by identifying quantitatively the magnitude and the duration of facilitatory and depressive behavior. The variations in STP characteristics secondary to changes in the stimulus intensity level and the addition of chemicals to the in vitro preparation were reflected in the differences of the calculated Volterra–Poisson kernels. Increasing the stimulus intensity caused an increase in the value of the first-order kernel (mean of the population spike amplitude) and decreased the peak facilitation of the second-order kernel. Picrotoxin caused an increase in the mean of PS amplitude, an increase in peak facilitation value, and an increase in the magnitude of the depressive phase of the second-order kernel. Decreasing the extracellular calcium concentration caused a decrease in the value of the first-order kernel and increased the peak facilitation value of the second-order kernel. Finally, the ability of the Volterra–Poisson modeling approach to track STP changes was confirmed through the low normalized mean square error associated with the prediction of in-sample and out-of-sample responses.",
                "year": 2005,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Computer education in architecture": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0010448580900299/pdfft?md5=02e47e0e648faa7d2558c09f42d69587&pid=1-s2.0-0010448580900299-main.pdf",
                "title": "Computer education in architecture",
                "abstract": "In conducting a review of the current state-of-the-art of computer education in architecture, the paper seeks to identify those issues relevant to such a review and assembles them as a framework for a discussion of the effectiveness of current provision in this area of computer education and training.\n\nA discursive account of the relevant literature is complemented by information taken from a survey of computer courses in architectural schools in the British Isles.\n\nThe paper concludes with proposals for a fresh approach to computer education for architects, an approach which acknowledges recent developments in both the technology of computing and an expanding range of applications in architectural practice.",
                "year": 1980,
                "publisher": "Computer-Aided Design"
            }
        }
    },
    "Building virtual web views": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169023X01000374/pdfft?md5=4bf3c726c0275fee62cac6a7e53fc4e5&pid=1-s2.0-S0169023X01000374-main.pdf",
                "title": "Building virtual web views",
                "abstract": "We are so used to the ubiquitous World-Wide Web (WWW) that we take it for granted. There is no need to emphasize how dynamic, large, rich, and unstructured, yet important the web is. From researchers and engineers to children and retired elderly, everyone uses the WWW for a variety of needs. A multitude of tools and search engines were developed to find and retrieve resources from the web. However, everyone knows how frustrating the experience with search engines can be. It is very difficult to find, if ever found, relevant information or patterns from within resources on the Internet. The idea presented in this paper is to “warehouse” the web in a structure that would allow efficient information retrieval and knowledge discovery from the Internet. Warehousing the web in this context consists of creating different virtual web views with layered databases of descriptors organized hierarchicly. Using a declarative adhoc mining language, one can find and pinpoint explicit as well as implicit knowledge from the web warehouse.",
                "year": 2001,
                "publisher": "Data & Knowledge Engineering"
            }
        }
    },
    "Experiences With a Self-Reported Mobile Phone-Based System Among Patients With Colorectal Cancer: A Qualitative Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Experiences With a Self-Reported Mobile Phone-Based System Among Patients With Colorectal Cancer: A Qualitative Study",
                "abstract": "Background\n\nIn cancer care, mobile phone-based systems are becoming more widely used in the assessment, monitoring, and management of side effects.",
                "year": 2016,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Cancer modeling: From mechanistic to data-driven approaches, and from fundamental insights to clinical applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877750320304993/pdfft?md5=7cbea43054c565503413259fb1b7cedc&pid=1-s2.0-S1877750320304993-main.pdf",
                "title": "Cancer modeling: From mechanistic to data-driven approaches, and from fundamental insights to clinical applications",
                "abstract": "Cancer is still one of the major causes of death worldwide. Even if its comprehension is improving continuously, the complexity and heterogeneity of this group of diseases invariably make some cancer cases incurable and lethal. By focusing only on one or two cancerous molecular species simultaneously, traditional in vitro and in vivo approaches do not provide a global view on this disease and are sometimes unable to generate significant insights about cancer. In silico techniques are increasingly used in the oncology domain for their remarkable integration capacity. In basic cancer research, a vast number of mathematical and computational models has been implemented in the past decades, allowing for a better understanding of these complex diseases, generating new hypotheses and predictions, and guiding scientists towards the most impactful experiments. Although clinical uptake of such in silico approaches is still limited, some treatment strategies are currently under investigation in phase I or II clinical trials. Besides being responsible for new therapeutic ideas, in silico models could play a significant role in optimizing clinical trial design and patient stratification. This review provides a non-exhaustive overview of models according to their intrinsic features. In silico contributions to basic cancer science are discussed, using the hallmarks of cancer as a guidance. Subsequently, in silico cancer models, that are a part of currently ongoing clinical trials, are addressed. In a forward-looking section, issues such as the need for adequate regulatory processes related to in silico models, and advances in model technologies are discussed.",
                "year": 2020,
                "publisher": "Journal of Computational Science"
            }
        }
    },
    "Enterprise design through complex adaptive systems and efficiency measurement": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221718310245/pdfft?md5=dd4299cc97f14f459ea8edf0b5450311&pid=1-s2.0-S0377221718310245-main.pdf",
                "title": "Enterprise design through complex adaptive systems and efficiency measurement",
                "abstract": "Managerial policies affect enterprise operations facilitating or impeding the achievement of goals. Enterprises can operate individually or might also be part of a group of interconnected enterprises. For an enterprise operating as part of a (de)centralized group of enterprises with a similar or complementary mission and under the same or different ownership, the coordination of decisions and actions is important. Using the concept of network as an arrangement of interconnected nodes, this paper deals with enterprises as the members of a network, and with enterprise transactions as the connections among its members. We consider coordination exhibiting connectivity, feedback, and adaptation. These features are not only typical of enterprise networks, but also of complex adaptive systems (CAS). By considering enterprise networks as CAS, we study managerial policies that affect the coordination among the members of enterprise networks and the subsequent effect on technical efficiency both at the individual member enterprise and network levels. To approximate the effects of coordination-driven managerial policies, we consider flocking behavior from natural ecosystems, which the CAS literature studies, as a proxy for managerial policies. We simulate the relationships between managerial policies and technical efficiency in an enterprise network of deregulated power plants through an agent-based model. Experimental results inform when and how managerial policies enhance the coordination among network members and allow for the investigation of the relationships that exist between individual decisions and collective influences. These relationships result in an emergent goal seeking network behavior with respect to the goal of achieving technical efficiency.",
                "year": 2019,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Propagation of program control: A tool for distributed disease surveillance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505606000463/pdfft?md5=6cd560b0dccc4bfe1cbb7b3f8271c02b&pid=1-s2.0-S1386505606000463-main.pdf",
                "title": "Propagation of program control: A tool for distributed disease surveillance",
                "abstract": "Purpose\n\nThe purpose of the study was (1) to identify the requirements for syndromic, disease surveillance and epidemiology systems arising from events such as the SARS outbreak in March 2003, and the deliberate spread of Bacillus anthracis, or anthrax, in the US in 2001; and (2) to use these specifications as input to the construction of a system intended to meet these requirements. An important goal was to provide information about the diffusion of a communicable disease without being dependent on centralised storage of information about individual patients or revealing patient-identifiable information.",
                "year": 2007,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "Searching the web by constrained spreading activation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457399000734/pdfft?md5=9d10ccb0ce4e768775dff37a7bb6f747&pid=1-s2.0-S0306457399000734-main.pdf",
                "title": "Searching the web by constrained spreading activation",
                "abstract": "Intelligent Information Retrieval is concerned with the application of intelligent techniques, like for example semantic networks, neural networks and inference nets to Information Retrieval. This field of research has seen a number of applications of Constrained Spreading Activation (CSA) techniques on domain knowledge networks. However, there has never been any application of these techniques to the World Wide Web. The Web is a very important information resource, but users find that looking for a relevant piece of information in the Web can be like ‘looking for a needle in a haystack’. We were therefore motivated to design and develop a prototype system, WebSCSA (Web Search by CSA), that applied a CSA technique to retrieve information from the Web using an ostensive approach to querying similar to query-by-example. In this paper we describe the system and its underlying model. Furthermore, we report on an experiment carried out with human subjects to evaluate the effectiveness of WebSCSA. We tested whether WebSCSA improves retrieval of relevant information on top of Web search engines results and how well WebSCSA serves as an agent browser for the user. The results of the experiments are promising, and show that there is much potential for further research on the use of CSA techniques to search the Web.",
                "year": 2000,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Adaptive request scheduling for the I/O forwarding layer using reinforcement learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X20300856/pdfft?md5=0f00074b2336fa7d1d69f98162b10287&pid=1-s2.0-S0167739X20300856-main.pdf",
                "title": "Adaptive request scheduling for the I/O forwarding layer using reinforcement learning",
                "abstract": "In this paper, we propose an approach to adapt the I/O forwarding layer of HPC systems to applications’ access patterns. I/O optimization techniques can improve performance for the access patterns they were designed to target, but they often decrease performance for others. Furthermore, these techniques usually depend on the precise tune of their parameters, which commonly falls back to the users. Instead, we propose to do it dynamically at runtime based on the I/O workload observed by the system. Our approach uses a reinforcement learning technique – contextual bandits – to make the system capable of learning the best parameter value to each observed access pattern during its execution. That eliminates the need of a complicated and time-consuming previous training phase. Our case study is the TWINS scheduling algorithm, where performance improvements depend on the time window parameter, which in turn depends on the workload. We evaluate our proposal and demonstrate it can reach a precision of 88% on the parameter selection in the first hundreds of observations of an access pattern, achieving 99% of the optimal performance. We demonstrate that the system – which is expected to live for years – will be able to adapt to changes and optimize its performance after having observed an access pattern for a few (not necessarily contiguous) minutes.",
                "year": 2020,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Cyberbullying experience and bystander behavior in cyberbullying incidents: The serial mediating roles of perceived incident severity and empathy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222003041/pdfft?md5=deb1dace0e8295c832906822580325aa&pid=1-s2.0-S0747563222003041-main.pdf",
                "title": "Cyberbullying experience and bystander behavior in cyberbullying incidents: The serial mediating roles of perceived incident severity and empathy",
                "abstract": "Bystanders' behavior can influence the development of cyberbullying incidents. Previous research has demonstrated cyberbullying experience as a significant predictor of bystander behavior in cyberbullying. However, few studies have explored the explanatory mechanisms underlying this relationship. To fill in this gap, the present study investigated whether perceived incident severity and empathy mediated the relationship between cyberbullying experience and bystander behavior in cyberbullying. A total of 434 college students (female: 65.4%) aged from 18 to 25 years old (M = 20.42; SD = 1.84) were recruited to complete an online survey measuring the variables of interest. Qualitative analysis showed that individuals who experienced cyberbullying perpetration only and those who experienced both cyberbullying perpetration and victimization had stronger intentions to engage in negative bystander behavior. Besides, cyberbullying perpetration experience positively predicted negative bystander behavior, whereas cyberbullying victimization experience did not predict any type of bystander behavior. Perceived incident severity played a mediating role between cyberbullying perpetration experience and all types of bystander behavior except remaining an outsider. Empathy played a mediating role between cyberbullying perpetration experience and negative bystander behavior. Perceived incident severity and empathy played the serial mediating roles between cyberbullying perpetration experience and negative bystander behavior. When analyzing the two components of empathy, the results showed that cognitive and affective empathy had different roles in explaining the relationship between cyberbullying experience and bystander behavior. The gender differences in the mediating effects were also demonstrated. The results of the study can provide empirical support for formulating the cyberbullying intervention measures based on the bystanders’ perspective.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Semantic Web Mining: State of the art and future directions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826806000084/pdfft?md5=019a79770bfb58963d9a6b13555f92f1&pid=1-s2.0-S1570826806000084-main.pdf",
                "title": "Semantic Web Mining: State of the art and future directions",
                "abstract": "Semantic Web Mining aims at combining the two fast-developing research areas Semantic Web and Web Mining. This survey analyzes the convergence of trends from both areas: More and more researchers are working on improving the results of Web Mining by exploiting semantic structures in the Web, and they make use of Web Mining techniques for building the Semantic Web. Last but not least, these techniques can be used for mining the Semantic Web itself.\n\nThe Semantic Web is the second-generation WWW, enriched by machine-processable information which supports the user in his tasks. Given the enormous size even of today’s Web, it is impossible to manually enrich all of these resources. Therefore, automated schemes for learning the relevant information are increasingly being used. Web Mining aims at discovering insights about the meaning of Web resources and their usage. Given the primarily syntactical nature of the data being mined, the discovery of meaning is impossible based on these data only. Therefore, formalizations of the semantics of Web sites and navigation behavior are becoming more and more common. Furthermore, mining the Semantic Web itself is another upcoming application. We argue that the two areas Web Mining and Semantic Web need each other to fulfill their goals, but that the full potential of this convergence is not yet realized. This paper gives an overview of where the two areas meet today, and sketches ways of how a closer integration could be profitable.",
                "year": 2006,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "A self-tuned graph-based framework for localization and grading prostate cancer lesions: An initial evaluation based on multiparametric magnetic resonance imaging": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S001048251830074X/pdfft?md5=a37434d02ad991d28b0c2e4e9d733e01&pid=1-s2.0-S001048251830074X-main.pdf",
                "title": "A self-tuned graph-based framework for localization and grading prostate cancer lesions: An initial evaluation based on multiparametric magnetic resonance imaging",
                "abstract": "Multiparametric magnetic resonance imaging (mpMRI) has been established as the state-of-the-art examination for the detection and localization of prostate cancer lesions. Prostate Imaging-Reporting and Data System (PI-RADS) has been established as a scheme to standardize the reporting of mpMRI findings. Although lesion delineation and PI-RADS ratings could be performed manually, human delineation and ratings are subjective and time-consuming. In this article, we developed and validated a self-tuned graph-based model for PI-RADS rating prediction. 34 features were obtained at the pixel level from T2-weighted (T2W), apparent diffusion coefficient (ADC) and dynamic contrast enhanced (DCE) images, from which PI-RADS scores were predicted. Two major innovations were involved in this self-tuned graph-based model. First, graph-based approaches are sensitive to the choice of the edge weight. The proposed model tuned the edge weights automatically based on the structure of the data, thereby obviating empirical edge weight selection. Second, the feature weights were tuned automatically to give heavier weights to features important for PI-RADS rating estimation. The proposed framework was evaluated for its lesion localization performance in mpMRI datasets of 12 patients. In the evaluation, the PI-RADS score distribution map generated by the algorithm and from the observers' ratings were binarized by thresholds of 3 and 4. The sensitivity, specificity and accuracy obtained in these two threshold settings ranged from 65 to 77%, 86 to 93% and 85 to 88% respectively, which are comparable to results obtained in previous studies in which non-clinical T2 maps were available. The proposed algorithm took 10s to estimate the PI-RADS score distribution in an axial image. The efficiency achievable suggests that this technique can be developed into a prostate MR analysis system suitable for clinical use after a thorough validation involving more patients.",
                "year": 2018,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "The effects of mathematics instruction using spatial temporal cognition on teacher efficacy and instructional practices": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563211002111/pdfft?md5=590eedafd930f3624f9d0619594d65b1&pid=1-s2.0-S0747563211002111-main.pdf",
                "title": "The effects of mathematics instruction using spatial temporal cognition on teacher efficacy and instructional practices",
                "abstract": "This paper examined the effects of an instructional approach known as Spatial Temporal Mathematics (ST Math) on teacher beliefs about mathematics teaching. Participants were 339 elementary teachers teaching grades 2–5 who were randomly assigned to a control or treatment group. Hierarchical linear modeling was used to determine the effects of the intervention on self-efficacy, outcome expectancy, and instructional practices using scientific reasoning. While the treatment did not yield significant effects in teacher outcomes, our secondary analysis indicated that time on ST Math and the integration of ST Math into daily instructions were positively associated with teacher efficacy and instructional practices using scientific reasoning. Implications of the results on teacher beliefs about mathematics teaching are discussed.",
                "year": 2012,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Advanced Information Retrieval": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1571066108005458/pdfft?md5=2927c49cf6e3807a3c6f0ef53bacdc74&pid=1-s2.0-S1571066108005458-main.pdf",
                "title": "Advanced Information Retrieval",
                "abstract": "In this paper we explore some of the most important areas of advanced information retrieval. In particular, we look at cross-lingual information retrieval, multimedia information retrieval and semantic-based information retrieval. Cross-lingual information retrieval deals with asking questions in one language and retrieving documents in one or more different languages. With an increasingly globalized economy, the ability to find information in other languages is becoming a necessity. Multimedia information retrieval deals with finding media other than text, i.e. music and pictures. With the explosion of digital media that is available on the Internet and present on users' computers techniques for quickly and accurately finding desired media is important. Semantic based information retrieval goes beyond classical information retrieval and uses semantic information to understand the documents and queries in order to aid retrieval. Semantic based information retrieval goes beyond standard surface information by using the concepts represented in documents and queries to improve retrieval performance.",
                "year": 2009,
                "publisher": "Electronic Notes in Theoretical Computer Science"
            }
        }
    },
    "Link prediction and path analysis using Markov chains": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S138912860000044X/pdfft?md5=5fc65e7f28553a3d7072496718407334&pid=1-s2.0-S138912860000044X-main.pdf",
                "title": "Link prediction and path analysis using Markov chains",
                "abstract": "The enormous growth in the number of documents in the World Wide Web increases the need for improved link navigation and path analysis models. Link prediction and path analysis are important problems with a wide range of applications ranging from personalization to Web server request prediction. The sheer size of the World Wide Web coupled with the variation in users' navigation patterns makes this a very difficult sequence modelling problem. In this paper, the notion of probabilistic link prediction and path analysis using Markov chains is proposed and evaluated. Markov chains allow the system to dynamically model the URL access patterns that are observed in navigation logs based on the previous state. Furthermore, the Markov chain model can also be used in a generative mode to automatically obtain tours. The Markov transition matrix can be analysed further using eigenvector decomposition to obtain `personalized hubs/authorities'. The utility of the Markov chain approach is demonstrated in many domains: HTTP request prediction, system-driven adaptive Web navigation, tour generation, and detection of `personalized hubs/authorities' from user navigation profiles. The generality and power of Markov chains is a first step towards the application of powerful probabilistic models to Web path analysis and link prediction.",
                "year": 2000,
                "publisher": "Computer Networks"
            }
        }
    },
    "A model of the information security investment decision-making process": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404816301043/pdfft?md5=c6436554f0a86d2f25408a38a750e973&pid=1-s2.0-S0167404816301043-main.pdf",
                "title": "A model of the information security investment decision-making process",
                "abstract": "Following recent developments affecting the information security threat landscape, information security has become a complex managerial issue. Using grounded theory, we present a conceptual model that reflects the most up-to-date decision-making practices regarding information security investment in organizations for several industries. The framework described in this article generalizes the current decision-making processes, while taking into consideration that organizations may differ in many respects, including: the stakeholder who administers the information security budget, the Chief Information Security Officer's (CISO) role in the organization, the organization's industry sector, the organizational structure, and so on. Our findings indicate that the information security investment decision-making process contains 14 phases and 16 concepts that affect and are affected by these phases. The study shows that the decision-making process is heavily biased by different organizational and psychological factors. The conceptual model derived can assist decision makers/stakeholders in performing, reviewing, and manipulating the decision-making process in their organizations. It can also assist vendors and consultants in understanding and prioritizing various aspects of their sales cycle.",
                "year": 2016,
                "publisher": "Computers & Security"
            }
        }
    },
    "Counterfactual regret minimization for integrated cyber and air defense resource allocation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221720308912/pdfft?md5=9c8cc7d83d4c479e40a74ea9b08c13ef&pid=1-s2.0-S0377221720308912-main.pdf",
                "title": "Counterfactual regret minimization for integrated cyber and air defense resource allocation",
                "abstract": "This research presents a new application of optimal and approximate solution techniques to solve resource allocation problems with imperfect information in the cyber and air-defense domains. We develop a two-player, zero-sum, extensive-form game to model attacker and defender roles in both physical and cyber space. We reformulate the problem to find a Nash equilibrium using an efficient, sequence-form linear program. Solving this linear program produces optimal defender strategies for the multi-domain security game. We address large problem instances with an application of the approximate counterfactual regret minimization algorithm. This approximation reduces computation time by 95% while maintaining an optimality gap of less than 3%. Our application of discounted counterfactual regret results in a further 36% reduction in computation time from the base algorithm. We develop domain insights through a designed experiment to explore the parameter space of the problem and algorithm. We also address robust opponent exploitation by combining existing techniques to extend the counterfactual regret algorithm to include a discounted, constrained variant. A comparison of robust linear programming, data-biased response, and constrained counterfactual regret approaches clarifies trade-offs between exploitation and exploitability for each method. The robust linear programming approach is the most effective, producing an exploitation to exploitability ratio of 10.8 to 1.",
                "year": 2021,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "CANTO: An actor model-based distributed fog framework supporting neural networks training in IoT applications": {
        "accordingTo": {
            "scienceDirect": {
                "title": "CANTO: An actor model-based distributed fog framework supporting neural networks training in IoT applications",
                "abstract": "The large volumes of Internet of Things (IoT) data transmission to and from the cloud leads to one of cloud-centric processing’s major drawbacks: latency. Fog computing gives a solution to this by bringing the processing closer to the edge devices. Although a lot of work has been done which makes use of fog nodes like aggregators and offloaders, data intensive tasks like machine learning are still, for the most part, being performed on cloud. In a fog network, there are multiple fog devices, albeit resource-constrained ones. We can make use of distributed processing to collectively utilize these resources. So, we propose CANTO, a general distributed framework which uses the actor model to train neural networks on a network of fog devices in an IoT setting. Since the actors communicate using messages and act according to the type of message they receive, the framework provides a new message type wherein we can specify parameters like the dataset, the size of each dataset part, the activation function, learning rate, etc. The framework is containerized and deployed in a docker swarm. The framework is demonstrated with the use case on IoT-based forest fire prediction. In addition, the effectiveness of the framework is demonstrated with respect to accuracy, latency and load distribution.",
                "year": 2023,
                "publisher": "Computer Communications"
            }
        }
    },
    "Automatic generation of natural language nursing shift summaries in neonatal intensive care: BT-Nurse": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0933365712001170/pdfft?md5=95fb3819ff652080ec18f19ac72cf31c&pid=1-s2.0-S0933365712001170-main.pdf",
                "title": "Automatic generation of natural language nursing shift summaries in neonatal intensive care: BT-Nurse",
                "abstract": "Introduction\n\nOur objective was to determine whether and how a computer system could automatically generate helpful natural language nursing shift summaries solely from an electronic patient record system, in a neonatal intensive care unit (NICU).",
                "year": 2012,
                "publisher": "Artificial Intelligence in Medicine"
            }
        }
    },
    "Automatic estimation of aortic and mitral valve displacements in dynamic CTA with 4D graph-cuts": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841520301122/pdfft?md5=2030345bc5b631b90985919f5aba324f&pid=1-s2.0-S1361841520301122-main.pdf",
                "title": "Automatic estimation of aortic and mitral valve displacements in dynamic CTA with 4D graph-cuts",
                "abstract": "The location of the mitral and aortic valves in dynamic cardiac imaging is useful for extracting functional derived parameters such as ejection fraction, valve excursions, and global longitudinal strain, and when performing anatomical structures tracking using slice following or valve intervention's planning. Completely automatic segmentation methods are still challenging tasks because of their fast movements and the different positions that prevent good visibility of the leaflets along the full cardiac cycle. In this article, we propose a processing pipeline to track the displacement of the aortic and mitral valve annuli from high-resolution cardiac four-dimensional computed tomographic angiography (4D-CTA). The proposed method is based on the dynamic separation of left ventricle, left atrium and aorta using statistical shape modeling and an energy minimization algorithm based on graph-cuts and has been evaluated on a set of 15 electrocardiography-gated 4D-CTAs. We report a mean agreement distance between manual annotations and our proposed method of 2.52±1.06 mm for the mitral annulus and 2.00±0.69 mm for the aortic valve annulus based on valve locations detected from manual anatomical landmarks. In addition, we show the effect of detecting the valvular planes on derived functional parameters (ejection fraction, global longitudinal strain, and excursions of the mitral and aortic valves).",
                "year": 2020,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "The dynamics of contextual forces of ERP implementation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0963868705000284/pdfft?md5=773173856f7d4e9865c2770851389be4&pid=1-s2.0-S0963868705000284-main.pdf",
                "title": "The dynamics of contextual forces of ERP implementation",
                "abstract": "This paper reports the findings of an in-depth case study of implementing Enterprise Resource Planning Systems (ERP) in a long-established multinational company within the telecommunication sector. The company streamlined its operations through an ambitious business process redesign initiative and introduced an ERP system. The study examines the emergence of the ERP system and the enactment of simultaneous changes to the system and to the company context, along with the intentions and actions of the individuals involved. The study seeks to understand the ERP implementation process, in particular, the link between the ERP implementation process and the underlying and often subtle influences within the context. By drawing on the case study the paper offers a theoretical conceptualization of triggers and consequences of the cycles of control and drift of ERP implementation. We argue that by taking account of managers' intentions, the affordance of technology and the power and cultural context, we can provide richer insights into the ERP implementation process and the dynamics of the underlying, subtle influences within the context.",
                "year": 2005,
                "publisher": "The Journal of Strategic Information Systems"
            }
        }
    },
    "Who overuses Smartphones? Roles of virtues and parenting style in Smartphone addiction among Chinese college students": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216305921/pdfft?md5=835034ccb9090a89759b05a692f06931&pid=1-s2.0-S0747563216305921-main.pdf",
                "title": "Who overuses Smartphones? Roles of virtues and parenting style in Smartphone addiction among Chinese college students",
                "abstract": "Several empirical studies have linked negative parenting style (i.e., parental rejection and overprotection) with a higher degree of Smartphone addiction. However, few studies have analyzed the potential protective factors (e.g., virtues) that may prevent addiction and promote effective Smartphone addiction interventions. Therefore, we examined if virtues (i.e., relationship, vitality, and conscientiousness) mediated the association between parenting style and Smartphone addiction among college students. Moreover, we examined whether these relationships were moderated by gender. Chinese college students (N = 742) ages 16–25 years completed the: 1) short-Egna Minnen av Barndoms Uppfostran-Chinese measure of parenting style, 2) 96-item Chinese Virtues Questionnaire, and the 3) Mobile Phone Addiction Index of Smartphone addiction. The results showed that negative parenting style significantly influenced college students' Smartphone addiction. Furthermore, virtues acted as a cognitive mechanism that mediated the association between negative parenting style and Smartphone addiction. Moreover, male virtues may be more sensitive to negative parenting style than female virtues. Relevant suggestions for college administrators, educators, parents, and future researchers are offered.",
                "year": 2016,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Bit-serial SIMD on the CM-2 and the Cray-2": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/074373159190119T/pdfft?md5=6fed50c4c5d26ebf95a20ae49ff88ec7&pid=1-s2.0-074373159190119T-main.pdf",
                "title": "Bit-serial SIMD on the CM-2 and the Cray-2",
                "abstract": "The term Single Instruction, Multiple Data or SIMD has been used to categorize a particular architecture in which each instruction is broadcast to a large number of processors. Each processor then executes that instruction using data local to itself. However, the SIMD concept can also be viewed as a model of computation. In this paper, two different machine architectures for supporting a SIMD programming model with one-bit granularity are investigated. One machine, the CM-2, is a bit-level parallel processor. The other machine, the Cray-2, is a vector processor with 64-bit-wide functional units that is programmed to support a bit-serial SIMD programming model. Benchmarking studies are used to compare its ability to support bit-serial SIMD to that of the CM-2.",
                "year": 1991,
                "publisher": "Journal of Parallel and Distributed Computing"
            }
        }
    },
    "Deformable registration network based on multi-scale features and cumulative optimization for medical image alignment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809424002301/pdfft?md5=cea4fff8c9b1166a2b986df92be5728c&pid=1-s2.0-S1746809424002301-main.pdf",
                "title": "Deformable registration network based on multi-scale features and cumulative optimization for medical image alignment",
                "abstract": "When faced with significant structural differences or topological changes in medical image registration, existing deep learning methods still encounter challenges. To address the issue of large deformations in medical image registration, this paper proposes a deformable registration network based on multi-scale features and cumulative optimization. A multi-scale feature fusion module is designed in the encoding phase of the network to extract features from different dimensions and merge them into a high-dimensional representation. Simultaneously, an attention mechanism is employed to enhance registration accuracy in local anatomical regions. Additionally, a sustainable optimization mechanism for deformation field prediction is introduced, incorporating long-range and short-range information within the network to generate the final spatial displacement vectors. This strengthens the model's ability to handle large deformations. This framework is proven through extensive experiments to be an efficient medical image registration scheme.",
                "year": 2024,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Evaluation of a portable MOS electronic nose to detect root rots in shade tree species": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169913001075/pdfft?md5=d103b88d8a44e48b01f93e12e1ee7f7c&pid=1-s2.0-S0168169913001075-main.pdf",
                "title": "Evaluation of a portable MOS electronic nose to detect root rots in shade tree species",
                "abstract": "The early detection of wood decays in high-value standing trees is very important in urban areas because mitigating control measures must be implemented long before tree failures result in property damage or injuries to citizens. Adverse urban environments increase physiological stresses in trees, causing greater susceptibility to attacks by pathogenic decay fungi. The detection of fungal root rots in urban trees is particularly difficult because conventional detection tools, currently used for diagnosis of wood decays, are not feasible below ground level. Portable electronic olfactory systems or electronic noses (e-noses), currently used in many different scientific fields and industries, previously have been tested for the early diagnosis of wood decay fungi and wood rots. We evaluated the accuracy and effectiveness of the portable PEN3 electronic nose to discriminate between healthy and decayed root segments of five shade trees species, artificially inoculated separately with three species of root-rot fungi and incubated in different soil types under laboratory conditions. The PEN3 e-nose discriminated between healthy and inoculated root fragments and between different decay fungi in different soil types for most host-fungus combinations, but the discrimination power of this e-nose varied depending on tree species and strain of root-rot fungus analyzed. We provide explanations for the ineffectiveness of the e-nose to detect low levels of decay for certain host-fungus combinations. The advantages of e-nose detection over conventional wood decay detection tools also are discussed.",
                "year": 2013,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Semantic-based information retrieval in support of concept design": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S147403461000073X/pdfft?md5=06dfc6661bec243cf529ff2648e6e83b&pid=1-s2.0-S147403461000073X-main.pdf",
                "title": "Semantic-based information retrieval in support of concept design",
                "abstract": "This research is motivated by the realisation that semantic technology can be used to develop computational tools in support of designers’ creativity by focusing on the inspirational stage of design. The paper describes a semantic-based image retrieval tool developed for the needs of concept cars designers from two renowned European companies. It is created to help them find and interpret sources of inspiration. The core innovation of the tool is its ability to provide a degree of diversity, ambiguity and uncertainty in the information gathering and idea generation process. The tool is based on the assumption that there is a semantic link between the images in a web page and the text around them. Furthermore, it uses the idea that the more frequently a term occurs in a document and the fewer documents it occurs in, the more representative this term is of that document. The new contribution is linking the most meaningful words in a document with ontological concepts, and then finding the most powerful set of concepts representing that document and consequently the images in it. This is based on the observation that monosemic words (with a single meaning) are more domain-oriented than polysemic ones (that have multiple meanings), and provide a greater amount of domain information. The tool tags images by first processing all significant words in the text around them, extracting all keywords and key phrases in it, ranking them according to their significance, and linking them to ontological concepts. It generates a set of concept numbers for each text, which is then used to retrieve information in a process called semantic expansion, where a keyword query is also processed semantically. The proposed approach is illustrated with examples using the tool developed for the needs of Stile Bertone and Fiat, Italy, two of the industrial partners in the TRENDS project sponsored by the European Community.",
                "year": 2011,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "Dynamic parameter identification method for wireless charging system of AUV based on multi-strategy nonlinear rime algorithm": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142061524005672/pdfft?md5=c37975cddb42bdc9da7deb91be6c3d80&pid=1-s2.0-S0142061524005672-main.pdf",
                "title": "Dynamic parameter identification method for wireless charging system of AUV based on multi-strategy nonlinear rime algorithm",
                "abstract": "The magnetic coupling wireless charging system may experience changes in mutual inductance and load due to the impact of actual underwater environment, which would lower the system’s transmission efficiency. In response to the circumstance, firstly, this paper proposes the circuit structure of the dynamic autonomous underwater vehicle wireless charging system with LCC-S-Buck-Boost, and a dynamic parameter identification method based on the multi-strategy nonlinear Rime algorithm. Secondly, it is proposed that the difference between the actual input current value of the primary side of the system and the current value calculated by the identification method, and the difference between the actual coil current value of the secondary side of the system and the coil current value calculated by the identification method are used as the adaptability function of the identification method, and the construction of the actual model of the system and the identification algorithm has been completed, and the results of the identification of the system’s mutual inductance and the equivalent load have been obtained. Finally, based on the identification results, it is confirmed that the proposed identification method is efficient and accurate in identifying system mutual inductance and equivalent loads in complex underwater environments by comparing with other algorithms under different values of mutual inductance and equivalent loads.",
                "year": 2024,
                "publisher": "International Journal of Electrical Power & Energy Systems"
            }
        }
    },
    "Improving adversarial robustness of medical imaging systems via adding global attention noise": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482523007163/pdfft?md5=2caf0767e4ab6c94fcc4e23cd3944e97&pid=1-s2.0-S0010482523007163-main.pdf",
                "title": "Improving adversarial robustness of medical imaging systems via adding global attention noise",
                "abstract": "Recent studies have found that medical images are vulnerable to adversarial attacks. However, it is difficult to protect medical imaging systems from adversarial examples in that the lesion features of medical images are more complex with high resolution. Therefore, a simple and effective method is needed to address these issues to improve medical imaging systems’ robustness. We find that the attackers generate adversarial perturbations corresponding to the lesion characteristics of different medical image datasets, which can shift the model’s attention to other places. In this paper, we propose global attention noise (GATN) injection, including global noise in the example layer and attention noise in the feature layers. Global noise enhances the lesion features of the medical images, thus keeping the examples away from the sharp areas where the model is vulnerable. The attention noise further locally smooths the model from small perturbations. According to the characteristic of medical image datasets, we introduce Global attention lesion-unrelated noise (GATN-UR) for datasets with unclear lesion boundaries and Global attention lesion-related noise (GATN-R) for datasets with clear lesion boundaries. Extensive experiments on ChestX-ray, Dermatology, and Fundoscopy datasets show that GATN improves the robustness of medical diagnosis models against a variety of powerful attacks and significantly outperforms the existing adversarial defense methods. To be specific, the robust accuracy is 86.66% on ChestX-ray, 72.49% on Dermatology, and 90.17% on Fundoscopy under PGD attack. Under the AA attack, it achieves robust accuracy of 87.70% on ChestX-ray, 66.85% on Dermatology, and 87.83% on Fundoscopy.",
                "year": 2023,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Single-leader multi-follower games for the regulation of two-sided mobility-as-a-service markets": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221722005161/pdfft?md5=8fa8f74d5bc253de372d7ea556421e3d&pid=1-s2.0-S0377221722005161-main.pdf",
                "title": "Single-leader multi-follower games for the regulation of two-sided mobility-as-a-service markets",
                "abstract": "Mobility-as-a-Service (MaaS) is an emerging business model in transportation enabled through mobile internet technologies. A MaaS platform can be viewed as a two-sided market, where travelers and transportation service providers (TSPs) are two groups of interacting agents. We propose an optimization framework for the regulation of two-sided MaaS markets. We cast this problem as a single-leader multi-follower game (SLMFG) where the leader is the MaaS regulator and two groups of follower problems represent the travelers and the TSPs. The MaaS regulator aims to maximize its profits by optimizing service prices and resource allocation. In response, travelers (resp. TSPs) adjust their participation level in the MaaS platform to minimize their travel costs (resp. maximize their profits). We analyze network effects in the MaaS market and formulate SLMFGs without/with network effects leading to mixed-integer linear/quadratic bilevel programming problems. We propose single-level reformulations based on mathematical programming with equilibrium constraints (MPECs) and prove the equivalence between the solutions obtained using the MPECs and the original bilevel problems. Customized branch-and-bound algorithms based on strong duality reformulations are developed to solve these MPECs. Extensive numerical experiments conducted on large-scale instances generated from realistic mobility data highlight the performance of the proposed algorithms relative to a benchmarking approach, and provide meaningful managerial insights for the regulation of two-sided MaaS markets.",
                "year": 2024,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Enhancing parametric design through non-manifold topology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0142694X17300285/pdfft?md5=d33b942e1e7f0e9e05f821821a4b1d5e&pid=1-s2.0-S0142694X17300285-main.pdf",
                "title": "Enhancing parametric design through non-manifold topology",
                "abstract": "This paper aims to build a theoretical foundation for parametric design thinking by exploring its cognitive roots, unfolding its basic tenets, expanding its definition through new concepts, and exemplifying its potential through a use-case scenario. The paper focuses on a specific type of topological parameter, called non-manifold topology as a novel approach to thinking about designing cellular spaces and voids. The approach is illustrated within the context of additive manufacturing of non-conformal cellular structures. The paper concludes that parametric design thinking that omits a definition of topological relationships risks brittleness and failure in later design stages while a consideration of topology can create enhanced and smarter solutions as it can modify parameters based on an accommodation of the design context.",
                "year": 2017,
                "publisher": "Design Studies"
            }
        }
    },
    "Analysis of individual characteristics influencing user polarization in COVID-19 vaccine hesitancy.": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563222004691/pdfft?md5=6f73b430b255c0fbe2a388b41bc13e10&pid=1-s2.0-S0747563222004691-main.pdf",
                "title": "Analysis of individual characteristics influencing user polarization in COVID-19 vaccine hesitancy.",
                "abstract": "During the COVID-19 pandemic, vaccine hesitancy proved to be a major obstacle in efforts to control and mitigate the negative consequences of COVID-19. This study centered on the degree of polarization on social media about vaccine use and contributing factors to vaccine hesitancy among social media users. Examining the discussion about COVID-19 vaccine on the Weibo platform, a relatively comprehensive system of user features was constructed based on psychological theories and models such as the curiosity-drive theory and the big five model of personality. Then machine learning methods were used to explore the paramount impacting factors that led users into polarization. Findings revealed that factors reflecting the activity and effectiveness of social media use promoted user polarization. In contrast, features reflecting users' information processing ability and personal qualities had a negative impact on polarization. This study hopes to help healthcare organizations and governments understand and curb social media polarization around vaccine development in the face of future surges of pandemics.",
                "year": 2023,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "A statistical learning method for image-based monitoring of the plume signature in laser powder bed fusion": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S073658451830139X/pdfft?md5=603b453ed2b1c81e281170dba4489df1&pid=1-s2.0-S073658451830139X-main.pdf",
                "title": "A statistical learning method for image-based monitoring of the plume signature in laser powder bed fusion",
                "abstract": "The industrial breakthrough of metal additive manufacturing processes mainly involves highly regulated sectors, e.g., aerospace and healthcare, where both part and process qualification are of paramount importance. Because of this, there is an increasing interest for in-situ monitoring tools able to detect process defects and unstable states since their onset stage during the process itself. In-situ measured quantities can be regarded as “signatures” of the process behaviour and proxies of the final part quality. This study relies on the idea that the by-products of laser powder bed fusion (LPBF) can be used as process signatures to design and implement statistical monitoring methods. In particular, this paper proposes a methodology to monitor the LPBF process via in-situ infrared (IR) video imaging of the plume formed by material evaporation and heating of the surrounding gas. The aspect of the plume naturally changes from one frame to another following the natural dynamics of the process: this yields a multimodal pattern of the plume descriptors that limits the effectiveness of traditional statistical monitoring techniques. To cope with this, a nonparametric control charting scheme is proposed, called K-chart, which allows adapting the alarm threshold to the dynamically varying patterns of the monitored data. A real case study in LPBF of zinc powder is presented to demonstrate the capability of detecting the onset of unstable conditions in the presence of a material that, despite being particularly interesting for biomedical applications, imposes quality challenges in LPBF because of its low melting and boiling points. A comparison analysis is presented to highlight the benefits provided by the proposed approach against competitor methods.",
                "year": 2019,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "The complex problem of monetizing virtual electronic social networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923609001298/pdfft?md5=3fe2c54a0be2a32069acd89d872fb7ad&pid=1-s2.0-S0167923609001298-main.pdf",
                "title": "The complex problem of monetizing virtual electronic social networks",
                "abstract": "As traditional advertising is losing its impact, both advertisers and the media owners who are dependent upon them are desperately seeking alternative ways to reach consumers and alternative ways to earn revenues by doing so. Although there are many ways to earn money from social network traffic, attempting to do so by treating social networks as just another entertainment medium is doomed to failure. Traditional entertainment and advertising media were unidirectional message-sending systems, where paid messages were pushed at viewers as part of the price they paid for being entertained. In contrast, online social networks are better viewed as milieux, or as meeting places where people congregate to exchange information, observe and emulate each other, assess status, and above all entertain themselves. People are not captives, and when they are not interested, are not entertained, or feel that they are being manipulated, they simply leave. The guidelines for monetizing milieux and meeting places are still being discovered.",
                "year": 2009,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Iterated sequence databank search methods": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097848599000170/pdfft?md5=f105771e9eebb026e55b72d5b56d9a89&pid=1-s2.0-S0097848599000170-main.pdf",
                "title": "Iterated sequence databank search methods",
                "abstract": "Iterated sequence databank search methods were assessed from the viewpoint of someone with the sequence of a novel gene product wishing to find distant relatives to their protein and, with the specific searches against the PDB, also hoping to find a relative of known structure. We examined three methods in detail, spanning a range from simple pattern-matching to sophisticated weighted profiles. Rather than apply these methods ‘blindly’ (with default parameters) to a large number of test queries, we have concentrated on the globins, so allowing a more detailed investigation of each method on different data subsets with different parameter settings. Despite their widespread use, regular-expression matching proved to be very limited—seldom extending beyond the sub-family from which the pattern was derived. To attain any generality, the patterns had to be ‘stripped-down’ to include only the most highly conserved parts. The QUEST program avoided these problems by introducing a more flexible (weighted) matching. On the PDB sequences this was highly effective, missing only a few globins with probes based on each sub-family or even a single representative from each sub-family. In addition, very few false-positives were encountered, and those that did match, often only did so for a few cycles before being lost again. On the larger sequence collection, however, QUEST encountered problems with maintaining (or achieving) the alignment of the full globin family. Ψ-BLAST also recognised almost all the globins when matching against the PDB sequences, typically, missing three or four of the most distantly related sequences while picking-up a few false-positives. In contrast to QUEST, Ψ-BLAST performed very well on the larger databank, getting almost a full collection of globins although still retaining the same proportion of false-positives. SAM applied to the PDB sequences performed reasonably well with the myoglobin and hemoglobin families as probes, missing, typically several of the more difficult proteins but performed poorly with the leghemoglobin probe. Only with the full family range as a probe did it produce results comparable to Ψ-BLAST and QUEST. With the larger databank, SAM produced a good result but, again, this was only achieved using the full range of sequence variation with the default regulariser and use of Dirichlet mixtures completely failed in this situation.",
                "year": 1999,
                "publisher": "Computers & Chemistry"
            }
        }
    },
    "Fast corotational simulation for example-driven deformation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0097849314000120/pdfft?md5=f730ccc3f7aeaaa927763756cd06c3f7&pid=1-s2.0-S0097849314000120-main.pdf",
                "title": "Fast corotational simulation for example-driven deformation",
                "abstract": "We present a fast corotational finite element framework for example-driven deformation of 3-dimensional solids. An example-driven deformation space and an example space energy is constructed by introducing the modified linear Cauchy strain with rotation compensation. During this simulation, our adopted total energy functional is quadratic, and its corresponding optimization can be quickly worked out by solving a linear system. For addressing the possible errors, we propose an effective error-correction algorithm. Some related factors including the parameters and example weights are also discussed. Various experiments are demonstrated to show that the proposed method can achieve high quality results. Moreover, our method can avoid complex non-linear optimization, and it outperforms previous methods in terms of the calculation cost and implementation efficiency. Finally, other acceleration algorithms, such as the embedding technique for handling highly detailed meshes, can be easily integrated into our framework.",
                "year": 2014,
                "publisher": "Computers & Graphics"
            }
        }
    },
    "Stochastic simulation of multiple process calculi for biology": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0304397511010231/pdfft?md5=46ffcd36aa51517382bf3a22968c79c9&pid=1-s2.0-S0304397511010231-main.pdf",
                "title": "Stochastic simulation of multiple process calculi for biology",
                "abstract": "Numerous programming languages based on process calculi have been developed for biological modelling, many of which can generate potentially unbounded numbers of molecular species and reactions. As a result, such languages cannot rely on standard reaction-based simulation methods, and are generally implemented using custom stochastic simulation algorithms. As an alternative, this paper proposes a generic abstract machine that can be instantiated to simulate a range of process calculi using a range of simulation methods. The abstract machine functions as a just-in-time compiler, which dynamically updates the set of possible reactions and chooses the next reaction in an iterative cycle. We instantiate the generic abstract machine with two Markovian simulation methods and provide encodings for four process calculi: the agent-based pi-calculus, the compartment-based bioambient calculus, the rule-based kappa calculus and the domain-specific DNA strand displacement calculus. We present a generic method for proving that the encoding of an arbitrary process calculus into the abstract machine is correct, and we use this method to prove the correctness of all four calculus encodings. Finally, we demonstrate how the generic abstract machine can be used to simulate heterogeneous models in which discrete communicating sub-models are written using different domain-specific languages and then simulated together. Our approach forms the basis of a multi-language environment for the simulation of heterogeneous biological models.",
                "year": 2012,
                "publisher": "Theoretical Computer Science"
            }
        }
    },
    "A standardized approach to deal with firewall and mobility policies in the IoT": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A standardized approach to deal with firewall and mobility policies in the IoT",
                "abstract": "Internet of Things (IoT) is intended to provide a network where information flows could easily be set up between any kinds of products, devices, users and information systems in general. This vision is getting closer to become real due to the continuous development of new information system concepts and technologies. Nonetheless, this new reality requires special attention on particular aspects of the IoT such as security and mobility. First, people and companies want to secure their assets/data using firewalls, which inevitably leads to a challenging conflict between data security and usability. Second, products are becoming increasingly mobile, operating in environments where it can be difficult to contact them directly using their IP address (e.g., due to the presence of NAT or to access restrictions). It might therefore be necessary in some IoT applications to enable two-way communications through any type of firewall, e.g. to enable real-time control and maintenance. Quantum Lifecycle Management (QLM) messaging standards have been designed to provide generic and standardized application-level interfaces for the IoT that make it possible, among other things, to achieve such two-way communication. This paper provides a high-level description of QLM messaging standards with a particular focus on this QLM feature, along with proofs-of-concept through real-life implementations in building and automotive domains.",
                "year": 2015,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "An automatic approach for ontology-based feature extraction from heterogeneous textualresources": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0952197612002035/pdfft?md5=5bc1a6640fdaba17011a95610f6b243a&pid=1-s2.0-S0952197612002035-main.pdf",
                "title": "An automatic approach for ontology-based feature extraction from heterogeneous textualresources",
                "abstract": "Data mining algorithms such as data classification or clustering methods exploit features of entities to characterise, group or classify them according to their resemblance. In the past, many feature extraction methods focused on the analysis of numerical or categorical properties. In recent years, motivated by the success of the Information Society and the WWW, which has made available enormous amounts of textual electronic resources, researchers have proposed semantic data classification and clustering methods that exploit textual data at a conceptual level. To do so, these methods rely on pre-annotated inputs in which text has been mapped to their formal semantics according to one or several knowledge structures (e.g. ontologies, taxonomies). Hence, they are hampered by the bottleneck introduced by the manual semantic mapping process. To tackle this problem, this paper presents a domain-independent, automatic and unsupervised method to detect relevant features from heterogeneous textual resources, associating them to concepts modelled in a background ontology. The method has been applied to raw text resources and also to semi-structured ones (Wikipedia articles). It has been tested in the Tourism domain, showing promising results.",
                "year": 2013,
                "publisher": "Engineering Applications of Artificial Intelligence"
            }
        }
    },
    "Conceptual modelling: an essential pillar for quality software development": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950705198000483/pdfft?md5=94511582191fdd087e1b804910dbbf48&pid=1-s2.0-S0950705198000483-main.pdf",
                "title": "Conceptual modelling: an essential pillar for quality software development",
                "abstract": "After many years of stressing the importance of the product and the process in software development, emphasis has now switched to the role played by the person. This paper, however, underlines the importance of understanding and modelling the problem, as this is a necessary, and often sufficient, condition for developing good quality software. Firstly, a formal definition is given of what the problem is and how it can be classified. In view of the confusion in the field of software development, where the word model is used very vaguely, an explanation is given of what modelling means, and a generally applicable form of modelling is briefly discussed. Finally, conceptualisation is defined, first declaratively and then procedurally, and a method of building conceptual models is presented which particularly stresses the information map as a visual overview of the entire process.",
                "year": 1998,
                "publisher": "Knowledge-Based Systems"
            }
        }
    },
    "Pushing open the door to reality: On facilitating the transitions from virtual to real environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687021001824/pdfft?md5=489f829cf0a46f695d499b4113099b8e&pid=1-s2.0-S0003687021001824-main.pdf",
                "title": "Pushing open the door to reality: On facilitating the transitions from virtual to real environments",
                "abstract": "The recent rise of virtual reality technology has led researchers to investigate how to adapt transitions to virtual environments. Transitions play a key role in facilitating the return to reality, which is of particular importance when the virtual world is far more agreeable than the real world. In the present study, the efficacy of a door transition − an almost “transparent” door falling out the top of the virtual environment and controlled by the user − was evaluated and compared to two basic transitions: a direct transition and a fading transition. Participants reported a strong preference for the door transition that was evaluated as being smoother, more controllable, and greatly facilitated the return to reality. Moreover, the results showed that the door transition triggered no greater sickness in participants than the two other types of transition.",
                "year": 2021,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Development of MyTeen Text Messaging Program to Support Parents of Adolescents: Qualitative Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Development of MyTeen Text Messaging Program to Support Parents of Adolescents: Qualitative Study",
                "abstract": "Background\n\nParents play an important role in the lives of adolescents, and supporting and addressing the needs of families continue to be the focus of many researchers and policy makers. Mobile health interventions have great potential for supporting parents at a population level because of their broad reach and convenience. However, limited evidence exists for such interventions for parents of adolescents. This study reports on the formative work conducted with parents and/or primary caregivers to identify their needs and preferences for the development of MyTeen—an SMS text messaging program on promoting parental competence and mental health literacy for parents of adolescents (aged 10-15 years).",
                "year": 2019,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "A process algebra framework for multi-scale modelling of biological systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0304397513002314/pdfft?md5=e2963022ddc117688ccf5e22cc95cdc3&pid=1-s2.0-S0304397513002314-main.pdf",
                "title": "A process algebra framework for multi-scale modelling of biological systems",
                "abstract": "We introduce a novel process algebra for modelling biological systems at multiple scales, called process algebra with hooks (PAH). Processes represent biological entities, such as molecules, cells and tissues, while two algebraic operators, both symmetric, define composition of processes within and between scales. Composed actions allow for biological events to interact within and between scales at the same time. The algebra has a stochastic semantics based on functional rates of reactions. Two bisimulations are defined on PAH processes. The first bisimulation is used to aid model development by checking that two biological scales can interact correctly. The second bisimulation is a congruence that relates models, or part of models, that can perform the same timed events at a specified scale. Finally, we provide a PAH model of pattern formation in a tissue and illustrate reasoning about its behaviour using the PAH framework.",
                "year": 2013,
                "publisher": "Theoretical Computer Science"
            }
        }
    },
    "Human factors in patient safety as an innovation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0003687009001719/pdfft?md5=16f87286fa12b0bc08c1a02b40e89296&pid=1-s2.0-S0003687009001719-main.pdf",
                "title": "Human factors in patient safety as an innovation",
                "abstract": "The use of Human Factors and Ergonomics (HFE) tools, methods, concepts and theories has been advocated by many experts and organizations to improve patient safety. To facilitate and support the spread of HFE knowledge and skills in healthcare and patient safety, we propose to conceptualize HFE as innovations whose diffusion, dissemination, implementation and sustainability need to be understood and specified. Using Greenhalgh et al. (2004) model of innovation, we identified various factors that can either hinder or facilitate the spread of HFE innovations in healthcare organizations. Barriers include lack of systems thinking, complexity of HFE innovations and lack of understanding about the benefits of HFE innovations. Positive impact of HFE interventions on task performance and the presence of local champions can facilitate the adoption, implementation and sustainability of HFE innovations. This analysis concludes with a series of recommendations for HFE professionals, researchers and educators.",
                "year": 2010,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "A Smartphone App (TRIANGLE) to Change Cardiometabolic Risk Behaviors in Women Following Gestational Diabetes Mellitus: Intervention Mapping Approach": {
        "accordingTo": {
            "scienceDirect": {
                "title": "A Smartphone App (TRIANGLE) to Change Cardiometabolic Risk Behaviors in Women Following Gestational Diabetes Mellitus: Intervention Mapping Approach",
                "abstract": "Background\n\nGestational diabetes mellitus (GDM) is the most common complication during pregnancy and is associated with an increased risk for the development of cardiometabolic diseases. Behavioral interventions can reduce this risk, but current solutions insufficiently address the requirements for such a program. The systematic development of a scalable mobile health (mHealth) promotion program for mothers during the first years post-GDM may contribute to solving this problem.",
                "year": 2021,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "A plausible contributor to multiple sclerosis; presentation of antigenic myelin protein epitopes by major histocompatibility complexes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482522006102/pdfft?md5=755161e1536cb9570c013ee34e0d63c3&pid=1-s2.0-S0010482522006102-main.pdf",
                "title": "A plausible contributor to multiple sclerosis; presentation of antigenic myelin protein epitopes by major histocompatibility complexes",
                "abstract": "Background\n\nMultiple sclerosis (MS) can be induced upon successful presentation of myelin antigens by MHC I/II. Antigenic similarity between the myelin and viral proteins may worsen the immunological responses.",
                "year": 2022,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Possible pharmacological targets and mechanisms of sivelestat in protecting acute lung injury": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482524001641/pdfft?md5=3f1d19033abc6d875a81a8b3bda1693a&pid=1-s2.0-S0010482524001641-main.pdf",
                "title": "Possible pharmacological targets and mechanisms of sivelestat in protecting acute lung injury",
                "abstract": "Acute lung injury/acute respiratory distress syndrome (ALI/ARDS) is a life-threatening syndrome induced by various diseases, including COVID-19. In the progression of ALI/ARDS, activated neutrophils play a central role by releasing various inflammatory mediators, including elastase. Sivelestat is a selective and competitive inhibitor of neutrophil elastase. Although its protective effects on attenuating ALI/ARDS have been confirmed in several models of lung injury, clinical trials have presented inconsistent results on its therapeutic efficacy. Therefore, in this report, we used a network pharmacology approach coupled with animal experimental validation to unravel the concrete therapeutic targets and biological mechanisms of sivelestat in treating ALI/ARDS. In bioinformatic analyses, we found 118 targets of sivelestat against ALI/ARDS, and identified six hub genes essential for sivelestat treatment of ALI/ARDS, namely ERBB2, GRB2, PTK2, PTPN11, ESR1, and CCND1. We also found that sivelestat targeted several genes expressed in human lung microvascular endothelial cells after lipopolysaccharide (LPS) treatment at 4 h (ICAM-1, PTGS2, RND1, BCL2A1, TNF, CA2, and ADORA2A), 8 h (ICAM-1, PTGS2, RND1, BCL2A1, MMP1, BDKRB1 and SLC40A1), and 24 h (ICAM-1). Further animal experiments showed that sivelestat was able to attenuate LPS-induced ALI by inhibiting the overexpression of ICAM-1, VCAM-1, and PTGS2 and increasing the phosphorylation of PTK2. Taken together, the bioinformatic findings and experimentative data indicate that the therapeutic effects of sivelestat against ALI/ARDS mainly focus on the early stage of ALI/ARDS by pharmacological modulation of inflammatory reaction, vascular endothelial injury, and cell apoptosis–related molecules.",
                "year": 2024,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Insights into novel inhibitors intending HCMV protease a computational molecular modelling investigation for antiviral drug repurposing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914824000789/pdfft?md5=91e97f3f6a19b2ec1cb18fd7bd21ca90&pid=1-s2.0-S2352914824000789-main.pdf",
                "title": "Insights into novel inhibitors intending HCMV protease a computational molecular modelling investigation for antiviral drug repurposing",
                "abstract": "Human cytomegalovirus (HCMV) is an important opportunistic pathogen that is the most significant viral cause of congenital birth abnormalities and is responsible for a high morbidity and death rate in immunocompromised patients. HCMV's rising severity and the limitations of current vaccines in preventing infection highlight the urgent need for effective antiviral drugs. This study aimed to identify small compounds using extensive & appropriate in-silico drug design techniques, including molecular docking, Post-docking MM-GBSA, ADMET, MD simulation, PCA, and DCCM were employed in this study capable of binding to the viral protease and inhibiting its activity, thereby preventing proteolytic processing during capsid maturation. 3516 compounds from life chemical were used in molecular docking, and the top four compounds having high binding affinity and promising ADMET properties with life chemicals ID: F3407-0101 (CID: 49667672), F6559-3323 (CID: 121022124), F6456-1266 (CID: 71810903), and F3411-7969 (CID: 50785034). MD simulation was also used to assess the stability of the protein-ligand complex structure. Finally, after MD simulation, principal component analysis (PCA), and dynamic cross-correlation matrix (DCCM) analysis were performed using trajectories, and we can suggest the best drug candidate which is CID: 50785034 (N-(3-fluoro-4-methylphenyl)-2-({7-oxo-8-[(oxolan-2-yl) methyl]-5-thia-1,8,10,11-tetraazatricyclo [7.3.0.0^ {2,6}] dodeca-2(6),3,9,11-tetraen-12-yl}sulfanyl)acetamide), another two compounds CID: 121022124, and CID: 71810903 which comes after CID: 50785034. All three compounds may have the potential to be developed as a therapy option for HCMV infection.",
                "year": 2024,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Abdominal multi-organ segmentation with cascaded convolutional and adversarial deep networks": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Abdominal multi-organ segmentation with cascaded convolutional and adversarial deep networks",
                "abstract": "Abdominal anatomy segmentation is crucial for numerous applications from computer-assisted diagnosis to image-guided surgery. In this context, we address fully-automated multi-organ segmentation from abdominal CT and MR images using deep learning. The proposed model extends standard conditional generative adversarial networks. Additionally to the discriminator which enforces the model to create realistic organ delineations, it embeds cascaded partially pre-trained convolutional encoder-decoders as generator. Encoder fine-tuning from a large amount of non-medical images alleviates data scarcity limitations. The network is trained end-to-end to benefit from simultaneous multi-level segmentation refinements using auto-context. Employed for healthy liver, kidneys and spleen segmentation, our pipeline provides promising results by outperforming state-of-the-art encoder-decoder schemes. Followed for the Combined Healthy Abdominal Organ Segmentation (CHAOS) challenge organized in conjunction with the IEEE International Symposium on Biomedical Imaging 2019, it gave us the first rank for three competition categories: liver CT, liver MR and multi-organ MR segmentation. Combining cascaded convolutional and adversarial networks strengthens the ability of deep learning pipelines to automatically delineate multiple abdominal organs, with good generalization capability. The comprehensive evaluation provided suggests that better guidance could be achieved to help clinicians in abdominal image interpretation and clinical decision making.",
                "year": 2021,
                "publisher": "Artificial Intelligence in Medicine"
            }
        }
    },
    "OmniPHR: A distributed architecture model to integrate personal health records": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046417301089/pdfft?md5=715779c5a3fe726b9821ccc0fa4eae4e&pid=1-s2.0-S1532046417301089-main.pdf",
                "title": "OmniPHR: A distributed architecture model to integrate personal health records",
                "abstract": "The advances in the Information and Communications Technology (ICT) brought many benefits to the healthcare area, specially to digital storage of patients’ health records. However, it is still a challenge to have a unified viewpoint of patients’ health history, because typically health data is scattered among different health organizations. Furthermore, there are several standards for these records, some of them open and others proprietary. Usually health records are stored in databases within health organizations and rarely have external access. This situation applies mainly to cases where patients’ data are maintained by healthcare providers, known as EHRs (Electronic Health Records). In case of PHRs (Personal Health Records), in which patients by definition can manage their health records, they usually have no control over their data stored in healthcare providers’ databases. Thereby, we envision two main challenges regarding PHR context: first, how patients could have a unified view of their scattered health records, and second, how healthcare providers can access up-to-date data regarding their patients, even though changes occurred elsewhere. For addressing these issues, this work proposes a model named OmniPHR, a distributed model to integrate PHRs, for patients and healthcare providers use. The scientific contribution is to propose an architecture model to support a distributed PHR, where patients can maintain their health history in an unified viewpoint, from any device anywhere. Likewise, for healthcare providers, the possibility of having their patients data interconnected among health organizations. The evaluation demonstrates the feasibility of the model in maintaining health records distributed in an architecture model that promotes a unified view of PHR with elasticity and scalability of the solution.",
                "year": 2017,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Semi-automatic motion compensation of contrast-enhanced ultrasound images from abdominal organs for perfusion analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482514002716/pdfft?md5=d644a87b80483f626fc9f97d13468ace&pid=1-s2.0-S0010482514002716-main.pdf",
                "title": "Semi-automatic motion compensation of contrast-enhanced ultrasound images from abdominal organs for perfusion analysis",
                "abstract": "This paper presents a system for correcting motion influences in time-dependent 2D contrast-enhanced ultrasound (CEUS) images to assess tissue perfusion characteristics. The system consists of a semi-automatic frame selection method to find images with out-of-plane motion as well as a method for automatic motion compensation. Translational and non-rigid motion compensation is applied by introducing a temporal continuity assumption. A study consisting of 40 clinical datasets was conducted to compare the perfusion with simulated perfusion using pharmacokinetic modeling. Overall, the proposed approach decreased the mean average difference between the measured perfusion and the pharmacokinetic model estimation. It was non-inferior for three out of four patient cohorts to a manual approach and reduced the analysis time by 41% compared to manual processing.",
                "year": 2015,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "System identification of photosensitiser uptake kinetics in photodynamic therapy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809407000511/pdfft?md5=6a0debdcddb1b01e8f56696ee0d42b89&pid=1-s2.0-S1746809407000511-main.pdf",
                "title": "System identification of photosensitiser uptake kinetics in photodynamic therapy",
                "abstract": "This study draws the interest of system identification to the experimental modelling of in vitro uptake kinetics of photosensitising agents (PS) into cancer cells. The proposed identification methodology must be usable and valid for every PS. Therefore, three PSs characterized by opposing chemical and biological properties have been selected: (1) PC: a second generation photosensitising agent conjugated via a spacer to a VEGF receptor-specific heptapeptide, (2) Ce6: Chlorin e6, and (3) TPP: TetraPhenylPorphyrin. Experiments have been carried out with two rates (2% and 9%) of foetal bovin serum in the culture medium and one cancer cell line U87 a human malignant glioma. Difficulties of such an application are triple: (i) lack of data, (ii) low signal-to-noise ratio and (iii) ‘poor’ stimulus signals. The proposed identification methodology deals with the design of experiments, the selection of a model structure, the estimation of the model parameters and the estimation of the parameter uncertainties. The photosensitiser uptake phenomenon is described by a first-order transfer function. Estimates of the time constant and the static gain provide quantitative information about the uptake rate and yield of the PS. The parameter uncertainty is described by confidence regions in parameters space. This representation is presented as an efficient way to discriminate the uptake characteristics of different photosensitisers. This representation also emphasizes the effects of some biological factors, such as the serum rate, on the uptake yield.",
                "year": 2007,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Multi-criteria modelling of irrigation water market at basin level: A Spanish case study": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221705000214/pdfft?md5=ff4c33a1c514db15526de9e13b6aabb6&pid=1-s2.0-S0377221705000214-main.pdf",
                "title": "Multi-criteria modelling of irrigation water market at basin level: A Spanish case study",
                "abstract": "This paper develops a multi-criteria methodology to simulate irrigation water markets at basin level. For this purpose it is assumed that irrigators try to optimise personal multi-attribute utility functions via their productive decision making process (crop mix), subject to a set of constraints based upon the structural features of their farms. In this sense, farmers with homogeneous behaviour regarding water use have been grouped, such groups being established as “types” to be considered in the whole water market simulation model. This model calculates the market equilibrium through a solution that maximises aggregate welfare, which is quantified as the sum of the multi-attribute utilities reached by each of the participating agents. This methodology has been empirically applied for the Duero Basin (Northern Spain), finding that the implementation of this economic institution would increase economic efficiency and agricultural labour demand, particularly during droughts.",
                "year": 2006,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Understanding the link between organizational learning capability and ERP system usage: An empirical examination": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563214000351/pdfft?md5=1dd02bb0690774afaa6ef8498a4efb1f&pid=1-s2.0-S0747563214000351-main.pdf",
                "title": "Understanding the link between organizational learning capability and ERP system usage: An empirical examination",
                "abstract": "Although significant research attention has been directed at understanding ERP system adoption and deployment, very little attention has been paid to understanding ERP system usage among these adopting firms. This paper seeks to fill this void. We examine the concept of organizational learning capability (OLC), defined by dimensions of managerial commitment, systems perspective, openness and experimentation and transfer and integration to understand how firms can appropriate ERP systems to capture their potential benefits. Specifically, we examine the impact of OLC on ERP systems usage. We also incorporate the concept of user satisfaction to argue that OLC has an indirect effect on user satisfaction as well as a direct effect on ERP system usage. The empirical results show that OLC has a positive effect on user satisfaction. Besides, managerial commitment was found to have a positive effect on both user satisfaction and ERP system usage. Finally, user satisfaction was found to be a strong predictor of ERP system usage.",
                "year": 2014,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Loss, post-processing and standard architecture improvements of liver deep learning segmentation from Computed Tomography and magnetic resonance": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914821000757/pdfft?md5=1be6bc7d24344397771dee3ccb0ad7fa&pid=1-s2.0-S2352914821000757-main.pdf",
                "title": "Loss, post-processing and standard architecture improvements of liver deep learning segmentation from Computed Tomography and magnetic resonance",
                "abstract": "As deep learning is increasingly applied to segmentation of organs from Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) sequences, we should understand the importance of certain operations that can improve the quality of results. For segmentation of the liver from those sequences, we quantify the improvement achieved with segmentation network, loss function and post-processing steps. Our results on a publicly available dataset show an improvement of 11% points (pp) by using DeepLabV3 instead of UNet or FCN, 4 pp by applying post-processing operations and 2pp using the top-performing loss function. The conclusions of this work help researchers and practitioners choosing the network and loss function and implementing effective post-processing operations.",
                "year": 2021,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "TrackGen: An interactive track generator for TORCS and Speed-Dreams": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494614005705/pdfft?md5=dfca9a6bf054e1afc2c9ab159d6b90df&pid=1-s2.0-S1568494614005705-main.pdf",
                "title": "TrackGen: An interactive track generator for TORCS and Speed-Dreams",
                "abstract": "TrackGen is an online tool for the generation of tracks for two open-source 3D car racing games (TORCS and Speed Dreams). It integrates interactive evolution with procedural content generation and comprises two components: (i) a web frontend that maintains the database of all the evolved populations and manages the interaction with users (by collecting users evaluations and providing access to the evolved tracks) and (ii) an evolutionary/content-generation backend that runs both the evolutionary algorithm and generates the actual game content that is available through the web frontend. The first prototype of the tool was presented in July 2011 but advertised only to researchers; the first official version which generated tracks only for TORCS was released to the game community in September 2011; due to the many requests, we released a new version soon afterwards, in January 2012, with support for Speed Dreams (the fork of TORCS focused on visual realism and graphic quality) that has been online since then. From January 2012 until July 2014, TrackGen had more than 7600 unique visitors who visited the website around 11,500 times and viewed 85,500 pages; it was employed to evolve more than 8853 tracks, and it was used to download 12,218 tracks. Some of the tracks evolve by our system have been also included in the TORCS distribution.",
                "year": 2015,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Semantator: Semantic annotator for converting biomedical text to linked data": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046413001020/pdfft?md5=4c2d3c17d73000179848cbc279d7f46c&pid=1-s2.0-S1532046413001020-main.pdf",
                "title": "Semantator: Semantic annotator for converting biomedical text to linked data",
                "abstract": "More than 80% of biomedical data is embedded in plain text. The unstructured nature of these text-based documents makes it challenging to easily browse and query the data of interest in them. One approach to facilitate browsing and querying biomedical text is to convert the plain text to a linked web of data, i.e., converting data originally in free text to structured formats with defined meta-level semantics. In this paper, we introduce Semantator (Semantic Annotator), a semantic-web-based environment for annotating data of interest in biomedical documents, browsing and querying the annotated data, and interactively refining annotation results if needed. Through Semantator, information of interest can be either annotated manually or semi-automatically using plug-in information extraction tools. The annotated results will be stored in RDF and can be queried using the SPARQL query language. In addition, semantic reasoners can be directly applied to the annotated data for consistency checking and knowledge inference. Semantator has been released online and was used by the biomedical ontology community who provided positive feedbacks. Our evaluation results indicated that (1) Semantator can perform the annotation functionalities as designed; (2) Semantator can be adopted in real applications in clinical and transactional research; and (3) the annotated results using Semantator can be easily used in Semantic-web-based reasoning tools for further inference.",
                "year": 2013,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Validating, verifying and testing timed data-flow reactive systems in Coq from controlled natural-language requirements": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167642320301453/pdfft?md5=496d2f435070a8579fe11a45c577abd3&pid=1-s2.0-S0167642320301453-main.pdf",
                "title": "Validating, verifying and testing timed data-flow reactive systems in Coq from controlled natural-language requirements",
                "abstract": "Data-flow reactive systems (DFRSs) form a class of embedded systems whose inputs and outputs are always available as signals. Input signals can be seen as data provided by sensors, whereas the output data are provided to system actuators. In previous works, verifying well-formedness properties of DFRS models was accomplished in a programmatic way, with no formal guarantees, and test cases were generated by translating these models into other notations. Here, we use Coq as a single framework to specify, validate and verify DFRS models. Moreover, the specification of DFRSs in Coq is automatically derived from controlled natural-language requirements, and well-formedness properties are formally verified with no user intervention. System validation is supported by bounded exploration of models; general and domain-specific system property verification is supported by the development of proof scripts, and test generation is achieved with the aid of the QuickChick tool. Considering examples from the literature, but also from the aerospace (Embraer) and the automotive (Mercedes) industries, our automatic testing strategy was evaluated in terms of performance and the ability to detect defects generated by mutation. Within seconds, test cases were generated automatically from the requirements, achieving an average mutation score of about 75%.",
                "year": 2021,
                "publisher": "Science of Computer Programming"
            }
        }
    },
    "Transcriptome analysis displays new molecular insights into the mechanisms of action of Mebendazole in gastric cancer cells": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482524015002/pdfft?md5=b7b0a9de99c894a5bf5b5be9ff6f7855&pid=1-s2.0-S0010482524015002-main.pdf",
                "title": "Transcriptome analysis displays new molecular insights into the mechanisms of action of Mebendazole in gastric cancer cells",
                "abstract": "Gastric cancer (GC) is a common cancer worldwide. Therefore, searching for effective treatments is essential, and drug repositioning can be a promising strategy to find new potential drugs for GC therapy. For the first time, we sought to identify molecular alterations and validate new mechanisms related to Mebendazole (MBZ) treatment in GC cells through transcriptome analysis using microarray technology. Data revealed 1066 differentially expressed genes (DEGs), of which 345 (2.41 %) genes were upregulated, 721 (5.04 %) genes were downregulated, and 13,231 (92.54 %) genes remained unaltered after MBZ exposure. The overexpressed genes identified were CCL2, IL1A, and CDKN1A. In contrast, the H3C7, H3C11, and H1-5 were the top 3 underexpressed genes. Gene set enrichment analysis (GSEA) identified 8 pathways significantly overexpressed in the treated group (p < 0.05 and FDR<0.25). The validation of the expression of top desregulated genes by RT-qPCR confirmed the transcriptome results, where MBZ increased the CCL2, IL1A, and CDKN1A and reduced the H3C7, H3C11, and H1-5 transcript levels. Expression analysis in samples from TCGA databases correlated that the lower ILI1A and higher H3C11 and H1-5 gene expression are associated with decreased overall survival rates in patients with GC, indicating that MBZ treatment can improve the prognosis of patients. Thus, the data demonstrated that the drug MBZ alters the transcriptome of the AGP-01 lineage, mainly modulating the expression of histone proteins and inflammatory cytokines, indicating a possible epigenetic and immunological effect on tumor cells, these findings highlight new mechanisms of action related to MBZ treatment. Additional studies are still needed to better clarify the epigenetic and immune mechanism of MBZ in the therapy of GC.",
                "year": 2025,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Optimization of fused deposition modeling process using a virus-evolutionary genetic algorithm": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Optimization of fused deposition modeling process using a virus-evolutionary genetic algorithm",
                "abstract": "Fused Deposition modelling (FDM) is one of the most widely used Additive Manufacturing technologies that extrude a melted plastic filament through a heated nozzle in order to build final physical models layer-by-layer. In this research, a virus-evolutionary genetic algorithm (MOVEGA) is developed and implemented to solve a multi-objective optimization problem related to fused deposition modelling. Taguchi approach was first employed for the experimental procedure design and nine test parts were built according to L9 orthogonal array. The examined process parameters were the deposition angle, layer thickness, and infill ratio each one having three levels. Infill pattern was constant to honeycomb selection. Fabrication time of ABS (Acrylonitrile-Butadiene-Styrene) 3D printed specimens was measured during experiments and analyzed by using Analysis of Means (ANOM) and Analysis of Variance (ANOVA) techniques. Shape accuracy was measured by considering the parts’ dimensions in X, Y and Z axes and expressed as the overall error for control. Regression models were developed to use them as objective functions for a group of multi-objective optimization algorithms. Multi-objective Greywolf (MOGWO), and multi-universe (MOMVO) algorithms where also selected for optimizing the FDM problem to compare results. To evaluate the algorithms and judge superiority with reference to the non-dominated solution sets obtained, the hypervolume indicator was adopted. It has been verified that MOVEGA exhibited superiority in its performance for optimizing FDM problems when compared to heuristics such as MOGWO and MOMVO algorithms whilst it has strong potentials to be coupled with “Internet of Things” (IoT) platforms to facilitate the intelligent optimization control referring to a range of resources, consumables and software.",
                "year": 2021,
                "publisher": "Computers in Industry"
            }
        }
    },
    "Impact of the Growing Healthy mHealth Program on Maternal Feeding Practices, Infant Food Preferences, and Satiety Responsiveness: Quasi-Experimental Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Impact of the Growing Healthy mHealth Program on Maternal Feeding Practices, Infant Food Preferences, and Satiety Responsiveness: Quasi-Experimental Study",
                "abstract": "Background\n\nInfancy is an important life stage for obesity prevention efforts. Parents’ infant feeding practices influence the development of infants’ food preferences and eating behaviors and subsequently diet and weight. Mobile health (mHealth) may provide a feasible medium through which to deliver programs to promote healthy infant feeding as it allows low cost and easy access to tailored content.",
                "year": 2018,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Management Information Systems: Evolution and Status": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Management Information Systems: Evolution and Status",
                "abstract": "This chapter describes the evolution of management information systems (MIS). It also discusses the current status of this field. The chapter emphasizes on the development of MIS thought, the body of knowledge that exists through the performance of research, and the evolution of support mechanisms. MIS essentially deals with all informational and decision-making activity associated with operating an organization. In functional context, management information-decision systems are a blend of organizational theory, accounting, statistics, mathematical modeling, and economics, together with exposure to advanced computer hardware and software systems. This field is centered on expanding the horizons and integrating the decisions of organizations, both public and private, that must operate within the dynamics and functional demands imposed by their organizational size and degree of complexity. The chapter considers a historical approach that emphasizes on the academic and professional development of the MIS area. Decision support for management is very important to MIS.",
                "year": 1981,
                "publisher": "Advances in Computers"
            }
        }
    },
    "An ontological engineering approach for integrating CAD and GIS in support of infrastructure management": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1474034605000492/pdfft?md5=08c67ad6a5d395fa419ab01b06aaf0f7&pid=1-s2.0-S1474034605000492-main.pdf",
                "title": "An ontological engineering approach for integrating CAD and GIS in support of infrastructure management",
                "abstract": "Infrastructure managers rely on capabilities of Computer-Aided Design (CAD) and Geospatial Information Systems (GIS) for making decisions during the implementation of engineering tasks. However, despite the fact that there are domains in which both CAD and GIS are used and despite the overlaps between the data and operations they support, CAD and GIS have been developed independently over many years resulting in platforms that are not easily integrated. Engineers in infrastructure management must gain knowledge and skills in both CAD and GIS to perform infrastructure management tasks. In most cases, they need to manually transfer data queried from one system to another, due to the heterogeneous nature, such as data and operations heterogeneity, of CAD and GIS. Interoperability is seen as a solution to overcome the problems associated with heterogeneous environments and may occur at different levels and for different purposes. In this paper, we discuss the need for semantic interoperability between GIS and CAD and present an ontological engineering methodology as a possible means to enable this interoperability. This methodology uses ontological-based techniques for resolving the semantic differences in queries requiring CAD and GIS data and operations.",
                "year": 2006,
                "publisher": "Advanced Engineering Informatics"
            }
        }
    },
    "An investigation on human dynamics in enclosed spaces": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045790617327891/pdfft?md5=056cf15c586d098b82260e436b2deaa9&pid=1-s2.0-S0045790617327891-main.pdf",
                "title": "An investigation on human dynamics in enclosed spaces",
                "abstract": "In this article, we introduce a method for analysing specific dynamical properties associated with the movement of people on a two-dimensional (compact) space. We focus on a variety of features defined by the topology and dynamics of the system to investigate human dynamics in enclosed spaces. This can potentially have significant applications within systems defined by human behaviour, particularly relevant to disaster management, internet of things (IoT), and big data analytics.",
                "year": 2018,
                "publisher": "Computers & Electrical Engineering"
            }
        }
    },
    "Left ventricular non-compaction cardiomyopathy automatic diagnosis using a deep learning approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260721006222/pdfft?md5=cdff6ac64411c26ec3ef01dd4fc08e91&pid=1-s2.0-S0169260721006222-main.pdf",
                "title": "Left ventricular non-compaction cardiomyopathy automatic diagnosis using a deep learning approach",
                "abstract": "Background and Objective: Left ventricular non-compaction (LVNC) is an uncommon cardiomyopathy characterised by a thick and spongy left ventricle wall caused by the high presence of trabeculae (hyper-trabeculation). Recently, the percentage of the trabecular volume to the total volume of the external wall of the left ventricle (VT%) has been proposed to diagnose this illness.\n\nMethods: This paper presents the use of a deep learning-based method to measure the (VT%) value and diagnose this rare cardiomyopathy.\n\nThe population used in this research was composed of 277 patients suffering from hypertrophic cardiomyopathy. 134 patients only suffered hypertrophic cardiomyopathy, and 143 also suffered left ventricular non-compaction.\n\nOur deep learning solution is based on a 2D U-Net. This artificial neural network (ANN) was trained on short-axis magnetic resonance imaging to segment the left ventricle’s internal cavity, external wall, and trabecular tissue. 5-fold cross-validation was performed to ensure the robustness of the results. The Dice coefficient of the three classes was computed as a measure of the precision of the segmentation.\n\nBased on this segmentation, the percentage of the trabecular volume (VT%) was computed. Two specialist cardiologists rated the segmentation produced by the neural network for 25 patients to evaluate the clinical validity of the outputs. The computed VT% was used to automatically diagnose the 277 patients depending on whether or not a given threshold was exceeded. A receiver operating characteristic analysis was also performed.\n\nResults: According to the cross-validation results, the average and standard deviation of the Dice coefficient for the internal cavity, external wall, and trabeculae were 0.96±0.00, 0.89±0.00, and 0.84±0.00, respectively. The cardiologists rated 99.5% of the evaluated segmentations as clinically valid for diagnosis, outperforming existing automatic traditional tools. The area under the ROC curve was 0.94 (95% confidence interval, 0.91–0.96). The accuracy, sensitivity, and specificity values of diagnosis using a threshold of 25% were 0.87, 0.93, and 0.80, respectively.\n\nConclusions: The U-Net neural network can achieve excellent results in the delineation of different cardiac structures of short-axis cardiac MRI. The high-quality segmentation allows for the correct measurement of left ventricular hyper-trabeculation and a definitive diagnosis of LVNC illness. Using this kind of solution could lead to more objective and faster analysis, reducing human error and time spent by cardiologists.",
                "year": 2022,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "A refined circular template matching method for classification of human cytomegalovirus capsids in TEM images": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260704000896/pdfft?md5=b492910a34b965c670f085a98b0cd9e1&pid=1-s2.0-S0169260704000896-main.pdf",
                "title": "A refined circular template matching method for classification of human cytomegalovirus capsids in TEM images",
                "abstract": "An automatic image analysis method for describing, segmenting, and classifying human cytomegalovirus capsids in transmission electron micrograph (TEM) images of host cell nuclei has been developed. Three stages of the capsid assembly process in the host cell nucleus have been investigated. Each class is described by a radial density profile, which is the average grey-level at each radial distance from the center. A template, constructed from the profile, is used to find possible capsid locations by correlation based matching. The matching results are further refined by size and distortion analysis of each possible capsid, resulting in a final segmentation and classification.",
                "year": 2004,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Overexpression of SLC40A1 inhibits the malignancy of hepatocellular carcinoma MHCC-97H cells by stimulation of autophagy": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1746809422000763/pdfft?md5=bcab7e98bcc5f6ab3f1e027eb87af093&pid=1-s2.0-S1746809422000763-main.pdf",
                "title": "Overexpression of SLC40A1 inhibits the malignancy of hepatocellular carcinoma MHCC-97H cells by stimulation of autophagy",
                "abstract": "Our previous findings demonstrated that SLC40A1 expression in hepatocellular carcinoma (HCC) tissue was significantly lower than para-tumor tissues and normal liver tissue. SLC40A1 expression was associated with tumor staging, intrahepatic metastasis and portal vein invasion. However, the mechanism of SLC40A1 as a modulator of autophagy involving the invasion and metastasis of HCC was not clear. In the present study, we found that SLC40A1 overexpressed in MHCC-97H could activate the autophagic flux and further inhibit cells’ capacity of proliferation, migration, and invasion. Correspondingly, the autophagy inhibitor chloroquine could restrain the autophagic level of MHCC-97H-SLC40A1 cells and increase their invasive capacity. Moreover, SLC40A1 modulated the autophagic level of MHCC-97H via the AMPK/mTOR/ULK1 and AMPK/ULK1 signaling pathways. Our findings showed that elevated expression of SLC40A1 suppressed the invasion and metastasis of HCC via activating autophagy. Therapeutics that interfere with the expression of SLC40A1 in HCC cells and promotion of autophagy may attenuate tumor progression and metastasis.",
                "year": 2022,
                "publisher": "Biomedical Signal Processing and Control"
            }
        }
    },
    "Tungsten imido-catalysed dimerisation of α-olefins: insight into the Lewis acid's function revealed from computational studies": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1877050911001876/pdfft?md5=3fe326fe4e2bf7b8cb61ee8ed0a592dd&pid=1-s2.0-S1877050911001876-main.pdf",
                "title": "Tungsten imido-catalysed dimerisation of α-olefins: insight into the Lewis acid's function revealed from computational studies",
                "abstract": "Alternative channels of the tungsten imido-catalysed olefin dimerisation proceeding through mono(imido) and bis(imido) compounds have been examined by means of the DFT method simulating authentic reaction conditions on catalyst models that closely mimic the real catalyst system. All relevant steps of a plausible metallacycle mechanism to follow alternative pathways together with the divergent path towards azatungstanacycle intermediates have been characterised by considering up to three AlClMe2 moieties explicitly. This study disclosed that the Lewis acid is actively engaged in the productive catalytic cycle, this in addition to its presumed role, among others, as alkylating and chloride ligand abstraction agent in the formation of the active catalyst species. The Lewis acid preferably forms four-membered chelates across tungsten–imido or tungsten–chlorine bonds featuring a μ-Al–Cl–W bridge. The formation of the five-membered tungstanacycle is found to be less affected by the cocatalyst, but it acts profoundly in modulating the energetics of its degradation into the dimer product. DFT predicts a smooth energy profile for the most accessible path via mono(imido) tungsten species, having the Lewis acid bridged across tungsten–chlorine bonds, which is in consonance with observed activity and selectivity. This type of Lewis acid association appears as being pivotal for an effective catalysis. The alternative channel to engage bis(imido) tungsten compounds features significantly more demanding kinetics for the dimer generating route, thereby rendering it as being almost impossible to be accessed. The present study constitutes evidence, based on reliable DFT calculations, that mono(imido) tungsten compounds complexed by the Lewis acid component is effectively triggering dimerisation, whilst bis(imido) tungsten species are unlikely to play a role in the productive cycle.",
                "year": 2011,
                "publisher": "Procedia Computer Science"
            }
        }
    },
    "Viral marketing branching processes": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Viral marketing branching processes",
                "abstract": "We consider the inherent timeline structure of the appearance of content in online social networks (OSNs) while studying content propagation. We model the propagation of a post/content of interest by an appropriate multi-type branching process. The branching process allows one to predict the emergence of global macro properties (e.g., the spread of a post in the network) from the laws and parameters that determine local interactions. The local interactions largely depend upon the timeline (an inverse stack capable of holding many posts and one dedicated to each user) structure and the number of friends (i.e., connections) of users, etc. We explore the use of multi-type branching processes to analyze the viral properties of the post, e.g., to derive the expected number of shares, the probability of virality of the content, etc.\n\nIn OSNs, the new posts push down the existing contents in timelines, which can greatly influence content propagation; our analysis considers this influence. We find that one leads to draw incorrect conclusions when the timeline (TL) structure is ignored: (a) for instance, even less attractive posts are shown to get viral; (b) ignoring TL structure also indicates erroneous growth rates. More importantly, one cannot capture some interesting paradigm shifts/phase transitions; for example, virality chances are not monotone with network activity parameter, as shown by analysis including TL influence.\n\nIn the last part, we integrate the online auctions into our viral marketing model. We study the optimization problem considering real-time bidding. We again compared the study with and without considering the TL structure for varying activity levels of the network. We find that the analysis without TL structure fails to capture the relevant phase transitions, thereby making the study incomplete.",
                "year": 2023,
                "publisher": "Computer Communications"
            }
        }
    },
    "Scalability issues with using FSMWeb to test web applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0950584909000858/pdfft?md5=226d2ad4167e9c64f84321e2c403bb41&pid=1-s2.0-S0950584909000858-main.pdf",
                "title": "Scalability issues with using FSMWeb to test web applications",
                "abstract": "Web applications are fast becoming more widespread, larger, more interactive, and more essential to the international use of computers. It is well understood that web applications must be highly dependable, and as a field we are just now beginning to understand how to model and test Web applications. One straightforward technique is to model Web applications as finite state machines. However, large numbers of input fields, input choices and the ability to enter values in any order combine to create a state space explosion problem. This paper evaluates a solution that uses constraints on the inputs to reduce the number of transitions, thus compressing the FSM. The paper presents an analysis of the potential savings of the compression technique and reports actual savings from two case studies.",
                "year": 2010,
                "publisher": "Information and Software Technology"
            }
        }
    },
    "Multidecadal mapping of status and trends in annual burn probability over Canada’s forested ecosystems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0924271624000510/pdfft?md5=83dd02c2a490cbda75359821fedf2812&pid=1-s2.0-S0924271624000510-main.pdf",
                "title": "Multidecadal mapping of status and trends in annual burn probability over Canada’s forested ecosystems",
                "abstract": "Globally, wildfires burn an average of approximately 5.5 Mha of forest per year. Deriving a detailed inventory of forest fuel conditions is critical to managing resources both before and during a fire. However, data products that form the basis of these inventories often come from disparate sources, may not be subject to update, or may not capture information relevant to fuels and fire behaviour. Satellite-derived data products present a valuable solution to these concerns due to exhaustive spatial coverage and capacity for frequent temporal revisit. In this study, we modeled annual burn probability in Canadian forests for a 27-year period (1994–2020) using satellite-derived data products informing on topographic, biotic, and climatic variables. While not predicting the presence or absence of a wildfire, a high burn probability as output from the models showed strong correspondence with actual burned areas. Results indicated that biotic variables representing canopy cover and stand volume were among the most important to estimate burn probability, with the most relevant climatic variables being spring precipitation and the annual range of temperature. Analysis of the temporal trends in burn probability highlighted that 15% of tiles analyzed had significantly declining trends in mean burn probability across the study period. These decreasing trends did not always correspond to reductions in total burned area, potentially indicating a shift in forest composition of some regions and a change in the conditions which burn in others. This study highlights the importance and potential of annual burn probability mapping in order to capture these longer-term trends as well as changes in climatic conditions, which are more dynamic than other biotic or topographic drivers of burn probability. This study demonstrates the use of freely available satellite-derived datasets to generate maps of burn probability across a large, structurally and compositionally diverse forested extent, aimed to inform effective fire management.",
                "year": 2024,
                "publisher": "ISPRS Journal of Photogrammetry and Remote Sensing"
            }
        }
    },
    "Co-constructing local meanings for child health indicators in community-based information systems: The UThukela District Child Survival Project in KwaZulu-Natal": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505606001390/pdfft?md5=a75590674477798fd29defb0347b0b0b&pid=1-s2.0-S1386505606001390-main.pdf",
                "title": "Co-constructing local meanings for child health indicators in community-based information systems: The UThukela District Child Survival Project in KwaZulu-Natal",
                "abstract": "In changing the context regarding the situation of vulnerable children in rural South Africa, understanding the role of communication in the design of community-based child health information systems (HIS) is key. Communication goes beyond language. The importance of translation of terms and concepts used to negotiate between different meanings and logics is explored in this paper. In striving for the ‘ideal speech situation’ [J. Habermas, Structural Transformation of the Public Sphere, The MIT Press, Cambridge, MA, 1989], or, in other words, creating an enabling environment in which people can participate in debate and discussion on equal terms, there is a need to develop a codetermined vision; to understand local meanings of and for childhood illness; to understand communication systems and the context in which they occur; and to connect with networks beyond the localised setting, such as provincial or national health authorities. We provide a theoretical and practical framework in which important aspects of communication related to IS design can be highlighted and against which the implementation of an IS can be reviewed. The South African case study from the UThukela District Child Survival Project in KwaZulu-Natal, illustrates how this approach was used in co-constructing local meanings for child health indicators in a community-based information system.",
                "year": 2007,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "Integrated product-process design: Material and manufacturing process selection for additive manufacturing using multi-criteria decision making": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584517300832/pdfft?md5=81c8c3172d6fe006811c730d409052fe&pid=1-s2.0-S0736584517300832-main.pdf",
                "title": "Integrated product-process design: Material and manufacturing process selection for additive manufacturing using multi-criteria decision making",
                "abstract": "Market dynamics of today are constantly evolving in the presence of emerging technologies such as Additive Manufacturing (AM). Drivers such as mass customization strategies, high part-complexity needs, shorter product development cycles, a large pool of materials to choose from, abundant manufacturing processes, diverse streams of applications (e.g. aerospace, motor vehicles, and health care) and high cost incurred due to manufacturability of the part have made it essential to choose the right compromise of materials, manufacturing processes and associated machines in early stages of design considering the Design for Additive Manufacturing guidelines. There exists a complex relationship between AM products and their process data. However, the literature to-date shows very less studies targeting this integration. As several criteria, material attributes and process functionality requirements are involved for decision making in the industries, this paper introduces a generic decision methodology, based on multi-criteria decision-making tools, that will not only provide a set of compromised AM materials, processes and machines but will also act as a guideline for designers to achieve a strong foothold in the AM industry by providing practical solutions containing design oriented and feasible material-machine combinations from a current database of 38 renowned AM vendors in the world. An industrial case study, related to aerospace, has also been tested in detail via the proposed methodology.",
                "year": 2018,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "A scalable method for the estimation of spatial disaggregation models": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0098300422001169/pdfft?md5=83541c712ba1bd1534b076a60ee31854&pid=1-s2.0-S0098300422001169-main.pdf",
                "title": "A scalable method for the estimation of spatial disaggregation models",
                "abstract": "Gaining information about detailed processes using aggregation information is a frequent challenge in research involving geospatial data, with examples in different fields of knowledge such as agronomy, soil science, meteorology, public health, epidemiology, and others. Analyses using aggregated data lead to distorted conclusions since they disregard local patterns, and such a problem has motivated different approaches for reconstructing the information in a finer resolution from the aggregated data. However, most existing methods focus on the particular case where the volume of data does not exceed the amount of memory available for computations, a situation that has become increasingly less frequent with the fast pace of data generation nowadays. In practice, this problem limits either spatial resolution or coverage of applications, thus precluding their use in a more general context. In this paper, we address the problem of disaggregation of spatial data with huge datasets by proposing a scalable method to estimate the parameters of a well-established model. We propose an iterative scheme for model estimation and prove its convergence to a critical point of the likelihood function derived. To test the method, we provide a controlled simulation and a real example for sugarcane production in Brazil. In the simulation, the results indicate a successful reconstruction of 1 million pixels from 90 block areas. In the real example, the results had a compatible match with the agronomic literature, indicating a reasonable prediction of sugarcane production in a 100 m spatial resolution (i.e., approx. 5×108 pixels) from 5,565 block-areas. Compared to the most similar previous work, scalability allowed us to use a nearly 100 times higher resolution, which corresponds to 10,000 times more pixels. With our methods, we expect to assist researchers from different fields in disaggregating spatial information to larger areas or higher resolutions.",
                "year": 2022,
                "publisher": "Computers & Geosciences"
            }
        }
    },
    "Designing a parallel cloud based comparative genomics workflow to improve phylogenetic analyses": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X13000654/pdfft?md5=d2e4463132144ebac3002aea1343b7a3&pid=1-s2.0-S0167739X13000654-main.pdf",
                "title": "Designing a parallel cloud based comparative genomics workflow to improve phylogenetic analyses",
                "abstract": "Over the last years, comparative genomics analyses have become more compute-intensive due to the explosive number of available genome sequences. Comparative genomics analysis is an important a prioristep for experiments in various bioinformatics domains. This analysis can be used to enhance the performance and quality of experiments in areas such as evolution and phylogeny. A common phylogenetic analysis makes extensive use of Multiple Sequence Alignment (MSA) in the construction of phylogenetic trees, which are used to infer evolutionary relationships between homologous genes. Each phylogenetic analysis aims at exploring several different MSA methods to verify which execution produces trees with the best quality. This phylogenetic exploration may run during weeks, even when executed in High Performance Computing (HPC) environments. Although there are many approaches that model and parallelize phylogenetic analysis as scientific workflows, exploring all MSA methods becomes a complex and expensive task to be performed. If scientists determine a priorithe most adequate MSA method to use in the phylogenetic analysis, it would save time, and, in some cases, financial resources. Comparative genomics analyses play an important role in optimizing phylogenetic analysis workflows. In this paper, we extend the SciHmm scientific workflow, aimed at determining the most suitable MSA method, to use it in a phylogenetic analysis. SciHmm uses SciCumulus, a cloud workflow execution engine, for parallel execution. Experimental results show that using SciHmm considerably reduces the total execution time of the phylogenetic analysis (up to 80%). Experiments also show that trees built with the MSA program elected by using SciHmm presented more quality than the remaining, as expected. In addition, the parallel execution of SciHmm shows that this kind of bioinformatics workflow has an excellent cost/benefit when executed in cloud environments.",
                "year": 2013,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Augmented probability simulation methods for sequential games": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221722005173/pdfft?md5=69b11a1d9ea3d2ba33ac5b814d333ad5&pid=1-s2.0-S0377221722005173-main.pdf",
                "title": "Augmented probability simulation methods for sequential games",
                "abstract": "We present a robust framework with computational algorithms to support decision makers in sequential games. Our framework includes methods to solve games with complete information, assess the robustness of such solutions and, finally, approximate adversarial risk analysis solutions when lacking complete information. Existing simulation based approaches can be inefficient when dealing with large sets of feasible decisions; the game of interest may not even be solvable to the desired precision for continuous decisions. Hence, we provide a novel alternative solution method based on the use of augmented probability simulation. While the proposed framework conceptually applies to multi-stage sequential games, the discussion focuses on two-stage sequential defend-attack problems.",
                "year": 2023,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Properties of a federated epidemiology query system": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505606001584/pdfft?md5=316aca42556b1dceb33e9738e4e9f07f&pid=1-s2.0-S1386505606001584-main.pdf",
                "title": "Properties of a federated epidemiology query system",
                "abstract": "Purpose\n\nThe purpose of the study was to establish knowledge about how online access to epidemiological data from general practitioners (GPs) electronic health record (EHR) system should be provided. Before such systems are developed and deployed a decision about the appropriate system architecture must be made. Such a decision should ideally be based on knowledge about the properties of different system architectures. This choice is important because the system architecture may affect the willingness of GPs to participate in providing epidemiological data from their EHR system.",
                "year": 2007,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "QuantuneV2: Compiler-based local metric-driven mixed precision quantization for practical embedded AI applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X25000135/pdfft?md5=f24884d4051c53bf43fed8627228ce02&pid=1-s2.0-S0167739X25000135-main.pdf",
                "title": "QuantuneV2: Compiler-based local metric-driven mixed precision quantization for practical embedded AI applications",
                "abstract": "Mixed-precision quantization methods have been proposed to reduce model size while minimizing accuracy degradation. However, existing studies require retraining and do not consider the computational overhead and intermediate representations (IR) generated during the compilation process, limiting their application at the compiler level. This computational overhead refers to the runtime latency caused by frequent quantization and de-quantization operations during inference. Performing these operations at the individual operator level causes significant runtime delays. To address these issues, we propose QuantuneV2, a compiler-based mixed-precision quantization method designed for practical embedded AI applications. QuantuneV2 performs inference only twice—once before quantization and once after quantization—and operates with a computational complexity off O(n) that increases linearly with the number of model parameters. We also made the sensitivity analysis more stable by using local metrics like weights, activation values, the Signal-to-Quantization-Noise Ratio (SQNR), and the Mean Squared Error (MSE). We also cut down on computational overhead by choosing the best IR and using operator fusion. Experimental results show that QuantuneV2 achieved up to a 10.28% improvement in accuracy and a 12.52% increase in speed compared to existing methods across five models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This demonstrates that QuantuneV2 enhances model performance while maintaining computational efficiency, making it suitable for deployment in embedded AI environments.",
                "year": 2025,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "The influence of colour on CRT reading performance and subjective comfort under operational conditions": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0003687087901402/pdfft?md5=1f51bedc84788eeedd616f271afd2f59&pid=1-s2.0-0003687087901402-main.pdf",
                "title": "The influence of colour on CRT reading performance and subjective comfort under operational conditions",
                "abstract": "Performance and reports of comfort when proof-reading text displayed on a CRT were investigated as a function of the text/background colour. Luminances were those typically produced by commercially available hardware and software, and task conditions were designed to simulate those experienced by typical users. Colours from the extremes of the spectrum (red, blue) were shown to produce poorer reading performance, higher ratings of discomfort and a higher incidence of reported symptoms of discomfort than those from mid-spectrum and white stimulus conditions.",
                "year": 1987,
                "publisher": "Applied Ergonomics"
            }
        }
    },
    "Computing policies and problems: A stage theory approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0308596181900045/pdfft?md5=9dc998c1c9f5f86d693e58f145c2925d&pid=1-s2.0-0308596181900045-main.pdf",
                "title": "Computing policies and problems: A stage theory approach",
                "abstract": "Computing policies have been considered a major mechanism for reducing and preventing the emergence of problems in computing operations. The authors examine the relationship between computing policies and problems, and formulate a theoru of the interactions between problems, policies and commitment of resources for computing activity. The data presented are from an international comparative study of computing operations and impacts in 40 US and 16 other cities.",
                "year": 1981,
                "publisher": "Telecommunications Policy"
            }
        }
    },
    "HANet: a framework toward ultimately reliable network services": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0164121204002092/pdfft?md5=43eeec51b6c3e0dbd8f542a6b8ddb4b0&pid=1-s2.0-S0164121204002092-main.pdf",
                "title": "HANet: a framework toward ultimately reliable network services",
                "abstract": "High availability is becoming an essential part of network services because even a little downtime may lead to a great loss of money. According to previous research, network failure is one of the major causes of system unavailability. In this paper, we propose a framework called HANet for building highly available network services. The main goal of HANet is to allow a server to continue providing services when all its network interfaces to the outside world (i.e., public interfaces) have failed. This is achieved by two techniques. First, a network interface can be backed up not only by other public network interfaces, but also by other inter-server I/O communication interfaces (i.e., private interfaces) such as Ethernet, USB, RS232, etc. Therefore, IP packets can still be transmitted and received via these I/O links, even when all of the public network interfaces have failed. Second, HANet allows a server to take over the packet transmission job of another network-failed server.\n\nThe benefit of HANet is that a network-failed server will not lose any requests which are being processed. And, it is efficient since no synchronization overhead or replaying process is required. Moreover, it is totally transparent to server applications and clients. To demonstrate the feasibility of HANet, we implemented it in the Linux kernel. According to the performance results, using a private Fast Ethernet interface for data communication leads to only 1% overhead in user-perceived latency when the public Fast Ethernet interface of the server has failed. This indicates that HANet is efficient, and hence is feasible for commercial network services.",
                "year": 2005,
                "publisher": "Journal of Systems and Software"
            }
        }
    },
    "Direct distributed memory access for CMPs": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0743731513002311/pdfft?md5=da5251a69a8b2e7f80a1045e24429cb8&pid=1-s2.0-S0743731513002311-main.pdf",
                "title": "Direct distributed memory access for CMPs",
                "abstract": "On-chip distributed memory has emerged as a promising memory organization for future many-core systems, since it efficiently exploits memory level parallelism and can lighten off the load on each memory module by providing a comparable number of memory interfaces with on-chip cores. The packet-based memory access model (PDMA) has provided a scalable and flexible solution for distributed memory management, but suffers from complicated and costly on-chip network protocol translation and massive interferences among packets, which leads to unpredictable performance. In this paper we propose a direct distributed memory access (DDMA) model, in which remote memory can be directly accessed by local cores via remote-to-local virtualization, without network protocol translation. From the perspective of local cores, remote memory controllers (MC) can be directly manipulated through accessing the local agent MC, which is responsible for accessing remote memory through high-performance inter-tile communication. We further discuss some detailed architecture supports for the DDMA model, including the memory interface design, work flow and the protocols involved. Simulation results of executing PARSEC benchmarks show that our DDMA architecture outperforms PDMA in terms of both average memory access latency and IPC by 17.8% and 16.6% respectively on average. Besides, DDMA can better manage congested memory traffic, since a reduction of bandwidth in running memory-intensive SPEC2006 workloads only incurs 18.9% performance penalty, compared with 38.3% for PDMA.",
                "year": 2014,
                "publisher": "Journal of Parallel and Distributed Computing"
            }
        }
    },
    "Analysis of binding residues between scorpion neurotoxins and D2 dopamine receptor: A computational docking study": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482508001157/pdfft?md5=7751793a9b6ec222cdf65f10a16b4702&pid=1-s2.0-S0010482508001157-main.pdf",
                "title": "Analysis of binding residues between scorpion neurotoxins and D2 dopamine receptor: A computational docking study",
                "abstract": "We report the results on the computation of binding affinity, electrostatic free energies, contact free energies, secondary structures, stabilization centers and stabilizing residues of binding residues during the molecular docking of selected scorpion neurotoxins with D2 dopamine receptor. All the scorpion neurotoxins showed a good and satisfactory docking with the D2 receptor molecule except one neurotoxin 2SN3. We computed multiple alignment studies, solvent accessibility calculations, secondary structure analysis, stabilization centers and stabilizing residues before and after the docking process. Overall, we emphasize that the results obtained in this work will be very helpful in further enhancement of understanding the research on modeling and drug design with respect to the D2 dopamine receptor.",
                "year": 2008,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Automatic hidden-web table interpretation, conceptualization, and semantic annotation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169023X09000172/pdfft?md5=da7c4b0fcef5b945ad16e58591879fce&pid=1-s2.0-S0169023X09000172-main.pdf",
                "title": "Automatic hidden-web table interpretation, conceptualization, and semantic annotation",
                "abstract": "The longstanding problem of automatic table interpretation still eludes us. Its solution would not only be an aid to table processing applications such as large volume table conversion, but would also be an aid in solving related problems such as information extraction, semantic annotation, and semi-structured data management. In this paper, we offer a solution for the common special case in which so-called sibling pages are available. The sibling pages we consider are pages on the hidden web, commonly generated from underlying databases. Our system compares them to identify and connect nonvarying components (category labels) and varying components (data values). We tested our solution using more than 2000 tables in source pages from three different domains—car advertisements, molecular biology, and geopolitical information. Experimental results show that the system can successfully identify sibling tables, generate structure patterns, interpret tables using the generated patterns, and automatically adjust the structure patterns as it processes a sequence of hidden-web pages. For these activities, the system was able to achieve an overall F-measure of 94.5%. Further, given that we can automatically interpret tables, we next show that this leads immediately to a conceptualization of the data in these interpreted tables and thus also to a way to semantically annotate these interpreted tables with respect to the ontological conceptualization. Labels in nested table structures yield ontological concepts and interrelationships among these concepts, and associated data values become annotated information. We further show that semantically annotated data leads immediately to queriable data. Thus, the entire process, which is fully automatic, transform facts embedded within tables into facts accessible by standard query engines.",
                "year": 2009,
                "publisher": "Data & Knowledge Engineering"
            }
        }
    },
    "Generating complex ontology instances from documents": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0196677409000054/pdfft?md5=ea201c6141f854aa078bfaa93a7e2b33&pid=1-s2.0-S0196677409000054-main.pdf",
                "title": "Generating complex ontology instances from documents",
                "abstract": "This paper presents a novel Information Extraction system able to generate complex instances from free text available on the Web. The approach is based on a non-monotonical processing over ontologies, and makes use of entity recognizers and disambiguators in order to adequately extract and combine instances and their relations. Experiments conducted over the archaeology research domain provide encouraging results in both efficiency and efficacy and suggest that the tool is suitable for its application on other similar Semantic Web resources.",
                "year": 2009,
                "publisher": "Journal of Algorithms"
            }
        }
    },
    "Design for manufacturing of surfaces to improve accuracy in Fused Deposition Modeling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584515000848/pdfft?md5=5ae121c89e1d9c90a2d53a58eab2f314&pid=1-s2.0-S0736584515000848-main.pdf",
                "title": "Design for manufacturing of surfaces to improve accuracy in Fused Deposition Modeling",
                "abstract": "Fused Deposition Modeling is an Additive Manufacturing technology able to fabricate prototypes, tooling and functional parts without geometrical complexity limitations. Despite of the potential advantages of this technology, a limiting aspect of its industrial diffusion is the obtainable accuracy. The literature highlighted that significant deviations from the nominal values are observed: these deviations are not constant over all the part surfaces but strictly depend upon the process parameters, i.e. the layer thickness and the deposition angle. This involves poor surface quality: the parts could not satisfy the design specifications nor assure the functionality and the assembly fit with other components. The aim of this work is the development of a design for manufacturing methodology able to improve the dimensional accuracy obtainable by this technology. It operates in the design model step performing a virtual model preprocessing: an anisotropic offset is applied to the surfaces, defined by a mathematical formulation, in order to compensate for the abovementioned dimensional deviations. This way, without to eliminate the physical sources of the errors, it is possible to obtain a part with dimensional values very close to nominal ones. This method does not require any additional resources for its application such as preliminary artifact construction and measurements.",
                "year": 2016,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Semantically enhanced Information Retrieval: An ontology-based approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1570826810000910/pdfft?md5=d1a2f9ca0afe53ccd9d7c3ec033af1c0&pid=1-s2.0-S1570826810000910-main.pdf",
                "title": "Semantically enhanced Information Retrieval: An ontology-based approach",
                "abstract": "Currently, techniques for content description and query processing in Information Retrieval (IR) are based on keywords, and therefore provide limited capabilities to capture the conceptualizations associated with user needs and contents. Aiming to solve the limitations of keyword-based models, the idea of conceptual search, understood as searching by meanings rather than literal strings, has been the focus of a wide body of research in the IR field. More recently, it has been used as a prototypical scenario (or even envisioned as a potential “killer app”) in the Semantic Web (SW) vision, since its emergence in the late nineties. However, current approaches to semantic search developed in the SW area have not yet taken full advantage of the acquired knowledge, accumulated experience, and technological sophistication achieved through several decades of work in the IR field. Starting from this position, this work investigates the definition of an ontology-based IR model, oriented to the exploitation of domain Knowledge Bases to support semantic search capabilities in large document repositories, stressing on the one hand the use of fully fledged ontologies in the semantic-based perspective, and on the other hand the consideration of unstructured content as the target search space. The major contribution of this work is an innovative, comprehensive semantic search model, which extends the classic IR model, addresses the challenges of the massive and heterogeneous Web environment, and integrates the benefits of both keyword and semantic-based search. Additional contributions include: an innovative rank fusion technique that minimizes the undesired effects of knowledge sparseness on the yet juvenile SW, and the creation of a large-scale evaluation benchmark, based on TREC IR evaluation standards, which allows a rigorous comparison between IR and SW approaches. Conducted experiments show that our semantic search model obtained comparable and better performance results (in terms of MAP and P@10 values) than the best TREC automatic system.",
                "year": 2011,
                "publisher": "Journal of Web Semantics"
            }
        }
    },
    "Multi-layer Intrabody Terahertz Wave Propagation Model for Nanobiosensing Applications": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Multi-layer Intrabody Terahertz Wave Propagation Model for Nanobiosensing Applications",
                "abstract": "Enabling wireless communication between intrabody nanosensors and wearable devices can transform the field of nanobiosensing and, ultimately, lead to revolutionary healthcare systems. Recently, it has been demonstrated that such communication can occur at Terahertz (THz) band frequencies (0.1–10 THz). For the time being, existing studies are focused on characterizing the propagation of THz waves in a uniform medium. However, in a practical system, the THz waves will traverse different body tissues as they go in/out of the body. In this paper, the propagation of THz waves across human tissues is analytically modeled and numerically analyzed. More specifically, an impedance model that accounts for the discrepancies and the thicknesses of the human tissue layers is developed to allow us to predict the loss encountered as the wave propagates through the human body at THz band frequencies. The results show the necessity of accounting for the lost power due to multi-layer reflection in order to formulate a complete intrabody communication model. At the same time, the viability of utilizing the THz band for developing a feasible intrabody communication link is demonstrated.",
                "year": 2017,
                "publisher": "Nano Communication Networks"
            }
        }
    },
    "Market mechanisms for resource allocation in pervasive sensor applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119212000442/pdfft?md5=0f2554b590e6f9a46e7861674671a3c8&pid=1-s2.0-S1574119212000442-main.pdf",
                "title": "Market mechanisms for resource allocation in pervasive sensor applications",
                "abstract": "This paper describes the use of market mechanisms for resource allocation in pervasive sensor applications to maximize their Value of Information (VoI), which combines the objectively measured Quality of Information (QoI) with the subjective value assigned to it by the users. The unique challenge of pervasive sensor applications that we address is the need for adjusting resource allocation in response to the changing application requirements and evolving sensor network conditions. We use two market mechanisms: auctions at individual sensor nodes to optimize routing, and switch options to optimize dynamic selection of sensor network services as well as switching between modes of operation in pervasive security applications. We also present scenarios of transient congestion management and home security system to motivate the proposed techniques.",
                "year": 2012,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "Personalized Smartphone-Enabled Assessment of Blood Pressure and Its Treatment During the SARS-CoV-2 COVID-19 Pandemic in Patients From the CURE-19 Study: Longitudinal Observational Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Personalized Smartphone-Enabled Assessment of Blood Pressure and Its Treatment During the SARS-CoV-2 COVID-19 Pandemic in Patients From the CURE-19 Study: Longitudinal Observational Study",
                "abstract": "Background\n\nThe use of digital interventions by patients for remote monitoring and management of health and disease is increasing. This observational study examined the feasibility, use, and safety of a digital smartphone app for routine monitoring of blood pressure (BP), medication, and symptoms of COVID-19 during the COVID-19 pandemic.",
                "year": 2024,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "Geneapp: A web application for visualizing alternative splicing for biomedicine": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482524008746/pdfft?md5=7b74c1c5056fd49cbb9dfcc746275402&pid=1-s2.0-S0010482524008746-main.pdf",
                "title": "Geneapp: A web application for visualizing alternative splicing for biomedicine",
                "abstract": "Alternative Splicing (AS) is an essential mechanism for eukaryotes. However, the consequences of deleting a single exon can be dramatic for the organism and can lead to cancer in humans. Additionally, alternative 5′ and 3′ splice sites, which define the boundaries of exons, also play key roles to human disorders. Therefore, Investigating AS events is crucial for understanding the molecular basis of human diseases and developing therapeutic strategies. Workflow for AS event analysis can be sampling followed by data analysis with bioinformatics to identify the different AS events in the control and case samples, data visualization for curation, and selection of relevant targets for experimental validation. The raw output of the analysis software does not favor the inspection of events by bioinformaticians requiring custom scripts for data visualization. In this work, we propose the Geneapp application with three modules: GeneappScript, GeneappServer, and GeneappExplorer. GeneappScript is a wrapper that assists in identifying AS in samples compared in two different approaches, while GeneappServer integrates data from AS analysis already performed by the user. In GeneappExplorer, the user visualizes the previous dataset by exploring AS events in genes with functional annotation. This targeted screens that Geneapp allows to perform helps in the identification of targets for experimental validation to confirm the hypotheses under study. The Geneapp is freely available for non-commercial use at https://geneapp.net to advance research on AS for bioinformatics.",
                "year": 2024,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "A rough-fuzzy document grading system for customized text information retrieval": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457303000827/pdfft?md5=18fb677801e7b4346936d61750156f4e&pid=1-s2.0-S0306457303000827-main.pdf",
                "title": "A rough-fuzzy document grading system for customized text information retrieval",
                "abstract": "Due to the large repository of documents available on the web, users are usually inundated by a large volume of information, most of which is found to be irrelevant. Since user perspectives vary, a client-side text filtering system that learns the user's perspective can reduce the problem of irrelevant retrieval. In this paper, we have provided the design of a customized text information filtering system which learns user preferences and modifies the initial query to fetch better documents. It uses a rough-fuzzy reasoning scheme. The rough-set based reasoning takes care of natural language nuances, like synonym handling, very elegantly. The fuzzy decider provides qualitative grading to the documents for the user's perusal. We have provided the detailed design of the various modules and some results related to the performance analysis of the system.",
                "year": 2005,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Sub-messages extraction for industrial control protocol reverse engineering": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Sub-messages extraction for industrial control protocol reverse engineering",
                "abstract": "The Industrial Internet of Things (IIoT) connects various industrial devices and processes for smart manufacturing purposes. The industrial devices and processes may employ standard or private communication protocols. Protocol Reverse Engineering (PRE) can infer the format of the unknown protocol by analyzing traffic traces. Existing work in the field mainly focuses on Internet protocol only, handling text messages. PRE for industrial control protocols is difficult and particularly designed for IIoT for real-time interconnection among industrial devices. Given the phenomenon that many consecutive sub-messages are often embedded in a lengthy message payload and have a similar format, a novel sub-messages extraction algorithm is proposed in this work by using template iteration as an intermediate step to form a full message format inference framework. An improved evaluation criterion is also proposed to evaluate the sub-messages extraction results. We carry out our algorithm on three standard industrial control protocols and two unknown protocols. Experiments show that adding our sub-messages extraction in PRE for IIoT can greatly improve the accuracy of the overall protocol format inference compared with the existing work.",
                "year": 2022,
                "publisher": "Computer Communications"
            }
        }
    },
    "Automatic quality control of brain T1-weighted magnetic resonance images for a clinical data warehouse": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841521002644/pdfft?md5=519c3ef1bc8293e29f08039b78201233&pid=1-s2.0-S1361841521002644-main.pdf",
                "title": "Automatic quality control of brain T1-weighted magnetic resonance images for a clinical data warehouse",
                "abstract": "Many studies on machine learning (ML) for computer-aided diagnosis have so far been mostly restricted to high-quality research data. Clinical data warehouses, gathering routine examinations from hospitals, offer great promises for training and validation of ML models in a realistic setting. However, the use of such clinical data warehouses requires quality control (QC) tools. Visual QC by experts is time-consuming and does not scale to large datasets. In this paper, we propose a convolutional neural network (CNN) for the automatic QC of 3D T1-weighted brain MRI for a large heterogeneous clinical data warehouse. To that purpose, we used the data warehouse of the hospitals of the Greater Paris area (Assistance Publique-Hôpitaux de Paris [AP-HP]). Specifically, the objectives were: 1) to identify images which are not proper T1-weighted brain MRIs; 2) to identify acquisitions for which gadolinium was injected; 3) to rate the overall image quality. We used 5000 images for training and validation and a separate set of 500 images for testing. In order to train/validate the CNN, the data were annotated by two trained raters according to a visual QC protocol that we specifically designed for application in the setting of a data warehouse. For objectives 1 and 2, our approach achieved excellent accuracy (balanced accuracy and F1-score >90%), similar to the human raters. For objective 3, the performance was good but substantially lower than that of human raters. Nevertheless, the automatic approach accurately identified (balanced accuracy and F1-score >80%) low quality images, which would typically need to be excluded. Overall, our approach shall be useful for exploiting hospital data warehouses in medical image computing.",
                "year": 2022,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Utilizing bloom filters for detecting flooding attacks against SIP based services": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404809000443/pdfft?md5=99051915356d6a9325ee6b006ede4ba5&pid=1-s2.0-S0167404809000443-main.pdf",
                "title": "Utilizing bloom filters for detecting flooding attacks against SIP based services",
                "abstract": "Any application or service utilizing the Internet is exposed to both general Internet attacks and other specific ones. Most of the times the latter are exploiting a vulnerability or misconfiguration in the provided service and/or in the utilized protocol itself. Consequently, the employment of critical services, like Voice over IP (VoIP) services, over the Internet is vulnerable to such attacks and, on top of that, they offer a field for new attacks or variations of existing ones. Among the various threats–attacks that a service provider should consider are the flooding attacks, at the signaling level, which are very similar to those against TCP servers but have emerged at the application level of the Internet architecture. This paper examines flooding attacks against VoIP architectures that employ the Session Initiation Protocol (SIP) as their signaling protocol. The focus is on the design and implementation of the appropriate detection method. Specifically, a bloom filter based monitor is presented and a new metric, named session distance, is introduced in order to provide an effective protection scheme against flooding attacks. The proposed scheme is evaluated through experimental test bed architecture under different scenarios. The results of the evaluation demonstrate that the required time to detect such an attack is negligible and also that the number of false alarms is close to zero.",
                "year": 2009,
                "publisher": "Computers & Security"
            }
        }
    },
    "Impact of spatial effects on income segregation indices": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0198971511000834/pdfft?md5=895b6d2702063a81ed42fbda8c6d00ab&pid=1-s2.0-S0198971511000834-main.pdf",
                "title": "Impact of spatial effects on income segregation indices",
                "abstract": "Residential segregation is an inherently spatial phenomenon as it measures the separation of different types of people within a region. Whether measured with an explicitly spatial index, or a classic aspatial index, a region’s underlying spatial properties could manifest themselves in the magnitude of measured segregation. In this paper we implement a Monte Carlo simulation approach to investigate the properties of four segregation indices in regions built with specific spatial properties. This approach allows us to control the experiment in ways that empirical data do not. In general we confirm the expected results for the indices under various spatial properties, but some unexpected results emerge. Both the Dissimilarity Index and Neighborhood Sorting Index are sensitive to region size, but their spatial counterparts, the Adjusted Dissimilarity Index and Generalized Neighborhood Sorting Index, are generally immune to this problem. The paper also lends weight to concerns about the downward pressure on measured segregation when multiple neighborhoods are grouped into a single census tract. Finally, we discuss concerns about the way space is incorporated into segregation indices since the expected value of the spatial indices tested is lower than their aspatial counterparts.",
                "year": 2011,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "DC-Vegas: A delay-based TCP congestion control algorithm for datacenter applications": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1084804515000661/pdfft?md5=e3d7d4ff343c9d82a7d095aeaa9a7397&pid=1-s2.0-S1084804515000661-main.pdf",
                "title": "DC-Vegas: A delay-based TCP congestion control algorithm for datacenter applications",
                "abstract": "TCP congestion control in datacenter networks is very different from in traditional network environments. Datacenter applications require TCP to provide soft real-time latency and have the ability to avoid incast throughput collapses. To meet the special requirements of datacenter congestion control, numerous solutions have been proposed, such as DCTCP, D2TCP, and D3. However, several deployment drawbacks, including significant modifications to switch hardware, the Operating System protocol stack, and/or upper-layer applications, as well as switch ECN requirements, which are not always available in already existing datacenters, limit deployment of these solutions. To address these deployment problems, in this paper, we proposed a delay-based TCP algorithm for datacenter congestion control, namely DC-Vegas. DC-Vegas combines the performance advantages of DCTCP with the deployment advantages of delay-based TCP Vegas. DC-Vegas can meet both soft real-time and incast avoidance requirements of datacenters, requiring minimal deployment modification to existing datacenter hardware/software (with only sender-side update and without ECN requirements). Experimental results obtained using the real datacenter test bed and an ns-2 simulator demonstrate that DC-Vegas has similar performance with the state-of-the-art Data Center TCP algorithm.",
                "year": 2015,
                "publisher": "Journal of Network and Computer Applications"
            }
        }
    },
    "Revelation of potent ES-62 activity of a hypothetical protein from Wuchereria bancrofti proteome by in-silico based characterization and functional annotation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914824001011/pdfft?md5=9479672c37fef970f5a15a679be5e3c3&pid=1-s2.0-S2352914824001011-main.pdf",
                "title": "Revelation of potent ES-62 activity of a hypothetical protein from Wuchereria bancrofti proteome by in-silico based characterization and functional annotation",
                "abstract": "Lymphatic filariasis (LF) is a well-known disease caused primarily by the parasitic worm Wuchereria bancrofti, although other Filarioidea parasites, including Brugia malayi and Brugia timori, can also be responsible. These parasites can frequently affect the lymphatic system during their microfilariae stage. These parasites often produce numerous excretory-secretory (ES) products that can modulate or impede the activities of diverse immune system cells. Thus, this study utilized computational characterization and functional annotation to assess the impact of a hypothetical protein from Wuchereria brancrofti, specifically its role as an ES-62 product. The analysis of functional domain annotation indicates that the hypothetical protein shares functional domains comparable to those found in ES-62 (Zinc-binding metallopeptidase family protein). Additionally, the hypothetical protein has significant sequence similarities (73.83 %) with the ES-62 of Acanthocheilonema viteae, as shown by the multiple sequence alignment and phylogeny analyses. Furthermore, the hypothetical protein's secondary and tertiary structure and its tertiary model structure's quality possess an acceptable standard quality (Z-score of −9.45). Moreover, the molecular docking study demonstrates that the hypothetical protein shows a significant affinity for the TLR-4 receptor (center energy score of −1145.9 and the lowest energy score of −1096.5), a finding that is subsequently confirmed by normal mode analysis. These in silico findings suggest that the observed hypothetical protein could aid in studying filarial infection and developing pharmaceuticals to treat lymphatic filariasis.",
                "year": 2024,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Adaptive cache pre-forwarding policy for distributed deep learning": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045790619313266/pdfft?md5=4c1871b81a257659958cd0cc3f3a4d6a&pid=1-s2.0-S0045790619313266-main.pdf",
                "title": "Adaptive cache pre-forwarding policy for distributed deep learning",
                "abstract": "With the rapid growth of deep learning algorithms, several high-accuracy models have been developed and applied to many real-world domains. Deep learning is parallel and suitable for distributed computing, which can significantly improve the system throughput. However, there is a bottleneck for cross-machine training, that is, network latency. Nodes frequently need to wait for synchronization, and the content of each synchronization may range from several megabytes to hundred megabytes. Thus, network communication takes considerable time in the training process, which reduces system performance. Therefore, many computing architectures have been proposed. This paper proposes a type of distributed computing system for deep learning. Our design aims to reduce synchronization times and network blocking times by using a new cache mechanism, called cache pre-forwarding. The design concept of cache pre-forwarding aims to exploit reinforcement learning to train a pre-forwarding policy to increase the cache hit rate. Because of the features of reinforcement learning, our policy is adaptive and applicable to different computing environments. Finally, we experimentally demonstrate that our system is feasible.",
                "year": 2020,
                "publisher": "Computers & Electrical Engineering"
            }
        }
    },
    "On the reproducibility of results of pathway analysis in genome-wide expression studies of colorectal cancers": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046409001336/pdfft?md5=e3aee3be72c972946421ea6e23df64f8&pid=1-s2.0-S1532046409001336-main.pdf",
                "title": "On the reproducibility of results of pathway analysis in genome-wide expression studies of colorectal cancers",
                "abstract": "One of the major problems in genomics and medicine is the identification of gene networks and pathways deregulated in complex and polygenic diseases, like cancer. In this paper, we address the problem of assessing the variability of results of pathways analysis identified in different and independent genome wide expression studies, in which the same phenotypic conditions are assayed. To this end, we assessed the deregulation of 1891 curated gene sets in four independent gene expression data sets of subjects affected by colorectal cancer (CRC). In this comparison we used two well-founded statistical models for evaluating deregulation of gene networks. We found that the results of pathway analysis in expression studies are highly reproducible. Our study revealed 53 pathways identified by the two methods in all the four data sets analyzed with high statistical significance and strong biological relevance with the pathology examined. This set of pathways associated to single markers as well as to whole biological processes altered constitutes a signature of the disease which sheds light on the genetics bases of CRC.",
                "year": 2010,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Grand challenges to computational science": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0167739X89900381/pdfft?md5=19da3388ec8f16667a053a19d55ad888&pid=1-s2.0-0167739X89900381-main.pdf",
                "title": "Grand challenges to computational science",
                "abstract": "Computational Science is at the very beginning of centuries of growth, comparable to the four centuries of experimental advances since Galileo. The Grand Challenges to Computational Science are unsolved scientific problems of extraordinary breadth and importance which will demand continuing computational advances throughout the forthcoming computational era. Supercomputers can be used to see phenomena not directly accessible to experiment in key scientific and engineering areas such as atmospheric science, astronomy, materials science, molecular biology, aerodynamics, and elementary particle physics. However, the benefits of supercomputers will be greatly increased if some major difficulties are overcome. In this paper, I address some of the tougher requirements on current grand challenge research to ensure that it has enduring value. The problems of algorithm development, error control, software productivity, and the fostering of technological advances are especially important.",
                "year": 1989,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Molecular modeling and dynamics simulation of human cyclin-dependent kinase 3 complexed with inhibitors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482508001728/pdfft?md5=97468c10d533782be643a923bd33f9a4&pid=1-s2.0-S0010482508001728-main.pdf",
                "title": "Molecular modeling and dynamics simulation of human cyclin-dependent kinase 3 complexed with inhibitors",
                "abstract": "The complex CDK3–cyclin is involved in the control of the progression of G0. While the mechanisms governing early and late G1 progression are well understood, very little is known about the G0–G1 transition. Human CDK3 is closely related to cyclin-dependent kinase 2 (CDK2). Since there is no crystallographic structure of human CDK3, this work describes for the first time a molecular model of human CDK3 complexed with several inhibitors. Comparison of the binary complexes with different inhibitors strongly indicates that those inhibitors should inhibit CDK3 as well as CDK2.",
                "year": 2009,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Management challenges in creating value from business analytics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221717301455/pdfft?md5=e418dc05dc721be8768c1b781ab4aaa0&pid=1-s2.0-S0377221717301455-main.pdf",
                "title": "Management challenges in creating value from business analytics",
                "abstract": "The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.",
                "year": 2017,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Digital Health Opportunities to Improve Primary Health Care in the Context of COVID-19: Scoping Review": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Digital Health Opportunities to Improve Primary Health Care in the Context of COVID-19: Scoping Review",
                "abstract": "Background\n\nThe COVID-19 pandemic brought social, economic, and health impacts, requiring fast adaptation of health systems. Although information and communication technologies were essential for achieving this objective, the extent to which health systems incorporated this technology is unknown.",
                "year": 2022,
                "publisher": "JMIR Human Factors"
            }
        }
    },
    "Vital triangle: A new concept to evaluate urban vitality": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0198971522001302/pdfft?md5=36e9d71e42032024cf2af9b6b1ff59b0&pid=1-s2.0-S0198971522001302-main.pdf",
                "title": "Vital triangle: A new concept to evaluate urban vitality",
                "abstract": "Urban vitality is a driving force of urban sustainable development and influences the well-being of its residents. To better understand and evaluate it, we defined urban vitality from the perspective of the urban organism and proposed the “vital triangle” conceptual framework linked to the three aspects of growth, diversity, and mobility. We designed an evaluation index system that includes the six dimensions of population, economy, society, government, environment, and external links by integrating multiple sets of heterogeneous data, and applied it to cities in the Yangtze River Delta. The results show that Shanghai, Nanjing, Suzhou, and Hangzhou have the highest vitality levels but are weak regarding their eco-environments. Cities with high vitality are distributed on the Z-shaped key development belt of the Yangtze River Delta urban agglomeration planning. Regional differences in external links and population vitality are significant, and these two dimensions pose weaknesses to many cities. The interregional differences linked to social vitality are the smallest. Vitality not only comes from within the urban system, but also greatly depends on the external environment. The concept “vital triangle” and relevant evaluation framework are intended to serve as a basis for future research, and the case study could provide some key decision-making support for urban planning and management.",
                "year": 2022,
                "publisher": "Computers, Environment and Urban Systems"
            }
        }
    },
    "DNA sequence watermarking based on random circular angle": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1051200413002753/pdfft?md5=b8fce73f3d39f925b26783529a9bd21e&pid=1-s2.0-S1051200413002753-main.pdf",
                "title": "DNA sequence watermarking based on random circular angle",
                "abstract": "This paper discusses DNA watermarking for copyright protection and authentication of a DNA sequence. We then propose a DNA watermarking method that confers mutation resistance, amino acid residue conservation, and watermark security. Our method allocates codons to random circular angles using a random mapping table and selects a number of codons for embedding targets using the Lipschitz regularity that is measured from the evolution across scales of local modulus maxima of codon circular angles. We then embed the watermark into random circular angles of codons without changing the amino acid residue. The length and location of target codons depend on the random mapping table and the singularity of detection of Lipschitz regularity. This table is used as the watermark key and can be applied to any codon sequence regardless of sequence length. Without knowledge of this table, it is very difficult to detect the length and location of sequences for extracting the watermark. From experimental results on the suitability of similar watermark capacities, we verified that our method has a lower bit-rate error for point mutations compared with previous methods. Further, we established that the entropies of the random mapping table and the location of target codons are high, indicating that the watermark is secure.",
                "year": 2014,
                "publisher": "Digital Signal Processing"
            }
        }
    },
    "Effects of individual characteristics, organizational factors and task characteristics on computer programmer productivity and job satisfaction": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/037872068490020X/pdfft?md5=52c947a024e1d7c3face0777e0cd120e&pid=1-s2.0-037872068490020X-main.pdf",
                "title": "Effects of individual characteristics, organizational factors and task characteristics on computer programmer productivity and job satisfaction",
                "abstract": "Several individual characteristics of computer programmers (self- esteem, level of experience, and mathematical aptitude); four organizational factors (perceived supervisory initiated structure, perceived supervisor consideration, perceived level of performance feedback, and perceived degree of participation in organizational decisions); and five task characteristics (skill variety, task variety, task significance, autonomy, and feedback) are related to computer programmer productivity and job satisfaction. Measurement was accomplished via questionnaires; least squares multiple regression was used to test the hypotheses.",
                "year": 1984,
                "publisher": "Information & Management"
            }
        }
    },
    "New potential anticancer drug-like compounds for squamous cell lung cancer using transcriptome network analysis": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914821000897/pdfft?md5=62f372a67eb4dbd00fcc2b4af7b10cd6&pid=1-s2.0-S2352914821000897-main.pdf",
                "title": "New potential anticancer drug-like compounds for squamous cell lung cancer using transcriptome network analysis",
                "abstract": "Lung Squamous Cell Carcinoma (SQCC) is one of the deadliest non-small cell lung cancers that does not respond well to chemotherapy or radiation therapy. Targeted therapy can prevent SQCC progression by blocking messages sent to SQCC cells. Understanding the genetic causes of SQCC can help predict the most effective targeted therapeutic options by exploring novel candidate genes. In this respect, considering the importance of module or co-expressed genes, the RNA-seq data of SQCC from The Cancer Genome Atlas (TCGA) database was applied to create and analyze genetic modules of co-expressed genes using Weighted Gene Correlation Network Analysis (WGCNA). The results revealed that the genes were clustered into five modules. Among them, we have observed two modules with a high number of associated oncogenes for SQCC. Focusing on a subset of genes carries the advantage of removing challenges in large network analysis; ergo, from network analysis and module enrichment analysis, reported five important genes and nine pathways and biological processes associated with SQCC. The results of the co-expression network analysis were then applied to enable drug repurposing to candidate novel drug-like compounds for SQCC. This strategy can be different from other drug repurposing approaches since it employed co-expression network analysis results. Seven potential drug-like compounds associated with SQCC targeted therapy were then found. Though the study has a formal procedure flow, the introduction of a few drug-like compounds for SQCC not only presents a brighter vision for further follow-up studies but can also help scientists with positive biological evidence.",
                "year": 2021,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "An empirical approach based on quantile regression for estimating citation ageing": {
        "accordingTo": {
            "scienceDirect": {
                "title": "An empirical approach based on quantile regression for estimating citation ageing",
                "abstract": "An aspect of citation behavior, which has received longstanding attention in research, is how articles’ received citations evolve as time passes since their publication (i.e., citation ageing). Citation ageing has been studied mainly by the formulation and fit of mathematical models of diverse complexity. Commonly, these models restrict the shape of citation ageing functions and explicitly take into account factors known to influence citation ageing. An alternative—and less studied—approach is to estimate citation ageing functions using data-driven strategies. However, research following the latter approach has not been consistent in taking into account those factors known to influence citation ageing. In this article, we propose a model-free approach for estimating citation ageing functions which combines quantile regression with a non-parametric specification able to capture citation inflation. The proposed strategy allows taking into account field of research effects, impact level effects, citation inflation effects and skewness in the distribution of cites effects. To test our methodology, we collected a large dataset consisting of more than five million citations to 59,707 research articles spanning 12 dissimilar fields of research and, with this data in hand, tested the proposed strategy.",
                "year": 2019,
                "publisher": "Journal of Informetrics"
            }
        }
    },
    "Modelling marine particle dynamics with LTRANS-Zlev: implementation and validation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815219305304/pdfft?md5=1f070b985335a6363d52027f7f85d490&pid=1-s2.0-S1364815219305304-main.pdf",
                "title": "Modelling marine particle dynamics with LTRANS-Zlev: implementation and validation",
                "abstract": "This paper presents the release of LTRANS-Zlev, which is a new version of the off-line Lagrangian ocean particle-tracking model LTRANS v.2b that is compatible with a Z-coordinate (constant-depth layers) discretization of the hydrodynamic equations. The model capitalizes on and massively extends the capabilities of the original code LTRANS, which is already quite popular, but can be used only adopting a sigma-coordinate (terrain-following layers) discretization. Among the additional features included in LTRANS-Zlev, there are the backward-in-time particle-tracking algorithm and some new customizable larval behaviour options. The new version also includes the OILTRANS-module for oil spill simulations. The new implementations were validated by using the output of the Z-coordinate Massachusetts Institute of Technology general circulation model (MITgcm) for an idealized case study describing a cyclonic gyre in a mid-latitude closed basin. Another test-case, in which larval dispersal is modelled in the northern Adriatic Sea, illustrates some of the new features of LTRANS-Zlev.",
                "year": 2020,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Characterization of aboveground biomass in an unmanaged boreal forest using Landsat temporal segmentation metrics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0924271614000665/pdfft?md5=e4c77647aec87ce0ac661addf5606caa&pid=1-s2.0-S0924271614000665-main.pdf",
                "title": "Characterization of aboveground biomass in an unmanaged boreal forest using Landsat temporal segmentation metrics",
                "abstract": "Canada is dominated by forested ecosystems which are subject to various inventory and management practices, with more northern boreal forests subject to neither. Our objectives were to measure the capacity of temporal trajectory metrics for estimating selected forest attributes in a northern Canadian boreal forest context using Landsat imagery and investigate the importance of different types of temporal trajectory metrics. Results indicated that Wetness was the best Tasseled Cap (TC) component for aboveground biomass estimation (R2 = 50%, RMSE% = 56%), and the combination of simple and complex metrics from all TC components produced the highest R2 (62%) and lowest RMSE% (49%). Using a similar combination of variables, other forest attributes were estimated equally reliably with lower RMSE% values. The most important temporal trajectory metrics were simple and described TC component values at each point of change in the temporal trajectory, however the most important variables overall were environmental variables.",
                "year": 2014,
                "publisher": "ISPRS Journal of Photogrammetry and Remote Sensing"
            }
        }
    },
    "A software framework for construction of process-based stochastic spatio-temporal models and data assimilation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815209002643/pdfft?md5=09835af2395294500d0c1d5e2c1827c4&pid=1-s2.0-S1364815209002643-main.pdf",
                "title": "A software framework for construction of process-based stochastic spatio-temporal models and data assimilation",
                "abstract": "Process-based spatio-temporal models simulate changes over time using equations that represent real world processes. They are widely applied in geography and earth science. Software implementation of the model itself and integrating model results with observations through data assimilation are two important steps in the model development cycle. Unlike most software frameworks that provide tools for either implementation of the model or data assimilation, this paper describes a software framework that integrates both steps. The software framework includes generic operations on 2D map and 3D block data that can be combined in a Python script using a framework for time iterations and Monte Carlo simulation. In addition, the framework contains components for data assimilation with the Ensemble Kalman Filter and the Particle filter. Two case studies of distributed hydrological models show how the framework integrates model construction and data assimilation.",
                "year": 2010,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "A Randomized Trial Comparing Telemedicine Case Management with Usual Care in Older, Ethnically Diverse, Medically Underserved Patients with Diabetes Mellitus: 5 Year Results of the IDEATel Study": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1067502709000851/pdfft?md5=7963eff5030ca9e96fa302c58c318f30&pid=1-s2.0-S1067502709000851-main.pdf",
                "title": "A Randomized Trial Comparing Telemedicine Case Management with Usual Care in Older, Ethnically Diverse, Medically Underserved Patients with Diabetes Mellitus: 5 Year Results of the IDEATel Study",
                "abstract": "Context\n\nTelemedicine is a promising but largely unproven technology for providing case management services to patients with chronic conditions and lower access to care.",
                "year": 2009,
                "publisher": "Journal of the American Medical Informatics Association"
            }
        }
    },
    "Structure based in-silico study on UDP-N-acetylmuramoyl-L-alanyl-D-glutamate-2,6-diaminopimelate ligase (MurE) from Acinetobacter baumannii as a drug target against nosocomial infections": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352914819301066/pdfft?md5=31be005adf6aa20d91ff31f84d601d10&pid=1-s2.0-S2352914819301066-main.pdf",
                "title": "Structure based in-silico study on UDP-N-acetylmuramoyl-L-alanyl-D-glutamate-2,6-diaminopimelate ligase (MurE) from Acinetobacter baumannii as a drug target against nosocomial infections",
                "abstract": "UDP-N-acetylmuramoyl-L-alanyl-d-glutamate--2,6-diaminopimelate ligase (MurE) initiates reaction by adding meso-diaminopimelic acid to the nucleotide precursor UDP-N-acetylmuramoyl-L-alanyl-d-glutamate, during the synthesis of murein in the cytoplasm. This enzyme is crucial for microorganisms including A. baumannii, and is non-homologous to mammals; therefore, it can be used as potential antibacterial drug target. The crystallographic structure of UDP-N-acetylmuramoyl-L-alanyl-d-glutamate--l-lysine ligase (MurE) from Staphylococcus aureus with UDP-MurNAc- Ala-Glu-Lys (4C13) was used to model the tertiary structure of UDP-N-acetylmuramoyl-l-alanine-d-glutamate:meso diaminopimelate ligase (MurE) from A. baumannii. The evaluated structure of MurE was aligned on the template and nitrogen-termini, central, and carbon-termini domains were obtained (the first, second, and third domain, respectively). It has been found from the conserved region that the active site residues, Arg383, Asp407, Asn408, Arg410, and Glu463, participated in the binding site, whereas Asn408 was the vital residue involved in the catalysis during synthesis of murein in Acinetobacter baumannii, by adding meso-diaminopimelic acid to a nucleotide precursor. Consequently, disrupting the above-mentioned amino acid chain could hamper the usual role of MurE. Overall, six thousand seven hundred and twenty-five (6725) compounds were capable of binding to MurE with low binding energy. The natural ligands were screened to eliminate molecules with unwanted properties for drug-likeness on the basis of physicochemical properties. Eight (8) ligands such as ZINC15675880, ZINC15675922, ZINC15675928, ZINC15707240, ZINC15707335, ZINC15675930, ZINC20503844, and ZINC30879537 have satisfied the ADMET parameters. Out of these, two compounds that had the best binding energies and docking results were selected for molecular dynamics simulations. The results for all parameters indicated stability and less fluctuation of the complexes. Therefore, after wet-lab confirmation, these compounds could be recommended as potential drugs against Acinetobacter baumannii infections.",
                "year": 2019,
                "publisher": "Informatics in Medicine Unlocked"
            }
        }
    },
    "Status of clinical gene sequencing data reporting and associated risks for information loss": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046406000293/pdfft?md5=a0eefada8451539ad248cb035deb374d&pid=1-s2.0-S1532046406000293-main.pdf",
                "title": "Status of clinical gene sequencing data reporting and associated risks for information loss",
                "abstract": "Clinical gene sequencing is growing in importance and cost-effectiveness. In the past two years, the number of genes associated with disease has grown by roughly 25%. Knowledge of genetic variations will soon guide drug selection and dosages, predict risks from toxin exposures, and inform nutritional needs. Despite the significance of sequencing, methods for reporting results are problematic. Frequent use of paper and infrequent use of naming standards impede data exchange and make incorporation into the electronic medical record difficult. Reports often describe only variations found, rather than all data (all patient bases sequenced). Also, reports frequently do not describe reference data used to define variations. These practices create risks for loss of both data and information. Standardized electronic reporting of all data (all bases sequenced and all reference data) and electronic record systems capable of storing these results would both prevent data loss and simplify the preservation of information those data provide.",
                "year": 2007,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Studies on striping and buffer caching issues for the software RAID file system": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1383762102000528/pdfft?md5=669626a353e8f23811f0d8908d180172&pid=1-s2.0-S1383762102000528-main.pdf",
                "title": "Studies on striping and buffer caching issues for the software RAID file system",
                "abstract": "A software RAID file system is defined as a system that distributes data redundantly across an array of disks attached to each of the workstations connected on a high-speed network. This configuration provides higher throughput and availability compared to conventional file systems. In this paper, we consider two specific issues regarding the distribution of data among the cluster, namely, striping and buffer caching for such an environment. Through simulation studies we compare the performance of various striping methods and show that for effective striping in software RAID file systems, it must take advantage of its flexible nature. Further, for buffer caching, we show that conventional caching schemes developed for distributed systems are insufficient, and that the Exclusively Old Data and Parity scheme that is presented in this paper, overcomes the limitations of the previously proposed schemes.",
                "year": 2002,
                "publisher": "Journal of Systems Architecture"
            }
        }
    },
    "Cluster-based lateral transshipments for the Zambian health supply chain": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221723006094/pdfft?md5=24899a708ca76896aeab6d40374fcb0b&pid=1-s2.0-S0377221723006094-main.pdf",
                "title": "Cluster-based lateral transshipments for the Zambian health supply chain",
                "abstract": "Many low- and middle-income countries, including Zambia, suffer from unreliable distribution of health commodities leading to high variation in service levels across health facilities. Our work investigates how transshipment can improve system-wide service levels, equity across facilities, and average inventory levels. We focus on the distribution of malaria medicines in Zambia’s public pharmaceutical supply chain, which is heavily impacted by the rainy season leading to seasonality and uncertainty in demand and lead times. We use the more advanced deep reinforcement learning method Proximal Policy Optimization to develop transshipment policies and compare their performance with currently available, easy-to-use heuristics. To ensure that the model applies to settings of a realistic scale, we adopt a policy architecture that effectively decouples the policy’s complexity from the problem dimensions. We find that deep reinforcement learning is mainly useful in resource-constrained environments where system-wide inventory is scarce. When sufficient inventory is available, transshipment heuristics are more appealing from an overall cost-effectiveness perspective. Based on our numerical experiments, we formulate policy insights that can support policymakers in a humanitarian health context.",
                "year": 2024,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Performance analysis of Cellular Automata HPC implementations": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045790615003225/pdfft?md5=383da969e86ebbfe1a5c2670f0b83d98&pid=1-s2.0-S0045790615003225-main.pdf",
                "title": "Performance analysis of Cellular Automata HPC implementations",
                "abstract": "Cellular Automata (CA) are of interest in several research areas and there are many available serial implementations of CA. However, there are relatively few studies analyzing in detail High Performance Computing (HPC) implementations of CA which allow research on large systems. Here, we present a parallel implementation of a CA with distributed memory based on MPI. As a first step to insure fast performance, we study several possible serial implementations of the CA. The simulations are performed in three infrastructures, comparing two different microarchitectures. The parallel code is tested with both Strong and Weak scaling, and we obtain parallel efficiencies of ∼ 75%–85%, for 64 cores, comparable to efficiencies for other mature parallel codes in similar architectures. We report communication time and multiple hardware counters, which reveal that performance losses are related to cache references with misses, branches and memory access.",
                "year": 2015,
                "publisher": "Computers & Electrical Engineering"
            }
        }
    },
    "Experimental aspects of Mode I and Mode II fracture toughness testing of fibre-reinforced polymer-matrix composites": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782599002571/pdfft?md5=84c52083d79c43c249c4bcc616e84bd2&pid=1-s2.0-S0045782599002571-main.pdf",
                "title": "Experimental aspects of Mode I and Mode II fracture toughness testing of fibre-reinforced polymer-matrix composites",
                "abstract": "Research on fracture mechanics has yielded a number of different test set-ups for determining the critical fracture toughness of fibre-reinforced polymer-matrix (FRP) composite materials under tensile/opening (Mode I), in-plane shear (Mode II), and anti-plane shear/torsion (Mode III) loads, as well as for mixed mode loads I/II. Test procedures for Mode I and Mode II critical fracture toughness testing of unidirectional FRP-composites are currently being evaluated for international standardisation. This paper discusses recent developments with the emphasis on experimental aspects of fracture toughness testing of FRP-composites in Mode I and Mode II.\n\nExperimental determination of the critical fracture toughness of FRP-composites involves a number of issues. Most activities to date have aimed at reducing in-laboratory and inter-laboratory variation to “reasonable” bounds (typical coefficient of variation 10–20%) when testing identical material. Specifically, the following problems will be addressed and discussed: (1) the type of starter crack or delamination starter, (2) the definition of delamination initiation from an existing crack, (3) the distribution and number of data points used in the analysis, (4) the accuracy of the experimental data, and (5) validation criteria for test method and data.\n\nBeyond the specific technical issues mentioned above, there are more general issues that relate to the applicability of the test procedures. So far, some of these issues have received little attention. A crucial point is the definition of the scope of the procedure, but differences in interpretation and realisation of the procedures by the operator are also important. From the point of view of the users of FRP-composites, general experimental aspects, such as, e.g., the simplicity of the test procedure and the availability of test set-ups, as well as the time needed for test preparation, performance, and analysis (which translate directly into cost), and the applicability of the procedures to engineering materials will gain in importance, once the technical test problems have been resolved.",
                "year": 2000,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Exploring HCV genome to construct multi-epitope based subunit vaccine to battle HCV infection: Immunoinformatics based approach": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S153204642030126X/pdfft?md5=6d2ec65d82df4215003a553380030f24&pid=1-s2.0-S153204642030126X-main.pdf",
                "title": "Exploring HCV genome to construct multi-epitope based subunit vaccine to battle HCV infection: Immunoinformatics based approach",
                "abstract": "Hepatitis C Virus (HCV) infection is a major cause of chronic liver disease, hepatocellular carcinoma, and the single most common indication for liver transplantation. HCV vaccines eliciting specific T-cell responses, have been considered as potent method to prevent HCV infection. Despite several reports on progress of vaccine, these vaccine failed in mediating clinical relevance activity against HCV in humans. In this study we integrated both immunoinformatic and molecular docking approach to present a multiepitope vaccine against HCV by designating 17 conserved epitopes from eight viral proteins such as Core protein, E1, E2, NS2, NS34A, NS4B, NS5A, and NS5B. The epitopes were prioritized based on conservation among epitopes of T cell, B cell and IFN-γ that were then scanned for non-homologous to host and antigenicity. The prioritized epitopes were then linked together by AAY linker and adjuvant (β-defensin) were attached at N-terminal to enhance immunogenic potential. The construct thus formed were subjected to structural modeling and physiochemical characteristics. The modeled structure were successfully docked to antigenic receptor TLR-3 and In-silico cloning confers the authenticity of its expression efficiency. However, the proposed construct need to be validate experimentally to ensure its safety and immunogenic profile.",
                "year": 2020,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "A game theoretic approach for minimal connected dominating set": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0304397520302966/pdfft?md5=c91b7978755fd049a6c1d56a413facbe&pid=1-s2.0-S0304397520302966-main.pdf",
                "title": "A game theoretic approach for minimal connected dominating set",
                "abstract": "Connected dominating set (CDS) is a subset of vertices in a graph which dominates all vertices and induces a connected subgraph. This paper proposes a game theoretic approach to find a CDS, and proves that starting from a non-trivial initial state, every non-trivial Nash equilibrium is a minimal CDS and reaching a non-trivial Nash equilibrium needs O(n) rounds in the worst case.",
                "year": 2020,
                "publisher": "Theoretical Computer Science"
            }
        }
    },
    "WARNE: A stalkerware evidence collection tool": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2666281723001968/pdfft?md5=1b6f141e02aa6980d7dac8f91ca37e2d&pid=1-s2.0-S2666281723001968-main.pdf",
                "title": "WARNE: A stalkerware evidence collection tool",
                "abstract": "Intimate partner violence (IPV) is a form of abuse in romantic relationships, more frequently, against the female partner. IPV can vary in severity and frequency, ranging from emotional abuse or stalking to recurring and severe violent episodes over a long period. Easy access to stalkerware apps helps foster such behaviors by allowing non-tech-savvy individuals to spy on their victims. These apps offer features for discreetly monitoring and remotely controlling compromised mobile devices, thereby infringing the victim's privacy and the security of their data. In this work, we investigate methods for gathering evidence about an abuser and the stalkerware they employ on a victim's device. We develop a semi-automated tool intended for use by investigators, helping them to analyze Android phones for potential threats in cases of IPV stalkerware. As a first step towards this goal, we perform an experimental privacy and security study to investigate currently available stalkerware apps. We specifically study the vectors through which vulnerabilities found in stalkerware apps could be exploited by investigators, allowing them to gather information about the IPV services, IPV abusers, and the victims' stolen data. We then design and implement a tool called WARNE, leveraging the identified flaws to facilitate the information and evidence collection process. In our experiments, we identified 50 unique stalkerware apps and their corresponding download websites that are still reachable, including one available on the Google Play Store. Among these apps, we found 30 that were free or offered a free trial. We enumerated and experimentally verified several invasive capabilities offered by these apps to clearly identify the severe privacy risks posed by them. We also found that most stalkerware apps store private information locally on the compromised device, potentially giving away information about the abuser. Our evidence-gathering tool found data related to the abuser and/or the stalkerware company, such as account credentials, dashboard URLs, and API tokens in 20 apps out of 30 tested apps. We hope our tool will help IPV victims and investigators against the growing threat of stalkerware abuse.",
                "year": 2024,
                "publisher": "Forensic Science International: Digital Investigation"
            }
        }
    },
    "Prediction of diabetic retinopathy among type 2 diabetic patients in University of Gondar Comprehensive Specialized Hospital, 2006–2021: A prognostic model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1386505624001990/pdfft?md5=d25edb0902f3b7152602ff59bfa0943a&pid=1-s2.0-S1386505624001990-main.pdf",
                "title": "Prediction of diabetic retinopathy among type 2 diabetic patients in University of Gondar Comprehensive Specialized Hospital, 2006–2021: A prognostic model",
                "abstract": "Background\n\nThere has been a paucity of evidence for the development of a prediction model for diabetic retinopathy (DR) in Ethiopia. Predicting the risk of developing DR based on the patient’s demographic, clinical, and behavioral data is helpful in resource-limited areas where regular screening for DR is not available and to guide practitioners estimate the future risk of their patients.",
                "year": 2024,
                "publisher": "International Journal of Medical Informatics"
            }
        }
    },
    "Analysis of a new therapeutic target and construction of a prognostic model for breast cancer based on ferroptosis genes": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482523008351/pdfft?md5=0f617a56ecd5f09138ac272d5e927d98&pid=1-s2.0-S0010482523008351-main.pdf",
                "title": "Analysis of a new therapeutic target and construction of a prognostic model for breast cancer based on ferroptosis genes",
                "abstract": "Breast cancer, which is the most common malignant tumor among women worldwide and an important cause of death in women. The existing prognostic model for patients with breast cancer is not accurate as breast cancer is resistant to commonly used antitumor drugs. Ferroptosis is a novel mechanism of programmed cell death that depends on iron accumulation and lipid peroxidation. Various studies have confirmed the role of ferroptosis in tumor regulation and ferroptosis is now considered to play an important role in breast cancer development. At present, the association between breast cancer prognosis and ferroptosis-related gene expression remains unclear. Further exploration of this research area may optimize the evaluation and prediction of prognosis of patients with breast cancer and finding of new therapeutic targets. In this study, clinical factors and the expression of multiple genes were evaluated in breast cancer samples from the Cancer Genome Atlas (TCGA) database and Gene Expression Omnibus (GEO) database database. Eleven prognostication-related genes (TP63, IFNG, MT3, ANO6, FLT3, PTGS2, SLC1A4, JUN, SLC7A5, CHAC1, and TF) were identified from differentially expressed genes to construct a survival prediction model, which showed a good prediction ability. KEGG pathway analysis revealed that immune-related pathways were the primary pathways. ssGSEA analysis showed significant differences in the distribution of certain immune-related cell subsets, such as CD8+T cells and B cells, and in the expression of multiple immune genes, including type II IFN response and APC coinhibition. In addition, 10 immune targets related to ferroptosis in breast cancer were found: CD276, CD80, HHLA2, LILRA2, NCR3LG1, NECTIN3, PVR, SLAMF9,TNFSF4, and BTN1A1. Using TCGA, new ferroptosis genes related to breast cancer prognosis were identified, a new reliable and accurate prognosis model was developed, and 10 new potential therapeutic targets different from the traditional targeted drugs were identified to provide a reference for improving the poor prognosis of patients with breast cancer.",
                "year": 2023,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Predicting 30-Day Readmission Risk for Patients With Chronic Obstructive Pulmonary Disease Through a Federated Machine Learning Architecture on Findable, Accessible, Interoperable, and Reusable (FAIR) Data: Development and Validation Study": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Predicting 30-Day Readmission Risk for Patients With Chronic Obstructive Pulmonary Disease Through a Federated Machine Learning Architecture on Findable, Accessible, Interoperable, and Reusable (FAIR) Data: Development and Validation Study",
                "abstract": "Background\n\nOwing to the nature of health data, their sharing and reuse for research are limited by legal, technical, and ethical implications. In this sense, to address that challenge and facilitate and promote the discovery of scientific knowledge, the Findable, Accessible, Interoperable, and Reusable (FAIR) principles help organizations to share research data in a secure, appropriate, and useful way for other researchers.",
                "year": 2022,
                "publisher": "JMIR Medical Informatics"
            }
        }
    },
    "Conceptual model for online auditing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923610001387/pdfft?md5=f7563791b9fd40b45baeccc525aaf2d0&pid=1-s2.0-S0167923610001387-main.pdf",
                "title": "Conceptual model for online auditing",
                "abstract": "The independent verification of the right applications of business rules in an information system is a task for auditors. The increasing complexity of information systems, and the high risks associated with violations of business rules, have created the need for Online Auditing Tools. In this paper we sketch a conceptual design for such a tool. The components of the tool are described briefly. The focus is on the database and the conformance checker, which are described in detail. The approach is illustrated with an example and some preliminary case studies from industry.",
                "year": 2011,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "A computational model relating changes in cerebral blood volume to synaptic activity in neurons": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231206003614/pdfft?md5=63e27fc2c8c32cd3cfe2ade28cc136c2&pid=1-s2.0-S0925231206003614-main.pdf",
                "title": "A computational model relating changes in cerebral blood volume to synaptic activity in neurons",
                "abstract": "Brain imaging methods, and in particular fMRI (functional Magnetic Resonance Imaging), do not detect neural activity directly, but rather changes in blood flow and oxygenation in neighboring blood vessels, this being the BOLD (Blood Oxygenation Level Dependent) effect. We have constructed a model of the steps leading from neural activity to increased blood flow in arterioles, in which astrocytes play a crucial role. Glutamate released from neuronal synapses binds to metabotropic receptors on the astrocyte processes that ensheath these synapses. This initiates a calcium wave that travels along the endfeet of astrocytes that abut the endothelial cells forming the walls of blood capillaries; this calcium wave is propagated by the extracellular diffusion of ATP (adenosine triphosphate) that acts on metabotropic purinergic receptors on the astrocytes. A further second messenger (taken to be nitric oxide) relays this signal to the smooth muscle cells forming the outer walls of arterioles, and the subsequent wave of hyperpolarization reduces calcium influx and allows relaxation of the muscle cells and hence increased blood flow. The model gives results that are in agreement with experimental measurements of blood volume changes in the arterioles in the visual cortex of optically stimulated cats.",
                "year": 2007,
                "publisher": "Neurocomputing"
            }
        }
    },
    "CROPGRO-soybean model – Validation and application for the southern Amazon, Brazil": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169923008669/pdfft?md5=a8df7fe3ede45561fc4f3372674323e9&pid=1-s2.0-S0168169923008669-main.pdf",
                "title": "CROPGRO-soybean model – Validation and application for the southern Amazon, Brazil",
                "abstract": "Agricultural models are useful tools for predicting phenology, biomass, and crop yield under different environmental conditions during the growth period. Notwithstanding their use in the Southern Cone of Rondônia (southern Amazon, Brazil) is still limited. Thus, the objective in this work was to calibrate and validate the CROPGRO-Soybean model with soybean data collected in harvests of 2017 to 2019, being the data from the first growing period devoted to model calibration, and crop data from 2018/2019 to model validation. Six soybean cultivars were planted at the municipality of Vilhena, State of Rondônia, Brazil. The model was parameterized using climatic, hydric and physical soil properties, and growth data of the evaluated soybean cultivars. To evaluate the accuracy of model prediction we used the coefficient of determination (r2), percentage difference (Pd), root mean square error (RMSE), and Willmott’s index of agreement (d-value). The model accurately simulates the vegetative and reproductive stages, and the yields of the soybean cultivars under the climatic conditions of the Southern Cone of Rondônia. However, the CROPGRO-Soybean model tended to overestimate soil moisture content especially in upper soil layer (0.0–0.1 m depth). Taking the observed data, as the baseline, the model predicted the time to flowering, first pod, seeding, and physiological maturity, with a discrepancy of just one to seven days. In most of the soybean cultivars (five of the six cultivar evaluated), the differences between observed and simulated yields (growing cycle of 2018/2019 – validation harvest) ranged from –6 % to (underestimation) to 27 % (overestimation).",
                "year": 2024,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "New advances in chemistry and materials science with CPMD and parallel computing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167819100000144/pdfft?md5=27319a2317c9a8611b29e39f3e227715&pid=1-s2.0-S0167819100000144-main.pdf",
                "title": "New advances in chemistry and materials science with CPMD and parallel computing",
                "abstract": "A short overview is presented of the density functional theory and molecular dynamics (DFT–MD) method and of a code (CPMD) based on a plane wave scheme. Its power is shown through the survey of specific applications to diverse frontier areas of chemistry and materials science that make use of parallel computing.",
                "year": 2000,
                "publisher": "Parallel Computing"
            }
        }
    },
    "The role of the champion in DSS implementation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0378720683900071/pdfft?md5=8c92ba7c377940fd69ce95efc85fe59e&pid=1-s2.0-0378720683900071-main.pdf",
                "title": "The role of the champion in DSS implementation",
                "abstract": "Adoption of a DSS usually involves changes or innovation in an organization's way of doing things. The process of innovation has been the subject of study in other contexts, and research has indicated that the efforts of various champions can be critical to successful innovation. Observations of DSS implementations in several organizations suggest that a system champion can likewise be a key factor in implementation success. Such persons play a variety of roles, including that of reference leader, of missionary or change agent, or of top management surrogate. Implementation strategies can be designed which capitalize on the presence of a champion.",
                "year": 1983,
                "publisher": "Information & Management"
            }
        }
    },
    "Taxonomy-based prompt engineering to generate synthetic drug-related patient portal messages": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001709/pdfft?md5=8df268d696149d13b9670db987fe4a98&pid=1-s2.0-S1532046424001709-main.pdf",
                "title": "Taxonomy-based prompt engineering to generate synthetic drug-related patient portal messages",
                "abstract": "Objective:\n\nThe objectives of this study were to: (1) create a corpus of synthetic drug-related patient portal messages to address the current lack of publicly available datasets for model development, (2) assess differences in language used and linguistics among the synthetic patient portal messages, and (3) assess the accuracy of patient-reported drug side effects for different racial groups.",
                "year": 2024,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "Komodo Mlipir Algorithm": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494621009637/pdfft?md5=d55538c4b69bf4f5901aedbac0342763&pid=1-s2.0-S1568494621009637-main.pdf",
                "title": "Komodo Mlipir Algorithm",
                "abstract": "This paper proposes Komodo Mlipir Algorithm (KMA) as a new metaheuristic optimizer. It is inspired by two phenomena: the behavior of Komodo dragons living in the East Nusa Tenggara, Indonesia, and the Javanese gait named mlipir. Adopted the foraging and reproduction of Komodo dragons, the population of a few Komodo individuals (candidate solutions) in KMA are split into three groups based on their qualities: big males, female, and small males. First, the high-quality big males do a novel movement called high-exploitation low-exploration to produce better solutions. Next, the middle-quality female generates a better solution by either mating the highest-quality big male (exploitation) or doing parthenogenesis (exploration). Finally, the low-quality small males diversify candidate solutions using a novel movement called mlipir (a Javanese term defined as a walk on the side of the road to reach a particular destination safely), which is implemented by following the big males in a part of their dimensions. A self-adaptation of the population is also proposed to control the exploitation–exploration balance. An examination using the well-documented twenty-three benchmark functions shows that KMA outperforms the recent metaheuristic algorithms. Besides, it provides high scalability to optimize thousand-dimensional functions. The source code of KMA is publicly available at: https://suyanto.staff.telkomuniversity.ac.id/komodo-mlipir-algorithm and https://www.mathworks.com/matlabcentral/fileexchange/102514-komodo-mlipir-algorithm.",
                "year": 2022,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "Crop season planning tool: Adjusting sowing decisions to reduce the risk of extreme weather events": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0168169918304666/pdfft?md5=4123e7c2e8f2477df0b232a20fd6d4ef&pid=1-s2.0-S0168169918304666-main.pdf",
                "title": "Crop season planning tool: Adjusting sowing decisions to reduce the risk of extreme weather events",
                "abstract": "Frost, extreme high temperatures, dry spells, and other weather extremes influence crop development, often causing major variations in yield from one season to the next. Selected growth stages such as flowering and grain filling are particularly sensitive to temperature and/or precipitation extremes and farmers are often unable to manage this risk. Crop simulation models have matured into powerful tools for identifying strategies in fully utilizing scarce natural resources under variable climate conditions. However, the use of crop models often requires expert knowledge and an extensive number of input datasets. Combining the power of Internet, computing technology, and web-based decision-support tools is becoming increasingly popular in situations where these requirements cannot be met. The objective of this study is to develop a simple decision-support tool that combines freely available information with the functionality of complex crop models in a user friendly interface for real-time assessment of soil, plant, and weather information. The resulting tool is intended to be used ahead of the growing season by producers, and it enables the estimation and minimization of the likelihood of extreme weather events during critical stages of crop development.",
                "year": 2019,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "Breath emulator for simulation and modelling of expired tidal breath carbon dioxide characteristics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S016926072031659X/pdfft?md5=700f0237c5ed49e281ab33e95dd2c842&pid=1-s2.0-S016926072031659X-main.pdf",
                "title": "Breath emulator for simulation and modelling of expired tidal breath carbon dioxide characteristics",
                "abstract": "Background\n\nIn this work we describe a breath emulator system, used to simulate temporal characteristics of exhaled carbon dioxide (CO2) concentration waveform versus time simulating how much CO2 is present at each phase of the human lung respiratory process. The system provides a method for testing capnometers incorporating fast response non-dispersive infrared (NDIR) CO2 gas sensing devices - in a clinical setting, capnography devices assess ventilation which is the CO2 movement in and out of the lungs. A mathematical model describing the waveform of the expired CO2 characteristic and influence of CO2 gas sensor noise factors and speed of response is presented and compared with measured and emulated data.",
                "year": 2021,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "A nonlocal method for modeling interfaces: Numerical simulation of decohesion and sliding at grain boundaries": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045782520300177/pdfft?md5=2a74d0490b93bed6dae46a960c0c7536&pid=1-s2.0-S0045782520300177-main.pdf",
                "title": "A nonlocal method for modeling interfaces: Numerical simulation of decohesion and sliding at grain boundaries",
                "abstract": "Understanding and modeling the interface behavior is an important task for predicting materials response in various applications. To formulate the behavior of an arbitrary interface, one needs to construct the relation between acting tractions and displacement jumps at the interface. In addition to capturing the correct physics of the interface, the so-called traction–separation relation must also be thermodynamically consistent and satisfy the basic balance laws. Apart from many attempts in the literature to address these issues, a new and simple method to capture the complex mechanical behavior at an arbitrary interface is proposed. The new formulation is based on introducing a new quantity called “traction density”. As a result, the traction–separation relation for any arbitrary interface is automatically computed by integrating the traction density over the interface. The traction density can be formulated based on understandings and observations from lower scales. As will be shown, the mathematical representation of the traction density is relatively simple and therefore its consistency can be verified easily. When it comes to the grain boundary (GB) behavior, the proposed methodology is able to represent not only intergranular fracture but also grain boundary sliding. For calibration and verification of the model, molecular dynamics (MD) simulations for aluminum Σ5 GB are utilized. Interestingly, the calculations from current MD simulations show size-dependent behavior for the GB. By introducing a healing parameter in the new interface model, it is now possible to explain and predict possible GB size-dependent behavior.",
                "year": 2020,
                "publisher": "Computer Methods in Applied Mechanics and Engineering"
            }
        }
    },
    "Recursive partitioning techniques for modeling irrigation behavior": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815213001321/pdfft?md5=3e3500cfad44f4c3ff2e6a3393dc5634&pid=1-s2.0-S1364815213001321-main.pdf",
                "title": "Recursive partitioning techniques for modeling irrigation behavior",
                "abstract": "Accurate forecasts of short-term irrigation demands can provide information useful for canal operators to manage water diversions and deliveries more efficiently. This can be accomplished by analyzing the actions of the farmers who make water use decisions. Readily available data on biophysical conditions in farmers' fields and the irrigation delivery system during the growing season can be utilized to anticipate irrigation water orders in the absence of any predictive socio-economic information that could be used to provide clues into future irrigation decisions. Decision classification and the common factors, form a basis for division of farmers into groups, which can be then used to make predictions of future decisions to irrigate. In this paper, we have implemented three tree algorithms, viz. classification and regression trees (CART), random forest (RF), and conditional inference trees (Ctree), to analyze farmers' irrigation decisions. These tools were then used to forecast future decisions. During the training process, the models inferred connections between input variables and the decision output. These variables were a time series of the biophysical conditions during the days prior to irrigation. Data from the Canal B region of the Lower Sevier River Basin, near the town of Delta, Utah were used. The main crops in the region are alfalfa, barley and corn. While irrigation practices for alfalfa are dependent on the timing of cuts, for barley and corn the critical crop growth stages are often used as indicators of farmer decisions to irrigate. Though all the models performed well in forecasting farmer decisions to irrigate, the best prediction accuracies by crop type were: 99.3% for alfalfa using all the three models; 98.7% for barley, using the CART model; and 97.6% for corn, with Ctree approximately. Crop water use, which is the amount of water lost through evapotranspiration, was the prime factor across all the crops to prompt irrigation, which complies with the irrigation principles. The analyses showed that the tree algorithms used here are able to handle large as well as small data sets, they can achieve high classification accuracy, and they offer potential tools to forecast future farmer actions. This can be subsequently useful in making short-term demand forecasts.",
                "year": 2013,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "ERT: Data placement based on estimated response time for P2P storage systems": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0743731524001862/pdfft?md5=aa7ec4afdfe3599de0a9e5f0b13d6372&pid=1-s2.0-S0743731524001862-main.pdf",
                "title": "ERT: Data placement based on estimated response time for P2P storage systems",
                "abstract": "Storage system using P2P architecture is an alternative to traditional client-server system that offers better scalability and fault tolerance while eliminating the single point of failure. P2P storage systems with heterogeneous nodes face performance and scalability challenges, one of which is contributed by their data placement method. Existing data placement methods, such as the implementation of the Kademlia protocol, store data at the closest node, where the distance is measured by bitwise XOR operation between identifiers of data and the node. This approach is highly scalable because it does not require global knowledge for placing and retrieving the data. It does not however consider the heterogeneous performance of the nodes, which can result in imbalanced resource usage. Other works implement criteria-based node selection that addresses the heterogeneity of nodes. However, such approaches often cause subsequent data retrieval to require global knowledge of the data location. This paper introduces a novel data placement method called Estimated Response Time-based (ERT), which stores data to selected nodes based on dynamic estimation of nodes response time. ERT models the data placement process using queueing theory. Besides, it incorporates the predicted response time, alongside other criteria, for node selection decision-making. The experimental results in a real P2P storage system indicate that ERT reduces the standard deviation of response time of nodes by 17.57% compared to the native Kademlia, 39.01% compared Kademlia with Virtual Node, and by 49.24% compared to Throughput-based data placement. Further, relying on a modified DHT data structure of Kademlia allows ERT to eliminate the need for global knowledge during data retrieval.",
                "year": 2025,
                "publisher": "Journal of Parallel and Distributed Computing"
            }
        }
    },
    "Exploring the relationship between organizational culture and software process improvement deployment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0378720610000509/pdfft?md5=1ffdc80a22a17859fc32d55b86355e8e&pid=1-s2.0-S0378720610000509-main.pdf",
                "title": "Exploring the relationship between organizational culture and software process improvement deployment",
                "abstract": "We explored the relationship between organizational culture and deployment of software process improvement (SPI) approaches using a competing values framework. Our results indicated that the organizational culture had an influence on SPI deployment, primarily made possible by a hierarchic culture with its emphasis on procedures, order, and stability. Clan culture, with its emphasis on human development, commitment to others, and participation, appears to be a necessary condition in creating skills development and sharing SPI knowledge in the process of its deployment. Software Engineering Program Group leaders should ensure that internal values are in place to enhance SPI deployment.",
                "year": 2010,
                "publisher": "Information & Management"
            }
        }
    },
    "Weakly-supervised convolutional neural networks for multimodal image registration": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841518301051/pdfft?md5=89679e4a2cf8fd941704c4ab51a7728f&pid=1-s2.0-S1361841518301051-main.pdf",
                "title": "Weakly-supervised convolutional neural networks for multimodal image registration",
                "abstract": "One of the fundamental challenges in supervised learning for multimodal image registration is the lack of ground-truth for voxel-level spatial correspondence. This work describes a method to infer voxel-level transformation from higher-level correspondence information contained in anatomical labels. We argue that such labels are more reliable and practical to obtain for reference sets of image pairs than voxel-level correspondence. Typical anatomical labels of interest may include solid organs, vessels, ducts, structure boundaries and other subject-specific ad hoc landmarks. The proposed end-to-end convolutional neural network approach aims to predict displacement fields to align multiple labelled corresponding structures for individual image pairs during the training, while only unlabelled image pairs are used as the network input for inference. We highlight the versatility of the proposed strategy, for training, utilising diverse types of anatomical labels, which need not to be identifiable over all training image pairs. At inference, the resulting 3D deformable image registration algorithm runs in real-time and is fully-automated without requiring any anatomical labels or initialisation. Several network architecture variants are compared for registering T2-weighted magnetic resonance images and 3D transrectal ultrasound images from prostate cancer patients. A median target registration error of 3.6 mm on landmark centroids and a median Dice of 0.87 on prostate glands are achieved from cross-validation experiments, in which 108 pairs of multimodal images from 76 patients were tested with high-quality anatomical labels.",
                "year": 2018,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Prosocial video game as an intimate partner violence prevention tool among youth: A randomised controlled trial": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563218306149/pdfft?md5=439371ae04236a1de16009723001a78f&pid=1-s2.0-S0747563218306149-main.pdf",
                "title": "Prosocial video game as an intimate partner violence prevention tool among youth: A randomised controlled trial",
                "abstract": "Evidence demonstrates that exposure to prosocial video games can increase players' prosocial behaviour, prosocial thoughts, and empathic responses. Prosocial gaming has also been used to reduce gender-based violence among young people, but the use of video games to this end as well as evaluations of their effectiveness are rare. The objective of this study was to assess the effectiveness of a context-specific, prosocial video game, Jesse, in increasing affective and cognitive responsiveness (empathy) towards victims of intimate partner violence (IPV) among children and adolescents (N = 172, age range 9–17 years, M = 12.27, SD = 2.26). A randomised controlled trial was conducted in seven schools in Barbados. Participants were randomly assigned to an experimental (prosocial video game) or control (standard school curriculum) condition. Experimental and control group enrolled 86 participants each. Girls and boys in the experimental condition, but not their counterparts in the control condition, recorded a significant increase in affective responsiveness after intervention. This change was sustained one week after game exposure. No significant effects were recorded for cognitive responsiveness. Findings suggest that Jesse is a promising new IPV prevention tool among girls and boys, which can be used in educational settings.",
                "year": 2019,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Improving aggregated channel performance through decentralized channel monitoring": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S138912860500246X/pdfft?md5=d4966e1f4d8bdb2f0f0d858c72d42d02&pid=1-s2.0-S138912860500246X-main.pdf",
                "title": "Improving aggregated channel performance through decentralized channel monitoring",
                "abstract": "Aggregating low-speed WAN links into a higher-speed logical link promises to improve data-transfer rates to collaborating communities of wireless mobile multihomed devices. Such bandwidth aggregation systems must adapt to link dynamics as the number of links and the channel conditions vary with time due to mobility, power dissipation, and channel interference. A monitoring architecture that accurately measures the link dynamics and promptly feeds this information to the system is vital to realize significant bandwidth aggregation performance gains. In this paper we present various architectural design alternatives for such a monitoring system, and evaluate them using both analysis and simulation. We show that a properly-designed monitoring system can accurately measure and quickly respond to changes in communication link performance while minimizing the control overhead.",
                "year": 2006,
                "publisher": "Computer Networks"
            }
        }
    },
    "A neuro evolutionary algorithm for patient calibrated prediction of survival in Glioblastoma patients": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S153204642100023X/pdfft?md5=9c38bb71ab8cef76c7de90e3731abfcd&pid=1-s2.0-S153204642100023X-main.pdf",
                "title": "A neuro evolutionary algorithm for patient calibrated prediction of survival in Glioblastoma patients",
                "abstract": "Background and objectives\n\nGlioblastoma multiforme (GBM) is the most common and malignant type of primary brain tumors. Radiation therapy (RT) plus concomitant and adjuvant Temozolomide (TMZ) constitute standard treatment of GBM. Existing models for GBM growth do not consider the effect of different schedules on tumor growth and patient survival. However, clinical trials show that treatment schedule and drug dosage significantly affect patient survival. The goal is to provide a patient calibrated model for predicting survival according to the treatment schedule.",
                "year": 2021,
                "publisher": "Journal of Biomedical Informatics"
            }
        }
    },
    "How good is good? Bayesian machine-learning estimation of probabilistic benchmarks in noisy datasets and an application to nanofinance+": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2772941922000023/pdfft?md5=fe456e80ebe699354c1b27514711e387&pid=1-s2.0-S2772941922000023-main.pdf",
                "title": "How good is good? Bayesian machine-learning estimation of probabilistic benchmarks in noisy datasets and an application to nanofinance+",
                "abstract": "Benchmarks are reference standards that are calculated using key performance indicators (KPIs). Calculating benchmarks in noisy datasets is challenging because noise in KPIs reduces the accuracy and precision of benchmarks. The purpose of this research is to propose a two-step methodology for calculating probabilistic benchmarks in noisy datasets. The research methods are based on swarm optimization and Bayesian machine-learning. Swarm optimization simulates the behavior of birds and applies a novel double-hyperbolic undersampling algorithm to denoise KPIs. Bayesian machine-learning estimates probabilistic benchmarks with denoised KPIs through relevance vector machines. The practical implementation of the methods is illustrated with an application to a database of nanofinance+. The results indicate that the proposed methodology is able to denoise KPIs, estimate probabilistic benchmarks, and properly identify the continuous and discrete factors influencing the accuracy and precision of benchmarks.",
                "year": 2022,
                "publisher": "Systems and Soft Computing"
            }
        }
    },
    "Busting up Monopoly: Methods for modern darknet marketplace forensics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2666281723001166/pdfft?md5=4c1e8ac3e50049ed7b28311b39d4b179&pid=1-s2.0-S2666281723001166-main.pdf",
                "title": "Busting up Monopoly: Methods for modern darknet marketplace forensics",
                "abstract": "Darknet marketplaces represent the most delinquent evolution step in distributing illicit goods such as drugs, steroids, firearms, warez, or leaked personal information. On the one hand, law enforcement agencies try to catch vendors, buyers, and operators of darknet marketplaces. On the other hand, the criminals mentioned above constantly stretch the limits of overlay networks, applied cryptography, and cryptocurrency pseudonymity. This paper intends to provide relevant and up-to-date (for the year 2022) information about potential ways to deal with darknet marketplaces from the perspective of investigators. The paper outlines methods (based on periodic web scraping) that may help sworn officers to gather evidence about darknet marketplace (ab)users. The potential is demonstrated in a real-life case study of the Monopoly Market. For instance, suggested approaches seem capable: monitoring the demography and activities of darknet marketplace users, estimating the number of procurements and their value, and correlating user identities with their cryptocurrency addresses. The paper also provides an applicability analysis of proposed methods on the subset of currently trending darknet marketplaces.",
                "year": 2023,
                "publisher": "Forensic Science International: Digital Investigation"
            }
        }
    },
    "Graph neural networks for representing multivariate resource usage: A multiplayer mobile gaming case-study": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2667096823000058/pdfft?md5=67a50c6c11f53c75683a5d61ca117283&pid=1-s2.0-S2667096823000058-main.pdf",
                "title": "Graph neural networks for representing multivariate resource usage: A multiplayer mobile gaming case-study",
                "abstract": "The emergence of Multiplayer Mobile Gaming (MMG) applications is intertwined with a plethora of Quality of Service and Quality of Experience requirements. Resource usage prediction can provide valuable insights into the corresponding orchestration and management process in the form of several proactive functionalities in resource scaling, service migration, task offloading and scheduling. These processes are crucial in the Cloud and Edge environments exploited by MMG applications. Thus, producing accurate resource usage predictions concerning these types of applications is of paramount importance. To that end, we propose a resource usage representation paradigm based on Graph Neural Networks (GNNs). The novelty of this approach is based on the process of leveraging the dependencies that exist among the various types of computational resources. Furthermore, we expand upon this representation approach to develop a GNN-based Encoder-Decoder model that caters to the complexities of resource usage and can provide multi-step resource usage predictions. This model is compared against numerous well-established Encoder-Decoder and Deep Learning prediction models to assess its efficiency. Finally, the proposed model is incorporated in a proactive Horizontal Autoscaling solution that manages to outperform a standard reactive Horizontal Autoscaling approach in the context of a large-scale simulation, in terms of various performance metrics, while keeping the volume of the required computational resources to a minimum. The findings of this work showcase the importance of developing novel approaches in order to represent resource usage and the numerous benefits in the context of application performance and resource consumption that may derive from such scientific endeavors.",
                "year": 2023,
                "publisher": "International Journal of Information Management Data Insights"
            }
        }
    },
    "Retweeting #WorldEnvironmentDay: A study of content features and visual rhetoric in an environmental movement": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0747563216308238/pdfft?md5=4ec75af1ace5fbacd608f4b1c0e28cbd&pid=1-s2.0-S0747563216308238-main.pdf",
                "title": "Retweeting #WorldEnvironmentDay: A study of content features and visual rhetoric in an environmental movement",
                "abstract": "Much work has been done on the role and impacts of social media in social movements, but less attention has been paid to the use of content features such as hashtags and URLs, and visual rhetoric used in persuading and mobilizing in social movements. We address this gap by analyzing a sample of 1271 tweets during World Environment Day, a yearly and global movement to commemorate and promote awareness about the environment. We examine the impacts of content features and the use of three types of visual rhetoric (ethos, pathos and logos) on retweeting, as retweeting is a measure of retransmission and expressive participation in the movement. The use of URLs was positively associated with retweeting, but the relationship is reverse for hashtags and retweeting. Ethos was the dominant persuasion strategy present in the visual rhetoric, and the use of both Ethos and Pathos were significantly associated with retweeting.",
                "year": 2017,
                "publisher": "Computers in Human Behavior"
            }
        }
    },
    "Flexible fault tolerance in cloud through replicated cooperative resource group": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Flexible fault tolerance in cloud through replicated cooperative resource group",
                "abstract": "Fault tolerance (FT) is a highly focused issue in cloud due to high reliability requirements especially for delay sensitive tasks. Numerous FT frameworks in cloud have been proposed in literature. By and large, proactive and reactive FT approaches are followed in the existing frameworks. Irrespective of any of the applied approaches, FT implementation remains service provider centric in the existing frameworks. Cloud users are not given any control over deciding the level of FT. However, cloud follows the service orientation model with greater flexibility in using the computing services on demand basis and in pay-per-use manner. The user’s control in deciding the level of FT while executing their tasks will further reinforce the model of service orientation in cloud. Therefore, this paper proposes a flexible fault tolerance framework (FFTF) in cloud. FFTF provision the users to categorize their tasks as premium/standard/economy to implement the corresponding level of fault tolerance (FT). To implement a FT level, user task is executed on a cooperative resource group in cloud. Flexibility of FFTF is endorsed by maintaining the performance parameters of deadline guarantee ratio and average task delay for different task categories. The performance of FFTF is analyzed and compared through extensive simulation experiments on artificial and real workload in term of its FT capability, resource consumption and utilization with an existing FT framework.",
                "year": 2019,
                "publisher": "Computer Communications"
            }
        }
    },
    "The design and evaluation of accessibility on web navigation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167923606001138/pdfft?md5=5e4f0855da7f0b26e56f4fe03f1d844a&pid=1-s2.0-S0167923606001138-main.pdf",
                "title": "The design and evaluation of accessibility on web navigation",
                "abstract": "Web sites have been deployed to create and sustain business competitiveness in a trend of emerging Web technologies and growing e-commerce. One critical success factor of e-commerce is the ability to allow information to be retrieved from a Web site in an efficient and effective manner. Such ability, being determined by both the Web site structure and the Web page organization, can be measured in terms of accessibility and popularity of Web pages. The relationship between accessibility and popularity of web pages is dynamic in nature and can be analyzed to enhance a Web design. Having observed the lack of means to measure information retrieval of a Web site, this paper purports to introduce a guideline to evaluate Web page accessibility based on several structural-based accessibility models where an innovative accessibility–popularity (A–P) analysis is deployed to measure and, thereby, to modify a Web structure. Both push (i.e. demand driven) strategies and pull (i.e. design driven) strategies are incorporated into such guideline. Further, accessibility models are analyzed and compared in order to identify appropriate applications for each model. The paper is concluded by a summary of future directions of the accessibility models.",
                "year": 2007,
                "publisher": "Decision Support Systems"
            }
        }
    },
    "Delegate the smartphone user? Security awareness in smartphone platforms": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404812001733/pdfft?md5=7f62c30d4f0f95e3472b455920399522&pid=1-s2.0-S0167404812001733-main.pdf",
                "title": "Delegate the smartphone user? Security awareness in smartphone platforms",
                "abstract": "Smartphone users increasingly download and install third-party applications from official application repositories. Attackers may use this centralized application delivery architecture as a security and privacy attack vector. This risk increases since application vetting mechanisms are often not in place and the user is delegated to authorize which functionality and protected resources are accessible by third-party applications. In this paper, we mount a survey to explore the security awareness of smartphone users who download applications from official application repositories (e.g. Google Play, Apple's App Store, etc.). The survey findings suggest a security complacency, as the majority of users trust the app repository, security controls are not enabled or not added, and users disregard security during application selection and installation. As a response to this security complacency we built a prediction model to identify users who trust the app repository. The model is assessed, evaluated and proved to be statistically significant and efficient.",
                "year": 2013,
                "publisher": "Computers & Security"
            }
        }
    },
    "Stochastic Simulation Methods Applied to a Secure Electronic Voting Model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1571066106003562/pdfft?md5=9b6ed10fafc8d67829ebc387363dacbb&pid=1-s2.0-S1571066106003562-main.pdf",
                "title": "Stochastic Simulation Methods Applied to a Secure Electronic Voting Model",
                "abstract": "We demonstrate a novel simulation technique for analysing large stochastic process algebra models, applying this to a secure electronic voting system example. By approximating the discrete state space of a PEPA model by a continuous equivalent, we can draw on rate equation simulation techniques from both chemical and biological modelling to avoid having to directly enumerate the huge state spaces involved. We use stochastic simulation techniques to provide traces of course-of-values time series representing the number of components in a particular state. Using such a technique we can get simulation results for models exceeding 1010000 states within only a few seconds.",
                "year": 2006,
                "publisher": "Electronic Notes in Theoretical Computer Science"
            }
        }
    },
    "A decentralized approach for mining event correlations in distributed system monitoring": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0743731512002225/pdfft?md5=70efb00b99b0df07193f7871de89d0ba&pid=1-s2.0-S0743731512002225-main.pdf",
                "title": "A decentralized approach for mining event correlations in distributed system monitoring",
                "abstract": "Nowadays, there is an increasing demand to monitor, analyze, and control large scale distributed systems. Events detected during monitoring are temporally correlated, which is helpful to resource allocation, job scheduling, and failure prediction. To discover the correlations among detected events, many existing approaches concentrate detected events into an event database and perform data mining on it. We argue that these approaches are not scalable to large scale distributed systems as monitored events grow so fast that event correlation discovering can hardly be done with the power of a single computer. In this paper, we present a decentralized approach to efficiently detect events, filter irrelative events, and discover their temporal correlations. We propose a MapReduce-based algorithm, MapReduce-Apriori, to data mining event association rules, which utilizes the computational resource of multiple dedicated nodes of the system. Experimental results show that our decentralized event correlation mining algorithm achieves nearly ideal speedup compared to centralized mining approaches.",
                "year": 2013,
                "publisher": "Journal of Parallel and Distributed Computing"
            }
        }
    },
    "Smoothed particle hydrodynamics for root growth mechanics": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0955799718305563/pdfft?md5=b571fd4bdb41ae61783c6f6428e40f2f&pid=1-s2.0-S0955799718305563-main.pdf",
                "title": "Smoothed particle hydrodynamics for root growth mechanics",
                "abstract": "A major challenge of plant developmental biology is to understand how cells grow during the formation of an organ. To date, it has proved difficult to develop computational models of entire organs at cellular resolution and, as a result, the testing of hypotheses on the biophysics of self-organisation is currently limited.\n\nHere, we formulate a model for plant tissue growth in an Smoothed Particles Hydrodynamics (SPH) framework. The framework identifies the SPH particle with individual cells in a tissue, but the tissue growth is performed at the macroscopic level using SPH approximations. Plant tissue is represented as an anisotropic poro-elastic material where turgor pressure deforms the cell walls and biosynthesis and cell division control the density of the tissue.\n\nThe performance of the model is evaluated through a series of tests and benchmarks. Results demonstrate good stability and convergence of simulations as well as readiness of the technique for more complex biological problems.",
                "year": 2019,
                "publisher": "Engineering Analysis with Boundary Elements"
            }
        }
    },
    "Assessing Apps for Patients with Genitourinary Tumors Using the Mobile Application Rating Scale (MARS): Systematic Search in App Stores and Content Analysis": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Assessing Apps for Patients with Genitourinary Tumors Using the Mobile Application Rating Scale (MARS): Systematic Search in App Stores and Content Analysis",
                "abstract": "Background\n\nThe large number of available cancer apps and their impact on the population necessitates a transparent, objective, and comprehensive evaluation by app experts, health care professionals, and users. To date, there have been no analyses or classifications of apps for patients with genitourinary cancers, which are among the most prevalent types of cancer.",
                "year": 2020,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "An improved particle swarm optimization algorithm for unit commitment": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S014206150600055X/pdfft?md5=1dc73a58a8123c559433b86563af5fcc&pid=1-s2.0-S014206150600055X-main.pdf",
                "title": "An improved particle swarm optimization algorithm for unit commitment",
                "abstract": "This paper presents an improved particle swarm optimization algorithm (IPSO) for power system unit commitment. IPSO is an extension of the standard particle swarm optimization algorithm (PSO) which uses more particles’ information to control the mutation operation, and is similar to the social society in that a group of leaders could make better decisions. The convergence property of the proposed IPSO method is analyzed using standard results from the dynamic system theory and some guidelines are derived for proper algorithm parameter selection. A new adaptive strategy for choosing parameters is also proposed to assure convergence of IPSO method, and the proposed algorithm adopts the orthogonal design to generate initial population that are scattered uniformly over feasible solution space. Furthermore, this method combines relaxation technique to zero-one variable and penalty function method to transform the problem to a nonlinear continuous variable optimization one by taking into account more constraints. The feasibility of the proposed method is demonstrated from 10 to 100 unit systems, and the test results are compared with those obtained by Evolutionary Programming (EP) and Genetic Algorithm (GA) in terms of solution quality and convergence properties. The simulation results show that the proposed method is capable of obtaining higher quality solutions.",
                "year": 2006,
                "publisher": "International Journal of Electrical Power & Energy Systems"
            }
        }
    },
    "Time-critical underwater sensor diffusion with no proactive exchanges and negligible reactive floods": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S157087050700025X/pdfft?md5=179d5a65288751aa560f122320190a18&pid=1-s2.0-S157087050700025X-main.pdf",
                "title": "Time-critical underwater sensor diffusion with no proactive exchanges and negligible reactive floods",
                "abstract": "In this paper we study multi-hop ad hoc routing in a scalable underwater sensor network (UWSN), which is a novel network paradigm for ad hoc investigation of the world below the water surface. Unlike existing underwater acoustic networks (UAN), the new UWSN paradigm dispatches large number (in the thousands) of unmanned low-cost sensor nodes to locally monitor and report otherwise not easily accessible underwater events in a time-critical manner. Due to the large propagation latency and very low bandwidth of the acoustic channel, a new protocol stack and corresponding models are required as conventional approaches fail. In particular, we show that neither proactive routing message exchange nor reactive/on-demand flooding is adequate in the challenging new underwater environment. Unlike the terrestrial scenarios, on-demand flooding cannot be both reliable and efficient due to widespread collisions caused by the large propagation delay. On the other hand, as in terrestrial scenarios, proactive routing is more expensive and less efficient than on-demand routing in typical underwater environments. We propose a “conservative” communications architecture that minimizes the number of all packet transmissions to avoid possible acoustic collisions. This is implemented in the non-intrusive underwater diffusion (UWD), which is a multi-hop ad hoc routing and in-network processing protocol with no proactive routing message exchange and negligible amount of on-demand floods. To achieve its design goal, UWD does not rely on GPS or power hungry motors to control currents. Instead, UWD is designed in a minimalist’s framework, which assumes homogeneous GPS-free nodes and random node mobility. Our simulation study verifies the effectiveness and efficiency of our design.",
                "year": 2007,
                "publisher": "Ad Hoc Networks"
            }
        }
    },
    "Myocardial deformation recovery from cine MRI using a nearly incompressible biventricular model": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1361841507001090/pdfft?md5=b4df3a3d4be224c804a9043c12278a07&pid=1-s2.0-S1361841507001090-main.pdf",
                "title": "Myocardial deformation recovery from cine MRI using a nearly incompressible biventricular model",
                "abstract": "This paper presents a method for biventricular myocardial deformation recovery from cine MRI. The method is based on a deformable model that is nearly incompressible, a desirable property since the myocardium has been shown to be nearly incompressible. The model uses a matrix-valued radial basis function to represent divergence-free displacement fields, which is a first order approximation of incompressibility. This representation allows for deformation modeling of an arbitrary topologies with a relatively small number of parameters, which is suitable for representing the motion of the multi-chamber structure of the heart. The myocardium needs to be segmented in an initial frame after which the method automatically determines the tissue deformation everywhere in the myocardium throughout the cardiac cycle.\n\nTwo studies were carried out to validate the method. In the first study the myocardial deformation was recovered from a 3D anatomical cine MRI sequence of a healthy volunteer and then validated against the manual segmentation of the biventricular wall and against the corresponding 3D tagged cine MRI sequence. The average volume agreement between the model and the manual segmentation had a false positive rate of 3.2%, false negative rate of 2.8% and true positive rate of 91.4%. The average distance between the model and manually determined intersections of perpendicular tag planes was 1.7 mm (1.2 pixel). The same procedures was repeated on another set of 3D anatomical and tagged MRI scans of the same volunteer taken four months later. The recovered deformation was very similar to the one obtained from the first set of scans. In the second study the method was applied to 3D anatomical cine MRI scans of three patients with ventricular dyssynchrony and three age-matched healthy volunteers. The recovered strains of the normal subjects were clearly stronger than the recovered strains of the patients and they were similar to those reported by other researchers. The recovered deformation of all six subjects was validated against manual segmentation of the biventricular wall and against corresponding tagged MRI scans. The agreement was similar to that of the first study.",
                "year": 2008,
                "publisher": "Medical Image Analysis"
            }
        }
    },
    "Selective disassembly planning for waste electrical and electronic equipment with case studies on liquid crystaldisplays": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584513000070/pdfft?md5=1454e32d7eb2950acb683c308fcea308&pid=1-s2.0-S0736584513000070-main.pdf",
                "title": "Selective disassembly planning for waste electrical and electronic equipment with case studies on liquid crystaldisplays",
                "abstract": "Waste Electrical and Electronic Equipments (WEEEs) are one of the most significant waste streams in modern societies. In the past decade, disassembly of WEEE to support remanufacturing and recycling has been growingly adopted by industries. With the increasing customisation and diversity of Electrical and Electronic Equipment (EEE) and more complex assembly processes, full disassembly of WEEE is rarely an ideal solution due to high disassembly cost. Selective disassembly, which prioritises operations for partial disassembly according to the legislative and economic considerations of specific stakeholders, is becoming an important but still a challenging research topic in recent years. In order to address the issue effectively, in this paper, a Particle Swarm Optimisation (PSO)-based selective disassembly planning method embedded with customisable decision making models and a novel generic constraint handling algorithm has been developed. With multi-criteria and adaptive decision making models, the developed method is flexible to handle WEEE to meet the various requirements of stakeholders. Based on the generic constraint handling and intelligent optimisation algorithms, the developed research is capable to process complex constraints and achieve optimised selective disassembly plans. Industrial cases on Liquid Crystal Display (LCD) televisions have been used to verify and demonstrate the effectiveness and robustness of the research in different application scenarios.",
                "year": 2013,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Productivity growth: The take-off point": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0306457396850102/pdfft?md5=ca74a80a91d57d75c600f61fa1fb53e3&pid=1-s2.0-S0306457396850102-main.pdf",
                "title": "Productivity growth: The take-off point",
                "abstract": "Two major and apparently contradictory themes have emerged and taken center stage in the management literature in the last several years—the “productivity paradox” and “business process reengineering.” These apparently contradictory themes are in fact logically related to each other, and the theme of this piece is that we are on the cusp of emerging from the former to the latter.",
                "year": 1996,
                "publisher": "Information Processing & Management"
            }
        }
    },
    "Automated TIMI frame counting using 3-d modeling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0895611112001280/pdfft?md5=f9b562eb56f296c7d3f1af4beb2de937&pid=1-s2.0-S0895611112001280-main.pdf",
                "title": "Automated TIMI frame counting using 3-d modeling",
                "abstract": "Three dimensional coronary modeling and reconstruction can assist in the quantitative analysis of coronary flow velocity from 2-d coronary images. In this paper a novel method to assess coronary flow velocity is proposed. First, 3-d models of the coronary arteries are estimated from bi-plane X-ray images using epipolar constraint energy minimization for the selected fiducial points like bifurcations, and subsequently 3-d B-spline energy minimization for the arterial segments. A 4-d model is assembled from a set of 3-d models representing different phases of the cardiac cycle. The 4-d model is fitted to the 2-d image sequences containing basal or hyperemic blood flow information. Then, by counting the frames in analogy with TIMI frame counting, an index of the mean coronary flow velocity can be estimated. Our experimental results show that the algorithm correlates with r = 0.98 (P<0.0001, 95% CI 0.92–0.99) to the clinical measurements of the TFC.",
                "year": 2012,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "Mobile App Design, Development, and Publication for Adverse Drug Reaction Assessments of Causality, Severity, and Preventability": {
        "accordingTo": {
            "scienceDirect": {
                "title": "Mobile App Design, Development, and Publication for Adverse Drug Reaction Assessments of Causality, Severity, and Preventability",
                "abstract": "Background\n\nAdverse drug reactions (ADRs) cause significant morbidity and mortality. Improved assessment of ADRs to identify the causal relationship, the severity, and the preventability will aid ADRs prevention or reduce patient burden.",
                "year": 2017,
                "publisher": "JMIR mHealth and uHealth"
            }
        }
    },
    "DiscPOP: Power-aware buffer management for disk accesses": {
        "accordingTo": {
            "scienceDirect": {
                "title": "DiscPOP: Power-aware buffer management for disk accesses",
                "abstract": "Much research has been conducted on energy efficient cache buffer management for disk based storage systems. Some of them use greedy prefetching technique to artificially increase disk idle intervals if there is a large number of known future requests. However, this might result in sub-optimal solution by not exploiting the relationship between I/O access pattern (sequential/random) and application pattern (cpu time required for computing). In a cpu-bound application, by explicitly taking into account the required CPU time it may reduce energy conservation by up to 50% and increase power cycle number by 100% compared to an existing efficient prefetching scheme without this consideration. In this paper, we consider the tradeoff between disk power consumption, performance guarantee and disk reliability all together by proposing a Disk characteristic based Power-Optimal Prefetching (DiscPOP) scheme. Specifically, we make three contributions: (i) A theoretical model is proposed to analyze energy-efficient cache buffer management in disk I/O system and it was formulated as an optimization problem. We have shown it can be solved via an Integer Linear Programming (ILP) technique, we further conduct the following research. (ii) For offline cases, we proposed a Greedy Partition algorithm (GP) to divide the problem into several small ones and solve them separately via the proposed ILP algorithm. (iii) For online cases, we proposed two heuristic algorithms based on Lazy Start Power-Optimal Prefetching (LSPOP) technique. Both of them use simple threshold controlled algorithms to select a prefetching start judiciously and cautiously. We use a trace-driven simulation to evaluate our proposed schemes. The results show GP outperforms the traditional aggressive prefetching by up to 26.9% more disk energy conservation and 17.8% power cycle reduction. The online heuristic algorithms can also improve disk energy saving by up to 20.5% and reduce power cycle by 14.3%.",
                "year": 2013,
                "publisher": "Sustainable Computing: Informatics and Systems"
            }
        }
    },
    "Accounting for institutional quality in global forest modeling": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815216310647/pdfft?md5=157b1faf204876714da995f7709c8113&pid=1-s2.0-S1364815216310647-main.pdf",
                "title": "Accounting for institutional quality in global forest modeling",
                "abstract": "The current state of the art in modeling forest cover change is to combine a detailed representation of biophysical processes with economic decision-making principles. Yet, there is an increasing consensus that the quality of political institutions is another relevant component in determining forest cover change patterns.\n\nIn this paper, the Global Forest Model is used to analyze whether including an index, measuring the capacity of political institutions to guarantee sustainable natural resource management, allows to improve the precision of the modeled forest cover trend. The analysis shows that incorporating the index indeed allows reducing the gap between the estimated and observed forest cover trends for the 2000 to 2010 calibration period.",
                "year": 2018,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "Formal verification of context and situation models in pervasive computing": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1574119212000417/pdfft?md5=1f4616b2777f2dca8bb20302bace7cbe&pid=1-s2.0-S1574119212000417-main.pdf",
                "title": "Formal verification of context and situation models in pervasive computing",
                "abstract": "Pervasive computing is a paradigm that focuses on the availability of computer resources anytime anywhere for any application and supports non-intrusive integration of computing services into everyday life. Context awareness is the core feature of pervasive computing. High-level context awareness can be enhanced by situation awareness that represents the ability to detect and reason about the real-life situations. In this article we propose, analyze and validate the formal verification method for situation definitions and demonstrate its feasibility and efficiency. Situations are often defined manually by domain experts and are, therefore, susceptible to definition inconsistencies and possible errors, which in turn can cause situation reasoning problems. The proposed method takes as an input properties of situations and dependencies among them as well as situation definitions in terms of low-level context features, and then either formally proves that the definitions do comply with the expected properties, or provides a complete set of counterexamples — context parameters that prove situation inconsistency. Evaluation and complexity analysis of the proposed approach are also presented and discussed. Examples and evaluation results demonstrate that the proposed approach can be used to verify real-life situation definitions, and detect non-obvious errors in situation specifications.",
                "year": 2013,
                "publisher": "Pervasive and Mobile Computing"
            }
        }
    },
    "Coeliac disease under a microscope: Histological diagnostic features and confounding factors": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482518303196/pdfft?md5=90654b08ba75f76c68daed67d7e61c86&pid=1-s2.0-S0010482518303196-main.pdf",
                "title": "Coeliac disease under a microscope: Histological diagnostic features and confounding factors",
                "abstract": "Coeliac disease (CD) and gluten-related disorders represent an important cornerstone of the daily practice of gastroenterologists, endoscopists and dedicated histopathologists. Despite the knowledge of clinical, serological and histological typical lesions, there are some conditions to consider for differential diagnosis. From the first description of histology of CD, several studies were conducted to define similar findings suggestive for microscopic enteritis. Considering the establishment of early precursor lesions, the imbalance of gut microbiota is another point still requiring a detailed definition. This review assesses the importance of a right overview in case of suspected gluten-related disorders and the several conditions mimicking a similar histology.",
                "year": 2019,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "An ontology-based policy for deploying secure SIP-based VoIP services": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167404808000424/pdfft?md5=083c422fb6e7bcb939c74df7142dfcc7&pid=1-s2.0-S0167404808000424-main.pdf",
                "title": "An ontology-based policy for deploying secure SIP-based VoIP services",
                "abstract": "Voice services over Internet Protocol (VoIP) are nowadays much promoted by telecommunication and Internet service providers. However, the utilization of open networks, like the Internet, raises several security issues that must be accounted for. On top of that, there are new sophisticated attacks against VoIP infrastructures that capitalize on vulnerabilities of the protocols employed for the establishment of a VoIP session (for example the Session Initiation Protocol – SIP).\n\nThis paper provides a categorization of potential attacks against VoIP services, followed by specific security recommendations and guidelines for protecting the underlying infrastructure from these attacks and thus ensuring the provision of robust and secure services. In order to utilize (share) the aforementioned security guidelines and recommendations into different domains, it is necessary to have them represented in some formal way. To this end, ontologies have been used for representing the proposed guidelines and recommendations in the form of a unified security policy for VoIP infrastructures. This ontology-based policy has been then transformed to a First Order Logic (FOL) formal representation.\n\nThe proposed ontology-based security policy can be applied in a real VoIP environment for detecting attacks against an SIP-based service, but it can be also utilized for security testing purposes and vulnerabilities identification.\n\nThe work presented in this paper has been focused to the SIP protocol. However, generalization to other signaling protocols is possible.",
                "year": 2008,
                "publisher": "Computers & Security"
            }
        }
    },
    "Meta-heuristic algorithms for optimized network flow wavelet-based image coding": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1568494613002901/pdfft?md5=95ce2a3c29a22131a24a7d9b5a02c5f8&pid=1-s2.0-S1568494613002901-main.pdf",
                "title": "Meta-heuristic algorithms for optimized network flow wavelet-based image coding",
                "abstract": "Optimal multipath selection to maximize the received multiple description coding (MDCs) in a lossy network model is proposed. Multiple description scalar quantization (MDSQ) has been applied to the wavelet coefficients of a color image to generate the MDCs which are combating transmission loss over lossy networks. In the networks, each received description raises the reconstruction quality of an MDC-coded signal (image, audio or video). In terms of maximizing the received descriptions, a greater number of optimal routings between source and destination must be obtained. The rainbow network flow (RNF) collaborated with effective meta-heuristic algorithms is a good approach to resolve it. Two meta-heuristic algorithms which are genetic algorithm (GA) and particle swarm optimization (PSO) have been utilized to solve the multi-objective optimization routing problem for finding optimal routings each of which is assigned as a distinct color by RNF to maximize the coded descriptions in a network model. By employing a local search based priority encoding method, each individual in GA and particle in PSO is represented as a potential solution. The proposed algorithms are compared with the multipath Dijkstra algorithm (MDA) for both finding optimal paths and providing reliable multimedia communication. The simulations run over various random network topologies and the results show that the PSO algorithm finds optimal routings effectively and maximizes the received MDCs with assistance of RNF, leading to reduce packet loss and increase throughput.",
                "year": 2014,
                "publisher": "Applied Soft Computing"
            }
        }
    },
    "An additive manufacturing process selection approach based on fuzzy Archimedean weighted power Bonferroni aggregation operators": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0736584518305945/pdfft?md5=733049a70491bf9a08b86d245e10934f&pid=1-s2.0-S0736584518305945-main.pdf",
                "title": "An additive manufacturing process selection approach based on fuzzy Archimedean weighted power Bonferroni aggregation operators",
                "abstract": "Selecting an appropriate additive manufacturing (AM) process or machine to fabricate an end-use product is an important issue in design for AM. One of many types of approaches for AM process selection is based on multi-criteria decision making (MCDM). Most of the MCDM based approaches have an advantage in taking into account the relative importance of performance parameter types and a few of them also consider the interrelationships of performance parameter types. Each of these approaches can work well in its specific context. They are however not entirely satisfactory, as they do not have the capabilities to reduce the influence of the deviation of performance parameter values on the decision-making result and to capture the risk attitudes of users in their decision-making models. In this paper, an MCDM approach based on fuzzy Archimedean weighted power Bonferroni aggregation operators with such capabilities is proposed for AM process selection. A fuzzy Archimedean weighted power Bonferroni mean operator and a fuzzy Archimedean weighted power geometric Bonferroni mean operator are firstly constructed. Based on these operators, an MCDM approach for selection of AM processes are then developed. After that, four practical examples are adopted to illustrate the developed approach and a set of sensitivity analysis experiments on the basis of these examples are carried out. Finally, qualitative and quantitative comparisons between the approach and the existing MCDM based approaches are reported to demonstrate its feasibility, effectiveness, and advantages.",
                "year": 2020,
                "publisher": "Robotics and Computer-Integrated Manufacturing"
            }
        }
    },
    "Non-rigid retinal image registration using an unsupervised structure-driven regression network": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0925231220307682/pdfft?md5=b23ee4bca77db49fa064c460915dae79&pid=1-s2.0-S0925231220307682-main.pdf",
                "title": "Non-rigid retinal image registration using an unsupervised structure-driven regression network",
                "abstract": "Retinal image registration is clinically significant to help clinicians obtain more complete details of the retinal structure by correlating the properties of the retina. However, existing methods suffer from great challenges due to time-consuming optimization and lack of ground truth. In this paper, we propose an unsupervised learning framework for non-rigid retinal image registration, which directly learns the mapping from a retinal image pair to their corresponding deformation field without any supervision such as ground truth registration fields. Specifically, we formulate the complex mapping as a parameterized deformation function, which can be represented and optimized by a deep neural network. Furthermore, the Structure-Driven Regression Network (SDRN) framework is applied to compute the multi-scale similarity combined with contextual structures (e.g., vessel distribution, optic disk appearance, and edge information) to guide the end-to-end learning procedure more effectively with unlabeled data. Given a new pair of images, our method can quickly register images by directly evaluating the parametric function using the learned parameters, which runs faster than traditional registration algorithms. Experimental results, performed on the public challenging dataset (FIRE), show that our method achieves an average Dice similarity coefficient (DSC) of 0.753 with short execution times (0.021 s), which is more accurate and robust than existing approaches and promises to significantly speed up retinal image analysis and processing.",
                "year": 2020,
                "publisher": "Neurocomputing"
            }
        }
    },
    "Modelling the influence of returns for an omni-channel retailer": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0377221722006646/pdfft?md5=ff2f3712fc339446ba46279151d03929&pid=1-s2.0-S0377221722006646-main.pdf",
                "title": "Modelling the influence of returns for an omni-channel retailer",
                "abstract": "More brick-and-mortar retailers open an online channel to increase sales. Often, they use the store to fulfil online orders and to receive returned products. The uncertain product returns however complicate the replenishment decision of a retailer. The inventory also has to be rationed over the offline and online sales channels. We therefore integrate the rationing and ordering decisions of an omni-channel retailer in a Markov Decision Process (MDP) that maximises the retailer’s profit. Contrary to previous studies, we explicitly model multi-period sales-dependent returns, which is more realistic and leads to higher profit and service levels. With Value Iteration (VI) an exact solution can only be computed for relatively small-scale instances. For solving large-scale instances, we constructed a Deep Reinforcement Learning (DRL) algorithm. The different methods are compared in an extensive numerical study of small-scale instances to gain insights. The results show that the running time of VI increases exponentially in the problem size, while the running time of DRL is high but scales well. DRL has a low optimality gap but the performance drops when there is a higher level of uncertainty or if the profit trade-off between different actions is minimal. Our approach of modelling multi-period sales-dependent product returns outperforms other methods. Furthermore, based on large-scale instances, we find that increasing online returns lowers the profit and the service level in the offline channel. However, longer return windows do not influence the retailer’s profit.",
                "year": 2023,
                "publisher": "European Journal of Operational Research"
            }
        }
    },
    "Evaluating the performance of a computer-based consultant": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0010468X79900229/pdfft?md5=31a37d65704a6f726044068fdbd88d0d&pid=1-s2.0-0010468X79900229-main.pdf",
                "title": "Evaluating the performance of a computer-based consultant",
                "abstract": "The performance of a computer-based clinical consultation system is evaluated. The program, called MYCIN, is designed to function as an aid for infectious disease diagnosis and therapy selection, with an initial emphasis on bacteremias. The evaluation methodology is discussed, as well as the difficulties encountered in attempting to evaluate clinical judgments. Specialists in infectious diseases judged MYCIN's final therapy recommendation, and intermediate conclusions about the significance of the infection and identity of infecting organisms. The evaluation techniques described may be useful in assessing the performance of other clinical decision aids. Results of the evaluation show that the program's therapy recommendations meet Stanford experts' standards of acceptable practice 90.9% of the time (table 2), with some variation noted both among individual experts and between Stanford experts and others (tables 1, 2).",
                "year": 1979,
                "publisher": "Computer Programs in Biomedicine"
            }
        }
    },
    "On a constitutive driver as a useful tool in soil plasticity": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0965997899000071/pdfft?md5=5f80c99706c6a20312a64ff3bef5e28b&pid=1-s2.0-S0965997899000071-main.pdf",
                "title": "On a constitutive driver as a useful tool in soil plasticity",
                "abstract": "A mathematical basis for the development of Constitutive Drivers in soil plasticity has recently been proposed by the authors. A Constitutive Driver is here understood as a computer program, containing a number of selected constitutive models, in which different laboratory and field tests can be simulated and model parameters optimised. As a pilot study of the mathematical concept, a Constitutive Driver for soils, in the form of a PC-program, has been developed. The paper discusses this particular program, i.e. its structure, the mathematical basis, included soil models and some application examples, to give an idea of how a general and user-friendly Constitutive Driver can be designed. Such a program can be used for practical, research and educational purposes. In fact, it is believed that so many important applications for Constitutive Drivers exist that it would be beneficial if such programs were easily accessible as complementary programs in commercial software.",
                "year": 1999,
                "publisher": "Advances in Engineering Software"
            }
        }
    },
    "Revisiting sequential composition in process calculi": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2352220815000796/pdfft?md5=08b6de700df027df230553a677f6ae55&pid=1-s2.0-S2352220815000796-main.pdf",
                "title": "Revisiting sequential composition in process calculi",
                "abstract": "The article reviews the various ways sequential composition is defined in traditional process calculi, and shows that such definitions are not optimal, thus limiting the dissemination of concurrency theory ideas among computer scientists. An alternative approach is proposed, based on a symmetric binary operator and write-many variables. This approach, which generalizes traditional process calculi, has been used to define the new LNT language implemented in the CADP toolbox. Feedback gained from university lectures and real-life case studies shows a high acceptance by computer-science students and industry engineers.",
                "year": 2015,
                "publisher": "Journal of Logical and Algebraic Methods in Programming"
            }
        }
    },
    "Modelling the role of self-weight consolidation on the morphodynamics of accretional mudflats": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1364815215300931/pdfft?md5=aab207d8d30db0e610528ecdc3c65965&pid=1-s2.0-S1364815215300931-main.pdf",
                "title": "Modelling the role of self-weight consolidation on the morphodynamics of accretional mudflats",
                "abstract": "We develop a consolidation module and merge it into a morphodynamic model to assess the role of consolidation on estuarine morphodynamics. We test the model using two settings: point models without hydrodynamic forcing to validate against two benchmark experimental datasets; and a profile model to simulate a mudflat restoration. The modelled self-weight consolidation influences the simulations by gradually reducing the bed level and decreasing the bed erodibility (i.e., increasing the critical bed shear stress). Both effects modify sediment transport processes on mudflats, leading to long-term morphodynamic effects. Depending on the initial bathymetry, the hydrodynamic forcing and the soil properties, the simulated morphological change of the restored mudflat may differ considerably with and without considering consolidation. The consolidation model developed can be utilised to assess the medium to long term effects related to estuarine development (e.g., wetland restoration) and aims to be a publicly available tool.",
                "year": 2016,
                "publisher": "Environmental Modelling & Software"
            }
        }
    },
    "A two-scale micromechanical model for aluminium foam based on results from nanoindentation": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0045794913002174/pdfft?md5=7a2d6e603182a0cbf199355fccd7160f&pid=1-s2.0-S0045794913002174-main.pdf",
                "title": "A two-scale micromechanical model for aluminium foam based on results from nanoindentation",
                "abstract": "The main aim of this paper is to develop and verify simple but effective model for elastic properties of a porous aluminium foam system and to compare results received from experimental micromechanics with solutions given by analytical or more advanced numerical methods. The material is characterized by a closed pore system with very thin but microscopically inhomogeneous pore walls (∼0.1 mm) and large air pores (∼2.9 mm). Therefore, two material levels can be distinguished. The lower level of the proposed model contains inhomogeneous solid matter of the foam cell walls produced from an aluminium melted with admixtures. Elastic parameters as well as volume fractions of microstructural material phases at this level are assessed with nanoindentation and effective properties computed via analytical and numerical homogenization schemes. The effective Young’s modulus of the cell walls was found close to 70 GPa irrespective to the used homogenization procedure.\n\nThe higher model scale contains homogenized cell walls and a significant volume fraction of air voids (91.4%). Since analytical schemes fail to predict effective properties of this highly porous structure, numerical homogenization based on a simple two dimensional finite element model is utilized. The model geometry is based on foam optical images from which an equivalent beam structure is produced using Voronoi tessellation. Effective foam Young’s modulus was found to be 1.36–1.38 GPa which is in relation with ∼1.45 GPa obtained from uniaxial compression experiments.",
                "year": 2013,
                "publisher": "Computers & Structures"
            }
        }
    },
    "ATOMMIC: An Advanced Toolbox for Multitask Medical Imaging Consistency to facilitate Artificial Intelligence applications from acquisition to analysis in Magnetic Resonance Imaging": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0169260724003705/pdfft?md5=cbdf31e815c85c2c4da5babf503b1cb8&pid=1-s2.0-S0169260724003705-main.pdf",
                "title": "ATOMMIC: An Advanced Toolbox for Multitask Medical Imaging Consistency to facilitate Artificial Intelligence applications from acquisition to analysis in Magnetic Resonance Imaging",
                "abstract": "Background and Objectives:\n\nArtificial intelligence (AI) is revolutionizing Magnetic Resonance Imaging (MRI) along the acquisition and processing chain. Advanced AI frameworks have been applied in various successive tasks, such as image reconstruction, quantitative parameter map estimation, and image segmentation. However, existing frameworks are often designed to perform tasks independently of each other or are focused on specific models or single datasets, limiting generalization. This work introduces the Advanced Toolbox for Multitask Medical Imaging Consistency (ATOMMIC), a novel open-source toolbox that streamlines AI applications for accelerated MRI reconstruction and analysis. ATOMMIC implements several tasks using deep learning (DL) models and enables MultiTask Learning (MTL) to perform related tasks in an integrated manner, targeting generalization in the MRI domain.",
                "year": 2024,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Research advances in nano-composite solders": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0026271408003818/pdfft?md5=0cf79a86584641449319828c9948d152&pid=1-s2.0-S0026271408003818-main.pdf",
                "title": "Research advances in nano-composite solders",
                "abstract": "Recently, nano-composite solders have been developed in the electronic packaging materials industry to improve the creep and thermo-mechanical fatigue resistance of solder joints to be used in service at high temperatures and under thermo-mechanical fatigue conditions. This paper reviews the driving force for the development of nano-composite solders in the electronic packaging industry and the research advances of the composite solders developed. The rationale for the preparation of nano-composite solders are presented at first. Examples of two nano-composite solder fabrication methods, a mechanical mixing method and an in-situ method, are explained in detail. The achievements and enhancements in the nano-composite prepared solders are summarized. The difficulties and problems existing in the fabrication of nano-composite solders are discussed. Finally, a novel nano-structure composite solder, which attempts to solve the problems encountered in the fabrication of nano-composite solders, is introduced in detail. Guidelines for the development of nano-composite solders are then provided.",
                "year": 2009,
                "publisher": "Microelectronics Reliability"
            }
        }
    },
    "A program which determines mutagenic concentrations of chemical carcinogens via a diffusion bioassay": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/0169260787900757/pdfft?md5=f8f31169c0e9b4451ae6d421e949c8a1&pid=1-s2.0-0169260787900757-main.pdf",
                "title": "A program which determines mutagenic concentrations of chemical carcinogens via a diffusion bioassay",
                "abstract": "A computer program implementing a mathematical model for determining mutagenic concentrations of chemical carcinogens was developed. The mathematical model describes the experiment in which a droplet of a suspected carcinogen is put at the center of a petri dish containing a bacterial lawn in an agar gel. After a period of incubation during which the chemical diffuses outward, one observes a concentric ring of mutants around the center. The largest radius at which mutation occurs, rmut, corresponds to the lowest (threshold) concentration of the chemical sufficient to produce bacterial mutation. Given a series of initial concentrations of a chemical and the resulting rmuts, the program computes and reports the threshold concentration and the decay time of the chemical. The program is also used as a method to determine the lowest mutagenic concentration for a particular time of exposure.",
                "year": 1987,
                "publisher": "Computer Methods and Programs in Biomedicine"
            }
        }
    },
    "Performance evaluation of an application-level checkpointing solution on grids": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0167739X10000804/pdfft?md5=c263c129c3a17841f85fc29ea4b59a62&pid=1-s2.0-S0167739X10000804-main.pdf",
                "title": "Performance evaluation of an application-level checkpointing solution on grids",
                "abstract": "In recent years there has been a significant effort to develop middleware that facilitates the execution of applications on Grid infrastructures. However, support for fault-tolerant execution continues to be scarce. The CPPC-G framework is a service-based architecture designed to provide efficient fault-tolerant mechanisms for the execution of sequential and parallel applications on grids. Applications to be managed by CPPC-G are expected to be preprocessed with CPPC (ComPiler for Portable Checkpointing), a tool for automatically inserting portable checkpoint instrumentation into the code of parallel applications. Built on top of existing Globus services, CPPC-G services are in charge of submitting and monitoring CPPC applications, managing generated checkpoint files, detecting failures and automatically restarting failed executions. In this paper the feasibility of this approach is assessed by measuring the performance of CPPC-G, quantitatively addressing its impact on application performance. Results show that the increase in overall throughput and availability comes with minor performance degradation.",
                "year": 2010,
                "publisher": "Future Generation Computer Systems"
            }
        }
    },
    "Preventing male infertility by marjoram and sage essential oils through modulating testicular lipid accumulation and androgens biosynthesis disruption in a rat model of dietary obesity": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S2314808X15000378/pdfft?md5=ca937955fc782f8c2e8dd4a709a4bb73&pid=1-s2.0-S2314808X15000378-main.pdf",
                "title": "Preventing male infertility by marjoram and sage essential oils through modulating testicular lipid accumulation and androgens biosynthesis disruption in a rat model of dietary obesity",
                "abstract": "Obesity has been recognized as a leading cause for male infertility. This study aimed to investigate reproductive disorders caused by obesity and the possible prevention through the use of marjoram and sage oil extracts. Obesity was achieved in adult male rats by feeding high fat diet (HFD) for 12 weeks, while marjoram (0.16 ml/kg b.wt) and sage (0.05 ml/kg b.wt) oils were given orally for the same duration. HFD-fed rats exhibited marked obesity features indicated by increased adiposity index, with higher weight gain compared to control rats. This goes with increased lipid accumulation in testis and serum of the obese rats. Increased serum levels of leptin, prolactinL (PRL) and estrogen (E2), with reduced serum androgens; dehydroepiandrosterone (DHEA), testosterone (T) and T/E2 ratio were also observed. Additionally, the results showed significant reduction in epididymal sperm count, as well as in steriodogenic enzymes; 3β-hydroxysteroid dehydrogenase (3β-HSD), alkaline phosphatase (ALP), and acid phosphatase (ACP), with marked elevation in aromatase activity in testis of the obese rats. Histopathological alterations, including degenerative changes in seminiferous tubules, with sloughing, vacuolization and reduction of spermatogenic cells were also detected. Oral administration of marjoram or sage oil extracts, along with HFD seemed to prevent overall mentioned alterations, as evident by reduced testicular lipid accumulation, elevated androgens and sperm count, in addition to improved testicular structure. Results thus suggested that both oils should be considered in future therapeutic approaches for controlling adverse impact of obesity on male fertility.",
                "year": 2015,
                "publisher": "Egyptian Journal of Basic and Applied Sciences"
            }
        }
    },
    "Whole farm economic evaluation of East Coast fever immunization strategies on farms in the Uasin Gishu District of Kenya": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/016816999400035O/pdfft?md5=38df12ac9034c2f78311187c2e23ea88&pid=1-s2.0-016816999400035O-main.pdf",
                "title": "Whole farm economic evaluation of East Coast fever immunization strategies on farms in the Uasin Gishu District of Kenya",
                "abstract": "East Coast feve (ECF) is a major constraint limiting livestock production in Eastern, Central, and Southern Africa. The infection and treatment method (ITM) has proven effective in research station and field trials. The work described in this paper was to quantify the economic payoffs to case study farms that adopt alternative ITM strategies and current ECF control methods. A whole farm simulation model was used to estimate the financial and economic payoffs to ITM and to estimate the probability distribution of returns for alternative ECF control strategies. Stochastic dominance was used to rank the alternative ECF controls for risk averse decision makers. The whole farm simulation model used for the analysis has the advantage of incorporating risk aspects involved in farm production, and ranking alternative strategies, as well as estimating the farmer's premium for risk preference associated with alternative ECF control strategies.",
                "year": 1995,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "3D surface-based registration of ultrasound and histology in prostate cancer imaging": {
        "accordingTo": {
            "scienceDirect": {
                "title": "3D surface-based registration of ultrasound and histology in prostate cancer imaging",
                "abstract": "Several transrectal ultrasound (TRUS)-based techniques aiming at accurate localization of prostate cancer are emerging to improve diagnostics or to assist with focal therapy. However, precise validation prior to introduction into clinical practice is required. Histopathology after radical prostatectomy provides an excellent ground truth, but needs accurate registration with imaging. In this work, a 3D, surface-based, elastic registration method was developed to fuse TRUS images with histopathologic results. To maximize the applicability in clinical practice, no auxiliary sensors or dedicated hardware were used for the registration. The mean registration errors, measured in vitro and in vivo, were 1.5 ± 0.2 and 2.1 ± 0.5 mm, respectively.",
                "year": 2016,
                "publisher": "Computerized Medical Imaging and Graphics"
            }
        }
    },
    "HeporCloud: An energy and performance efficient resource orchestrator for hybrid heterogeneous cloud computing environments": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1084804520303349/pdfft?md5=373a9b36df4f399406d6799de2712d9c&pid=1-s2.0-S1084804520303349-main.pdf",
                "title": "HeporCloud: An energy and performance efficient resource orchestrator for hybrid heterogeneous cloud computing environments",
                "abstract": "In major Information Technology (IT) companies such as Google, Rackspace and Amazon Web Services (AWS), virtualisation and containerisation technologies are usually used to execute customers' workloads and applications. The computational resources are provided through large-scale datacenters, which consume substantial amount of energy and have, therefore, ecological impacts. Since long, Google runs users' applications in containers, Rackspace offers bare-metal hardware, whereas AWS runs them either in VMs (EC2), containers (ECS) and/or containers inside VMs (Lambda); therefore, making resource management a tedious activity. The role of a resource management system is of the greatest importance, principally, if IT companies practice various kinds of sand-boxing technologies, for instance, bare-metal, VMs, containers, and/or nested containers in their datacenters (hybrid platforms). The absence of centralised, workload-aware resource managers and consolidation policies produces questions on datacenters energy efficiency, workloads performance, and users' costs. In this paper, we demonstrate, through several experiments, using the Google workload data for 12,583 hosts and approximately one million tasks that belong to four different kinds of workload, the likelihood of: (i) using workload-aware resource managers in hybrid clouds; (ii) achieving energy and cost savings, in heterogeneous hybrid datacenters such that the workload performance is not affected, negatively; and (iii) how various allocation policies, combined with different migration approaches, will impact on datacenter's energy and performance efficiencies. Using plausible assumptions for hybrid datacenters set-up, our empirical evaluation suggests that, for no migration, a single scheduler is at most 16.86% more energy efficient than distributed schedulers. Moreover, when migrations are considered, our resource manager can save up to 45.61% energy and can improve up to 17.9% workload performance.",
                "year": 2021,
                "publisher": "Journal of Network and Computer Applications"
            }
        }
    },
    "Adaptive congestion control framework and a simple implementation on high bandwidth-delay product networks": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S1389128614000875/pdfft?md5=9e843d9fa2663788378452dc32ffc426&pid=1-s2.0-S1389128614000875-main.pdf",
                "title": "Adaptive congestion control framework and a simple implementation on high bandwidth-delay product networks",
                "abstract": "As new link technologies and sub-networks proliferate and evolve, a large number of TCP variants have been developed for different types of the network environments. They can lead to major performance gains by taking advantage of local characteristics of the specific network. However, these TCP variants could not be automatically chosen according to the lower network environments. In this paper, we propose the ACCF, an adaptive congestion control framework, which can automatically transition among existing congestion control mechanisms according to the change of the network status. Then we perform a simple implementation of ACCF over the networks with high bandwidth-delay product (BDP). It can switch the congestion control approaches between the delay-based ones and the loss-based ones according to the network status. Extensive experiments are conducted based on network simulators as well as over real wired networks on different time periods of the day. For the simulation measures, the experimental results show that the performance of ACCF is significantly improved as compared to other state-of-the-art algorithms in term of throughput, fairness and TCP-friendliness. For the real network tests, the experimental results show that ACCF achieves speedup ratios up to 225.83% compared with average throughput of other TCP congestion control algorithms.",
                "year": 2014,
                "publisher": "Computer Networks"
            }
        }
    },
    "A window-based time series feature extraction method": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/S0010482517302706/pdfft?md5=0a8baed3bf2d98aed73af037b3840d50&pid=1-s2.0-S0010482517302706-main.pdf",
                "title": "A window-based time series feature extraction method",
                "abstract": "This study proposes a robust similarity score-based time series feature extraction method that is termed as Window-based Time series Feature ExtraCtion (WTC). Specifically, WTC generates domain-interpretable results and involves significantly low computational complexity thereby rendering itself useful for densely sampled and populated time series datasets. In this study, WTC is applied to a proprietary action potential (AP) time series dataset on human cardiomyocytes and three precordial leads from a publicly available electrocardiogram (ECG) dataset. This is followed by comparing WTC in terms of predictive accuracy and computational complexity with shapelet transform and fast shapelet transform (which constitutes an accelerated variant of the shapelet transform). The results indicate that WTC achieves a slightly higher classification performance with significantly lower execution time when compared to its shapelet-based alternatives. With respect to its interpretable features, WTC has a potential to enable medical experts to explore definitive common trends in novel datasets.",
                "year": 2017,
                "publisher": "Computers in Biology and Medicine"
            }
        }
    },
    "Modelling movement and mortality: killing tsetse flies in the field": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/016816999500022V/pdfft?md5=4ca549d2b1d18e442a5cefcfea175c75&pid=1-s2.0-016816999500022V-main.pdf",
                "title": "Modelling movement and mortality: killing tsetse flies in the field",
                "abstract": "Developments in the use of odour-baited traps offer exciting prospects for the effective control of tsetse flies and the trypanosome diseases of which they are the vectors. Work carried out at Nguruman in south-western Kenya has led to the development of cheap and simple traps that can be made and serviced by local communities. Various models have been developed that enable us to understand the factors that determine the efficiency of traps which in turn will help us to determine the density of traps needed to achieve a set level of control and to design more effective traps. Four models are discussed here. The first is a model of the population dynamics of tsetse that relates the overall population loss rate to the mortality that we impose on adult flies. The second is a model of movement on a large scale that makes it possible to relate the adult mortality to the movement patterns and population dynamics of the flies, and to the properties of the trap. The third is a more speculative attempt to model the way in which individual flies locate traps once they are close enough to detect the odours. This should eventually make it possible to refine the parameters in the large scale movement models. The last is a model of invasions of flies into a cleared area. The development and testing of these models has relied extensively on the data collected in the field at Nguruman and in turn the models have helped us to interpret the data and to formulate new questions and experiments.",
                "year": 1995,
                "publisher": "Computers and Electronics in Agriculture"
            }
        }
    },
    "A review of the growth and structure of thin films of germanium and silicon": {
        "accordingTo": {
            "scienceDirect": {
                "link": "https://www.sciencedirect.com/science/article/pii/002627146490246X/pdfft?md5=3dae0aebf797a21a4cf904e1067a01c8&pid=1-s2.0-002627146490246X-main.pdf",
                "title": "A review of the growth and structure of thin films of germanium and silicon",
                "abstract": "The growth of films of germanium and silicon by the techniques of (1) vacuum sublimation, (2) iodide disproportionation reactions and (3) the hydrogen reduction of suitable halides are compared and discussed in relation to the electrical properties and the structural perfection of the films. In general, films with semiconductor quality electrical properties have only been produced when the film and substrate are of the same material; exceptions are germanium deposits grown by method (2) on other semiconductor substrates. When foreign substrates such as insulators are used, imperfect epitaxy frequently occurs and difficulties are encountered due to differential contraction between the film and the substrate. It is concluded that the electrical properties of coherent epitaxial films are determined mainly by chemical impurities rather than lattice defects with the possible exception of evaporated germanium films. It is also thought that the presence of impurities leads to the introduction of structural defects into silicon films prepared by all three methods above.",
                "year": 1964,
                "publisher": "Microelectronics Reliability"
            }
        }
    }
}