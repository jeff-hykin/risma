affordance: 68.65418076365394, 281
learning: 52.04035766070566, 213
object: 50.08579023682939, 205
robot: 44.95505074915418, 184
visual: 22.721846302561627, 93
task: 19.545674238762686, 80
based: 18.079748670855487, 74
object affordance: 16.61583081570997, 34
action: 15.63653939101015, 64
optical flow: 14.661027190332327, 30
perception: 12.949009183180282, 53
grasp: 12.949009183180282, 53
interaction: 12.216046399226679, 50
feature: 11.727404543257611, 48
human: 11.727404543257611, 48
predict: 10.994441759304012, 45
deep: 10.505799903334944, 43
train: 10.505799903334944, 43
flow: 10.505799903334944, 43
representation: 9.772837119381343, 40
tool: 9.528516191396811, 39
reinforcement learning: 9.285317220543806, 19
affordance learning: 8.796616314199396, 18
image: 8.79555340744321, 36
network: 8.551232479458676, 35
environment: 8.551232479458676, 35
different: 8.551232479458676, 35
self supervised: 8.307915407854985, 17
deep learning: 8.307915407854985, 17
neural network: 8.307915407854985, 17
state art: 8.307915407854985, 17
optical: 8.306911551474142, 34
learn: 8.306911551474142, 34
learning affordance: 7.819214501510574, 16
grasp affordance: 7.819214501510574, 16
scene: 7.573948767520541, 31
result: 7.573948767520541, 31
supervised: 7.329627839536007, 30
goal: 7.085306911551474, 29
demonstrate: 7.085306911551474, 29
affordance based: 6.8418126888217525, 14
convolutional neural: 6.8418126888217525, 14
self: 6.840985983566941, 28
attention: 6.840985983566941, 28
video: 6.596665055582407, 27
real: 6.596665055582407, 27
agent: 6.352344127597873, 26
neural: 6.108023199613339, 25
recognition: 6.108023199613339, 25
effect: 6.108023199613339, 25
visual feature: 5.864410876132931, 12
real world: 5.864410876132931, 12
reinforcement: 5.863702271628806, 24
learned: 5.863702271628806, 24
given: 5.619381343644273, 23
state: 5.619381343644273, 23
present: 5.619381343644273, 23
behavior: 5.375060415659739, 22
novel: 5.375060415659739, 22
concept: 5.375060415659739, 22
object interaction: 4.887009063444109, 10
generative model: 4.887009063444109, 10
propose: 4.8864185596906715, 20
knowledge: 4.642097631706138, 19
framework: 4.642097631706138, 19
based affordance: 4.398308157099698, 9
experimental result: 4.398308157099698, 9
autonomous: 4.397776703721605, 18
space: 4.397776703721605, 18
3d: 4.153455775737071, 17
control: 4.153455775737071, 17
performance: 4.153455775737071, 17
problem: 4.153455775737071, 17
visual affordance: 3.909607250755287, 8
learning approach: 3.909607250755287, 8
affordance detection: 3.909607250755287, 8
visual attention: 3.909607250755287, 8
affordance robot: 3.909607250755287, 8
affordance model: 3.909607250755287, 8
unsupervised learning: 3.909607250755287, 8
affordance object: 3.909607250755287, 8
detection: 3.9091348477525374, 16
dataset: 3.9091348477525374, 16
system: 3.9091348477525374, 16
algorithm: 3.9091348477525374, 16
convolutional: 3.6648139197680036, 15
interactive: 3.6648139197680036, 15
cognitive: 3.6648139197680036, 15
world: 3.6648139197680036, 15
robot learning: 3.4209063444108763, 7
affordance recognition: 3.4209063444108763, 7
object recognition: 3.4209063444108763, 7
online learning: 3.4209063444108763, 7
learning task: 3.4209063444108763, 7
tool affordance: 3.4209063444108763, 7
humanoid robot: 3.4209063444108763, 7
direct perception: 3.4209063444108763, 7
rgb d: 3.4209063444108763, 7
autonomous robot: 3.4209063444108763, 7
human object: 3.4209063444108763, 7
high level: 3.4209063444108763, 7
robot able: 3.4209063444108763, 7
exploration: 3.4204929917834703, 14
survey: 3.4204929917834703, 14
end: 3.4204929917834703, 14
capture: 3.4204929917834703, 14
probabilistic: 3.4204929917834703, 14
unsupervised: 3.4204929917834703, 14
study: 3.4204929917834703, 14
future: 3.1761720637989366, 13
estimation: 3.1761720637989366, 13
multi: 3.1761720637989366, 13
developmental: 3.1761720637989366, 13
functional: 3.1761720637989366, 13
current: 3.1761720637989366, 13
available: 3.1761720637989366, 13
affordance predict: 2.9322054380664655, 6
end end: 2.9322054380664655, 6
contextual affordance: 2.9322054380664655, 6
flow estimation: 2.9322054380664655, 6
learning object: 2.9322054380664655, 6
supervised learning: 2.9322054380664655, 6
spatio temporal: 2.9322054380664655, 6
flow scene: 2.9322054380664655, 6
scene flow: 2.9322054380664655, 6
learning based: 2.9322054380664655, 6
latent space: 2.9322054380664655, 6
task relevant: 2.9322054380664655, 6
semantic: 2.931851135814403, 12
online: 2.931851135814403, 12
driven: 2.931851135814403, 12
rgb: 2.931851135814403, 12
architecture: 2.931851135814403, 12
hand: 2.931851135814403, 12
vision: 2.931851135814403, 12
generative: 2.931851135814403, 12
achieve: 2.931851135814403, 12
several: 2.931851135814403, 12
recent: 2.6875302078298695, 11
spatial: 2.6875302078298695, 11
policy: 2.6875302078298695, 11
temporal: 2.6875302078298695, 11
latent: 2.6875302078298695, 11
previously: 2.6875302078298695, 11
input: 2.6875302078298695, 11
experimental: 2.6875302078298695, 11
mechanism: 2.6875302078298695, 11
physical: 2.6875302078298695, 11
ecological approach: 2.4435045317220543, 5
visual perception: 2.4435045317220543, 5
based robot: 2.4435045317220543, 5
object based: 2.4435045317220543, 5
learning visual: 2.4435045317220543, 5
affordance segmentation: 2.4435045317220543, 5
deep network: 2.4435045317220543, 5
robot control: 2.4435045317220543, 5
learning robot: 2.4435045317220543, 5
perception affordance: 2.4435045317220543, 5
affordance map: 2.4435045317220543, 5
multiple object: 2.4435045317220543, 5
goal conditioned: 2.4435045317220543, 5
art method: 2.4435045317220543, 5
real robot: 2.4435045317220543, 5
demonstrate effectiveness: 2.4435045317220543, 5
effect action: 2.4435045317220543, 5
concept affordance: 2.4435045317220543, 5
simulated robot: 2.4435045317220543, 5
perception approach: 2.4435045317220543, 5
object parts: 2.4435045317220543, 5
predict effect: 2.4435045317220543, 5
learn affordance: 2.4435045317220543, 5
cues: 2.4432092798453358, 10
segmentation: 2.4432092798453358, 10
continuous: 2.4432092798453358, 10
depth: 2.4432092798453358, 10
understanding: 2.4432092798453358, 10
appearance: 2.4432092798453358, 10
generalize: 2.4432092798453358, 10
challenging: 2.4432092798453358, 10
simulated: 2.4432092798453358, 10
associated: 2.4432092798453358, 10
small: 2.4432092798453358, 10
conditional: 2.1988883518608024, 9
reasoning: 2.1988883518608024, 9
focused: 2.1988883518608024, 9
bayesian: 2.1988883518608024, 9
manipulation: 2.1988883518608024, 9
humanoid: 2.1988883518608024, 9
direct: 2.1988883518608024, 9
d: 2.1988883518608024, 9
general: 2.1988883518608024, 9
possible: 2.1988883518608024, 9
generate: 2.1988883518608024, 9
suitable: 2.1988883518608024, 9
ability: 2.1988883518608024, 9
important: 2.1988883518608024, 9
pixel: 2.1988883518608024, 9
understand: 2.1988883518608024, 9
label: 2.1988883518608024, 9
introduce: 2.1988883518608024, 9
various: 2.1988883518608024, 9
focus: 2.1988883518608024, 9
approach object: 1.9548036253776435, 4
sensorimotor object: 1.9548036253776435, 4
recent advances: 1.9548036253776435, 4
robot affordance: 1.9548036253776435, 4
detecting object: 1.9548036253776435, 4
learning predict: 1.9548036253776435, 4
random field: 1.9548036253776435, 4
interactive reinforcement: 1.9548036253776435, 4
task driven: 1.9548036253776435, 4
goal directed: 1.9548036253776435, 4
depth based: 1.9548036253776435, 4
navigation affordance: 1.9548036253776435, 4
computational model: 1.9548036253776435, 4
affordance rgb: 1.9548036253776435, 4
object detection: 1.9548036253776435, 4
predict human: 1.9548036253776435, 4
object scene: 1.9548036253776435, 4
affordance autonomous: 1.9548036253776435, 4
self exploration: 1.9548036253776435, 4
robot grasp: 1.9548036253776435, 4
control architecture: 1.9548036253776435, 4
learning perception: 1.9548036253776435, 4
markov decision: 1.9548036253776435, 4
current state: 1.9548036253776435, 4
performance predictor: 1.9548036253776435, 4
previously learned: 1.9548036253776435, 4
robot learn: 1.9548036253776435, 4
novel dataset: 1.9548036253776435, 4
pre train: 1.9548036253776435, 4
computer vision: 1.9548036253776435, 4
detection branch: 1.9548036253776435, 4
object appearance: 1.9548036253776435, 4
affordance terms: 1.9548036253776435, 4
3d feature: 1.9548036253776435, 4
paper present: 1.9548036253776435, 4
rl contextual: 1.9548036253776435, 4
human robot: 1.9548036253776435, 4
representation object: 1.9548036253776435, 4
attention mechanism: 1.9548036253776435, 4
action performed: 1.9548036253776435, 4
pixel wise: 1.9548036253776435, 4
learning algorithm: 1.9548036253776435, 4
reaching: 1.9545674238762687, 8
ecological: 1.9545674238762687, 8
sensorimotor: 1.9545674238762687, 8
systems: 1.9545674238762687, 8
driving: 1.9545674238762687, 8
development: 1.9545674238762687, 8
navigation: 1.9545674238762687, 8
computational: 1.9545674238762687, 8
examples: 1.9545674238762687, 8
analysis: 1.9545674238762687, 8
map: 1.9545674238762687, 8
motion: 1.9545674238762687, 8
via: 1.9545674238762687, 8
quality: 1.9545674238762687, 8
datasets: 1.9545674238762687, 8
key: 1.9545674238762687, 8
simple: 1.9545674238762687, 8
learns: 1.9545674238762687, 8
previous: 1.9545674238762687, 8
article: 1.9545674238762687, 8
affordancenet: 1.9545674238762687, 8
speed: 1.9545674238762687, 8
recently: 1.9545674238762687, 8
interact: 1.9545674238762687, 8
efficiently: 1.9545674238762687, 8
low: 1.9545674238762687, 8
build: 1.9545674238762687, 8
known: 1.9545674238762687, 8
vdac: 1.9545674238762687, 8
evaluation: 1.9545674238762687, 8
best: 1.9545674238762687, 8
describe: 1.9545674238762687, 8
controller: 1.9545674238762687, 8
structure: 1.9545674238762687, 8
imagine: 1.7102464958917352, 7
emergent: 1.7102464958917352, 7
language: 1.7102464958917352, 7
contextual: 1.7102464958917352, 7
point: 1.7102464958917352, 7
classification: 1.7102464958917352, 7
review: 1.7102464958917352, 7
basic: 1.7102464958917352, 7
properties: 1.7102464958917352, 7
detect: 1.7102464958917352, 7
towards: 1.7102464958917352, 7
basis: 1.7102464958917352, 7
markov: 1.7102464958917352, 7
develop: 1.7102464958917352, 7
diverse: 1.7102464958917352, 7
evaluate: 1.7102464958917352, 7
desired: 1.7102464958917352, 7
potential: 1.7102464958917352, 7
experiments: 1.7102464958917352, 7
specifically: 1.7102464958917352, 7
aspects: 1.7102464958917352, 7
context: 1.7102464958917352, 7
rl: 1.7102464958917352, 7
computer: 1.7102464958917352, 7
capabilities: 1.7102464958917352, 7
role: 1.7102464958917352, 7
directly: 1.7102464958917352, 7
related: 1.7102464958917352, 7
underlying: 1.7102464958917352, 7
motor: 1.7102464958917352, 7
common: 1.7102464958917352, 7
capable: 1.7102464958917352, 7
goal imagine: 1.4661027190332327, 3
emergent affordance: 1.4661027190332327, 3
tool synthesis: 1.4661027190332327, 3
language model: 1.4661027190332327, 3
learning perspective: 1.4661027190332327, 3
affordance function: 1.4661027190332327, 3
predict feature: 1.4661027190332327, 3
feature affordance: 1.4661027190332327, 3
robot perception: 1.4661027190332327, 3
perception systems: 1.4661027190332327, 3
dense conditional: 1.4661027190332327, 3
conditional random: 1.4661027190332327, 3
learning grasp: 1.4661027190332327, 3
attention control: 1.4661027190332327, 3
deep visuomotor: 1.4661027190332327, 3
visuomotor policy: 1.4661027190332327, 3
task focused: 1.4661027190332327, 3
ontology approach: 1.4661027190332327, 3
learning optical: 1.4661027190332327, 3
point clouds: 1.4661027190332327, 3
relational affordance: 1.4661027190332327, 3
object manipulation: 1.4661027190332327, 3
manipulation task: 1.4661027190332327, 3
haptic cues: 1.4661027190332327, 3
autonomous driving: 1.4661027190332327, 3
model affordance: 1.4661027190332327, 3
train data: 1.4661027190332327, 3
functional scene: 1.4661027190332327, 3
scene understanding: 1.4661027190332327, 3
object learning: 1.4661027190332327, 3
learning recognition: 1.4661027190332327, 3
d video: 1.4661027190332327, 3
human grasp: 1.4661027190332327, 3
robot task: 1.4661027190332327, 3
object tool: 1.4661027190332327, 3
visual grasp: 1.4661027190332327, 3
graphical model: 1.4661027190332327, 3
mobile robot: 1.4661027190332327, 3
cognitive control: 1.4661027190332327, 3
robot agent: 1.4661027190332327, 3
bayesian approach: 1.4661027190332327, 3
artificial agent: 1.4661027190332327, 3
approach based: 1.4661027190332327, 3
functional feature: 1.4661027190332327, 3
intrinsic motivation: 1.4661027190332327, 3
based grasp: 1.4661027190332327, 3
self learning: 1.4661027190332327, 3
approach learning: 1.4661027190332327, 3
input image: 1.4661027190332327, 3
existing research: 1.4661027190332327, 3
image generative: 1.4661027190332327, 3
step towards: 1.4661027190332327, 3
relevant object: 1.4661027190332327, 3
supervised goal: 1.4661027190332327, 3
propose framework: 1.4661027190332327, 3
problem affordance: 1.4661027190332327, 3
outperforms recent: 1.4661027190332327, 3
recent state: 1.4661027190332327, 3
robot demonstrate: 1.4661027190332327, 3
source code: 1.4661027190332327, 3
propose method: 1.4661027190332327, 3
information object: 1.4661027190332327, 3
understand interact: 1.4661027190332327, 3
deep convolutional: 1.4661027190332327, 3
network learn: 1.4661027190332327, 3
robot capabilities: 1.4661027190332327, 3
approach solving: 1.4661027190332327, 3
affordance specifically: 1.4661027190332327, 3
arbitrary visual: 1.4661027190332327, 3
visual cues: 1.4661027190332327, 3
more complex: 1.4661027190332327, 3
robot interaction: 1.4661027190332327, 3
compare three: 1.4661027190332327, 3
learning method: 1.4661027190332327, 3
graph representation: 1.4661027190332327, 3
everyday life: 1.4661027190332327, 3
train model: 1.4661027190332327, 3
model visual: 1.4661027190332327, 3
novel approach: 1.4661027190332327, 3
top down: 1.4661027190332327, 3
attention learned: 1.4661027190332327, 3
able predict: 1.4661027190332327, 3
learn object: 1.4661027190332327, 3
semantic segmentation: 1.4661027190332327, 3
ground truth: 1.4661027190332327, 3
object label: 1.4661027190332327, 3
supervised method: 1.4661027190332327, 3
while being: 1.4661027190332327, 3
flow algorithm: 1.4661027190332327, 3
haptic depth: 1.4661027190332327, 3
robot progressively: 1.4661027190332327, 3
progressively learn: 1.4661027190332327, 3
grasp configuration: 1.4661027190332327, 3
based behavior: 1.4661027190332327, 3
able learn: 1.4661027190332327, 3
continuous pose: 1.4661027190332327, 3
affordance memory: 1.4661027190332327, 3
grasp object: 1.4661027190332327, 3
easy task: 1.4661027190332327, 3
low level: 1.4661027190332327, 3
motivated: 1.4659255679072014, 6
dense: 1.4659255679072014, 6
relations: 1.4659255679072014, 6
flownet: 1.4659255679072014, 6
planning: 1.4659255679072014, 6
exploiting: 1.4659255679072014, 6
haptic: 1.4659255679072014, 6
spatio: 1.4659255679072014, 6
invariant: 1.4659255679072014, 6
toward: 1.4659255679072014, 6
imitation: 1.4659255679072014, 6
selection: 1.4659255679072014, 6
open: 1.4659255679072014, 6
decision: 1.4659255679072014, 6
geometric: 1.4659255679072014, 6
ground: 1.4659255679072014, 6
single: 1.4659255679072014, 6
generalization: 1.4659255679072014, 6
conditioned: 1.4659255679072014, 6
explore: 1.4659255679072014, 6
variety: 1.4659255679072014, 6
appropriate: 1.4659255679072014, 6
prior: 1.4659255679072014, 6
reach: 1.4659255679072014, 6
shows: 1.4659255679072014, 6
machine: 1.4659255679072014, 6
aims: 1.4659255679072014, 6
corresponding: 1.4659255679072014, 6
robust: 1.4659255679072014, 6
outperforms: 1.4659255679072014, 6
effectiveness: 1.4659255679072014, 6
types: 1.4659255679072014, 6
discover: 1.4659255679072014, 6
necessary: 1.4659255679072014, 6
terms: 1.4659255679072014, 6
relationships: 1.4659255679072014, 6
selected: 1.4659255679072014, 6
irl: 1.4659255679072014, 6
since: 1.4659255679072014, 6
similar: 1.4659255679072014, 6
frame: 1.4659255679072014, 6
static: 1.4659255679072014, 6
being: 1.4659255679072014, 6
performing: 1.4659255679072014, 6
performed: 1.4659255679072014, 6
pose: 1.4659255679072014, 6
embodied: 1.2216046399226679, 5
skills: 1.2216046399226679, 5
active: 1.2216046399226679, 5
variational: 1.2216046399226679, 5
discrete: 1.2216046399226679, 5
grounded: 1.2216046399226679, 5
function: 1.2216046399226679, 5
solving: 1.2216046399226679, 5
ontology: 1.2216046399226679, 5
directed: 1.2216046399226679, 5
icub: 1.2216046399226679, 5
case: 1.2216046399226679, 5
cnn: 1.2216046399226679, 5
makes: 1.2216046399226679, 5
synthetic: 1.2216046399226679, 5
few: 1.2216046399226679, 5
motivation: 1.2216046399226679, 5
artificial: 1.2216046399226679, 5
part: 1.2216046399226679, 5
modeling: 1.2216046399226679, 5
inspired: 1.2216046399226679, 5
categories: 1.2216046399226679, 5
aware: 1.2216046399226679, 5
supervision: 1.2216046399226679, 5
unseen: 1.2216046399226679, 5
infer: 1.2216046399226679, 5
states: 1.2216046399226679, 5
create: 1.2216046399226679, 5
particular: 1.2216046399226679, 5
outcomes: 1.2216046399226679, 5
step: 1.2216046399226679, 5
achieving: 1.2216046399226679, 5
predictor: 1.2216046399226679, 5
scenario: 1.2216046399226679, 5
along: 1.2216046399226679, 5
enables: 1.2216046399226679, 5
sample: 1.2216046399226679, 5
further: 1.2216046399226679, 5
those: 1.2216046399226679, 5
pre: 1.2216046399226679, 5
comprehensive: 1.2216046399226679, 5
presents: 1.2216046399226679, 5
challenge: 1.2216046399226679, 5
vector: 1.2216046399226679, 5
