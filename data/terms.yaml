affordance: 112.91144952058659, 544
learning: 89.87253243090807, 433
object: 69.53186689227299, 335
robot: 55.41793570219966, 267
based: 35.49238578680203, 171
visual: 30.510998307952626, 147
perception: 25.737168640721944, 124
deep: 23.869148336153412, 115
object affordance: 23.663258830994852, 57
reinforcement learning: 23.663258830994852, 57
action: 22.623801466441062, 109
task: 21.37845459672871, 103
human: 20.548223350253807, 99
network: 18.68020304568528, 90
image: 17.43485617597293, 84
predict: 16.81218274111675, 81
grasp: 16.604624929498026, 80
representation: 16.604624929498026, 80
interaction: 15.981951494641851, 77
optical flow: 15.775505887329901, 38
deep learning: 14.945216103786223, 36
neural network: 14.530071212014384, 35
feature: 14.113931190073323, 68
affordance based: 13.699781428470704, 33
reinforcement: 12.868584320360972, 62
train: 12.453468697123519, 60
learning affordance: 12.039201861383347, 29
grasp affordance: 11.624056969611507, 28
agent: 11.62323745064862, 56
ecological: 11.62323745064862, 56
recognition: 11.415679639029893, 55
flow: 11.415679639029893, 55
environment: 11.208121827411167, 54
tool: 11.000564015792442, 53
scene: 11.000564015792442, 53
learn: 10.793006204173716, 52
state art: 10.378622294295988, 25
neural: 10.377890580936267, 50
real: 9.75521714608009, 47
optical: 9.75521714608009, 47
machine learning: 9.54833251075231, 23
affordance learning: 9.54833251075231, 23
attention: 9.547659334461365, 46
video: 9.547659334461365, 46
vision: 9.34010152284264, 45
self supervised: 9.13318761898047, 22
supervised: 8.509870276367739, 41
different: 8.509870276367739, 41
self: 8.302312464749013, 40
goal: 8.094754653130288, 39
behavior: 8.094754653130288, 39
convolutional neural: 7.8877529436649505, 19
autonomous: 7.6796390298928365, 37
effect: 7.472081218274111, 36
concept: 7.472081218274111, 36
multi: 7.264523406655387, 35
detection: 7.264523406655387, 35
machine: 7.264523406655387, 35
control: 7.264523406655387, 35
real world: 7.057463160121272, 17
representation learning: 7.057463160121272, 17
object recognition: 7.057463160121272, 17
3d: 7.056965595036662, 34
framework: 7.056965595036662, 34
cognitive: 7.056965595036662, 34
end: 7.056965595036662, 34
convolutional: 7.056965595036662, 34
systems: 7.056965595036662, 34
language: 6.849407783417935, 33
demonstrate: 6.849407783417935, 33
result: 6.849407783417935, 33
end end: 6.642318268349433, 16
object interaction: 6.642318268349433, 16
state: 6.641849971799211, 32
ecological approach: 6.227173376577593, 15
affordance detection: 6.227173376577593, 15
study: 6.2267343485617594, 30
world: 6.2267343485617594, 30
knowledge: 6.019176536943034, 29
learned: 6.019176536943034, 29
present: 6.019176536943034, 29
visual affordance: 5.8120284848057535, 14
generative model: 5.8120284848057535, 14
semantic: 5.81161872532431, 28
system: 5.604060913705584, 27
propose: 5.604060913705584, 27
affordance object: 5.396883593033914, 13
learning object: 5.396883593033914, 13
visual feature: 5.396883593033914, 13
rgb d: 5.396883593033914, 13
international conference: 5.396883593033914, 13
human object: 5.396883593033914, 13
simulate: 5.396503102086858, 26
novel: 5.396503102086858, 26
segmentation: 5.396503102086858, 26
exploration: 5.1889452904681335, 25
understanding: 5.1889452904681335, 25
policy: 5.1889452904681335, 25
manipulation: 5.1889452904681335, 25
object detection: 4.981738701262075, 12
convolutional network: 4.981738701262075, 12
learning based: 4.981738701262075, 12
mobile robot: 4.981738701262075, 12
humanoid robot: 4.981738701262075, 12
direct perception: 4.981738701262075, 12
generative: 4.981387478849408, 24
performance: 4.981387478849408, 24
development: 4.981387478849408, 24
given: 4.981387478849408, 24
algorithm: 4.773829667230682, 23
problem: 4.773829667230682, 23
probabilistic: 4.773829667230682, 23
driven: 4.773829667230682, 23
space: 4.773829667230682, 23
affordance predict: 4.566593809490235, 11
affordance robot: 4.566593809490235, 11
affordance model: 4.566593809490235, 11
computer vision: 4.566593809490235, 11
spatio temporal: 4.566593809490235, 11
based approach: 4.566593809490235, 11
interactive: 4.566271855611957, 22
reasoning: 4.566271855611957, 22
physical: 4.566271855611957, 22
survey: 4.358714043993232, 21
functional: 4.358714043993232, 21
temporal: 4.358714043993232, 21
dataset: 4.358714043993232, 21
deep reinforcement: 4.151448917718395, 10
learning approach: 4.151448917718395, 10
based affordance: 4.151448917718395, 10
learning task: 4.151448917718395, 10
learning visual: 4.151448917718395, 10
tool affordance: 4.151448917718395, 10
unsupervised learning: 4.151448917718395, 10
robot control: 4.151448917718395, 10
future: 4.151156232374507, 20
bayesian: 4.151156232374507, 20
review: 4.151156232374507, 20
analysis: 4.151156232374507, 20
design: 4.151156232374507, 20
rgb: 4.151156232374507, 20
embodied: 3.9435984207557815, 19
artificial: 3.9435984207557815, 19
driving: 3.9435984207557815, 19
unsupervised: 3.9435984207557815, 19
context: 3.9435984207557815, 19
social: 3.9435984207557815, 19
deep neural: 3.7363040259465556, 9
based robot: 3.7363040259465556, 9
online learning: 3.7363040259465556, 9
visual attention: 3.7363040259465556, 9
autonomous robot: 3.7363040259465556, 9
robot grasp: 3.7363040259465556, 9
vision based: 3.7363040259465556, 9
experimental result: 3.7363040259465556, 9
spatial: 3.7360406091370555, 18
online: 3.7360406091370555, 18
estimation: 3.7360406091370555, 18
direct: 3.7360406091370555, 18
computer: 3.7360406091370555, 18
towards: 3.528482797518331, 17
recent: 3.528482797518331, 17
d: 3.528482797518331, 17
autonomous driving: 3.3211591341747164, 8
model based: 3.3211591341747164, 8
deep network: 3.3211591341747164, 8
affordance perception: 3.3211591341747164, 8
concept affordance: 3.3211591341747164, 8
semantic segmentation: 3.3211591341747164, 8
supervised learning: 3.3211591341747164, 8
perception affordance: 3.3211591341747164, 8
domain adaptation: 3.3211591341747164, 8
high level: 3.3211591341747164, 8
single: 3.3209249858996057, 16
via: 3.3209249858996057, 16
toward: 3.3209249858996057, 16
imitation: 3.3209249858996057, 16
mobile: 3.3209249858996057, 16
computational: 3.3209249858996057, 16
planning: 3.3209249858996057, 16
architecture: 3.3209249858996057, 16
current: 3.3209249858996057, 16
ai: 3.1133671742808797, 15
challenge: 3.1133671742808797, 15
grounded: 3.1133671742808797, 15
capture: 3.1133671742808797, 15
properties: 3.1133671742808797, 15
conference: 3.1133671742808797, 15
view: 3.1133671742808797, 15
navigation: 3.1133671742808797, 15
developmental: 3.1133671742808797, 15
artificial intelligence: 2.9060142424028768, 7
deep convolutional: 2.9060142424028768, 7
robot learning: 2.9060142424028768, 7
affordance recognition: 2.9060142424028768, 7
learning predict: 2.9060142424028768, 7
multiple object: 2.9060142424028768, 7
goal directed: 2.9060142424028768, 7
perceiving affordance: 2.9060142424028768, 7
human action: 2.9060142424028768, 7
visual object: 2.9060142424028768, 7
large scale: 2.9060142424028768, 7
robot systems: 2.9060142424028768, 7
computational model: 2.9060142424028768, 7
ecological psychology: 2.9060142424028768, 7
scene flow: 2.9060142424028768, 7
learning robot: 2.9060142424028768, 7
learn affordance: 2.9060142424028768, 7
imitation learning: 2.9060142424028768, 7
learning algorithm: 2.9060142424028768, 7
vision language: 2.9060142424028768, 7
current state: 2.9060142424028768, 7
robot able: 2.9060142424028768, 7
object parts: 2.9060142424028768, 7
efficient: 2.905809362662155, 14
modeling: 2.905809362662155, 14
perceiving: 2.905809362662155, 14
international: 2.905809362662155, 14
depth: 2.905809362662155, 14
humanoid: 2.905809362662155, 14
appearance: 2.905809362662155, 14
motion: 2.905809362662155, 14
role: 2.905809362662155, 14
pixel: 2.905809362662155, 14
domain: 2.905809362662155, 14
mechanism: 2.905809362662155, 14
available: 2.905809362662155, 14
active: 2.698251551043429, 13
motor: 2.698251551043429, 13
conditional: 2.698251551043429, 13
datasets: 2.698251551043429, 13
inference: 2.698251551043429, 13
sensorimotor: 2.698251551043429, 13
perspective: 2.698251551043429, 13
hand: 2.698251551043429, 13
experimental: 2.698251551043429, 13
robot affordance: 2.4908693506310375, 6
agent based: 2.4908693506310375, 6
based reinforcement: 2.4908693506310375, 6
artificial agent: 2.4908693506310375, 6
visual perception: 2.4908693506310375, 6
train deep: 2.4908693506310375, 6
pre train: 2.4908693506310375, 6
object based: 2.4908693506310375, 6
interactive reinforcement: 2.4908693506310375, 6
contextual affordance: 2.4908693506310375, 6
learning grasp: 2.4908693506310375, 6
affordance reasoning: 2.4908693506310375, 6
data driven: 2.4908693506310375, 6
behavior grounded: 2.4908693506310375, 6
affordance segmentation: 2.4908693506310375, 6
flow estimation: 2.4908693506310375, 6
theory affordance: 2.4908693506310375, 6
intelligent robot: 2.4908693506310375, 6
object manipulation: 2.4908693506310375, 6
predict human: 2.4908693506310375, 6
image based: 2.4908693506310375, 6
flow scene: 2.4908693506310375, 6
affordance autonomous: 2.4908693506310375, 6
latent space: 2.4908693506310375, 6
human robot: 2.4908693506310375, 6
affordance map: 2.4908693506310375, 6
task relevant: 2.4908693506310375, 6
robot learn: 2.4908693506310375, 6
simulate robot: 2.4908693506310375, 6
paper present: 2.4908693506310375, 6
experience: 2.490693739424704, 12
generate: 2.490693739424704, 12
latent: 2.490693739424704, 12
cues: 2.490693739424704, 12
intelligent: 2.490693739424704, 12
robust: 2.490693739424704, 12
cognition: 2.490693739424704, 12
psychology: 2.490693739424704, 12
scale: 2.490693739424704, 12
invariant: 2.490693739424704, 12
selection: 2.490693739424704, 12
general: 2.490693739424704, 12
environmental: 2.490693739424704, 12
input: 2.490693739424704, 12
label: 2.490693739424704, 12
previously: 2.490693739424704, 12
achieve: 2.490693739424704, 12
several: 2.490693739424704, 12
challenging: 2.490693739424704, 12
various: 2.490693739424704, 12
small: 2.490693739424704, 12
imagine: 2.2831359278059784, 11
egocentric: 2.2831359278059784, 11
open: 2.2831359278059784, 11
dynamic: 2.2831359278059784, 11
adaptation: 2.2831359278059784, 11
guided: 2.2831359278059784, 11
point: 2.2831359278059784, 11
potential: 2.2831359278059784, 11
evaluation: 2.2831359278059784, 11
spatio: 2.2831359278059784, 11
structure: 2.2831359278059784, 11
focus: 2.2831359278059784, 11
process: 2.2831359278059784, 11
associated: 2.2831359278059784, 11
map: 2.2831359278059784, 11
ability: 2.2831359278059784, 11
introduce: 2.2831359278059784, 11
language model: 2.0757244588591974, 5
affordance image: 2.0757244588591974, 5
affordance tool: 2.0757244588591974, 5
based deep: 2.0757244588591974, 5
deep visuomotor: 2.0757244588591974, 5
visuomotor policy: 2.0757244588591974, 5
robot perception: 2.0757244588591974, 5
perception systems: 2.0757244588591974, 5
task dependent: 2.0757244588591974, 5
egocentric video: 2.0757244588591974, 5
attention transition: 2.0757244588591974, 5
learn object: 2.0757244588591974, 5
object action: 2.0757244588591974, 5
object categories: 2.0757244588591974, 5
relational affordance: 2.0757244588591974, 5
multi object: 2.0757244588591974, 5
manipulation task: 2.0757244588591974, 5
perception autonomous: 2.0757244588591974, 5
affordance human: 2.0757244588591974, 5
model affordance: 2.0757244588591974, 5
scene understanding: 2.0757244588591974, 5
ieee international: 2.0757244588591974, 5
action recognition: 2.0757244588591974, 5
affordance rgb: 2.0757244588591974, 5
object tracking: 2.0757244588591974, 5
bayesian network: 2.0757244588591974, 5
bayesian approach: 2.0757244588591974, 5
robot interaction: 2.0757244588591974, 5
learning perception: 2.0757244588591974, 5
simulate real: 2.0757244588591974, 5
learning method: 2.0757244588591974, 5
ground truth: 2.0757244588591974, 5
problem affordance: 2.0757244588591974, 5
affordance terms: 2.0757244588591974, 5
grasp object: 2.0757244588591974, 5
predict affordance: 2.0757244588591974, 5
goal conditioned: 2.0757244588591974, 5
art method: 2.0757244588591974, 5
real robot: 2.0757244588591974, 5
demonstrate effectiveness: 2.0757244588591974, 5
effect action: 2.0757244588591974, 5
perception approach: 2.0757244588591974, 5
pixel wise: 2.0757244588591974, 5
predict effect: 2.0757244588591974, 5
emergent: 2.0755781161872533, 10
decision: 2.0755781161872533, 10
science: 2.0755781161872533, 10
inspired: 2.0755781161872533, 10
processing: 2.0755781161872533, 10
class: 2.0755781161872533, 10
classification: 2.0755781161872533, 10
generation: 2.0755781161872533, 10
hierarchical: 2.0755781161872533, 10
under: 2.0755781161872533, 10
categories: 2.0755781161872533, 10
simple: 2.0755781161872533, 10
movement: 2.0755781161872533, 10
possible: 2.0755781161872533, 10
continuous: 2.0755781161872533, 10
pose: 2.0755781161872533, 10
synthetic: 2.0755781161872533, 10
saliency: 2.0755781161872533, 10
interact: 2.0755781161872533, 10
basic: 2.0755781161872533, 10
generalization: 2.0755781161872533, 10
natural: 2.0755781161872533, 10
understand: 2.0755781161872533, 10
generalize: 2.0755781161872533, 10
including: 2.0755781161872533, 10
key: 2.0755781161872533, 10
important: 2.0755781161872533, 10
synthesis: 1.8680203045685277, 9
reaching: 1.8680203045685277, 9
communication: 1.8680203045685277, 9
intelligence: 1.8680203045685277, 9
function: 1.8680203045685277, 9
few: 1.8680203045685277, 9
activity: 1.8680203045685277, 9
application: 1.8680203045685277, 9
focused: 1.8680203045685277, 9
gaze: 1.8680203045685277, 9
user: 1.8680203045685277, 9
capabilities: 1.8680203045685277, 9
case: 1.8680203045685277, 9
geometric: 1.8680203045685277, 9
common: 1.8680203045685277, 9
range: 1.8680203045685277, 9
examples: 1.8680203045685277, 9
activities: 1.8680203045685277, 9
controller: 1.8680203045685277, 9
outcome: 1.8680203045685277, 9
sensory: 1.8680203045685277, 9
ecology: 1.8680203045685277, 9
static: 1.8680203045685277, 9
beyond: 1.8680203045685277, 9
speed: 1.8680203045685277, 9
underlying: 1.8680203045685277, 9
low: 1.8680203045685277, 9
diverse: 1.8680203045685277, 9
suitable: 1.8680203045685277, 9
recently: 1.8680203045685277, 9
build: 1.8680203045685277, 9
known: 1.8680203045685277, 9
decision making: 1.6605795670873582, 4
image generative: 1.6605795670873582, 4
data mining: 1.6605795670873582, 4
intrinsic motivation: 1.6605795670873582, 4
approach object: 1.6605795670873582, 4
sensorimotor object: 1.6605795670873582, 4
recent advances: 1.6605795670873582, 4
detecting object: 1.6605795670873582, 4
predict feature: 1.6605795670873582, 4
feature affordance: 1.6605795670873582, 4
random field: 1.6605795670873582, 4
task driven: 1.6605795670873582, 4
multi task: 1.6605795670873582, 4
dependent attention: 1.6605795670873582, 4
learned affordance: 1.6605795670873582, 4
action object: 1.6605795670873582, 4
based object: 1.6605795670873582, 4
d video: 1.6605795670873582, 4
approach affordance: 1.6605795670873582, 4
case study: 1.6605795670873582, 4
action effect: 1.6605795670873582, 4
reasoning object: 1.6605795670873582, 4
fully convolutional: 1.6605795670873582, 4
robot manipulation: 1.6605795670873582, 4
3d scene: 1.6605795670873582, 4
depth based: 1.6605795670873582, 4
navigation affordance: 1.6605795670873582, 4
multi scale: 1.6605795670873582, 4
human activities: 1.6605795670873582, 4
visual recognition: 1.6605795670873582, 4
multi view: 1.6605795670873582, 4
object scene: 1.6605795670873582, 4
temporal coherence: 1.6605795670873582, 4
visual representation: 1.6605795670873582, 4
model visual: 1.6605795670873582, 4
action selection: 1.6605795670873582, 4
self exploration: 1.6605795670873582, 4
graphical model: 1.6605795670873582, 4
control architecture: 1.6605795670873582, 4
towards affordance: 1.6605795670873582, 4
based grasp: 1.6605795670873582, 4
markov decision: 1.6605795670873582, 4
object perception: 1.6605795670873582, 4
approach perception: 1.6605795670873582, 4
self learning: 1.6605795670873582, 4
based simulate: 1.6605795670873582, 4
affordance concept: 1.6605795670873582, 4
approach learning: 1.6605795670873582, 4
performance predictor: 1.6605795670873582, 4
previously learned: 1.6605795670873582, 4
novel dataset: 1.6605795670873582, 4
agent learn: 1.6605795670873582, 4
detection branch: 1.6605795670873582, 4
object appearance: 1.6605795670873582, 4
propose method: 1.6605795670873582, 4
network learn: 1.6605795670873582, 4
3d feature: 1.6605795670873582, 4
rl contextual: 1.6605795670873582, 4
representation object: 1.6605795670873582, 4
attention mechanism: 1.6605795670873582, 4
able learn: 1.6605795670873582, 4
action performed: 1.6605795670873582, 4
influence: 1.6604624929498029, 8
effectiveness: 1.6604624929498029, 8
impact: 1.6604624929498029, 8
comprehensive: 1.6604624929498029, 8
search: 1.6604624929498029, 8
motivated: 1.6604624929498029, 8
non: 1.6604624929498029, 8
play: 1.6604624929498029, 8
contextual: 1.6604624929498029, 8
intrinsic: 1.6604624929498029, 8
across: 1.6604624929498029, 8
relations: 1.6604624929498029, 8
dependent: 1.6604624929498029, 8
transition: 1.6604624929498029, 8
body: 1.6604624929498029, 8
ontology: 1.6604624929498029, 8
does: 1.6604624929498029, 8
implications: 1.6604624929498029, 8
2d: 1.6604624929498029, 8
detect: 1.6604624929498029, 8
dynamics: 1.6604624929498029, 8
directed: 1.6604624929498029, 8
applications: 1.6604624929498029, 8
improve: 1.6604624929498029, 8
transfer: 1.6604624929498029, 8
evaluate: 1.6604624929498029, 8
relationships: 1.6604624929498029, 8
tracking: 1.6604624929498029, 8
graph: 1.6604624929498029, 8
basis: 1.6604624929498029, 8
ground: 1.6604624929498029, 8
sample: 1.6604624929498029, 8
terms: 1.6604624929498029, 8
quality: 1.6604624929498029, 8
explore: 1.6604624929498029, 8
experiments: 1.6604624929498029, 8
aspects: 1.6604624929498029, 8
previous: 1.6604624929498029, 8
article: 1.6604624929498029, 8
affordancenet: 1.6604624929498029, 8
types: 1.6604624929498029, 8
efficiently: 1.6604624929498029, 8
directly: 1.6604624929498029, 8
vdac: 1.6604624929498029, 8
best: 1.6604624929498029, 8
describe: 1.6604624929498029, 8
exploring: 1.4529046813310775, 7
higher: 1.4529046813310775, 7
person: 1.4529046813310775, 7
developing: 1.4529046813310775, 7
visuomotor: 1.4529046813310775, 7
pre: 1.4529046813310775, 7
discriminative: 1.4529046813310775, 7
discrete: 1.4529046813310775, 7
physics: 1.4529046813310775, 7
being: 1.4529046813310775, 7
motivation: 1.4529046813310775, 7
dense: 1.4529046813310775, 7
trustworthy: 1.4529046813310775, 7
joint: 1.4529046813310775, 7
urban: 1.4529046813310775, 7
principles: 1.4529046813310775, 7
recurrent: 1.4529046813310775, 7
indoor: 1.4529046813310775, 7
fully: 1.4529046813310775, 7
relational: 1.4529046813310775, 7
geometry: 1.4529046813310775, 7
haptic: 1.4529046813310775, 7
effective: 1.4529046813310775, 7
ieee: 1.4529046813310775, 7
prior: 1.4529046813310775, 7
local: 1.4529046813310775, 7
states: 1.4529046813310775, 7
frame: 1.4529046813310775, 7
acquisition: 1.4529046813310775, 7
supervision: 1.4529046813310775, 7
better: 1.4529046813310775, 7
benchmark: 1.4529046813310775, 7
levels: 1.4529046813310775, 7
part: 1.4529046813310775, 7
markov: 1.4529046813310775, 7
applied: 1.4529046813310775, 7
traditional: 1.4529046813310775, 7
category: 1.4529046813310775, 7
rendering: 1.4529046813310775, 7
target: 1.4529046813310775, 7
practice: 1.4529046813310775, 7
four: 1.4529046813310775, 7
feedback: 1.4529046813310775, 7
develop: 1.4529046813310775, 7
desired: 1.4529046813310775, 7
enables: 1.4529046813310775, 7
specifically: 1.4529046813310775, 7
shows: 1.4529046813310775, 7
certain: 1.4529046813310775, 7
rl: 1.4529046813310775, 7
outperforms: 1.4529046813310775, 7
necessary: 1.4529046813310775, 7
discuss: 1.4529046813310775, 7
significant: 1.4529046813310775, 7
related: 1.4529046813310775, 7
performing: 1.4529046813310775, 7
performed: 1.4529046813310775, 7
capable: 1.4529046813310775, 7
goal imagine: 1.2454346753155188, 3
emergent affordance: 1.2454346753155188, 3
tool synthesis: 1.2454346753155188, 3
learning act: 1.2454346753155188, 3
act predict: 1.2454346753155188, 3
learning framework: 1.2454346753155188, 3
learning agent: 1.2454346753155188, 3
model human: 1.2454346753155188, 3
policy search: 1.2454346753155188, 3
intrinsically motivated: 1.2454346753155188, 3
visual reinforcement: 1.2454346753155188, 3
curiosity driven: 1.2454346753155188, 3
supervised predict: 1.2454346753155188, 3
learning deep: 1.2454346753155188, 3
network based: 1.2454346753155188, 3
activity theory: 1.2454346753155188, 3
learning environment: 1.2454346753155188, 3
deep affordance: 1.2454346753155188, 3
learning perspective: 1.2454346753155188, 3
affordance function: 1.2454346753155188, 3
detection convolutional: 1.2454346753155188, 3
dense conditional: 1.2454346753155188, 3
conditional random: 1.2454346753155188, 3
based cues: 1.2454346753155188, 3
deep representation: 1.2454346753155188, 3
learning across: 1.2454346753155188, 3
information sources: 1.2454346753155188, 3
train agent: 1.2454346753155188, 3
learning driving: 1.2454346753155188, 3
predict future: 1.2454346753155188, 3
future object: 1.2454346753155188, 3
based visual: 1.2454346753155188, 3
attention control: 1.2454346753155188, 3
task focused: 1.2454346753155188, 3
ontology approach: 1.2454346753155188, 3
active vision: 1.2454346753155188, 3
bayesian deep: 1.2454346753155188, 3
method detect: 1.2454346753155188, 3
learning optical: 1.2454346753155188, 3
flow learning: 1.2454346753155188, 3
affordance theory: 1.2454346753155188, 3
information systems: 1.2454346753155188, 3
affordance probabilistic: 1.2454346753155188, 3
affordance knowledge: 1.2454346753155188, 3
knowledge base: 1.2454346753155188, 3
base representation: 1.2454346753155188, 3
image segmentation: 1.2454346753155188, 3
point clouds: 1.2454346753155188, 3
view geometry: 1.2454346753155188, 3
pose estimation: 1.2454346753155188, 3
task based: 1.2454346753155188, 3
grasp planning: 1.2454346753155188, 3
haptic cues: 1.2454346753155188, 3
learning depth: 1.2454346753155188, 3
affordance relations: 1.2454346753155188, 3
study learning: 1.2454346753155188, 3
affordance improve: 1.2454346753155188, 3
object class: 1.2454346753155188, 3
image recognition: 1.2454346753155188, 3
affordance planning: 1.2454346753155188, 3
rgb image: 1.2454346753155188, 3
affordance research: 1.2454346753155188, 3
weakly supervised: 1.2454346753155188, 3
train data: 1.2454346753155188, 3
network train: 1.2454346753155188, 3
functional scene: 1.2454346753155188, 3
robot icra: 1.2454346753155188, 3
conference computer: 1.2454346753155188, 3
conference intelligent: 1.2454346753155188, 3
activities object: 1.2454346753155188, 3
object learning: 1.2454346753155188, 3
learning recognition: 1.2454346753155188, 3
deep feature: 1.2454346753155188, 3
saliency predict: 1.2454346753155188, 3
real object: 1.2454346753155188, 3
temporal object: 1.2454346753155188, 3
policy learning: 1.2454346753155188, 3
supervised deep: 1.2454346753155188, 3
robust visual: 1.2454346753155188, 3
environment affordance: 1.2454346753155188, 3
human grasp: 1.2454346753155188, 3
symmetries invariant: 1.2454346753155188, 3
invariant solutions: 1.2454346753155188, 3
free energy: 1.2454346753155188, 3
art visual: 1.2454346753155188, 3
robot task: 1.2454346753155188, 3
inspired robot: 1.2454346753155188, 3
grounded approach: 1.2454346753155188, 3
perception action: 1.2454346753155188, 3
learning basic: 1.2454346753155188, 3
basic object: 1.2454346753155188, 3
object properties: 1.2454346753155188, 3
object tool: 1.2454346753155188, 3
visual grasp: 1.2454346753155188, 3
affordance toward: 1.2454346753155188, 3
feature selection: 1.2454346753155188, 3
representation robot: 1.2454346753155188, 3
recurrent neural: 1.2454346753155188, 3
different levels: 1.2454346753155188, 3
push affordance: 1.2454346753155188, 3
sensory motor: 1.2454346753155188, 3
model object: 1.2454346753155188, 3
object concept: 1.2454346753155188, 3
cognitive control: 1.2454346753155188, 3
robot agent: 1.2454346753155188, 3
visual descriptors: 1.2454346753155188, 3
approach towards: 1.2454346753155188, 3
multi model: 1.2454346753155188, 3
approach based: 1.2454346753155188, 3
functional feature: 1.2454346753155188, 3
affordance discovery: 1.2454346753155188, 3
embodied cognition: 1.2454346753155188, 3
3 d: 1.2454346753155188, 3
perception learning: 1.2454346753155188, 3
embodied embodied: 1.2454346753155188, 3
visually guided: 1.2454346753155188, 3
single image: 1.2454346753155188, 3
end learning: 1.2454346753155188, 3
video datasets: 1.2454346753155188, 3
conditional imitation: 1.2454346753155188, 3
visual navigation: 1.2454346753155188, 3
static image: 1.2454346753155188, 3
toward ecological: 1.2454346753155188, 3
development visual: 1.2454346753155188, 3
affordance action: 1.2454346753155188, 3
interaction design: 1.2454346753155188, 3
ecological perception: 1.2454346753155188, 3
role affordance: 1.2454346753155188, 3
affordance visual: 1.2454346753155188, 3
based model: 1.2454346753155188, 3
multi modal: 1.2454346753155188, 3
interaction affordance: 1.2454346753155188, 3
affordance behavior: 1.2454346753155188, 3
input image: 1.2454346753155188, 3
existing research: 1.2454346753155188, 3
step towards: 1.2454346753155188, 3
relevant object: 1.2454346753155188, 3
affordance given: 1.2454346753155188, 3
visual information: 1.2454346753155188, 3
natural language: 1.2454346753155188, 3
supervised goal: 1.2454346753155188, 3
propose framework: 1.2454346753155188, 3
outperforms recent: 1.2454346753155188, 3
recent state: 1.2454346753155188, 3
robot demonstrate: 1.2454346753155188, 3
source code: 1.2454346753155188, 3
information object: 1.2454346753155188, 3
understand interact: 1.2454346753155188, 3
present novel: 1.2454346753155188, 3
robot capabilities: 1.2454346753155188, 3
approach solving: 1.2454346753155188, 3
level affordance: 1.2454346753155188, 3
affordance specifically: 1.2454346753155188, 3
arbitrary visual: 1.2454346753155188, 3
visual cues: 1.2454346753155188, 3
affordance play: 1.2454346753155188, 3
more complex: 1.2454346753155188, 3
present method: 1.2454346753155188, 3
wide range: 1.2454346753155188, 3
existing method: 1.2454346753155188, 3
compare three: 1.2454346753155188, 3
three different: 1.2454346753155188, 3
graph representation: 1.2454346753155188, 3
everyday life: 1.2454346753155188, 3
train model: 1.2454346753155188, 3
novel approach: 1.2454346753155188, 3
top down: 1.2454346753155188, 3
attention learned: 1.2454346753155188, 3
propose model: 1.2454346753155188, 3
gaze predict: 1.2454346753155188, 3
able predict: 1.2454346753155188, 3
strong supervision: 1.2454346753155188, 3
affordance label: 1.2454346753155188, 3
object label: 1.2454346753155188, 3
sor3d aff: 1.2454346753155188, 3
supervised method: 1.2454346753155188, 3
while being: 1.2454346753155188, 3
given single: 1.2454346753155188, 3
train convolutional: 1.2454346753155188, 3
displacement field: 1.2454346753155188, 3
flow algorithm: 1.2454346753155188, 3
haptic depth: 1.2454346753155188, 3
robot progressively: 1.2454346753155188, 3
progressively learn: 1.2454346753155188, 3
grasp configuration: 1.2454346753155188, 3
based behavior: 1.2454346753155188, 3
continuous pose: 1.2454346753155188, 3
affordance memory: 1.2454346753155188, 3
easy task: 1.2454346753155188, 3
low level: 1.2454346753155188, 3
skills: 1.245346869712352, 6
act: 1.245346869712352, 6
making: 1.245346869712352, 6
distribution: 1.245346869712352, 6
life: 1.245346869712352, 6
variational: 1.245346869712352, 6
adversarial: 1.245346869712352, 6
fast: 1.245346869712352, 6
structured: 1.245346869712352, 6
output: 1.245346869712352, 6
tree: 1.245346869712352, 6
value: 1.245346869712352, 6
techniques: 1.245346869712352, 6
landscape: 1.245346869712352, 6
paradigm: 1.245346869712352, 6
solving: 1.245346869712352, 6
memory: 1.245346869712352, 6
adaptive: 1.245346869712352, 6
technology: 1.245346869712352, 6
random: 1.245346869712352, 6
benefits: 1.245346869712352, 6
categorization: 1.245346869712352, 6
qualitative: 1.245346869712352, 6
virtual: 1.245346869712352, 6
initial: 1.245346869712352, 6
place: 1.245346869712352, 6
flownet: 1.245346869712352, 6
particular: 1.245346869712352, 6
interacting: 1.245346869712352, 6
support: 1.245346869712352, 6
rich: 1.245346869712352, 6
makes: 1.245346869712352, 6
intention: 1.245346869712352, 6
exploiting: 1.245346869712352, 6
icub: 1.245346869712352, 6
cnn: 1.245346869712352, 6
semantics: 1.245346869712352, 6
maps: 1.245346869712352, 6
unified: 1.245346869712352, 6
descriptors: 1.245346869712352, 6
limitations: 1.245346869712352, 6
invariance: 1.245346869712352, 6
scenario: 1.245346869712352, 6
building: 1.245346869712352, 6
relevance: 1.245346869712352, 6
accuracy: 1.245346869712352, 6
considering: 1.245346869712352, 6
visually: 1.245346869712352, 6
platform: 1.245346869712352, 6
attributes: 1.245346869712352, 6
infer: 1.245346869712352, 6
conditioned: 1.245346869712352, 6
achieving: 1.245346869712352, 6
variety: 1.245346869712352, 6
appropriate: 1.245346869712352, 6
along: 1.245346869712352, 6
further: 1.245346869712352, 6
reach: 1.245346869712352, 6
evidence: 1.245346869712352, 6
aims: 1.245346869712352, 6
corresponding: 1.245346869712352, 6
discover: 1.245346869712352, 6
require: 1.245346869712352, 6
selected: 1.245346869712352, 6
fundamental: 1.245346869712352, 6
likely: 1.245346869712352, 6
irl: 1.245346869712352, 6
consider: 1.245346869712352, 6
similar: 1.245346869712352, 6
consists: 1.245346869712352, 6
sor3d: 1.245346869712352, 6
i: 1.0377890580936266, 5
situated: 1.0377890580936266, 5
global: 1.0377890580936266, 5
account: 1.0377890580936266, 5
dual: 1.0377890580936266, 5
free: 1.0377890580936266, 5
locomotion: 1.0377890580936266, 5
automatic: 1.0377890580936266, 5
imagined: 1.0377890580936266, 5
offline: 1.0377890580936266, 5
mining: 1.0377890580936266, 5
opportunities: 1.0377890580936266, 5
discovery: 1.0377890580936266, 5
together: 1.0377890580936266, 5
importance: 1.0377890580936266, 5
advances: 1.0377890580936266, 5
detecting: 1.0377890580936266, 5
top: 1.0377890580936266, 5
multimodal: 1.0377890580936266, 5
hole: 1.0377890580936266, 5
anticipation: 1.0377890580936266, 5
reality: 1.0377890580936266, 5
collaboration: 1.0377890580936266, 5
reduce: 1.0377890580936266, 5
oriented: 1.0377890580936266, 5
observation: 1.0377890580936266, 5
bottom: 1.0377890580936266, 5
distributed: 1.0377890580936266, 5
demonstration: 1.0377890580936266, 5
bootstrapping: 1.0377890580936266, 5
signal: 1.0377890580936266, 5
directions: 1.0377890580936266, 5
comparison: 1.0377890580936266, 5
localization: 1.0377890580936266, 5
faster: 1.0377890580936266, 5
physically: 1.0377890580936266, 5
shape: 1.0377890580936266, 5
moving: 1.0377890580936266, 5
relation: 1.0377890580936266, 5
group: 1.0377890580936266, 5
transformations: 1.0377890580936266, 5
coherence: 1.0377890580936266, 5
technique: 1.0377890580936266, 5
speech: 1.0377890580936266, 5
mapping: 1.0377890580936266, 5
limited: 1.0377890580936266, 5
afford: 1.0377890580936266, 5
constraints: 1.0377890580936266, 5
extraction: 1.0377890580936266, 5
second: 1.0377890580936266, 5
simultaneously: 1.0377890580936266, 5
feasible: 1.0377890580936266, 5
aware: 1.0377890580936266, 5
integrating: 1.0377890580936266, 5
everyday: 1.0377890580936266, 5
perspectives: 1.0377890580936266, 5
reduction: 1.0377890580936266, 5
easy: 1.0377890580936266, 5
realistic: 1.0377890580936266, 5
selective: 1.0377890580936266, 5
truth: 1.0377890580936266, 5
designed: 1.0377890580936266, 5
grounding: 1.0377890580936266, 5
extended: 1.0377890580936266, 5
mediated: 1.0377890580936266, 5
ways: 1.0377890580936266, 5
annotations: 1.0377890580936266, 5
wide: 1.0377890580936266, 5
unseen: 1.0377890580936266, 5
create: 1.0377890580936266, 5
step: 1.0377890580936266, 5
predictor: 1.0377890580936266, 5
exploit: 1.0377890580936266, 5
aim: 1.0377890580936266, 5
meaningful: 1.0377890580936266, 5
focusing: 1.0377890580936266, 5
compared: 1.0377890580936266, 5
useful: 1.0377890580936266, 5
vector: 1.0377890580936266, 5
decoder: 1.0377890580936266, 5
typically: 1.0377890580936266, 5
observed: 1.0377890580936266, 5
leads: 1.0377890580936266, 5
classify: 1.0377890580936266, 5
architectures: 1.0377890580936266, 5
evaluated: 1.0377890580936266, 5
promising: 1.0377890580936266, 5
arbitrary: 1.0377890580936266, 5
entities: 1.0377890580936266, 5
successfully: 1.0377890580936266, 5
bounding: 1.0377890580936266, 5
produce: 1.0377890580936266, 5
obtained: 1.0377890580936266, 5
includes: 1.0377890580936266, 5
scenarios: 1.0377890580936266, 5
compare: 1.0377890580936266, 5
resulting: 1.0377890580936266, 5
predicted: 1.0377890580936266, 5
success: 1.0377890580936266, 5
taking: 1.0377890580936266, 5
particularly: 1.0377890580936266, 5
structures: 1.0377890580936266, 5
uses: 1.0377890580936266, 5
performs: 1.0377890580936266, 5
wise: 1.0377890580936266, 5
robot exploration: 0.8302897835436791, 2
large language: 0.8302897835436791, 2
embodied ai: 0.8302897835436791, 2
imagine leveraging: 0.8302897835436791, 2
leveraging emergent: 0.8302897835436791, 2
affordance 3d: 0.8302897835436791, 2
3d tool: 0.8302897835436791, 2
predict explaining: 0.8302897835436791, 2
explaining affordance: 0.8302897835436791, 2
reaching task: 0.8302897835436791, 2
object centric: 0.8302897835436791, 2
centric generative: 0.8302897835436791, 2
efficient learning: 0.8302897835436791, 2
multi agent: 0.8302897835436791, 2
active perception: 0.8302897835436791, 2
perception based: 0.8302897835436791, 2
mutual influence: 0.8302897835436791, 2
influence language: 0.8302897835436791, 2
language perception: 0.8302897835436791, 2
language emergence: 0.8302897835436791, 2
cognitive science: 0.8302897835436791, 2
cognitive neuroscience: 0.8302897835436791, 2
ai model: 0.8302897835436791, 2
grasp novel: 0.8302897835436791, 2
novel object: 0.8302897835436791, 2
developing person: 0.8302897835436791, 2
person life: 0.8302897835436791, 2
life span: 0.8302897835436791, 2
approach visual: 0.8302897835436791, 2
data efficient: 0.8302897835436791, 2
learning fast: 0.8302897835436791, 2
end train: 0.8302897835436791, 2
primitives robot: 0.8302897835436791, 2
learning structured: 0.8302897835436791, 2
study count: 0.8302897835436791, 2
count based: 0.8302897835436791, 2
based exploration: 0.8302897835436791, 2
exploration deep: 0.8302897835436791, 2
learning via: 0.8302897835436791, 2
imagined goal: 0.8302897835436791, 2
language understanding: 0.8302897835436791, 2
off policy: 0.8302897835436791, 2
driven exploration: 0.8302897835436791, 2
exploration self: 0.8302897835436791, 2
supervised robot: 0.8302897835436791, 2
generative adversarial: 0.8302897835436791, 2
predict model: 0.8302897835436791, 2
able recognize: 0.8302897835436791, 2
mining knowledge: 0.8302897835436791, 2
high performance: 0.8302897835436791, 2
agency social: 0.8302897835436791, 2
based image: 0.8302897835436791, 2
survey visual: 0.8302897835436791, 2
recognition based: 0.8302897835436791, 2
end deep: 0.8302897835436791, 2
affordance grounded: 0.8302897835436791, 2
grounded sensorimotor: 0.8302897835436791, 2
advances deep: 0.8302897835436791, 2
deep robot: 0.8302897835436791, 2
affordance reinforcement: 0.8302897835436791, 2
affordance convolutional: 0.8302897835436791, 2
function survey: 0.8302897835436791, 2
network dense: 0.8302897835436791, 2
visual learning: 0.8302897835436791, 2
open challenge: 0.8302897835436791, 2
trustworthy representation: 0.8302897835436791, 2
across domains: 0.8302897835436791, 2
learning top: 0.8302897835436791, 2
learning imitation: 0.8302897835436791, 2
knowledge guided: 0.8302897835436791, 2
agent interactive: 0.8302897835436791, 2
learning contextual: 0.8302897835436791, 2
urban environment: 0.8302897835436791, 2
spatial relations: 0.8302897835436791, 2
attention privileged: 0.8302897835436791, 2
privileged reinforcement: 0.8302897835436791, 2
driven object: 0.8302897835436791, 2
value driven: 0.8302897835436791, 2
driven attention: 0.8302897835436791, 2
attention capture: 0.8302897835436791, 2
focused visual: 0.8302897835436791, 2
visual spatial: 0.8302897835436791, 2
spatial attention: 0.8302897835436791, 2
proprioceptive data: 0.8302897835436791, 2
driven reinforcement: 0.8302897835436791, 2
peg hole: 0.8302897835436791, 2
hole task: 0.8302897835436791, 2
predict gaze: 0.8302897835436791, 2
gaze egocentric: 0.8302897835436791, 2
video learning: 0.8302897835436791, 2
tool based: 0.8302897835436791, 2
based interaction: 0.8302897835436791, 2
learning human: 0.8302897835436791, 2
task specific: 0.8302897835436791, 2
specific visual: 0.8302897835436791, 2
multi robot: 0.8302897835436791, 2
semantic robot: 0.8302897835436791, 2
robot behavior: 0.8302897835436791, 2
behavior development: 0.8302897835436791, 2
robot collaboration: 0.8302897835436791, 2
sofm learn: 0.8302897835436791, 2
programs machine: 0.8302897835436791, 2
control grasp: 0.8302897835436791, 2
grounded representation: 0.8302897835436791, 2
representation tool: 0.8302897835436791, 2
image feature: 0.8302897835436791, 2
function based: 0.8302897835436791, 2
recognition multiple: 0.8302897835436791, 2
probabilistic affordance: 0.8302897835436791, 2
joint object: 0.8302897835436791, 2
segmentation rgb: 0.8302897835436791, 2
2d image: 0.8302897835436791, 2
robot action: 0.8302897835436791, 2
deep recurrent: 0.8302897835436791, 2
particle image: 0.8302897835436791, 2
image velocimetry: 0.8302897835436791, 2
learning confidence: 0.8302897835436791, 2
confidence measure: 0.8302897835436791, 2
flownet evolution: 0.8302897835436791, 2
evolution optical: 0.8302897835436791, 2
estimation deep: 0.8302897835436791, 2
optical data: 0.8302897835436791, 2
rethinking affordance: 0.8302897835436791, 2
turing implications: 0.8302897835436791, 2
implications affordance: 0.8302897835436791, 2
theory approach: 0.8302897835436791, 2
affordance information: 0.8302897835436791, 2
study affordance: 0.8302897835436791, 2
affordance properties: 0.8302897835436791, 2
animal environment: 0.8302897835436791, 2
geometric feature: 0.8302897835436791, 2
object context: 0.8302897835436791, 2
network semantic: 0.8302897835436791, 2
affordance densities: 0.8302897835436791, 2
labeling 3d: 0.8302897835436791, 2
3d point: 0.8302897835436791, 2
whole body: 0.8302897835436791, 2
manipulation action: 0.8302897835436791, 2
rich landscape: 0.8302897835436791, 2
landscape affordance: 0.8302897835436791, 2
model robot: 0.8302897835436791, 2
affordance different: 0.8302897835436791, 2
human intention: 0.8302897835436791, 2
visual observations: 0.8302897835436791, 2
exploiting haptic: 0.8302897835436791, 2
robot navigation: 0.8302897835436791, 2
categories robot: 0.8302897835436791, 2
learning traversability: 0.8302897835436791, 2
traversability affordance: 0.8302897835436791, 2
affordance range: 0.8302897835436791, 2
range image: 0.8302897835436791, 2
learning perceiving: 0.8302897835436791, 2
image database: 0.8302897835436791, 2
dynamical systems: 0.8302897835436791, 2
learning adjectives: 0.8302897835436791, 2
adjectives nouns: 0.8302897835436791, 2
nouns affordance: 0.8302897835436791, 2
affordance appearance: 0.8302897835436791, 2
affordance direct: 0.8302897835436791, 2
improve object: 0.8302897835436791, 2
affordance multi: 0.8302897835436791, 2
potential field: 0.8302897835436791, 2
human demonstration: 0.8302897835436791, 2
deep residual: 0.8302897835436791, 2
manipulation affordance: 0.8302897835436791, 2
developmental approach: 0.8302897835436791, 2
approach autonomous: 0.8302897835436791, 2
social affordance: 0.8302897835436791, 2
emergent structuring: 0.8302897835436791, 2
structuring interdependent: 0.8302897835436791, 2
interdependent affordance: 0.8302897835436791, 2
segmentation deep: 0.8302897835436791, 2
research developmental: 0.8302897835436791, 2
developmental survey: 0.8302897835436791, 2
conference robot: 0.8302897835436791, 2
robot automation: 0.8302897835436791, 2
supervised affordance: 0.8302897835436791, 2
urban scene: 0.8302897835436791, 2
few examples: 0.8302897835436791, 2
learning action: 0.8302897835436791, 2
environment via: 0.8302897835436791, 2
cognitive robot: 0.8302897835436791, 2
proceedings 2006: 0.8302897835436791, 2
2006 ieee: 0.8302897835436791, 2
2012 international: 0.8302897835436791, 2
ieee conference: 0.8302897835436791, 2
vision pattern: 0.8302897835436791, 2
pattern recognition: 0.8302897835436791, 2
proceedings ieee: 0.8302897835436791, 2
2014 international: 0.8302897835436791, 2
long term: 0.8302897835436791, 2
anticipating human: 0.8302897835436791, 2
affordance reactive: 0.8302897835436791, 2
reactive robot: 0.8302897835436791, 2
robot response: 0.8302897835436791, 2
task oriented: 0.8302897835436791, 2
50 years: 0.8302897835436791, 2
sensorimotor learning: 0.8302897835436791, 2
object categorization: 0.8302897835436791, 2
faster r: 0.8302897835436791, 2
r towards: 0.8302897835436791, 2
towards real: 0.8302897835436791, 2
detection region: 0.8302897835436791, 2
region proposal: 0.8302897835436791, 2
proposal network: 0.8302897835436791, 2
physically grounded: 0.8302897835436791, 2
grounded spatio: 0.8302897835436791, 2
semi supervised: 0.8302897835436791, 2
learning rgb: 0.8302897835436791, 2
view 3d: 0.8302897835436791, 2
tracking based: 0.8302897835436791, 2
object segmentation: 0.8302897835436791, 2
segmentation model: 0.8302897835436791, 2
moving object: 0.8302897835436791, 2
brief survey: 0.8302897835436791, 2
few shot: 0.8302897835436791, 2
shot object: 0.8302897835436791, 2
interactive object: 0.8302897835436791, 2
motion field: 0.8302897835436791, 2
field optical: 0.8302897835436791, 2
bayesian inference: 0.8302897835436791, 2
learning temporal: 0.8302897835436791, 2
learning feature: 0.8302897835436791, 2
interact each: 0.8302897835436791, 2
focus attention: 0.8302897835436791, 2
video generation: 0.8302897835436791, 2
motion invariance: 0.8302897835436791, 2
task survey: 0.8302897835436791, 2
image processing: 0.8302897835436791, 2
macs approach: 0.8302897835436791, 2
affordance inspired: 0.8302897835436791, 2
tool behavior: 0.8302897835436791, 2
affordance icub: 0.8302897835436791, 2
icub humanoid: 0.8302897835436791, 2
affordance network: 0.8302897835436791, 2
perception developmental: 0.8302897835436791, 2
developmental learning: 0.8302897835436791, 2
developmental robot: 0.8302897835436791, 2
bio inspired: 0.8302897835436791, 2
discriminative model: 0.8302897835436791, 2
model grasp: 0.8302897835436791, 2
affordance self: 0.8302897835436791, 2
unknown object: 0.8302897835436791, 2
occluded object: 0.8302897835436791, 2
afford formalization: 0.8302897835436791, 2
formalization affordance: 0.8302897835436791, 2
toward affordance: 0.8302897835436791, 2
behavior affordance: 0.8302897835436791, 2
modeling affordance: 0.8302897835436791, 2
affordance bayesian: 0.8302897835436791, 2
grasp graphical: 0.8302897835436791, 2
based navigation: 0.8302897835436791, 2
synergy based: 0.8302897835436791, 2
feature extraction: 0.8302897835436791, 2
active sensing: 0.8302897835436791, 2
affordance consummatory: 0.8302897835436791, 2
consummatory motivation: 0.8302897835436791, 2
motivation driven: 0.8302897835436791, 2
driven adaptive: 0.8302897835436791, 2
adaptive perception: 0.8302897835436791, 2
embodiment specific: 0.8302897835436791, 2
driven robot: 0.8302897835436791, 2
hierarchical representation: 0.8302897835436791, 2
appearance based: 0.8302897835436791, 2
affordance relation: 0.8302897835436791, 2
line learning: 0.8302897835436791, 2
object grasp: 0.8302897835436791, 2
based imitation: 0.8302897835436791, 2
object feature: 0.8302897835436791, 2
supervised online: 0.8302897835436791, 2
object push: 0.8302897835436791, 2
object sensory: 0.8302897835436791, 2
motor coordination: 0.8302897835436791, 2
coordination imitation: 0.8302897835436791, 2
architecture cycle: 0.8302897835436791, 2
affordance maps: 0.8302897835436791, 2
action perception: 0.8302897835436791, 2
learning artificial: 0.8302897835436791, 2
world object: 0.8302897835436791, 2
affordance demonstration: 0.8302897835436791, 2
denoising auto: 0.8302897835436791, 2
auto encoders: 0.8302897835436791, 2
human instructions: 0.8302897835436791, 2
lifelong affordance: 0.8302897835436791, 2
based computational: 0.8302897835436791, 2
cognitive affordance: 0.8302897835436791, 2
maneuvering affordance: 0.8302897835436791, 2
perception system: 0.8302897835436791, 2
motivated affordance: 0.8302897835436791, 2
discovery modeling: 0.8302897835436791, 2
affordance leveraging: 0.8302897835436791, 2
ecological robot: 0.8302897835436791, 2
building affordance: 0.8302897835436791, 2
affordance framework: 0.8302897835436791, 2
3d environment: 0.8302897835436791, 2
flow based: 0.8302897835436791, 2
based control: 0.8302897835436791, 2
inverse reinforcement: 0.8302897835436791, 2
learning vision: 0.8302897835436791, 2
real data: 0.8302897835436791, 2
based autonomous: 0.8302897835436791, 2
learning dense: 0.8302897835436791, 2
flow approach: 0.8302897835436791, 2
geometric affordance: 0.8302897835436791, 2
interaction tensor: 0.8302897835436791, 2
properties tool: 0.8302897835436791, 2
learning model: 0.8302897835436791, 2
improving accuracy: 0.8302897835436791, 2
future challenge: 0.8302897835436791, 2
research development: 0.8302897835436791, 2
policy optimization: 0.8302897835436791, 2
multiple view: 0.8302897835436791, 2
geometry computer: 0.8302897835436791, 2
depth image: 0.8302897835436791, 2
image synthesis: 0.8302897835436791, 2
frustratingly easy: 0.8302897835436791, 2
easy domain: 0.8302897835436791, 2
learning evaluation: 0.8302897835436791, 2
based rendering: 0.8302897835436791, 2
physical object: 0.8302897835436791, 2
rich 3d: 0.8302897835436791, 2
3d model: 0.8302897835436791, 2
supersizing self: 0.8302897835436791, 2
doom based: 0.8302897835436791, 2
based ai: 0.8302897835436791, 2
ai research: 0.8302897835436791, 2
research platform: 0.8302897835436791, 2
platform visual: 0.8302897835436791, 2
proxy multi: 0.8302897835436791, 2
tracking analysis: 0.8302897835436791, 2
large collection: 0.8302897835436791, 2
driving model: 0.8302897835436791, 2
model large: 0.8302897835436791, 2
scale video: 0.8302897835436791, 2
real image: 0.8302897835436791, 2
2d 3d: 0.8302897835436791, 2
data indoor: 0.8302897835436791, 2
indoor scene: 0.8302897835436791, 2
domain randomization: 0.8302897835436791, 2
randomization transferring: 0.8302897835436791, 2
transferring deep: 0.8302897835436791, 2
network simulate: 0.8302897835436791, 2
image rendering: 0.8302897835436791, 2
stochastic grammars: 0.8302897835436791, 2
learning physical: 0.8302897835436791, 2
end driving: 0.8302897835436791, 2
driving via: 0.8302897835436791, 2
via conditional: 0.8302897835436791, 2
navigation complex: 0.8302897835436791, 2
vr goggles: 0.8302897835436791, 2
goggles real: 0.8302897835436791, 2
real sim: 0.8302897835436791, 2
sim domain: 0.8302897835436791, 2
adaptation visual: 0.8302897835436791, 2
visual control: 0.8302897835436791, 2
learning large: 0.8302897835436791, 2
driven visual: 0.8302897835436791, 2
scene deep: 0.8302897835436791, 2
per pixel: 0.8302897835436791, 2
educational affordance: 0.8302897835436791, 2
games learning: 0.8302897835436791, 2
perception theories: 0.8302897835436791, 2
task analysis: 0.8302897835436791, 2
behavior based: 0.8302897835436791, 2
user interface: 0.8302897835436791, 2
interface affordance: 0.8302897835436791, 2
affordance ecology: 0.8302897835436791, 2
field theory: 0.8302897835436791, 2
action affordance: 0.8302897835436791, 2
human perception: 0.8302897835436791, 2
emotion intuition: 0.8302897835436791, 2
affordance mobile: 0.8302897835436791, 2
crucial role: 0.8302897835436791, 2
affordance interaction: 0.8302897835436791, 2
sensory stimuli: 0.8302897835436791, 2
ecological dynamics: 0.8302897835436791, 2
ecological perspective: 0.8302897835436791, 2
perspective perception: 0.8302897835436791, 2
control action: 0.8302897835436791, 2
based agent: 0.8302897835436791, 2
cognitive niche: 0.8302897835436791, 2
rough fuzzy: 0.8302897835436791, 2
fuzzy data: 0.8302897835436791, 2
mining granular: 0.8302897835436791, 2
developmental framework: 0.8302897835436791, 2
human computer: 0.8302897835436791, 2
object category: 0.8302897835436791, 2
emergency evacuation: 0.8302897835436791, 2
affordance social: 0.8302897835436791, 2
social media: 0.8302897835436791, 2
object object: 0.8302897835436791, 2
field affordance: 0.8302897835436791, 2
understanding based: 0.8302897835436791, 2
design agent: 0.8302897835436791, 2
learning single: 0.8302897835436791, 2
affordance driven: 0.8302897835436791, 2
physical world: 0.8302897835436791, 2
robot object: 0.8302897835436791, 2
feature within: 0.8302897835436791, 2
environmental perception: 0.8302897835436791, 2
grasp point: 0.8302897835436791, 2
network robot: 0.8302897835436791, 2
scene affordance: 0.8302897835436791, 2
object level: 0.8302897835436791, 2
understanding human: 0.8302897835436791, 2
allow predict: 0.8302897835436791, 2
model learn: 0.8302897835436791, 2
model train: 0.8302897835436791, 2
demonstrate train: 0.8302897835436791, 2
train affordance: 0.8302897835436791, 2
conditioned policy: 0.8302897835436791, 2
based current: 0.8302897835436791, 2
outcome action: 0.8302897835436791, 2
action given: 0.8302897835436791, 2
explore richness: 0.8302897835436791, 2
richness information: 0.8302897835436791, 2
information captured: 0.8302897835436791, 2
captured latent: 0.8302897835436791, 2
based performance: 0.8302897835436791, 2
model capture: 0.8302897835436791, 2
factors variation: 0.8302897835436791, 2
imagine tool: 0.8302897835436791, 2
appropriate task: 0.8302897835436791, 2
tool geometries: 0.8302897835436791, 2
targeted deliberate: 0.8302897835436791, 2
robot equipped: 0.8302897835436791, 2
learned skills: 0.8302897835436791, 2
able perform: 0.8302897835436791, 2
many different: 0.8302897835436791, 2
robot encounters: 0.8302897835436791, 2
model still: 0.8302897835436791, 2
possible outcome: 0.8302897835436791, 2
raw image: 0.8302897835436791, 2
object via: 0.8302897835436791, 2
structured latent: 0.8302897835436791, 2
seen during: 0.8302897835436791, 2
knowledge object: 0.8302897835436791, 2
visual model: 0.8302897835436791, 2
analyze effect: 0.8302897835436791, 2
emergent communication: 0.8302897835436791, 2
representation may: 0.8302897835436791, 2
agent perform: 0.8302897835436791, 2
toward achieving: 0.8302897835436791, 2
human centric: 0.8302897835436791, 2
simple yet: 0.8302897835436791, 2
yet powerful: 0.8302897835436791, 2
learn discrete: 0.8302897835436791, 2
method allows: 0.8302897835436791, 2
high quality: 0.8302897835436791, 2
general purpose: 0.8302897835436791, 2
robot system: 0.8302897835436791, 2
system able: 0.8302897835436791, 2
goal feasible: 0.8302897835436791, 2
environment available: 0.8302897835436791, 2
enabling robot: 0.8302897835436791, 2
generalize object: 0.8302897835436791, 2
recent deep: 0.8302897835436791, 2
recognition method: 0.8302897835436791, 2
article provides: 0.8302897835436791, 2
result different: 0.8302897835436791, 2
approach simultaneously: 0.8302897835436791, 2
simultaneously detect: 0.8302897835436791, 2
detect multiple: 0.8302897835436791, 2
rgb affordancenet: 0.8302897835436791, 2
affordancenet object: 0.8302897835436791, 2
branch localize: 0.8302897835436791, 2
localize classify: 0.8302897835436791, 2
classify affordance: 0.8302897835436791, 2
branch assign: 0.8302897835436791, 2
assign each: 0.8302897835436791, 2
each pixel: 0.8302897835436791, 2
pixel object: 0.8302897835436791, 2
object probable: 0.8302897835436791, 2
probable affordance: 0.8302897835436791, 2
affordance propose: 0.8302897835436791, 2
framework employs: 0.8302897835436791, 2
employs three: 0.8302897835436791, 2
three key: 0.8302897835436791, 2
key components: 0.8302897835436791, 2
components effectively: 0.8302897835436791, 2
effectively handling: 0.8302897835436791, 2
handling multiclass: 0.8302897835436791, 2
multiclass problem: 0.8302897835436791, 2
affordance sequence: 0.8302897835436791, 2
sequence deconvolutional: 0.8302897835436791, 2
deconvolutional robust: 0.8302897835436791, 2
robust resizing: 0.8302897835436791, 2
resizing multi: 0.8302897835436791, 2
task loss: 0.8302897835436791, 2
loss experimental: 0.8302897835436791, 2
result public: 0.8302897835436791, 2
public datasets: 0.8302897835436791, 2
datasets affordancenet: 0.8302897835436791, 2
affordancenet outperforms: 0.8302897835436791, 2
method fair: 0.8302897835436791, 2
fair while: 0.8302897835436791, 2
while end: 0.8302897835436791, 2
end architecture: 0.8302897835436791, 2
architecture allows: 0.8302897835436791, 2
allows inference: 0.8302897835436791, 2
inference speed: 0.8302897835436791, 2
speed 150ms: 0.8302897835436791, 2
150ms per: 0.8302897835436791, 2
per makes: 0.8302897835436791, 2
makes affordancenet: 0.8302897835436791, 2
affordancenet suitable: 0.8302897835436791, 2
suitable real: 0.8302897835436791, 2
effectiveness affordancenet: 0.8302897835436791, 2
affordancenet different: 0.8302897835436791, 2
different testing: 0.8302897835436791, 2
testing environment: 0.8302897835436791, 2
environment real: 0.8302897835436791, 2
robot source: 0.8302897835436791, 2
code available: 0.8302897835436791, 2
namely types: 0.8302897835436791, 2
surpassing current: 0.8302897835436791, 2
method evaluated: 0.8302897835436791, 2
result demonstrate: 0.8302897835436791, 2
action agent: 0.8302897835436791, 2
specific object: 0.8302897835436791, 2
object more: 0.8302897835436791, 2
each category: 0.8302897835436791, 2
aspects affordance: 0.8302897835436791, 2
detect object: 0.8302897835436791, 2
input data: 0.8302897835436791, 2
encoder decoder: 0.8302897835436791, 2
order obtain: 0.8302897835436791, 2
learn feature: 0.8302897835436791, 2
full size: 0.8302897835436791, 2
size humanoid: 0.8302897835436791, 2
robot dominating: 0.8302897835436791, 2
dominating healthcare: 0.8302897835436791, 2
healthcare robot: 0.8302897835436791, 2
robot vision: 0.8302897835436791, 2
vision aims: 0.8302897835436791, 2
aims equip: 0.8302897835436791, 2
equip robot: 0.8302897835436791, 2
capabilities discover: 0.8302897835436791, 2
discover understand: 0.8302897835436791, 2
interact require: 0.8302897835436791, 2
require agent: 0.8302897835436791, 2
agent effectively: 0.8302897835436791, 2
effectively understand: 0.8302897835436791, 2
understand object: 0.8302897835436791, 2
affordance functions: 0.8302897835436791, 2
functions complex: 0.8302897835436791, 2
complex visual: 0.8302897835436791, 2
visual literature: 0.8302897835436791, 2
literature focused: 0.8302897835436791, 2
focused current: 0.8302897835436791, 2
art approach: 0.8302897835436791, 2
solving relevant: 0.8302897835436791, 2
relevant problem: 0.8302897835436791, 2
problem open: 0.8302897835436791, 2
open problem: 0.8302897835436791, 2
problem research: 0.8302897835436791, 2
research gaps: 0.8302897835436791, 2
gaps sub: 0.8302897835436791, 2
sub affordance: 0.8302897835436791, 2
affordance high: 0.8302897835436791, 2
specifically functional: 0.8302897835436791, 2
understanding prevalent: 0.8302897835436791, 2
prevalent descriptors: 0.8302897835436791, 2
descriptors literature: 0.8302897835436791, 2
literature survey: 0.8302897835436791, 2
survey provides: 0.8302897835436791, 2
provides necessary: 0.8302897835436791, 2
necessary background: 0.8302897835436791, 2
background sheds: 0.8302897835436791, 2
sheds light: 0.8302897835436791, 2
light highlights: 0.8302897835436791, 2
highlights existing: 0.8302897835436791, 2
existing challenge: 0.8302897835436791, 2
challenge affordance: 0.8302897835436791, 2
affordance functionality: 0.8302897835436791, 2
extension existing: 0.8302897835436791, 2
existing functional: 0.8302897835436791, 2
functional views: 0.8302897835436791, 2
views visual: 0.8302897835436791, 2
importance learning: 0.8302897835436791, 2
perception cueing: 0.8302897835436791, 2
cueing anticipation: 0.8302897835436791, 2
anticipation opportunities: 0.8302897835436791, 2
opportunities interaction: 0.8302897835436791, 2
interaction robot: 0.8302897835436791, 2
originally defined: 0.8302897835436791, 2
defined representational: 0.8302897835436791, 2
representational concept: 0.8302897835436791, 2
concept perception: 0.8302897835436791, 2
terms either: 0.8302897835436791, 2
either optical: 0.8302897835436791, 2
flow heuristically: 0.8302897835436791, 2
heuristically determined: 0.8302897835436791, 2
determined 3d: 0.8302897835436791, 2
feature perception: 0.8302897835436791, 2
perception entities: 0.8302897835436791, 2
entities generalized: 0.8302897835436791, 2
feature context: 0.8302897835436791, 2
context demonstrate: 0.8302897835436791, 2
demonstrate learning: 0.8302897835436791, 2
learning causal: 0.8302897835436791, 2
causal relationships: 0.8302897835436791, 2
relationships visual: 0.8302897835436791, 2
framework cueing: 0.8302897835436791, 2
play important: 0.8302897835436791, 2
important role: 0.8302897835436791, 2
role future: 0.8302897835436791, 2
future robot: 0.8302897835436791, 2
perception enable: 0.8302897835436791, 2
enable systems: 0.8302897835436791, 2
systems react: 0.8302897835436791, 2
react environment: 0.8302897835436791, 2
environment stimuli: 0.8302897835436791, 2
stimuli more: 0.8302897835436791, 2
more efficient: 0.8302897835436791, 2
provide potential: 0.8302897835436791, 2
potential plan: 0.8302897835436791, 2
plan basis: 0.8302897835436791, 2
responses more: 0.8302897835436791, 2
complex perception: 0.8302897835436791, 2
perception verify: 0.8302897835436791, 2
verify concept: 0.8302897835436791, 2
concept concrete: 0.8302897835436791, 2
concrete implementation: 0.8302897835436791, 2
applying state: 0.8302897835436791, 2
descriptors regions: 0.8302897835436791, 2
regions interest: 0.8302897835436791, 2
robot scenario: 0.8302897835436791, 2
scenario prove: 0.8302897835436791, 2
prove feature: 0.8302897835436791, 2
feature successfully: 0.8302897835436791, 2
successfully selected: 0.8302897835436791, 2
predict opportunities: 0.8302897835436791, 2
opportunities robot: 0.8302897835436791, 2
object detector: 0.8302897835436791, 2
propose approach: 0.8302897835436791, 2
approach outperforms: 0.8302897835436791, 2
field computer: 0.8302897835436791, 2
data during: 0.8302897835436791, 2
provide comprehensive: 0.8302897835436791, 2
ai systems: 0.8302897835436791, 2
propose trustworthy: 0.8302897835436791, 2
trustworthy framework: 0.8302897835436791, 2
dynamic interaction: 0.8302897835436791, 2
small number: 0.8302897835436791, 2
robot more: 0.8302897835436791, 2
more extensively: 0.8302897835436791, 2
extensively assistants: 0.8302897835436791, 2
assistants home: 0.8302897835436791, 2
home scenarios: 0.8302897835436791, 2
scenarios able: 0.8302897835436791, 2
able acquire: 0.8302897835436791, 2
acquire expertise: 0.8302897835436791, 2
expertise trainers: 0.8302897835436791, 2
trainers learning: 0.8302897835436791, 2
learning crossmodal: 0.8302897835436791, 2
crossmodal promising: 0.8302897835436791, 2
promising approach: 0.8302897835436791, 2
approach interactive: 0.8302897835436791, 2
learning external: 0.8302897835436791, 2
external trainer: 0.8302897835436791, 2
trainer advises: 0.8302897835436791, 2
advises apprentice: 0.8302897835436791, 2
apprentice action: 0.8302897835436791, 2
action speed: 0.8302897835436791, 2
speed learning: 0.8302897835436791, 2
learning paper: 0.8302897835436791, 2
present irl: 0.8302897835436791, 2
irl approach: 0.8302897835436791, 2
approach domestic: 0.8302897835436791, 2
domestic task: 0.8302897835436791, 2
task cleaning: 0.8302897835436791, 2
cleaning table: 0.8302897835436791, 2
table compare: 0.8302897835436791, 2
different learning: 0.8302897835436791, 2
method simulate: 0.8302897835436791, 2
simulate reinforcement: 0.8302897835436791, 2
learning rl: 0.8302897835436791, 2
affordance avoid: 0.8302897835436791, 2
avoid failed: 0.8302897835436791, 2
failed previously: 0.8302897835436791, 2
previously train: 0.8302897835436791, 2
train robot: 0.8302897835436791, 2
robot serving: 0.8302897835436791, 2
serving trainer: 0.8302897835436791, 2
trainer second: 0.8302897835436791, 2
second apprentice: 0.8302897835436791, 2
apprentice demonstrate: 0.8302897835436791, 2
demonstrate irl: 0.8302897835436791, 2
irl leads: 0.8302897835436791, 2
leads different: 0.8302897835436791, 2
different performance: 0.8302897835436791, 2
performance various: 0.8302897835436791, 2
various levels: 0.8302897835436791, 2
levels interaction: 0.8302897835436791, 2
interaction consistency: 0.8302897835436791, 2
consistency result: 0.8302897835436791, 2
result simulate: 0.8302897835436791, 2
robot completes: 0.8302897835436791, 2
completes task: 0.8302897835436791, 2
task although: 0.8302897835436791, 2
although working: 0.8302897835436791, 2
working slowly: 0.8302897835436791, 2
slowly low: 0.8302897835436791, 2
low rate: 0.8302897835436791, 2
rate rl: 0.8302897835436791, 2
affordance fewer: 0.8302897835436791, 2
fewer action: 0.8302897835436791, 2
action needed: 0.8302897835436791, 2
needed reach: 0.8302897835436791, 2
reach higher: 0.8302897835436791, 2
higher rates: 0.8302897835436791, 2
rates performance: 0.8302897835436791, 2
performance irl: 0.8302897835436791, 2
irl essential: 0.8302897835436791, 2
essential consider: 0.8302897835436791, 2
consider level: 0.8302897835436791, 2
level consistency: 0.8302897835436791, 2
consistency feedback: 0.8302897835436791, 2
feedback since: 0.8302897835436791, 2
since inconsistencies: 0.8302897835436791, 2
inconsistencies cause: 0.8302897835436791, 2
cause considerable: 0.8302897835436791, 2
considerable delay: 0.8302897835436791, 2
delay learning: 0.8302897835436791, 2
learning demonstrate: 0.8302897835436791, 2
demonstrate interactive: 0.8302897835436791, 2
interactive feedback: 0.8302897835436791, 2
feedback provides: 0.8302897835436791, 2
provides advantage: 0.8302897835436791, 2
advantage robot: 0.8302897835436791, 2
existing approach: 0.8302897835436791, 2
recently propose: 0.8302897835436791, 2
third direct: 0.8302897835436791, 2
low dimensional: 0.8302897835436791, 2
traffic lights: 0.8302897835436791, 2
present approach: 0.8302897835436791, 2
interaction taking: 0.8302897835436791, 2
interaction learned: 0.8302897835436791, 2
learned unsupervised: 0.8302897835436791, 2
scene interaction: 0.8302897835436791, 2
temporal aspect: 0.8302897835436791, 2
temporal interaction: 0.8302897835436791, 2
level qualitative: 0.8302897835436791, 2
interaction without: 0.8302897835436791, 2
visual data: 0.8302897835436791, 2
sample efficiency: 0.8302897835436791, 2
uses self: 0.8302897835436791, 2
actor critic: 0.8302897835436791, 2
access privileged: 0.8302897835436791, 2
diverse set: 0.8302897835436791, 2
model learning: 0.8302897835436791, 2
down attention: 0.8302897835436791, 2
scene biasing: 0.8302897835436791, 2
state representation: 0.8302897835436791, 2
representation model: 0.8302897835436791, 2
model consists: 0.8302897835436791, 2
visual processing: 0.8302897835436791, 2
visual cognitive: 0.8302897835436791, 2
vdac experiments: 0.8302897835436791, 2
participants completed: 0.8302897835436791, 2
visual search: 0.8302897835436791, 2
search task: 0.8302897835436791, 2
rewarded feature: 0.8302897835436791, 2
evidence vdac: 0.8302897835436791, 2
policy train: 0.8302897835436791, 2
object robot: 0.8302897835436791, 2
robot needs: 0.8302897835436791, 2
lighting hole: 0.8302897835436791, 2
hole surface: 0.8302897835436791, 2
task execution: 0.8302897835436791, 2
control model: 0.8302897835436791, 2
challenging lighting: 0.8302897835436791, 2
model physical: 0.8302897835436791, 2
robot performing: 0.8302897835436791, 2
bottom saliency: 0.8302897835436791, 2
method able: 0.8302897835436791, 2
learn meaningful: 0.8302897835436791, 2
able solve: 0.8302897835436791, 2
desired effect: 0.8302897835436791, 2
target object: 0.8302897835436791, 2
approach able: 0.8302897835436791, 2
ubiquitous robot: 0.8302897835436791, 2
neural architectures: 0.8302897835436791, 2
group transformations: 0.8302897835436791, 2
able handle: 0.8302897835436791, 2
associated optical: 0.8302897835436791, 2
learning understand: 0.8302897835436791, 2
understand infer: 0.8302897835436791, 2
infer object: 0.8302897835436791, 2
object functionalities: 0.8302897835436791, 2
functionalities important: 0.8302897835436791, 2
important step: 0.8302897835436791, 2
towards robust: 0.8302897835436791, 2
visual significant: 0.8302897835436791, 2
significant research: 0.8302897835436791, 2
research efforts: 0.8302897835436791, 2
efforts recently: 0.8302897835436791, 2
recently focused: 0.8302897835436791, 2
focused segmenting: 0.8302897835436791, 2
segmenting object: 0.8302897835436791, 2
parts enable: 0.8302897835436791, 2
enable specific: 0.8302897835436791, 2
specific types: 0.8302897835436791, 2
types human: 0.8302897835436791, 2
object called: 0.8302897835436791, 2
called works: 0.8302897835436791, 2
works treat: 0.8302897835436791, 2
treat static: 0.8302897835436791, 2
static semantic: 0.8302897835436791, 2
segmentation focusing: 0.8302897835436791, 2
focusing solely: 0.8302897835436791, 2
solely object: 0.8302897835436791, 2
appearance relying: 0.8302897835436791, 2
relying strong: 0.8302897835436791, 2
supervision object: 0.8302897835436791, 2
object novel: 0.8302897835436791, 2
approach exploits: 0.8302897835436791, 2
exploits spatio: 0.8302897835436791, 2
temporal nature: 0.8302897835436791, 2
nature human: 0.8302897835436791, 2
affordance design: 0.8302897835436791, 2
design autoencoder: 0.8302897835436791, 2
autoencoder train: 0.8302897835436791, 2
train ground: 0.8302897835436791, 2
truth label: 0.8302897835436791, 2
label last: 0.8302897835436791, 2
last frame: 0.8302897835436791, 2
frame able: 0.8302897835436791, 2
able infer: 0.8302897835436791, 2
infer pixel: 0.8302897835436791, 2
wise affordance: 0.8302897835436791, 2
label video: 0.8302897835436791, 2
video static: 0.8302897835436791, 2
static model: 0.8302897835436791, 2
model surpasses: 0.8302897835436791, 2
surpasses object: 0.8302897835436791, 2
label bounding: 0.8302897835436791, 2
bounding boxes: 0.8302897835436791, 2
boxes soft: 0.8302897835436791, 2
soft attention: 0.8302897835436791, 2
mechanism enables: 0.8302897835436791, 2
enables implicit: 0.8302897835436791, 2
implicit localization: 0.8302897835436791, 2
localization interaction: 0.8302897835436791, 2
interaction evaluation: 0.8302897835436791, 2
evaluation introduce: 0.8302897835436791, 2
introduce sor3d: 0.8302897835436791, 2
aff consists: 0.8302897835436791, 2
consists human: 0.8302897835436791, 2
interaction sequences: 0.8302897835436791, 2
sequences supports: 0.8302897835436791, 2
supports 9: 0.8302897835436791, 2
9 types: 0.8302897835436791, 2
types affordance: 0.8302897835436791, 2
terms pixel: 0.8302897835436791, 2
wise covering: 0.8302897835436791, 2
covering typical: 0.8302897835436791, 2
typical manipulations: 0.8302897835436791, 2
manipulations tool: 0.8302897835436791, 2
tool model: 0.8302897835436791, 2
model achieves: 0.8302897835436791, 2
achieves competitive: 0.8302897835436791, 2
competitive result: 0.8302897835436791, 2
result compared: 0.8302897835436791, 2
compared strongly: 0.8302897835436791, 2
strongly supervised: 0.8302897835436791, 2
method sor3d: 0.8302897835436791, 2
sor3d while: 0.8302897835436791, 2
being able: 0.8302897835436791, 2
affordance similar: 0.8302897835436791, 2
similar unseen: 0.8302897835436791, 2
unseen object: 0.8302897835436791, 2
effect predict: 0.8302897835436791, 2
affordance various: 0.8302897835436791, 2
fundamental challenge: 0.8302897835436791, 2
affordance recognized: 0.8302897835436791, 2
robot since: 0.8302897835436791, 2
epistemic variance: 0.8302897835436791, 2
densely predict: 0.8302897835436791, 2
overall result: 0.8302897835436791, 2
small displacements: 0.8302897835436791, 2
video installation: 0.8302897835436791, 2
mechanical device: 0.8302897835436791, 2
apas robot: 0.8302897835436791, 2
robot arm: 0.8302897835436791, 2
allows robot: 0.8302897835436791, 2
decision affordance: 0.8302897835436791, 2
transition model: 0.8302897835436791, 2
approach learn: 0.8302897835436791, 2
provides information: 0.8302897835436791, 2
task robot: 0.8302897835436791, 2
action possibilities: 0.8302897835436791, 2
different task: 0.8302897835436791, 2
effectiveness approach: 0.8302897835436791, 2
object though: 0.8302897835436791, 2
model generalize: 0.8302897835436791, 2
interaction object: 0.8302897835436791, 2
recognition task: 0.8302897835436791, 2
method online: 0.8302897835436791, 2
object present: 0.8302897835436791, 2
wheeled robot: 0.8302897835436791, 2
robot prototype: 0.8302897835436791, 2
validate propose: 0.8302897835436791, 2
depth sensory: 0.8302897835436791, 2
pan tilt: 0.8302897835436791, 2
tilt telescopic: 0.8302897835436791, 2
telescopic antenna: 0.8302897835436791, 2
object under: 0.8302897835436791, 2
field trials: 0.8302897835436791, 2
trials ability: 0.8302897835436791, 2
ability robot: 0.8302897835436791, 2
learn elements: 0.8302897835436791, 2
elements environment: 0.8302897835436791, 2
mediated perception: 0.8302897835436791, 2
map input: 0.8302897835436791, 2
representation provides: 0.8302897835436791, 2
notion affordance: 0.8302897835436791, 2
hierarchical structure: 0.8302897835436791, 2
basic affordance: 0.8302897835436791, 2
during robot: 0.8302897835436791, 2
robot discovers: 0.8302897835436791, 2
tested real: 0.8302897835436791, 2
dataset includes: 0.8302897835436791, 2
includes 83: 0.8302897835436791, 2
poke stack: 0.8302897835436791, 2
guided intrinsic: 0.8302897835436791, 2
large set: 0.8302897835436791, 2
single rgb: 0.8302897835436791, 2
affordance learned: 0.8302897835436791, 2
approach decomposes: 0.8302897835436791, 2
demonstrate approach: 0.8302897835436791, 2
remarkable progress: 0.8302897835436791, 2
object focus: 0.8302897835436791, 2
prior knowledge: 0.8302897835436791, 2
previously acquired: 0.8302897835436791, 2
challenging problem: 0.8302897835436791, 2
action scene: 0.8302897835436791, 2
labeled examples: 0.8302897835436791, 2
capable learning: 0.8302897835436791, 2
examples human: 0.8302897835436791, 2
representation invariant: 0.8302897835436791, 2
invariant selective: 0.8302897835436791, 2
simple complex: 0.8302897835436791, 2
socially aware: 0.8302897835436791, 2
aware robot: 0.8302897835436791, 2
general framework: 0.8302897835436791, 2
framework based: 0.8302897835436791, 2
based notion: 0.8302897835436791, 2
robot study: 0.8302897835436791, 2
performed current: 0.8302897835436791, 2
current paper: 0.8302897835436791, 2
best action: 0.8302897835436791, 2
achieve desired: 0.8302897835436791, 2
robot systematically: 0.8302897835436791, 2
given robot: 0.8302897835436791, 2
geometrical properties: 0.8302897835436791, 2
acquired knowledge: 0.8302897835436791, 2
estimating grasp: 0.8302897835436791, 2
grasp system: 0.8302897835436791, 2
object traversable: 0.8302897835436791, 2
learning system: 0.8302897835436791, 2
grasp control: 0.8302897835436791, 2
reaching grasp: 0.8302897835436791, 2
probabilistic graphical: 0.8302897835436791, 2
learning effect: 0.8302897835436791, 2
predict tool: 0.8302897835436791, 2
tool pose: 0.8302897835436791, 2
tool grasp: 0.8302897835436791, 2
task learned: 0.8302897835436791, 2
level attributes: 0.8302897835436791, 2
hard task: 0.8302897835436791, 2
stack action: 0.8302897835436791, 2
stabilized sparse: 0.8302897835436791, 2
neural agent: 0.8302897835436791, 2
leveraging: 0.8302312464749014, 4
abilities: 0.8302312464749014, 4
centric: 0.8302312464749014, 4
mutual: 0.8302312464749014, 4
games: 0.8302312464749014, 4
why: 0.8302312464749014, 4
emergence: 0.8302312464749014, 4
issues: 0.8302312464749014, 4
neuroscience: 0.8302312464749014, 4
teaching: 0.8302312464749014, 4
education: 0.8302312464749014, 4
factors: 0.8302312464749014, 4
improving: 0.8302312464749014, 4
relative: 0.8302312464749014, 4
inverse: 0.8302312464749014, 4
poke: 0.8302312464749014, 4
recognize: 0.8302312464749014, 4
james: 0.8302312464749014, 4
fuzzy: 0.8302312464749014, 4
computing: 0.8302312464749014, 4
designing: 0.8302312464749014, 4
content: 0.8302312464749014, 4
domains: 0.8302312464749014, 4
sources: 0.8302312464749014, 4
autoencoder: 0.8302312464749014, 4
investigation: 0.8302312464749014, 4
privileged: 0.8302312464749014, 4
conditions: 0.8302312464749014, 4
estimating: 0.8302312464749014, 4
full: 0.8302312464749014, 4
help: 0.8302312464749014, 4
distinctive: 0.8302312464749014, 4
functionality: 0.8302312464749014, 4
generic: 0.8302312464749014, 4
own: 0.8302312464749014, 4
confidence: 0.8302312464749014, 4
evolution: 0.8302312464749014, 4
computation: 0.8302312464749014, 4
base: 0.8302312464749014, 4
u: 0.8302312464749014, 4
library: 0.8302312464749014, 4
observations: 0.8302312464749014, 4
guidance: 0.8302312464749014, 4
traversability: 0.8302312464749014, 4
dynamical: 0.8302312464749014, 4
processes: 0.8302312464749014, 4
central: 0.8302312464749014, 4
evolving: 0.8302312464749014, 4
weakly: 0.8302312464749014, 4
systematic: 0.8302312464749014, 4
execution: 0.8302312464749014, 4
scaling: 0.8302312464749014, 4
proceedings: 0.8302312464749014, 4
optimization: 0.8302312464749014, 4
years: 0.8302312464749014, 4
mask: 0.8302312464749014, 4
brief: 0.8302312464749014, 4
shot: 0.8302312464749014, 4
assessment: 0.8302312464749014, 4
symmetries: 0.8302312464749014, 4
solutions: 0.8302312464749014, 4
media: 0.8302312464749014, 4
energy: 0.8302312464749014, 4
brain: 0.8302312464749014, 4
autonomy: 0.8302312464749014, 4
conceptual: 0.8302312464749014, 4
movements: 0.8302312464749014, 4
optimal: 0.8302312464749014, 4
cross: 0.8302312464749014, 4
modal: 0.8302312464749014, 4
property: 0.8302312464749014, 4
regions: 0.8302312464749014, 4
unknown: 0.8302312464749014, 4
formalization: 0.8302312464749014, 4
graphical: 0.8302312464749014, 4
discretization: 0.8302312464749014, 4
applying: 0.8302312464749014, 4
push: 0.8302312464749014, 4
coordination: 0.8302312464749014, 4
intermediate: 0.8302312464749014, 4
handle: 0.8302312464749014, 4
evolutionary: 0.8302312464749014, 4
combination: 0.8302312464749014, 4
3: 0.8302312464749014, 4
implicit: 0.8302312464749014, 4
gibson: 0.8302312464749014, 4
rates: 0.8302312464749014, 4
12: 0.8302312464749014, 4
progress: 0.8302312464749014, 4
emotion: 0.8302312464749014, 4
simulator: 0.8302312464749014, 4
light: 0.8302312464749014, 4
proxy: 0.8302312464749014, 4
collection: 0.8302312464749014, 4
car: 0.8302312464749014, 4
parallel: 0.8302312464749014, 4
augmented: 0.8302312464749014, 4
emerging: 0.8302312464749014, 4
early: 0.8302312464749014, 4
theories: 0.8302312464749014, 4
theoretical: 0.8302312464749014, 4
working: 0.8302312464749014, 4
material: 0.8302312464749014, 4
another: 0.8302312464749014, 4
engineering: 0.8302312464749014, 4
crucial: 0.8302312464749014, 4
stimuli: 0.8302312464749014, 4
reward: 0.8302312464749014, 4
combined: 0.8302312464749014, 4
plausible: 0.8302312464749014, 4
achieved: 0.8302312464749014, 4
involves: 0.8302312464749014, 4
requires: 0.8302312464749014, 4
always: 0.8302312464749014, 4
still: 0.8302312464749014, 4
investigate: 0.8302312464749014, 4
relate: 0.8302312464749014, 4
characterized: 0.8302312464749014, 4
improvement: 0.8302312464749014, 4
manipulations: 0.8302312464749014, 4
represent: 0.8302312464749014, 4
same: 0.8302312464749014, 4
binary: 0.8302312464749014, 4
main: 0.8302312464749014, 4
nature: 0.8302312464749014, 4
yet: 0.8302312464749014, 4
powerful: 0.8302312464749014, 4
encoder: 0.8302312464749014, 4
ideas: 0.8302312464749014, 4
extensive: 0.8302312464749014, 4
instead: 0.8302312464749014, 4
autonomously: 0.8302312464749014, 4
remarkable: 0.8302312464749014, 4
essential: 0.8302312464749014, 4
branch: 0.8302312464749014, 4
components: 0.8302312464749014, 4
loss: 0.8302312464749014, 4
code: 0.8302312464749014, 4
introduced: 0.8302312464749014, 4
made: 0.8302312464749014, 4
identify: 0.8302312464749014, 4
obtain: 0.8302312464749014, 4
functions: 0.8302312464749014, 4
cueing: 0.8302312464749014, 4
defined: 0.8302312464749014, 4
derived: 0.8302312464749014, 4
capturing: 0.8302312464749014, 4
address: 0.8302312464749014, 4
respect: 0.8302312464749014, 4
exploits: 0.8302312464749014, 4
trainer: 0.8302312464749014, 4
apprentice: 0.8302312464749014, 4
avoid: 0.8302312464749014, 4
failed: 0.8302312464749014, 4
consistency: 0.8302312464749014, 4
rate: 0.8302312464749014, 4
advantage: 0.8302312464749014, 4
define: 0.8302312464749014, 4
abstract: 0.8302312464749014, 4
significantly: 0.8302312464749014, 4
form: 0.8302312464749014, 4
salient: 0.8302312464749014, 4
find: 0.8302312464749014, 4
lighting: 0.8302312464749014, 4
jointly: 0.8302312464749014, 4
strong: 0.8302312464749014, 4
modeled: 0.8302312464749014, 4
thus: 0.8302312464749014, 4
next: 0.8302312464749014, 4
idea: 0.8302312464749014, 4
relying: 0.8302312464749014, 4
aff: 0.8302312464749014, 4
typical: 0.8302312464749014, 4
achieves: 0.8302312464749014, 4
competitive: 0.8302312464749014, 4
required: 0.8302312464749014, 4
estimate: 0.8302312464749014, 4
close: 0.8302312464749014, 4
device: 0.8302312464749014, 4
progressively: 0.8302312464749014, 4
notion: 0.8302312464749014, 4
discovered: 0.8302312464749014, 4
discovers: 0.8302312464749014, 4
leverage: 0.8302312464749014, 4
separate: 0.8302312464749014, 4
acquired: 0.8302312464749014, 4
hard: 0.8302312464749014, 4
geometrical: 0.8302312464749014, 4
guiding: 0.622673434856176, 3
solely: 0.622673434856176, 3
finding: 0.622673434856176, 3
insights: 0.622673434856176, 3
proactive: 0.622673434856176, 3
past: 0.622673434856176, 3
less: 0.622673434856176, 3
entropy: 0.622673434856176, 3
auto: 0.622673434856176, 3
intrinsically: 0.622673434856176, 3
primitives: 0.622673434856176, 3
game: 0.622673434856176, 3
variables: 0.622673434856176, 3
curiosity: 0.622673434856176, 3
covering: 0.622673434856176, 3
patterns: 0.622673434856176, 3
sense: 0.622673434856176, 3
agency: 0.622673434856176, 3
mental: 0.622673434856176, 3
technological: 0.622673434856176, 3
soft: 0.622673434856176, 3
assembled: 0.622673434856176, 3
enhancing: 0.622673434856176, 3
fair: 0.622673434856176, 3
empirical: 0.622673434856176, 3
proprioceptive: 0.622673434856176, 3
variable: 0.622673434856176, 3
shared: 0.622673434856176, 3
programs: 0.622673434856176, 3
behavioural: 0.622673434856176, 3
plus: 0.622673434856176, 3
measure: 0.622673434856176, 3
rethinking: 0.622673434856176, 3
sciences: 0.622673434856176, 3
identification: 0.622673434856176, 3
animal: 0.622673434856176, 3
labeling: 0.622673434856176, 3
clouds: 0.622673434856176, 3
hidden: 0.622673434856176, 3
outline: 0.622673434856176, 3
whole: 0.622673434856176, 3
configurations: 0.622673434856176, 3
database: 0.622673434856176, 3
tactile: 0.622673434856176, 3
hybrid: 0.622673434856176, 3
history: 0.622673434856176, 3
placement: 0.622673434856176, 3
structuring: 0.622673434856176, 3
automation: 0.622673434856176, 3
advanced: 0.622673434856176, 3
icra: 0.622673434856176, 3
long: 0.622673434856176, 3
term: 0.622673434856176, 3
anticipating: 0.622673434856176, 3
response: 0.622673434856176, 3
stochastic: 0.622673434856176, 3
classes: 0.622673434856176, 3
metrics: 0.622673434856176, 3
proposal: 0.622673434856176, 3
fixations: 0.622673434856176, 3
r: 0.622673434856176, 3
region: 0.622673434856176, 3
semi: 0.622673434856176, 3
regression: 0.622673434856176, 3
correlation: 0.622673434856176, 3
metric: 0.622673434856176, 3
detector: 0.622673434856176, 3
generalized: 0.622673434856176, 3
kernel: 0.622673434856176, 3
foundations: 0.622673434856176, 3
coding: 0.622673434856176, 3
classical: 0.622673434856176, 3
population: 0.622673434856176, 3
domestic: 0.622673434856176, 3
strategy: 0.622673434856176, 3
consequences: 0.622673434856176, 3
structural: 0.622673434856176, 3
pushing: 0.622673434856176, 3
synergy: 0.622673434856176, 3
sensing: 0.622673434856176, 3
experiences: 0.622673434856176, 3
embodiment: 0.622673434856176, 3
centered: 0.622673434856176, 3
grasps: 0.622673434856176, 3
encoders: 0.622673434856176, 3
instructions: 0.622673434856176, 3
naturally: 0.622673434856176, 3
flight: 0.622673434856176, 3
acting: 0.622673434856176, 3
surface: 0.622673434856176, 3
causal: 0.622673434856176, 3
relationship: 0.622673434856176, 3
household: 0.622673434856176, 3
imaging: 0.622673434856176, 3
statistical: 0.622673434856176, 3
post: 0.622673434856176, 3
critique: 0.622673434856176, 3
six: 0.622673434856176, 3
vehicle: 0.622673434856176, 3
test: 0.622673434856176, 3
phase: 0.622673434856176, 3
trials: 0.622673434856176, 3
change: 0.622673434856176, 3
transferring: 0.622673434856176, 3
unlabeled: 0.622673434856176, 3
10: 0.622673434856176, 3
educational: 0.622673434856176, 3
background: 0.622673434856176, 3
service: 0.622673434856176, 3
epistemic: 0.622673434856176, 3
characteristics: 0.622673434856176, 3
skill: 0.622673434856176, 3
interfaces: 0.622673434856176, 3
controlled: 0.622673434856176, 3
later: 0.622673434856176, 3
expertise: 0.622673434856176, 3
interface: 0.622673434856176, 3
analyses: 0.622673434856176, 3
unfamiliar: 0.622673434856176, 3
smart: 0.622673434856176, 3
points: 0.622673434856176, 3
differences: 0.622673434856176, 3
meaning: 0.622673434856176, 3
sport: 0.622673434856176, 3
niche: 0.622673434856176, 3
last: 0.622673434856176, 3
responses: 0.622673434856176, 3
camera: 0.622673434856176, 3
error: 0.622673434856176, 3
size: 0.622673434856176, 3
embedding: 0.622673434856176, 3
exhibit: 0.622673434856176, 3
automatically: 0.622673434856176, 3
explored: 0.622673434856176, 3
illustrate: 0.622673434856176, 3
captured: 0.622673434856176, 3
sub: 0.622673434856176, 3
utility: 0.622673434856176, 3
encoded: 0.622673434856176, 3
smooth: 0.622673434856176, 3
manipulate: 0.622673434856176, 3
equipped: 0.622673434856176, 3
operate: 0.622673434856176, 3
raw: 0.622673434856176, 3
employs: 0.622673434856176, 3
implicitly: 0.622673434856176, 3
therefore: 0.622673434856176, 3
shaped: 0.622673434856176, 3
ptlms: 0.622673434856176, 3
explores: 0.622673434856176, 3
implemented: 0.622673434856176, 3
apply: 0.622673434856176, 3
facilitate: 0.622673434856176, 3
specialized: 0.622673434856176, 3
experiment: 0.622673434856176, 3
complexity: 0.622673434856176, 3
rather: 0.622673434856176, 3
purpose: 0.622673434856176, 3
question: 0.622673434856176, 3
enabling: 0.622673434856176, 3
five: 0.622673434856176, 3
handling: 0.622673434856176, 3
sequence: 0.622673434856176, 3
public: 0.622673434856176, 3
testing: 0.622673434856176, 3
source: 0.622673434856176, 3
called: 0.622673434856176, 3
fact: 0.622673434856176, 3
possibilities: 0.622673434856176, 3
short: 0.622673434856176, 3
details: 0.622673434856176, 3
trains: 0.622673434856176, 3
represented: 0.622673434856176, 3
sets: 0.622673434856176, 3
gaps: 0.622673434856176, 3
prevalent: 0.622673434856176, 3
views: 0.622673434856176, 3
either: 0.622673434856176, 3
plan: 0.622673434856176, 3
interest: 0.622673434856176, 3
prove: 0.622673434856176, 3
substantial: 0.622673434856176, 3
heavily: 0.622673434856176, 3
depends: 0.622673434856176, 3
deployed: 0.622673434856176, 3
widely: 0.622673434856176, 3
upon: 0.622673434856176, 3
external: 0.622673434856176, 3
considerable: 0.622673434856176, 3
third: 0.622673434856176, 3
traffic: 0.622673434856176, 3
generalisation: 0.622673434856176, 3
depending: 0.622673434856176, 3
independent: 0.622673434856176, 3
aspect: 0.622673434856176, 3
especially: 0.622673434856176, 3
improved: 0.622673434856176, 3
down: 0.622673434856176, 3
location: 0.622673434856176, 3
developed: 0.622673434856176, 3
completed: 0.622673434856176, 3
expected: 0.622673434856176, 3
efforts: 0.622673434856176, 3
requirements: 0.622673434856176, 3
due: 0.622673434856176, 3
solve: 0.622673434856176, 3
cannot: 0.622673434856176, 3
created: 0.622673434856176, 3
far: 0.622673434856176, 3
integrated: 0.622673434856176, 3
recording: 0.622673434856176, 3
5: 0.622673434856176, 3
margin: 0.622673434856176, 3
ubiquitous: 0.622673434856176, 3
built: 0.622673434856176, 3
generated: 0.622673434856176, 3
segmenting: 0.622673434856176, 3
works: 0.622673434856176, 3
treat: 0.622673434856176, 3
surpasses: 0.622673434856176, 3
sequences: 0.622673434856176, 3
cnns: 0.622673434856176, 3
shown: 0.622673434856176, 3
recognized: 0.622673434856176, 3
additional: 0.622673434856176, 3
classifier: 0.622673434856176, 3
variance: 0.622673434856176, 3
finer: 0.622673434856176, 3
estimates: 0.622673434856176, 3
displacement: 0.622673434856176, 3
computed: 0.622673434856176, 3
major: 0.622673434856176, 3
mechanical: 0.622673434856176, 3
apparatus: 0.622673434856176, 3
4: 0.622673434856176, 3
calculator: 0.622673434856176, 3
arm: 0.622673434856176, 3
benefit: 0.622673434856176, 3
incrementally: 0.622673434856176, 3
traversable: 0.622673434856176, 3
antenna: 0.622673434856176, 3
elements: 0.622673434856176, 3
compact: 0.622673434856176, 3
tested: 0.622673434856176, 3
stack: 0.622673434856176, 3
emerged: 0.622673434856176, 3
accurate: 0.622673434856176, 3
annotated: 0.622673434856176, 3
continuously: 0.622673434856176, 3
observing: 0.622673434856176, 3
infant: 0.622673434856176, 3
frames: 0.622673434856176, 3
compute: 0.622673434856176, 3
configuration: 0.622673434856176, 3
predict guiding: 0.41514489177183955, 1
guiding robot: 0.41514489177183955, 1
affordance imagine: 0.41514489177183955, 1
imagine large: 0.41514489177183955, 1
based goal: 0.41514489177183955, 1
imagine embodied: 0.41514489177183955, 1
ai agent: 0.41514489177183955, 1
learning anticipate: 0.41514489177183955, 1
anticipate egocentric: 0.41514489177183955, 1
egocentric action: 0.41514489177183955, 1
action imagine: 0.41514489177183955, 1
i learning: 0.41514489177183955, 1
learning skills: 0.41514489177183955, 1
skills imagining: 0.41514489177183955, 1
imagining visual: 0.41514489177183955, 1
synthesis reaching: 0.41514489177183955, 1
probing object: 0.41514489177183955, 1
predict abilities: 0.41514489177183955, 1
abilities language: 0.41514489177183955, 1
model solely: 0.41514489177183955, 1
solely text: 0.41514489177183955, 1
based efficient: 0.41514489177183955, 1
framework collective: 0.41514489177183955, 1
collective perception: 0.41514489177183955, 1
agent active: 0.41514489177183955, 1
learning pomdp: 0.41514489177183955, 1
perception multi: 0.41514489177183955, 1
agent communication: 0.41514489177183955, 1
communication games: 0.41514489177183955, 1
influencing interaction: 0.41514489177183955, 1
interaction priming: 0.41514489177183955, 1
priming beliefs: 0.41514489177183955, 1
beliefs ai: 0.41514489177183955, 1
ai increase: 0.41514489177183955, 1
increase perception: 0.41514489177183955, 1
perception empathy: 0.41514489177183955, 1
empathy effectiveness: 0.41514489177183955, 1
why study: 0.41514489177183955, 1
study impact: 0.41514489177183955, 1
impact perception: 0.41514489177183955, 1
perception language: 0.41514489177183955, 1
emergence artificial: 0.41514489177183955, 1
interaction algorithm: 0.41514489177183955, 1
algorithm effect: 0.41514489177183955, 1
effect human: 0.41514489177183955, 1
human experience: 0.41514489177183955, 1
experience reinforcement: 0.41514489177183955, 1
detection scene: 0.41514489177183955, 1
scene perception: 0.41514489177183955, 1
perception self: 0.41514489177183955, 1
self driving: 0.41514489177183955, 1
driving open: 0.41514489177183955, 1
open issues: 0.41514489177183955, 1
research autonomous: 0.41514489177183955, 1
driving decision: 0.41514489177183955, 1
making strategies: 0.41514489177183955, 1
strategies based: 0.41514489177183955, 1
finding visualizing: 0.41514489177183955, 1
visualizing weaknesses: 0.41514489177183955, 1
weaknesses deep: 0.41514489177183955, 1
simulate based: 0.41514489177183955, 1
learning real: 0.41514489177183955, 1
world autonomous: 0.41514489177183955, 1
whatever predict: 0.41514489177183955, 1
predict situated: 0.41514489177183955, 1
situated future: 0.41514489177183955, 1
future cognitive: 0.41514489177183955, 1
disjuncture difference: 0.41514489177183955, 1
difference global: 0.41514489177183955, 1
global cultural: 0.41514489177183955, 1
cultural economy: 0.41514489177183955, 1
principal thin: 0.41514489177183955, 1
thin plate: 0.41514489177183955, 1
plate splines: 0.41514489177183955, 1
splines decomposition: 0.41514489177183955, 1
decomposition deformations: 0.41514489177183955, 1
neuroscience inspired: 0.41514489177183955, 1
inspired artificial: 0.41514489177183955, 1
hippocampus insights: 0.41514489177183955, 1
insights spatial: 0.41514489177183955, 1
spatial processing: 0.41514489177183955, 1
proactive analogies: 0.41514489177183955, 1
analogies associations: 0.41514489177183955, 1
associations generate: 0.41514489177183955, 1
generate predict: 0.41514489177183955, 1
exploring impact: 0.41514489177183955, 1
impact artificial: 0.41514489177183955, 1
intelligence teaching: 0.41514489177183955, 1
teaching learning: 0.41514489177183955, 1
learning higher: 0.41514489177183955, 1
higher education: 0.41514489177183955, 1
stalking elusive: 0.41514489177183955, 1
bayesian account: 0.41514489177183955, 1
neuroscience constructive: 0.41514489177183955, 1
constructive remembering: 0.41514489177183955, 1
remembering past: 0.41514489177183955, 1
past imagining: 0.41514489177183955, 1
imagining future: 0.41514489177183955, 1
towards understanding: 0.41514489177183955, 1
understanding interplay: 0.41514489177183955, 1
interplay generative: 0.41514489177183955, 1
generative artificial: 0.41514489177183955, 1
intelligence internet: 0.41514489177183955, 1
comprehensive review: 0.41514489177183955, 1
review latest: 0.41514489177183955, 1
latest advancements: 0.41514489177183955, 1
advancements large: 0.41514489177183955, 1
large generative: 0.41514489177183955, 1
generative ai: 0.41514489177183955, 1
tc uncovering: 0.41514489177183955, 1
uncovering distribution: 0.41514489177183955, 1
distribution data: 0.41514489177183955, 1
data generative: 0.41514489177183955, 1
generative factors: 0.41514489177183955, 1
generative approach: 0.41514489177183955, 1
approach zsar: 0.41514489177183955, 1
zsar right: 0.41514489177183955, 1
improving image: 0.41514489177183955, 1
human interaction: 0.41514489177183955, 1
dual teacher: 0.41514489177183955, 1
teacher class: 0.41514489177183955, 1
class incremental: 0.41514489177183955, 1
incremental learning: 0.41514489177183955, 1
learning data: 0.41514489177183955, 1
data free: 0.41514489177183955, 1
free generative: 0.41514489177183955, 1
generative replay: 0.41514489177183955, 1
model person: 0.41514489177183955, 1
person re: 0.41514489177183955, 1
re review: 0.41514489177183955, 1
deep dexterous: 0.41514489177183955, 1
dexterous grasp: 0.41514489177183955, 1
object single: 0.41514489177183955, 1
generative encodings: 0.41514489177183955, 1
encodings fare: 0.41514489177183955, 1
fare less: 0.41514489177183955, 1
less regular: 0.41514489177183955, 1
regular problem: 0.41514489177183955, 1
relative entropy: 0.41514489177183955, 1
entropy policy: 0.41514489177183955, 1
playing atari: 0.41514489177183955, 1
atari deep: 0.41514489177183955, 1
auto encoding: 0.41514489177183955, 1
encoding variational: 0.41514489177183955, 1
variational bayes: 0.41514489177183955, 1
biped dynamic: 0.41514489177183955, 1
dynamic walking: 0.41514489177183955, 1
walking reinforcement: 0.41514489177183955, 1
active learning: 0.41514489177183955, 1
learning inverse: 0.41514489177183955, 1
inverse model: 0.41514489177183955, 1
model intrinsically: 0.41514489177183955, 1
motivated goal: 0.41514489177183955, 1
goal exploration: 0.41514489177183955, 1
exploration robot: 0.41514489177183955, 1
based data: 0.41514489177183955, 1
efficient approach: 0.41514489177183955, 1
approach policy: 0.41514489177183955, 1
fast quadrupedal: 0.41514489177183955, 1
quadrupedal locomotion: 0.41514489177183955, 1
imagenet classification: 0.41514489177183955, 1
classification deep: 0.41514489177183955, 1
search motor: 0.41514489177183955, 1
motor primitives: 0.41514489177183955, 1
structured output: 0.41514489177183955, 1
output representation: 0.41514489177183955, 1
representation deep: 0.41514489177183955, 1
deep conditional: 0.41514489177183955, 1
conditional generative: 0.41514489177183955, 1
mastering game: 0.41514489177183955, 1
game deep: 0.41514489177183955, 1
network tree: 0.41514489177183955, 1
tree search: 0.41514489177183955, 1
conditional image: 0.41514489177183955, 1
image generation: 0.41514489177183955, 1
generation pixelcnn: 0.41514489177183955, 1
pixelcnn decoders: 0.41514489177183955, 1
fast reinforcement: 0.41514489177183955, 1
via slow: 0.41514489177183955, 1
slow reinforcement: 0.41514489177183955, 1
model agnostic: 0.41514489177183955, 1
agnostic meta: 0.41514489177183955, 1
meta learning: 0.41514489177183955, 1
fast adaptation: 0.41514489177183955, 1
adaptation deep: 0.41514489177183955, 1
automatic goal: 0.41514489177183955, 1
goal generation: 0.41514489177183955, 1
generation reinforcement: 0.41514489177183955, 1
information theoretic: 0.41514489177183955, 1
theoretic mpc: 0.41514489177183955, 1
mpc model: 0.41514489177183955, 1
efficient hierarchical: 0.41514489177183955, 1
hierarchical reinforcement: 0.41514489177183955, 1
learning imagined: 0.41514489177183955, 1
deep bidirectional: 0.41514489177183955, 1
bidirectional transformers: 0.41514489177183955, 1
transformers language: 0.41514489177183955, 1
unsupervised control: 0.41514489177183955, 1
control non: 0.41514489177183955, 1
non parametric: 0.41514489177183955, 1
parametric discriminative: 0.41514489177183955, 1
discriminative rewards: 0.41514489177183955, 1
efficient off: 0.41514489177183955, 1
policy meta: 0.41514489177183955, 1
meta reinforcement: 0.41514489177183955, 1
via probabilistic: 0.41514489177183955, 1
probabilistic context: 0.41514489177183955, 1
context variables: 0.41514489177183955, 1
adversarial feature: 0.41514489177183955, 1
feature learning: 0.41514489177183955, 1
variational information: 0.41514489177183955, 1
information maximizing: 0.41514489177183955, 1
maximizing exploration: 0.41514489177183955, 1
neural discrete: 0.41514489177183955, 1
discrete representation: 0.41514489177183955, 1
hindsight experience: 0.41514489177183955, 1
experience replay: 0.41514489177183955, 1
learning poke: 0.41514489177183955, 1
poke experiential: 0.41514489177183955, 1
experiential learning: 0.41514489177183955, 1
learning intuitive: 0.41514489177183955, 1
intuitive physics: 0.41514489177183955, 1
learning latent: 0.41514489177183955, 1
latent plans: 0.41514489177183955, 1
plans play: 0.41514489177183955, 1
contextual imagined: 0.41514489177183955, 1
goal self: 0.41514489177183955, 1
accelerating online: 0.41514489177183955, 1
online reinforcement: 0.41514489177183955, 1
learning offline: 0.41514489177183955, 1
offline datasets: 0.41514489177183955, 1
skew state: 0.41514489177183955, 1
state covering: 0.41514489177183955, 1
covering self: 0.41514489177183955, 1
supervised reinforcement: 0.41514489177183955, 1
adversarial nets: 0.41514489177183955, 1
universal value: 0.41514489177183955, 1
value function: 0.41514489177183955, 1
function approximators: 0.41514489177183955, 1
incentivizing exploration: 0.41514489177183955, 1
exploration reinforcement: 0.41514489177183955, 1
deep predict: 0.41514489177183955, 1
experience education: 0.41514489177183955, 1
mahalanobis distance: 0.41514489177183955, 1
distance based: 0.41514489177183955, 1
based classifiers: 0.41514489177183955, 1
classifiers able: 0.41514489177183955, 1
recognize eeg: 0.41514489177183955, 1
eeg patterns: 0.41514489177183955, 1
patterns few: 0.41514489177183955, 1
few eeg: 0.41514489177183955, 1
eeg electrodes: 0.41514489177183955, 1
historical imagine: 0.41514489177183955, 1
imagine nineteenth: 0.41514489177183955, 1
nineteenth century: 0.41514489177183955, 1
techniques theory: 0.41514489177183955, 1
theory delinquency: 0.41514489177183955, 1
temporality landscape: 0.41514489177183955, 1
ralph james: 0.41514489177183955, 1
james liberal: 0.41514489177183955, 1
liberal imagine: 0.41514489177183955, 1
mobilities paradigm: 0.41514489177183955, 1
crowdsourcing model: 0.41514489177183955, 1
model problem: 0.41514489177183955, 1
problem solving: 0.41514489177183955, 1
many memory: 0.41514489177183955, 1
memory systems: 0.41514489177183955, 1
artificial modern: 0.41514489177183955, 1
modern approach: 0.41514489177183955, 1
adaptive network: 0.41514489177183955, 1
based fuzzy: 0.41514489177183955, 1
fuzzy inference: 0.41514489177183955, 1
inference system: 0.41514489177183955, 1
induction decision: 0.41514489177183955, 1
decision trees: 0.41514489177183955, 1
knowledge making: 0.41514489177183955, 1
making sense: 0.41514489177183955, 1
sense data: 0.41514489177183955, 1
explainable artificial: 0.41514489177183955, 1
intelligence opportunities: 0.41514489177183955, 1
opportunities challenge: 0.41514489177183955, 1
challenge toward: 0.41514489177183955, 1
toward responsible: 0.41514489177183955, 1
responsible ai: 0.41514489177183955, 1
machine prospects: 0.41514489177183955, 1
knowledge discovery: 0.41514489177183955, 1
discovery databases: 0.41514489177183955, 1
performance convergence: 0.41514489177183955, 1
convergence human: 0.41514489177183955, 1
human artificial: 0.41514489177183955, 1
vision autonomic: 0.41514489177183955, 1
autonomic computing: 0.41514489177183955, 1
human agency: 0.41514489177183955, 1
social cognitive: 0.41514489177183955, 1
technology affordance: 0.41514489177183955, 1
being putting: 0.41514489177183955, 1
putting world: 0.41514489177183955, 1
world together: 0.41514489177183955, 1
together again: 0.41514489177183955, 1
motivation systems: 0.41514489177183955, 1
systems autonomous: 0.41514489177183955, 1
autonomous mental: 0.41514489177183955, 1
mental development: 0.41514489177183955, 1
theory framework: 0.41514489177183955, 1
framework designing: 0.41514489177183955, 1
designing constructivist: 0.41514489177183955, 1
constructivist learning: 0.41514489177183955, 1
myth language: 0.41514489177183955, 1
language language: 0.41514489177183955, 1
language diversity: 0.41514489177183955, 1
diversity importance: 0.41514489177183955, 1
importance cognitive: 0.41514489177183955, 1
technological pedagogical: 0.41514489177183955, 1
pedagogical content: 0.41514489177183955, 1
content knowledge: 0.41514489177183955, 1
explanation understanding: 0.41514489177183955, 1
geographic object: 0.41514489177183955, 1
image analysis: 0.41514489177183955, 1
analysis towards: 0.41514489177183955, 1
towards paradigm: 0.41514489177183955, 1
soft assembled: 0.41514489177183955, 1
assembled perception: 0.41514489177183955, 1
enhancing discrete: 0.41514489177183955, 1
discrete choice: 0.41514489177183955, 1
choice model: 0.41514489177183955, 1
model representation: 0.41514489177183955, 1
representation open: 0.41514489177183955, 1
survey trustworthy: 0.41514489177183955, 1
costs benefits: 0.41514489177183955, 1
benefits fair: 0.41514489177183955, 1
fair representation: 0.41514489177183955, 1
joint representation: 0.41514489177183955, 1
top n: 0.41514489177183955, 1
n recommendation: 0.41514489177183955, 1
recommendation heterogeneous: 0.41514489177183955, 1
heterogeneous information: 0.41514489177183955, 1
autoencoder based: 0.41514489177183955, 1
based representation: 0.41514489177183955, 1
learning application: 0.41514489177183955, 1
application intelligent: 0.41514489177183955, 1
intelligent fault: 0.41514489177183955, 1
fault review: 0.41514489177183955, 1
human representation: 0.41514489177183955, 1
multiscale benchmarks: 0.41514489177183955, 1
benchmarks multimodal: 0.41514489177183955, 1
multimodal representation: 0.41514489177183955, 1
empirical investigation: 0.41514489177183955, 1
investigation representation: 0.41514489177183955, 1
guided pre: 0.41514489177183955, 1
train framework: 0.41514489177183955, 1
framework improving: 0.41514489177183955, 1
improving molecular: 0.41514489177183955, 1
molecular representation: 0.41514489177183955, 1
affordance context: 0.41514489177183955, 1
conditional affordance: 0.41514489177183955, 1
driving urban: 0.41514489177183955, 1
reasoning semantic: 0.41514489177183955, 1
semantic relations: 0.41514489177183955, 1
mobile data: 0.41514489177183955, 1
driven language: 0.41514489177183955, 1
language affordance: 0.41514489177183955, 1
categorization affordance: 0.41514489177183955, 1
interaction qualitative: 0.41514489177183955, 1
qualitative spatial: 0.41514489177183955, 1
context deep: 0.41514489177183955, 1
learning design: 0.41514489177183955, 1
persistence value: 0.41514489177183955, 1
capture task: 0.41514489177183955, 1
computational task: 0.41514489177183955, 1
task offloading: 0.41514489177183955, 1
offloading algorithm: 0.41514489177183955, 1
algorithm based: 0.41514489177183955, 1
learning multi: 0.41514489177183955, 1
task dependency: 0.41514489177183955, 1
pay robustifying: 0.41514489177183955, 1
robustifying deep: 0.41514489177183955, 1
policy task: 0.41514489177183955, 1
attention proprioceptive: 0.41514489177183955, 1
learning robust: 0.41514489177183955, 1
robust peg: 0.41514489177183955, 1
task under: 0.41514489177183955, 1
under variable: 0.41514489177183955, 1
variable conditions: 0.41514489177183955, 1
solving tool: 0.41514489177183955, 1
interaction task: 0.41514489177183955, 1
task deep: 0.41514489177183955, 1
task reinforcement: 0.41514489177183955, 1
probabilistic learning: 0.41514489177183955, 1
anticipation architectural: 0.41514489177183955, 1
architectural computational: 0.41514489177183955, 1
computational neurophenomenology: 0.41514489177183955, 1
estimating influencing: 0.41514489177183955, 1
influencing user: 0.41514489177183955, 1
user mental: 0.41514489177183955, 1
mental model: 0.41514489177183955, 1
model perception: 0.41514489177183955, 1
perception capabilities: 0.41514489177183955, 1
affordance matching: 0.41514489177183955, 1
matching shared: 0.41514489177183955, 1
shared information: 0.41514489177183955, 1
information multi: 0.41514489177183955, 1
dynamic full: 0.41514489177183955, 1
full body: 0.41514489177183955, 1
body interaction: 0.41514489177183955, 1
interaction virtual: 0.41514489177183955, 1
virtual reality: 0.41514489177183955, 1
affordance semantic: 0.41514489177183955, 1
robot ontology: 0.41514489177183955, 1
exploring sketching: 0.41514489177183955, 1
sketching robot: 0.41514489177183955, 1
talk designing: 0.41514489177183955, 1
designing dialogue: 0.41514489177183955, 1
dialogue reduce: 0.41514489177183955, 1
reduce user: 0.41514489177183955, 1
user uncertainty: 0.41514489177183955, 1
does help: 0.41514489177183955, 1
help robot: 0.41514489177183955, 1
robot navigate: 0.41514489177183955, 1
navigate call: 0.41514489177183955, 1
call navigability: 0.41514489177183955, 1
purposive qualitative: 0.41514489177183955, 1
qualitative active: 0.41514489177183955, 1
cognition principles: 0.41514489177183955, 1
principles implications: 0.41514489177183955, 1
implications cognitive: 0.41514489177183955, 1
cognitive psychology: 0.41514489177183955, 1
modeling interaction: 0.41514489177183955, 1
interaction primate: 0.41514489177183955, 1
primate control: 0.41514489177183955, 1
motor facilitation: 0.41514489177183955, 1
facilitation following: 0.41514489177183955, 1
following action: 0.41514489177183955, 1
action behavioural: 0.41514489177183955, 1
behavioural study: 0.41514489177183955, 1
study prehensile: 0.41514489177183955, 1
prehensile action: 0.41514489177183955, 1
visual pathways: 0.41514489177183955, 1
pathways object: 0.41514489177183955, 1
object oriented: 0.41514489177183955, 1
oriented action: 0.41514489177183955, 1
object functional: 0.41514489177183955, 1
functional anatomy: 0.41514489177183955, 1
anatomy pet: 0.41514489177183955, 1
action initial: 0.41514489177183955, 1
initial steps: 0.41514489177183955, 1
steps towards: 0.41514489177183955, 1
towards artificial: 0.41514489177183955, 1
artificial cognition: 0.41514489177183955, 1
recognition functional: 0.41514489177183955, 1
functional parts: 0.41514489177183955, 1
distinctive image: 0.41514489177183955, 1
feature scale: 0.41514489177183955, 1
scale invariant: 0.41514489177183955, 1
invariant keypoints: 0.41514489177183955, 1
interactive recognition: 0.41514489177183955, 1
recognition representation: 0.41514489177183955, 1
representation functionality: 0.41514489177183955, 1
based generic: 0.41514489177183955, 1
generic recognition: 0.41514489177183955, 1
learning prospective: 0.41514489177183955, 1
prospective pick: 0.41514489177183955, 1
pick place: 0.41514489177183955, 1
place behavior: 0.41514489177183955, 1
exploratory behavior: 0.41514489177183955, 1
development acquiring: 0.41514489177183955, 1
acquiring knowledge: 0.41514489177183955, 1
cognition reality: 0.41514489177183955, 1
feature own: 0.41514489177183955, 1
own optical: 0.41514489177183955, 1
enhancing effect: 0.41514489177183955, 1
effect recognition: 0.41514489177183955, 1
recognition probabilistic: 0.41514489177183955, 1
reasoning segmentation: 0.41514489177183955, 1
segmentation image: 0.41514489177183955, 1
context based: 0.41514489177183955, 1
segmentation 2d: 0.41514489177183955, 1
image robot: 0.41514489177183955, 1
learning plus: 0.41514489177183955, 1
plus optical: 0.41514489177183955, 1
optical simple: 0.41514489177183955, 1
simple sensitive: 0.41514489177183955, 1
sensitive method: 0.41514489177183955, 1
detect cardioactive: 0.41514489177183955, 1
cardioactive drugs: 0.41514489177183955, 1
spatial statistics: 0.41514489177183955, 1
statistics optical: 0.41514489177183955, 1
flow convolutional: 0.41514489177183955, 1
recurrent optical: 0.41514489177183955, 1
learning particle: 0.41514489177183955, 1
velocimetry data: 0.41514489177183955, 1
measure optical: 0.41514489177183955, 1
leaf movement: 0.41514489177183955, 1
movement based: 0.41514489177183955, 1
based growth: 0.41514489177183955, 1
growth predict: 0.41514489177183955, 1
model optical: 0.41514489177183955, 1
flow analysis: 0.41514489177183955, 1
analysis machine: 0.41514489177183955, 1
learning plant: 0.41514489177183955, 1
plant factory: 0.41514489177183955, 1
scheduling machine: 0.41514489177183955, 1
based flow: 0.41514489177183955, 1
flow detection: 0.41514489177183955, 1
detection packet: 0.41514489177183955, 1
packet switched: 0.41514489177183955, 1
switched optical: 0.41514489177183955, 1
data center: 0.41514489177183955, 1
center network: 0.41514489177183955, 1
i theory: 0.41514489177183955, 1
affordance communication: 0.41514489177183955, 1
communication research: 0.41514489177183955, 1
machine mission: 0.41514489177183955, 1
mission possible: 0.41514489177183955, 1
theory computation: 0.41514489177183955, 1
affordance sciences: 0.41514489177183955, 1
sciences organisms: 0.41514489177183955, 1
interactive systems: 0.41514489177183955, 1
add nested: 0.41514489177183955, 1
nested affordance: 0.41514489177183955, 1
affordance reaching: 0.41514489177183955, 1
reaching perception: 0.41514489177183955, 1
perception complex: 0.41514489177183955, 1
complex particular: 0.41514489177183955, 1
user activity: 0.41514489177183955, 1
activity analysis: 0.41514489177183955, 1
analysis design: 0.41514489177183955, 1
design affordance: 0.41514489177183955, 1
organizational structuration: 0.41514489177183955, 1
structuration theory: 0.41514489177183955, 1
analyzing concept: 0.41514489177183955, 1
affordance affordance: 0.41514489177183955, 1
affordance feature: 0.41514489177183955, 1
feature identification: 0.41514489177183955, 1
identification user: 0.41514489177183955, 1
user observation: 0.41514489177183955, 1
properties animal: 0.41514489177183955, 1
environment system: 0.41514489177183955, 1
characterising descriptions: 0.41514489177183955, 1
descriptions affordance: 0.41514489177183955, 1
perceiving interacting: 0.41514489177183955, 1
interacting model: 0.41514489177183955, 1
model interaction: 0.41514489177183955, 1
affordance broken: 0.41514489177183955, 1
broken affordance: 0.41514489177183955, 1
dynamics affordance: 0.41514489177183955, 1
affordance implications: 0.41514489177183955, 1
implications design: 0.41514489177183955, 1
indoor segmentation: 0.41514489177183955, 1
segmentation support: 0.41514489177183955, 1
support inference: 0.41514489177183955, 1
inference rgbd: 0.41514489177183955, 1
rgbd image: 0.41514489177183955, 1
bottom learning: 0.41514489177183955, 1
effect logical: 0.41514489177183955, 1
logical continuous: 0.41514489177183955, 1
continuous manipulative: 0.41514489177183955, 1
manipulative exploration: 0.41514489177183955, 1
exploration symbolic: 0.41514489177183955, 1
symbolic planning: 0.41514489177183955, 1
detection tool: 0.41514489177183955, 1
tool parts: 0.41514489177183955, 1
parts geometric: 0.41514489177183955, 1
probabilistic ontological: 0.41514489177183955, 1
ontological approach: 0.41514489177183955, 1
robot iros: 0.41514489177183955, 1
iros international: 0.41514489177183955, 1
microsoft common: 0.41514489177183955, 1
common object: 0.41514489177183955, 1
u convolutional: 0.41514489177183955, 1
network biomedical: 0.41514489177183955, 1
biomedical image: 0.41514489177183955, 1
semantic labeling: 0.41514489177183955, 1
clouds object: 0.41514489177183955, 1
learning hidden: 0.41514489177183955, 1
hidden non: 0.41514489177183955, 1
non hidden: 0.41514489177183955, 1
hidden 0: 0.41514489177183955, 1
0 order: 0.41514489177183955, 1
order affordance: 0.41514489177183955, 1
detection real: 0.41514489177183955, 1
real scene: 0.41514489177183955, 1
outline theory: 0.41514489177183955, 1
scene segmentation: 0.41514489177183955, 1
segmentation autonomous: 0.41514489177183955, 1
extracting whole: 0.41514489177183955, 1
body affordance: 0.41514489177183955, 1
affordance multimodal: 0.41514489177183955, 1
multimodal exploration: 0.41514489177183955, 1
refining grasp: 0.41514489177183955, 1
model experience: 0.41514489177183955, 1
affordance goal: 0.41514489177183955, 1
directed task: 0.41514489177183955, 1
joining movement: 0.41514489177183955, 1
movement modified: 0.41514489177183955, 1
modified dynamic: 0.41514489177183955, 1
dynamic movement: 0.41514489177183955, 1
movement primitives: 0.41514489177183955, 1
robot applications: 0.41514489177183955, 1
applications exemplified: 0.41514489177183955, 1
exemplified handwriting: 0.41514489177183955, 1
probabilistic implications: 0.41514489177183955, 1
implications decisions: 0.41514489177183955, 1
decisions action: 0.41514489177183955, 1
toward library: 0.41514489177183955, 1
library manipulation: 0.41514489177183955, 1
action based: 0.41514489177183955, 1
based semantic: 0.41514489177183955, 1
semantic object: 0.41514489177183955, 1
action relations: 0.41514489177183955, 1
learning relational: 0.41514489177183955, 1
robot multi: 0.41514489177183955, 1
different motor: 0.41514489177183955, 1
motor skills: 0.41514489177183955, 1
action cue: 0.41514489177183955, 1
cue single: 0.41514489177183955, 1
single view: 0.41514489177183955, 1
makes chair: 0.41514489177183955, 1
scene geometry: 0.41514489177183955, 1
geometry human: 0.41514489177183955, 1
human workspace: 0.41514489177183955, 1
determining proper: 0.41514489177183955, 1
proper grasp: 0.41514489177183955, 1
grasp configurations: 0.41514489177183955, 1
configurations handovers: 0.41514489177183955, 1
handovers observation: 0.41514489177183955, 1
observation object: 0.41514489177183955, 1
object movement: 0.41514489177183955, 1
movement patterns: 0.41514489177183955, 1
patterns inter: 0.41514489177183955, 1
inter object: 0.41514489177183955, 1
interaction during: 0.41514489177183955, 1
during usage: 0.41514489177183955, 1
object vision: 0.41514489177183955, 1
intention visual: 0.41514489177183955, 1
observations interaction: 0.41514489177183955, 1
distributed reinforcement: 0.41514489177183955, 1
learning coordinate: 0.41514489177183955, 1
coordinate multi: 0.41514489177183955, 1
robot foraging: 0.41514489177183955, 1
moped object: 0.41514489177183955, 1
recognition pose: 0.41514489177183955, 1
estimation manipulation: 0.41514489177183955, 1
perceiving visual: 0.41514489177183955, 1
visual guidance: 0.41514489177183955, 1
guidance stair: 0.41514489177183955, 1
planning probabilistic: 0.41514489177183955, 1
probabilistic inference: 0.41514489177183955, 1
cues self: 0.41514489177183955, 1
exploring affordance: 0.41514489177183955, 1
tool icub: 0.41514489177183955, 1
image mobile: 0.41514489177183955, 1
relations humanoid: 0.41514489177183955, 1
scale hierarchical: 0.41514489177183955, 1
hierarchical image: 0.41514489177183955, 1
movement imitation: 0.41514489177183955, 1
imitation nonlinear: 0.41514489177183955, 1
nonlinear dynamical: 0.41514489177183955, 1
systems humanoid: 0.41514489177183955, 1
appearance feature: 0.41514489177183955, 1
goal emulation: 0.41514489177183955, 1
emulation planning: 0.41514489177183955, 1
planning perception: 0.41514489177183955, 1
perception space: 0.41514489177183955, 1
space learned: 0.41514489177183955, 1
grounded abstractions: 0.41514489177183955, 1
abstractions processes: 0.41514489177183955, 1
multi fingered: 0.41514489177183955, 1
fingered tactile: 0.41514489177183955, 1
tactile exploration: 0.41514489177183955, 1
exploration dynamic: 0.41514489177183955, 1
dynamic potential: 0.41514489177183955, 1
action inferring: 0.41514489177183955, 1
inferring object: 0.41514489177183955, 1
flexible hybrid: 0.41514489177183955, 1
hybrid framework: 0.41514489177183955, 1
framework modeling: 0.41514489177183955, 1
modeling complex: 0.41514489177183955, 1
complex manipulation: 0.41514489177183955, 1
affordance prospective: 0.41514489177183955, 1
prospective outline: 0.41514489177183955, 1
outline ontology: 0.41514489177183955, 1
understanding history: 0.41514489177183955, 1
history contemporary: 0.41514489177183955, 1
contemporary development: 0.41514489177183955, 1
development central: 0.41514489177183955, 1
central concept: 0.41514489177183955, 1
functional object: 0.41514489177183955, 1
class detection: 0.41514489177183955, 1
detection based: 0.41514489177183955, 1
based learned: 0.41514489177183955, 1
affordance cues: 0.41514489177183955, 1
foot placement: 0.41514489177183955, 1
placement selection: 0.41514489177183955, 1
selection non: 0.41514489177183955, 1
non geometric: 0.41514489177183955, 1
geometric visual: 0.41514489177183955, 1
visual properties: 0.41514489177183955, 1
residual learning: 0.41514489177183955, 1
learning image: 0.41514489177183955, 1
validation whole: 0.41514489177183955, 1
body loco: 0.41514489177183955, 1
loco manipulation: 0.41514489177183955, 1
affordance pushability: 0.41514489177183955, 1
pushability liftability: 0.41514489177183955, 1
robot tool: 0.41514489177183955, 1
tool developmental: 0.41514489177183955, 1
autonomous tool: 0.41514489177183955, 1
learning social: 0.41514489177183955, 1
learning refine: 0.41514489177183955, 1
refine object: 0.41514489177183955, 1
object segments: 0.41514489177183955, 1
semantic image: 0.41514489177183955, 1
convolutional atrous: 0.41514489177183955, 1
atrous fully: 0.41514489177183955, 1
fully connected: 0.41514489177183955, 1
connected crfs: 0.41514489177183955, 1
hierarchical nesting: 0.41514489177183955, 1
nesting affordance: 0.41514489177183955, 1
behavioural plasticity: 0.41514489177183955, 1
plasticity evolving: 0.41514489177183955, 1
evolving robot: 0.41514489177183955, 1
scale cnn: 0.41514489177183955, 1
cnn affordance: 0.41514489177183955, 1
bootstrapping relational: 0.41514489177183955, 1
object pairs: 0.41514489177183955, 1
pairs transfer: 0.41514489177183955, 1
2019 international: 0.41514489177183955, 1
pyramid scene: 0.41514489177183955, 1
scene parsing: 0.41514489177183955, 1
parsing network: 0.41514489177183955, 1
systems 2013: 0.41514489177183955, 1
2013 international: 0.41514489177183955, 1
advanced icar: 0.41514489177183955, 1
icar international: 0.41514489177183955, 1
affordance taxonomy: 0.41514489177183955, 1
taxonomy systematic: 0.41514489177183955, 1
systematic classification: 0.41514489177183955, 1
learning segment: 0.41514489177183955, 1
segment affordance: 0.41514489177183955, 1
makes synthetic: 0.41514489177183955, 1
synthetic train: 0.41514489177183955, 1
data learning: 0.41514489177183955, 1
learning disparity: 0.41514489177183955, 1
disparity optical: 0.41514489177183955, 1
effective synthetic: 0.41514489177183955, 1
synthetic data: 0.41514489177183955, 1
data urban: 0.41514489177183955, 1
scene semantic: 0.41514489177183955, 1
automatic differentiation: 0.41514489177183955, 1
differentiation pytorch: 0.41514489177183955, 1
history philosophy: 0.41514489177183955, 1
philosophy ecological: 0.41514489177183955, 1
ex paucis: 0.41514489177183955, 1
paucis learning: 0.41514489177183955, 1
segmentation few: 0.41514489177183955, 1
library implementing: 0.41514489177183955, 1
implementing generic: 0.41514489177183955, 1
generic robot: 0.41514489177183955, 1
robot execution: 0.41514489177183955, 1
execution framework: 0.41514489177183955, 1
framework manipulation: 0.41514489177183955, 1
action semantics: 0.41514489177183955, 1
action maps: 0.41514489177183955, 1
maps large: 0.41514489177183955, 1
large environment: 0.41514489177183955, 1
via person: 0.41514489177183955, 1
person vision: 0.41514489177183955, 1
semantic pose: 0.41514489177183955, 1
pose deep: 0.41514489177183955, 1
train synthetic: 0.41514489177183955, 1
synthetic rgb: 0.41514489177183955, 1
binge scaling: 0.41514489177183955, 1
scaling affordance: 0.41514489177183955, 1
learning sitcoms: 0.41514489177183955, 1
i around: 0.41514489177183955, 1
around deep: 0.41514489177183955, 1
deep functional: 0.41514489177183955, 1
understanding cognitive: 0.41514489177183955, 1
icra proceedings: 0.41514489177183955, 1
automation 2011: 0.41514489177183955, 1
2011 ieee: 0.41514489177183955, 1
systems 2012: 0.41514489177183955, 1
icra ieee: 0.41514489177183955, 1
2017 ieee: 0.41514489177183955, 1
scene semantics: 0.41514489177183955, 1
semantics long: 0.41514489177183955, 1
term observation: 0.41514489177183955, 1
convolutional lstm: 0.41514489177183955, 1
lstm machine: 0.41514489177183955, 1
approach precipitation: 0.41514489177183955, 1
precipitation nowcasting: 0.41514489177183955, 1
primal dual: 0.41514489177183955, 1
dual framework: 0.41514489177183955, 1
framework real: 0.41514489177183955, 1
real dense: 0.41514489177183955, 1
dense rgb: 0.41514489177183955, 1
d scene: 0.41514489177183955, 1
method stochastic: 0.41514489177183955, 1
stochastic optimization: 0.41514489177183955, 1
understanding difficulty: 0.41514489177183955, 1
difficulty train: 0.41514489177183955, 1
deep feedforward: 0.41514489177183955, 1
feedforward neural: 0.41514489177183955, 1
network large: 0.41514489177183955, 1
scale image: 0.41514489177183955, 1
individual comparisons: 0.41514489177183955, 1
comparisons ranking: 0.41514489177183955, 1
ranking method: 0.41514489177183955, 1
understanding task: 0.41514489177183955, 1
oriented object: 0.41514489177183955, 1
saliency context: 0.41514489177183955, 1
function visual: 0.41514489177183955, 1
visual analysis: 0.41514489177183955, 1
analysis physical: 0.41514489177183955, 1
physical methodology: 0.41514489177183955, 1
methodology recognition: 0.41514489177183955, 1
recognition generic: 0.41514489177183955, 1
generic classes: 0.41514489177183955, 1
classes object: 0.41514489177183955, 1
evaluate foreground: 0.41514489177183955, 1
foreground maps: 0.41514489177183955, 1
quaternionic signal: 0.41514489177183955, 1
signal processing: 0.41514489177183955, 1
processing techniques: 0.41514489177183955, 1
techniques evaluation: 0.41514489177183955, 1
evaluation dance: 0.41514489177183955, 1
dance performances: 0.41514489177183955, 1
performances mocap: 0.41514489177183955, 1
mocap data: 0.41514489177183955, 1
years object: 0.41514489177183955, 1
object directions: 0.41514489177183955, 1
directions forward: 0.41514489177183955, 1
distribution flora: 0.41514489177183955, 1
flora alpine: 0.41514489177183955, 1
view invariant: 0.41514489177183955, 1
invariant human: 0.41514489177183955, 1
recognition histograms: 0.41514489177183955, 1
histograms 3d: 0.41514489177183955, 1
3d joints: 0.41514489177183955, 1
saliency human: 0.41514489177183955, 1
human state: 0.41514489177183955, 1
art study: 0.41514489177183955, 1
study comparison: 0.41514489177183955, 1
comparison metrics: 0.41514489177183955, 1
stream convolutional: 0.41514489177183955, 1
network action: 0.41514489177183955, 1
recognition video: 0.41514489177183955, 1
feature discriminative: 0.41514489177183955, 1
discriminative localization: 0.41514489177183955, 1
framework object: 0.41514489177183955, 1
identity mappings: 0.41514489177183955, 1
mappings deep: 0.41514489177183955, 1
residual network: 0.41514489177183955, 1
object proposal: 0.41514489177183955, 1
proposal generation: 0.41514489177183955, 1
generation fully: 0.41514489177183955, 1
cascaded interactional: 0.41514489177183955, 1
interactional targeting: 0.41514489177183955, 1
targeting network: 0.41514489177183955, 1
network egocentric: 0.41514489177183955, 1
video analysis: 0.41514489177183955, 1
term recurrent: 0.41514489177183955, 1
recurrent convolutional: 0.41514489177183955, 1
network visual: 0.41514489177183955, 1
recognition description: 0.41514489177183955, 1
deepgaze reading: 0.41514489177183955, 1
reading fixations: 0.41514489177183955, 1
fixations deep: 0.41514489177183955, 1
feature train: 0.41514489177183955, 1
train object: 0.41514489177183955, 1
recognition 3d: 0.41514489177183955, 1
3d reconstruction: 0.41514489177183955, 1
reconstruction data: 0.41514489177183955, 1
visual saliency: 0.41514489177183955, 1
predict generative: 0.41514489177183955, 1
adversarial network: 0.41514489177183955, 1
affordance online: 0.41514489177183955, 1
online video: 0.41514489177183955, 1
end policy: 0.41514489177183955, 1
learning active: 0.41514489177183955, 1
active visual: 0.41514489177183955, 1
visual categorization: 0.41514489177183955, 1
fast efficient: 0.41514489177183955, 1
efficient weakly: 0.41514489177183955, 1
weakly semi: 0.41514489177183955, 1
deep complex: 0.41514489177183955, 1
complex isar: 0.41514489177183955, 1
isar object: 0.41514489177183955, 1
detection deep: 0.41514489177183955, 1
deep review: 0.41514489177183955, 1
devil regression: 0.41514489177183955, 1
regression gans: 0.41514489177183955, 1
mask r: 0.41514489177183955, 1
r cnn: 0.41514489177183955, 1
different evaluation: 0.41514489177183955, 1
evaluation metrics: 0.41514489177183955, 1
metrics tell: 0.41514489177183955, 1
tell saliency: 0.41514489177183955, 1
deep multi: 0.41514489177183955, 1
multi level: 0.41514489177183955, 1
level network: 0.41514489177183955, 1
network saliency: 0.41514489177183955, 1
categorization pose: 0.41514489177183955, 1
estimation multiviews: 0.41514489177183955, 1
multiviews unsupervised: 0.41514489177183955, 1
unsupervised viewpoints: 0.41514489177183955, 1
imperative high: 0.41514489177183955, 1
performance deep: 0.41514489177183955, 1
learning library: 0.41514489177183955, 1
deep sensorimotor: 0.41514489177183955, 1
d object: 0.41514489177183955, 1
grounded human: 0.41514489177183955, 1
interaction hotspots: 0.41514489177183955, 1
hotspots video: 0.41514489177183955, 1
reasoning human: 0.41514489177183955, 1
interaction dual: 0.41514489177183955, 1
dual attention: 0.41514489177183955, 1
attention network: 0.41514489177183955, 1
learning relationships: 0.41514489177183955, 1
relationships multi: 0.41514489177183955, 1
3d object: 0.41514489177183955, 1
based channel: 0.41514489177183955, 1
channel attention: 0.41514489177183955, 1
tracking review: 0.41514489177183955, 1
visual tracking: 0.41514489177183955, 1
tracking correlation: 0.41514489177183955, 1
correlation filters: 0.41514489177183955, 1
filters metric: 0.41514489177183955, 1
metric learning: 0.41514489177183955, 1
unsupervised offline: 0.41514489177183955, 1
offline video: 0.41514489177183955, 1
video object: 0.41514489177183955, 1
segmentation tracking: 0.41514489177183955, 1
parallelizable robust: 0.41514489177183955, 1
robust image: 0.41514489177183955, 1
based shape: 0.41514489177183955, 1
shape prior: 0.41514489177183955, 1
prior information: 0.41514489177183955, 1
unified deep: 0.41514489177183955, 1
deep framework: 0.41514489177183955, 1
framework moving: 0.41514489177183955, 1
survey semantic: 0.41514489177183955, 1
ego environment: 0.41514489177183955, 1
affordance egocentric: 0.41514489177183955, 1
detection attention: 0.41514489177183955, 1
attention rpn: 0.41514489177183955, 1
rpn multi: 0.41514489177183955, 1
multi relation: 0.41514489177183955, 1
relation detector: 0.41514489177183955, 1
transfer learning: 0.41514489177183955, 1
online multi: 0.41514489177183955, 1
tracking siamese: 0.41514489177183955, 1
siamese network: 0.41514489177183955, 1
network optical: 0.41514489177183955, 1
self assessment: 0.41514489177183955, 1
assessment grasp: 0.41514489177183955, 1
affordance transfer: 0.41514489177183955, 1
graph moving: 0.41514489177183955, 1
adaptive segmentation: 0.41514489177183955, 1
model liver: 0.41514489177183955, 1
liver ct: 0.41514489177183955, 1
ct image: 0.41514489177183955, 1
based neural: 0.41514489177183955, 1
network level: 0.41514489177183955, 1
level set: 0.41514489177183955, 1
set method: 0.41514489177183955, 1
group normalization: 0.41514489177183955, 1
passive interactive: 0.41514489177183955, 1
recognition self: 0.41514489177183955, 1
self identification: 0.41514489177183955, 1
identification humanoid: 0.41514489177183955, 1
symmetries local: 0.41514489177183955, 1
local transformations: 0.41514489177183955, 1
transformations translationally: 0.41514489177183955, 1
translationally invariant: 0.41514489177183955, 1
invariant matrix: 0.41514489177183955, 1
matrix product: 0.41514489177183955, 1
product states: 0.41514489177183955, 1
potential symmetries: 0.41514489177183955, 1
solutions generalized: 0.41514489177183955, 1
generalized porous: 0.41514489177183955, 1
porous medium: 0.41514489177183955, 1
medium equation: 0.41514489177183955, 1
solutions geometric: 0.41514489177183955, 1
geometric heat: 0.41514489177183955, 1
heat flows: 0.41514489177183955, 1
solutions absolutely: 0.41514489177183955, 1
absolutely unstable: 0.41514489177183955, 1
unstable media: 0.41514489177183955, 1
media equations: 0.41514489177183955, 1
image video: 0.41514489177183955, 1
video compression: 0.41514489177183955, 1
compression standards: 0.41514489177183955, 1
determining optical: 0.41514489177183955, 1
frame motion: 0.41514489177183955, 1
motion estimation: 0.41514489177183955, 1
estimation based: 0.41514489177183955, 1
based polynomial: 0.41514489177183955, 1
polynomial expansion: 0.41514489177183955, 1
active exploratory: 0.41514489177183955, 1
exploratory perception: 0.41514489177183955, 1
video based: 0.41514489177183955, 1
based descriptors: 0.41514489177183955, 1
descriptors object: 0.41514489177183955, 1
constraint verification: 0.41514489177183955, 1
verification kernel: 0.41514489177183955, 1
kernel machine: 0.41514489177183955, 1
foundations support: 0.41514489177183955, 1
support constraint: 0.41514489177183955, 1
constraint machine: 0.41514489177183955, 1
optical qualitative: 0.41514489177183955, 1
qualitative properties: 0.41514489177183955, 1
predict coding: 0.41514489177183955, 1
coding visual: 0.41514489177183955, 1
visual functional: 0.41514489177183955, 1
functional interpretation: 0.41514489177183955, 1
interpretation extra: 0.41514489177183955, 1
extra classical: 0.41514489177183955, 1
classical receptive: 0.41514489177183955, 1
receptive field: 0.41514489177183955, 1
field effect: 0.41514489177183955, 1
bayesian role: 0.41514489177183955, 1
role uncertainty: 0.41514489177183955, 1
uncertainty neural: 0.41514489177183955, 1
neural coding: 0.41514489177183955, 1
coding computation: 0.41514489177183955, 1
inference probabilistic: 0.41514489177183955, 1
probabilistic population: 0.41514489177183955, 1
population codes: 0.41514489177183955, 1
coherence video: 0.41514489177183955, 1
energy unified: 0.41514489177183955, 1
unified brain: 0.41514489177183955, 1
attention modeling: 0.41514489177183955, 1
representation video: 0.41514489177183955, 1
revisiting active: 0.41514489177183955, 1
deep intrinsic: 0.41514489177183955, 1
intrinsic video: 0.41514489177183955, 1
video representation: 0.41514489177183955, 1
representation exploring: 0.41514489177183955, 1
exploring temporal: 0.41514489177183955, 1
coherence graph: 0.41514489177183955, 1
graph structure: 0.41514489177183955, 1
feature watching: 0.41514489177183955, 1
watching object: 0.41514489177183955, 1
object move: 0.41514489177183955, 1
visual streams: 0.41514489177183955, 1
streams interact: 0.41514489177183955, 1
advances neural: 0.41514489177183955, 1
neural information: 0.41514489177183955, 1
information processing: 0.41514489177183955, 1
processing systems: 0.41514489177183955, 1
systems 29: 0.41514489177183955, 1
learning video: 0.41514489177183955, 1
video temporal: 0.41514489177183955, 1
temporal coherency: 0.41514489177183955, 1
coherency deep: 0.41514489177183955, 1
senses considered: 0.41514489177183955, 1
considered perception: 0.41514489177183955, 1
animate vision: 0.41514489177183955, 1
gravitational laws: 0.41514489177183955, 1
laws focus: 0.41514489177183955, 1
decomposing motion: 0.41514489177183955, 1
motion content: 0.41514489177183955, 1
content video: 0.41514489177183955, 1
predict deep: 0.41514489177183955, 1
learning successes: 0.41514489177183955, 1
successes limitations: 0.41514489177183955, 1
feature under: 0.41514489177183955, 1
under motion: 0.41514489177183955, 1
disentangling appearance: 0.41514489177183955, 1
appearance motion: 0.41514489177183955, 1
motion video: 0.41514489177183955, 1
flow survey: 0.41514489177183955, 1
function understanding: 0.41514489177183955, 1
mathematical problem: 0.41514489177183955, 1
problem image: 0.41514489177183955, 1
group equivariant: 0.41514489177183955, 1
equivariant convolutional: 0.41514489177183955, 1
perception visual: 0.41514489177183955, 1
visual world: 0.41514489177183955, 1
learning invariant: 0.41514489177183955, 1
invariant representation: 0.41514489177183955, 1
autonomy surgical: 0.41514489177183955, 1
surgical robot: 0.41514489177183955, 1
based conceptual: 0.41514489177183955, 1
conceptual framework: 0.41514489177183955, 1
framework spatial: 0.41514489177183955, 1
spatial behavior: 0.41514489177183955, 1
behavior social: 0.41514489177183955, 1
social robot: 0.41514489177183955, 1
disentangling effect: 0.41514489177183955, 1
effect robot: 0.41514489177183955, 1
robot autonomy: 0.41514489177183955, 1
autonomy human: 0.41514489177183955, 1
human team: 0.41514489177183955, 1
team members: 0.41514489177183955, 1
members mixed: 0.41514489177183955, 1
mixed initiative: 0.41514489177183955, 1
initiative task: 0.41514489177183955, 1
application ai: 0.41514489177183955, 1
ai technique: 0.41514489177183955, 1
technique robot: 0.41514489177183955, 1
robot field: 0.41514489177183955, 1
neural model: 0.41514489177183955, 1
recognition goal: 0.41514489177183955, 1
directed movements: 0.41514489177183955, 1
grasp dependent: 0.41514489177183955, 1
dependent tool: 0.41514489177183955, 1
generation behavior: 0.41514489177183955, 1
behavior automaton: 0.41514489177183955, 1
automaton neural: 0.41514489177183955, 1
learning speech: 0.41514489177183955, 1
speech guidance: 0.41514489177183955, 1
guidance domestic: 0.41514489177183955, 1
domestic scenario: 0.41514489177183955, 1
based optimal: 0.41514489177183955, 1
optimal strategy: 0.41514489177183955, 1
entropy based: 0.41514489177183955, 1
approach hierarchical: 0.41514489177183955, 1
hierarchical acquisition: 0.41514489177183955, 1
acquisition perception: 0.41514489177183955, 1
action capabilities: 0.41514489177183955, 1
learning consequences: 0.41514489177183955, 1
consequences representing: 0.41514489177183955, 1
representing effect: 0.41514489177183955, 1
effect feature: 0.41514489177183955, 1
feature changes: 0.41514489177183955, 1
supervised cross: 0.41514489177183955, 1
cross modal: 0.41514489177183955, 1
modal online: 0.41514489177183955, 1
affordance developmental: 0.41514489177183955, 1
decoupling control: 0.41514489177183955, 1
control autonomous: 0.41514489177183955, 1
autonomous learning: 0.41514489177183955, 1
goal oriented: 0.41514489177183955, 1
oriented dependable: 0.41514489177183955, 1
dependable action: 0.41514489177183955, 1
selection probabilistic: 0.41514489177183955, 1
inspired model: 0.41514489177183955, 1
robot adaptive: 0.41514489177183955, 1
adaptive learning: 0.41514489177183955, 1
learning mapping: 0.41514489177183955, 1
action property: 0.41514489177183955, 1
property learning: 0.41514489177183955, 1
learning causally: 0.41514489177183955, 1
causally dominant: 0.41514489177183955, 1
dominant properties: 0.41514489177183955, 1
properties cumulative: 0.41514489177183955, 1
cumulative explorative: 0.41514489177183955, 1
explorative interaction: 0.41514489177183955, 1
learning probabilistic: 0.41514489177183955, 1
probabilistic discriminative: 0.41514489177183955, 1
affordance under: 0.41514489177183955, 1
under limited: 0.41514489177183955, 1
limited supervision: 0.41514489177183955, 1
predict functional: 0.41514489177183955, 1
functional regions: 0.41514489177183955, 1
regions object: 0.41514489177183955, 1
affordance categorizing: 0.41514489177183955, 1
categorizing object: 0.41514489177183955, 1
initial development: 0.41514489177183955, 1
development object: 0.41514489177183955, 1
object knowledge: 0.41514489177183955, 1
knowledge learning: 0.41514489177183955, 1
learning structural: 0.41514489177183955, 1
structural affordance: 0.41514489177183955, 1
network ontology: 0.41514489177183955, 1
ontology robot: 0.41514489177183955, 1
tool autonomous: 0.41514489177183955, 1
learning contact: 0.41514489177183955, 1
contact locations: 0.41514489177183955, 1
locations pushing: 0.41514489177183955, 1
pushing orienting: 0.41514489177183955, 1
orienting unknown: 0.41514489177183955, 1
object search: 0.41514489177183955, 1
search relational: 0.41514489177183955, 1
learning detect: 0.41514489177183955, 1
detect visual: 0.41514489177183955, 1
approach forming: 0.41514489177183955, 1
forming object: 0.41514489177183955, 1
object separating: 0.41514489177183955, 1
separating containers: 0.41514489177183955, 1
containers noncontainers: 0.41514489177183955, 1
staged development: 0.41514489177183955, 1
development robot: 0.41514489177183955, 1
imitation motionese: 0.41514489177183955, 1
assimilation humanoid: 0.41514489177183955, 1
robot neurodynamical: 0.41514489177183955, 1
neurodynamical system: 0.41514489177183955, 1
exploiting object: 0.41514489177183955, 1
autonomous pile: 0.41514489177183955, 1
pile manipulation: 0.41514489177183955, 1
task constraints: 0.41514489177183955, 1
constraints robot: 0.41514489177183955, 1
discrete fuzzy: 0.41514489177183955, 1
fuzzy grasp: 0.41514489177183955, 1
robot manipulators: 0.41514489177183955, 1
knowledge propagation: 0.41514489177183955, 1
propagation relation: 0.41514489177183955, 1
relation learning: 0.41514489177183955, 1
predict action: 0.41514489177183955, 1
predict slippage: 0.41514489177183955, 1
slippage learning: 0.41514489177183955, 1
learning manipulation: 0.41514489177183955, 1
affordance gaussian: 0.41514489177183955, 1
gaussian process: 0.41514489177183955, 1
process regression: 0.41514489177183955, 1
affordance haptic: 0.41514489177183955, 1
symbol generation: 0.41514489177183955, 1
generation feature: 0.41514489177183955, 1
selection reinforcement: 0.41514489177183955, 1
agent affordance: 0.41514489177183955, 1
affordance u: 0.41514489177183955, 1
u trees: 0.41514489177183955, 1
extending sensorimotor: 0.41514489177183955, 1
sensorimotor contingency: 0.41514489177183955, 1
contingency action: 0.41514489177183955, 1
action generation: 0.41514489177183955, 1
structural feature: 0.41514489177183955, 1
extraction based: 0.41514489177183955, 1
based active: 0.41514489177183955, 1
sensing experiences: 0.41514489177183955, 1
specific representation: 0.41514489177183955, 1
model latent: 0.41514489177183955, 1
space discretization: 0.41514489177183955, 1
better vision: 0.41514489177183955, 1
vision manipulation: 0.41514489177183955, 1
primitive behavior: 0.41514489177183955, 1
behavior goal: 0.41514489177183955, 1
directed behavior: 0.41514489177183955, 1
study applying: 0.41514489177183955, 1
applying ecological: 0.41514489177183955, 1
approach mobile: 0.41514489177183955, 1
hallucinated human: 0.41514489177183955, 1
human hidden: 0.41514489177183955, 1
hidden context: 0.41514489177183955, 1
context labeling: 0.41514489177183955, 1
responding learning: 0.41514489177183955, 1
learning projecting: 0.41514489177183955, 1
projecting sensorimotor: 0.41514489177183955, 1
sensorimotor mapping: 0.41514489177183955, 1
acquisition intentionally: 0.41514489177183955, 1
intentionally indexed: 0.41514489177183955, 1
indexed object: 0.41514489177183955, 1
object centered: 0.41514489177183955, 1
centered affordance: 0.41514489177183955, 1
affordance biomimetic: 0.41514489177183955, 1
biomimetic controller: 0.41514489177183955, 1
controller mobile: 0.41514489177183955, 1
robot benchmark: 0.41514489177183955, 1
perception driven: 0.41514489177183955, 1
robot assembly: 0.41514489177183955, 1
assembly based: 0.41514489177183955, 1
based ecological: 0.41514489177183955, 1
learning generalization: 0.41514489177183955, 1
generalization behavior: 0.41514489177183955, 1
grounded tool: 0.41514489177183955, 1
detecting functional: 0.41514489177183955, 1
functional similarities: 0.41514489177183955, 1
similarities tool: 0.41514489177183955, 1
tool hierarchical: 0.41514489177183955, 1
representation outcome: 0.41514489177183955, 1
robot rehearses: 0.41514489177183955, 1
rehearses internally: 0.41514489177183955, 1
internally learn: 0.41514489177183955, 1
modeling tool: 0.41514489177183955, 1
tool body: 0.41514489177183955, 1
body assimilation: 0.41514489177183955, 1
assimilation second: 0.41514489177183955, 1
second order: 0.41514489177183955, 1
order recurrent: 0.41514489177183955, 1
increasing autonomy: 0.41514489177183955, 1
autonomy mobile: 0.41514489177183955, 1
robot line: 0.41514489177183955, 1
learning simultaneously: 0.41514489177183955, 1
simultaneously different: 0.41514489177183955, 1
levels abstraction: 0.41514489177183955, 1
autonomous acquisition: 0.41514489177183955, 1
acquisition pushing: 0.41514489177183955, 1
pushing action: 0.41514489177183955, 1
action support: 0.41514489177183955, 1
support object: 0.41514489177183955, 1
grasp humanoid: 0.41514489177183955, 1
development imitation: 0.41514489177183955, 1
sensing based: 0.41514489177183955, 1
based dynamical: 0.41514489177183955, 1
dynamical object: 0.41514489177183955, 1
semantic planning: 0.41514489177183955, 1
planning task: 0.41514489177183955, 1
specific stable: 0.41514489177183955, 1
stable robot: 0.41514489177183955, 1
robot grasps: 0.41514489177183955, 1
probabilistic concept: 0.41514489177183955, 1
concept web: 0.41514489177183955, 1
web humanoid: 0.41514489177183955, 1
novel formalization: 0.41514489177183955, 1
formalization robot: 0.41514489177183955, 1
robot cognition: 0.41514489177183955, 1
cognition based: 0.41514489177183955, 1
collision risk: 0.41514489177183955, 1
risk assessment: 0.41514489177183955, 1
assessment autonomous: 0.41514489177183955, 1
robot offline: 0.41514489177183955, 1
offline traversability: 0.41514489177183955, 1
traversability learning: 0.41514489177183955, 1
action grounded: 0.41514489177183955, 1
grounded push: 0.41514489177183955, 1
affordance bootstrapping: 0.41514489177183955, 1
bootstrapping unknown: 0.41514489177183955, 1
object specific: 0.41514489177183955, 1
specific grasp: 0.41514489177183955, 1
network model: 0.41514489177183955, 1
cycle robot: 0.41514489177183955, 1
recognition visuo: 0.41514489177183955, 1
visuo affordance: 0.41514489177183955, 1
kernel based: 0.41514489177183955, 1
approach direct: 0.41514489177183955, 1
direct action: 0.41514489177183955, 1
affordance local: 0.41514489177183955, 1
local visual: 0.41514489177183955, 1
model approach: 0.41514489177183955, 1
based 3d: 0.41514489177183955, 1
3d functional: 0.41514489177183955, 1
feature tool: 0.41514489177183955, 1
fill simple: 0.41514489177183955, 1
simple physics: 0.41514489177183955, 1
physics based: 0.41514489177183955, 1
approach containability: 0.41514489177183955, 1
containability reasoning: 0.41514489177183955, 1
learning intermediate: 0.41514489177183955, 1
intermediate object: 0.41514489177183955, 1
object towards: 0.41514489177183955, 1
towards development: 0.41514489177183955, 1
development tool: 0.41514489177183955, 1
tool concept: 0.41514489177183955, 1
bootstrapping semantics: 0.41514489177183955, 1
semantics affordance: 0.41514489177183955, 1
affordance analysis: 0.41514489177183955, 1
analysis real: 0.41514489177183955, 1
object per: 0.41514489177183955, 1
per part: 0.41514489177183955, 1
part basis: 0.41514489177183955, 1
learning haptic: 0.41514489177183955, 1
haptic affordance: 0.41514489177183955, 1
demonstration human: 0.41514489177183955, 1
human guided: 0.41514489177183955, 1
guided exploration: 0.41514489177183955, 1
localizing handle: 0.41514489177183955, 1
handle grasp: 0.41514489177183955, 1
encoders learning: 0.41514489177183955, 1
affordance continuous: 0.41514489177183955, 1
continuous space: 0.41514489177183955, 1
instructions robot: 0.41514489177183955, 1
robot formulation: 0.41514489177183955, 1
formulation affordance: 0.41514489177183955, 1
probabilistic planning: 0.41514489177183955, 1
approach finding: 0.41514489177183955, 1
finding substitute: 0.41514489177183955, 1
substitute tool: 0.41514489177183955, 1
tool 3d: 0.41514489177183955, 1
3d vision: 0.41514489177183955, 1
vision data: 0.41514489177183955, 1
affordance feasible: 0.41514489177183955, 1
feasible planning: 0.41514489177183955, 1
planning manipulator: 0.41514489177183955, 1
manipulator wrench: 0.41514489177183955, 1
wrench spaces: 0.41514489177183955, 1
task intrinsic: 0.41514489177183955, 1
motivation empirical: 0.41514489177183955, 1
empirical feature: 0.41514489177183955, 1
attribute based: 0.41514489177183955, 1
detection human: 0.41514489177183955, 1
interaction image: 0.41514489177183955, 1
planning anthropomorphic: 0.41514489177183955, 1
anthropomorphic hand: 0.41514489177183955, 1
hand human: 0.41514489177183955, 1
toward lifelong: 0.41514489177183955, 1
learning distributed: 0.41514489177183955, 1
distributed markov: 0.41514489177183955, 1
markov model: 0.41514489177183955, 1
logic based: 0.41514489177183955, 1
computational framework: 0.41514489177183955, 1
framework inferring: 0.41514489177183955, 1
inferring cognitive: 0.41514489177183955, 1
predict intention: 0.41514489177183955, 1
intention human: 0.41514489177183955, 1
activities real: 0.41514489177183955, 1
real human: 0.41514489177183955, 1
representation extraction: 0.41514489177183955, 1
extraction image: 0.41514489177183955, 1
feature associated: 0.41514489177183955, 1
associated maneuvering: 0.41514489177183955, 1
system vision: 0.41514489177183955, 1
based evolutionary: 0.41514489177183955, 1
evolutionary robot: 0.41514489177183955, 1
model shared: 0.41514489177183955, 1
shared grasp: 0.41514489177183955, 1
leveraging combination: 0.41514489177183955, 1
combination human: 0.41514489177183955, 1
human guidance: 0.41514489177183955, 1
guidance self: 0.41514489177183955, 1
anticipative generation: 0.41514489177183955, 1
generation situ: 0.41514489177183955, 1
situ adaptation: 0.41514489177183955, 1
adaptation maneuvering: 0.41514489177183955, 1
affordance naturally: 0.41514489177183955, 1
naturally complex: 0.41514489177183955, 1
complex scene: 0.41514489177183955, 1
robot visual: 0.41514489177183955, 1
interactive affordance: 0.41514489177183955, 1
map building: 0.41514489177183955, 1
building robot: 0.41514489177183955, 1
relations robot: 0.41514489177183955, 1
agent review: 0.41514489177183955, 1
affordance brief: 0.41514489177183955, 1
affordance multiple: 0.41514489177183955, 1
framework robot: 0.41514489177183955, 1
affordance equivalences: 0.41514489177183955, 1
equivalences formalism: 0.41514489177183955, 1
brief review: 0.41514489177183955, 1
review affordance: 0.41514489177183955, 1
manipulation research: 0.41514489177183955, 1
affordance survey: 0.41514489177183955, 1
human video: 0.41514489177183955, 1
video versatile: 0.41514489177183955, 1
versatile representation: 0.41514489177183955, 1
architecture online: 0.41514489177183955, 1
online perception: 0.41514489177183955, 1
perception planning: 0.41514489177183955, 1
based altruistic: 0.41514489177183955, 1
altruistic robot: 0.41514489177183955, 1
robot architecture: 0.41514489177183955, 1
architecture collaboration: 0.41514489177183955, 1
based multimodal: 0.41514489177183955, 1
multimodal fusion: 0.41514489177183955, 1
fusion natural: 0.41514489177183955, 1
natural human: 0.41514489177183955, 1
grasp manipulation: 0.41514489177183955, 1
manipulation real: 0.41514489177183955, 1
world applications: 0.41514489177183955, 1
defense direct: 0.41514489177183955, 1
human motion: 0.41514489177183955, 1
motion capture: 0.41514489177183955, 1
capture data: 0.41514489177183955, 1
open ended: 0.41514489177183955, 1
ended affordance: 0.41514489177183955, 1
discovery robot: 0.41514489177183955, 1
robot pertinent: 0.41514489177183955, 1
pertinent visual: 0.41514489177183955, 1
framework developmental: 0.41514489177183955, 1
developmental embodied: 0.41514489177183955, 1
affordance landscapes: 0.41514489177183955, 1
landscapes interaction: 0.41514489177183955, 1
interaction exploration: 0.41514489177183955, 1
exploration 3d: 0.41514489177183955, 1
paradigm study: 0.41514489177183955, 1
study social: 0.41514489177183955, 1
social physical: 0.41514489177183955, 1
physical affordance: 0.41514489177183955, 1
rediscovering reinforcement: 0.41514489177183955, 1
based markov: 0.41514489177183955, 1
decision process: 0.41514489177183955, 1
process control: 0.41514489177183955, 1
control strategies: 0.41514489177183955, 1
strategies autonomous: 0.41514489177183955, 1
autonomous mobile: 0.41514489177183955, 1
robot dynamic: 0.41514489177183955, 1
dynamic environment: 0.41514489177183955, 1
aggressive perception: 0.41514489177183955, 1
perception aware: 0.41514489177183955, 1
aware navigation: 0.41514489177183955, 1
navigation deep: 0.41514489177183955, 1
deep optical: 0.41514489177183955, 1
flow dynamics: 0.41514489177183955, 1
dynamics pixelmpc: 0.41514489177183955, 1
end efficient: 0.41514489177183955, 1
efficient indoor: 0.41514489177183955, 1
indoor navigation: 0.41514489177183955, 1
navigation optical: 0.41514489177183955, 1
enhancing optical: 0.41514489177183955, 1
control learning: 0.41514489177183955, 1
visual appearance: 0.41514489177183955, 1
appearance cues: 0.41514489177183955, 1
cues flying: 0.41514489177183955, 1
flying robot: 0.41514489177183955, 1
approximate inverse: 0.41514489177183955, 1
technology readiness: 0.41514489177183955, 1
readiness levels: 0.41514489177183955, 1
levels machine: 0.41514489177183955, 1
learning systems: 0.41514489177183955, 1
generalization integrating: 0.41514489177183955, 1
integrating simulate: 0.41514489177183955, 1
data deep: 0.41514489177183955, 1
autonomous flight: 0.41514489177183955, 1
deep approach: 0.41514489177183955, 1
hybrid deep: 0.41514489177183955, 1
deep model: 0.41514489177183955, 1
model deep: 0.41514489177183955, 1
dense optical: 0.41514489177183955, 1
approach human: 0.41514489177183955, 1
human activity: 0.41514489177183955, 1
activity recognition: 0.41514489177183955, 1
general flow: 0.41514489177183955, 1
flow foundation: 0.41514489177183955, 1
foundation affordance: 0.41514489177183955, 1
affordance scalable: 0.41514489177183955, 1
scalable robot: 0.41514489177183955, 1
optical perceiving: 0.41514489177183955, 1
perceiving acting: 0.41514489177183955, 1
acting 3: 0.41514489177183955, 1
d world: 0.41514489177183955, 1
affordance general: 0.41514489177183955, 1
general value: 0.41514489177183955, 1
value computational: 0.41514489177183955, 1
leveraging deep: 0.41514489177183955, 1
deep 3d: 0.41514489177183955, 1
3d saliency: 0.41514489177183955, 1
saliency interaction: 0.41514489177183955, 1
visual geometric: 0.41514489177183955, 1
geometric collaborative: 0.41514489177183955, 1
collaborative guidance: 0.41514489177183955, 1
guidance affordance: 0.41514489177183955, 1
perceiving surface: 0.41514489177183955, 1
surface ground: 0.41514489177183955, 1
ground object: 0.41514489177183955, 1
tool affect: 0.41514489177183955, 1
affect affordance: 0.41514489177183955, 1
perception toddlers: 0.41514489177183955, 1
map interactive: 0.41514489177183955, 1
interactive perception: 0.41514489177183955, 1
synergies affordance: 0.41514489177183955, 1
affordance 6: 0.41514489177183955, 1
6 dof: 0.41514489177183955, 1
dof grasp: 0.41514489177183955, 1
grasp detection: 0.41514489177183955, 1
detection via: 0.41514489177183955, 1
via implicit: 0.41514489177183955, 1
implicit representation: 0.41514489177183955, 1
i geometric: 0.41514489177183955, 1
affordance single: 0.41514489177183955, 1
single example: 0.41514489177183955, 1
example interaction: 0.41514489177183955, 1
relevance action: 0.41514489177183955, 1
action perceiving: 0.41514489177183955, 1
perceiving perception: 0.41514489177183955, 1
perception catchableness: 0.41514489177183955, 1
catchableness fly: 0.41514489177183955, 1
model development: 0.41514489177183955, 1
development process: 0.41514489177183955, 1
process learning: 0.41514489177183955, 1
causal approach: 0.41514489177183955, 1
approach tool: 0.41514489177183955, 1
detection relationship: 0.41514489177183955, 1
relationship aware: 0.41514489177183955, 1
aware network: 0.41514489177183955, 1
gibson real: 0.41514489177183955, 1
world perception: 0.41514489177183955, 1
perception embodied: 0.41514489177183955, 1
embodied agent: 0.41514489177183955, 1
learning development: 0.41514489177183955, 1
benchmark everyday: 0.41514489177183955, 1
everyday household: 0.41514489177183955, 1
household activities: 0.41514489177183955, 1
activities ecological: 0.41514489177183955, 1
ecological environment: 0.41514489177183955, 1
review supervised: 0.41514489177183955, 1
supervised machine: 0.41514489177183955, 1
algorithm applications: 0.41514489177183955, 1
applications ecological: 0.41514489177183955, 1
ecological data: 0.41514489177183955, 1
adoption machine: 0.41514489177183955, 1
learning techniques: 0.41514489177183955, 1
techniques ecology: 0.41514489177183955, 1
ecology earth: 0.41514489177183955, 1
earth science: 0.41514489177183955, 1
language navigation: 0.41514489177183955, 1
navigation random: 0.41514489177183955, 1
random environmental: 0.41514489177183955, 1
environmental mixup: 0.41514489177183955, 1
accuracy rainfall: 0.41514489177183955, 1
rainfall rates: 0.41514489177183955, 1
rates optical: 0.41514489177183955, 1
optical satellite: 0.41514489177183955, 1
satellite sensors: 0.41514489177183955, 1
sensors machine: 0.41514489177183955, 1
machine random: 0.41514489177183955, 1
random forests: 0.41514489177183955, 1
forests based: 0.41514489177183955, 1
approach applied: 0.41514489177183955, 1
applied msg: 0.41514489177183955, 1
msg seviri: 0.41514489177183955, 1
single pixel: 0.41514489177183955, 1
pixel imaging: 0.41514489177183955, 1
imaging 12: 0.41514489177183955, 1
12 years: 0.41514489177183955, 1
years review: 0.41514489177183955, 1
method comparative: 0.41514489177183955, 1
comparative plant: 0.41514489177183955, 1
plant population: 0.41514489177183955, 1
population ecology: 0.41514489177183955, 1
coupling machine: 0.41514489177183955, 1
machine tree: 0.41514489177183955, 1
tree based: 0.41514489177183955, 1
based statistical: 0.41514489177183955, 1
statistical model: 0.41514489177183955, 1
model cellular: 0.41514489177183955, 1
cellular automata: 0.41514489177183955, 1
automata simulate: 0.41514489177183955, 1
simulate urban: 0.41514489177183955, 1
urban growth: 0.41514489177183955, 1
application machine: 0.41514489177183955, 1
method forest: 0.41514489177183955, 1
forest recent: 0.41514489177183955, 1
recent progress: 0.41514489177183955, 1
progress future: 0.41514489177183955, 1
games simulations: 0.41514489177183955, 1
simulations online: 0.41514489177183955, 1
online research: 0.41514489177183955, 1
development research: 0.41514489177183955, 1
development frameworks: 0.41514489177183955, 1
critically considering: 0.41514489177183955, 1
considering embodied: 0.41514489177183955, 1
cognition research: 0.41514489177183955, 1
research theatre: 0.41514489177183955, 1
theatre performance: 0.41514489177183955, 1
embodied inquiry: 0.41514489177183955, 1
embodied emotion: 0.41514489177183955, 1
emotion turn: 0.41514489177183955, 1
turn traditional: 0.41514489177183955, 1
traditional post: 0.41514489177183955, 1
post cognitive: 0.41514489177183955, 1
cognitive perspectives: 0.41514489177183955, 1
embodied philosophical: 0.41514489177183955, 1
philosophical account: 0.41514489177183955, 1
account computer: 0.41514489177183955, 1
computer assisted: 0.41514489177183955, 1
assisted language: 0.41514489177183955, 1
language education: 0.41514489177183955, 1
view embodied: 0.41514489177183955, 1
embodied perspective: 0.41514489177183955, 1
perspective emotion: 0.41514489177183955, 1
emotion study: 0.41514489177183955, 1
embodied thought: 0.41514489177183955, 1
embodied representation: 0.41514489177183955, 1
representation part: 0.41514489177183955, 1
part grouping: 0.41514489177183955, 1
grouping representation: 0.41514489177183955, 1
simulating sensorimotor: 0.41514489177183955, 1
sensorimotor novel: 0.41514489177183955, 1
novel metaphors: 0.41514489177183955, 1
metaphors influence: 0.41514489177183955, 1
influence sensory: 0.41514489177183955, 1
sensory judgments: 0.41514489177183955, 1
model embodied: 0.41514489177183955, 1
embodied model: 0.41514489177183955, 1
total moving: 0.41514489177183955, 1
moving face: 0.41514489177183955, 1
face reconstruction: 0.41514489177183955, 1
critique pure: 0.41514489177183955, 1
pure vision: 0.41514489177183955, 1
world modelers: 0.41514489177183955, 1
modelers objectives: 0.41514489177183955, 1
objectives simulator: 0.41514489177183955, 1
simulator architecture: 0.41514489177183955, 1
convolutional inverse: 0.41514489177183955, 1
inverse graphics: 0.41514489177183955, 1
graphics network: 0.41514489177183955, 1
adapting visual: 0.41514489177183955, 1
visual category: 0.41514489177183955, 1
category model: 0.41514489177183955, 1
model domains: 0.41514489177183955, 1
trust region: 0.41514489177183955, 1
region policy: 0.41514489177183955, 1
computational principles: 0.41514489177183955, 1
principles movement: 0.41514489177183955, 1
movement neuroscience: 0.41514489177183955, 1
learning generate: 0.41514489177183955, 1
generate chairs: 0.41514489177183955, 1
chairs convolutional: 0.41514489177183955, 1
reduction imitation: 0.41514489177183955, 1
structured predict: 0.41514489177183955, 1
predict regret: 0.41514489177183955, 1
regret online: 0.41514489177183955, 1
elephants play: 0.41514489177183955, 1
play chess: 0.41514489177183955, 1
post rendering: 0.41514489177183955, 1
rendering 3d: 0.41514489177183955, 1
3d warping: 0.41514489177183955, 1
origin optics: 0.41514489177183955, 1
layered depth: 0.41514489177183955, 1
pascal visual: 0.41514489177183955, 1
object classes: 0.41514489177183955, 1
classes challenge: 0.41514489177183955, 1
light field: 0.41514489177183955, 1
field rendering: 0.41514489177183955, 1
rendering synthetic: 0.41514489177183955, 1
synthetic object: 0.41514489177183955, 1
object legacy: 0.41514489177183955, 1
legacy photographs: 0.41514489177183955, 1
sciences artificial: 0.41514489177183955, 1
view interpolation: 0.41514489177183955, 1
interpolation image: 0.41514489177183955, 1
action mach: 0.41514489177183955, 1
mach spatio: 0.41514489177183955, 1
temporal maximum: 0.41514489177183955, 1
maximum average: 0.41514489177183955, 1
average correlation: 0.41514489177183955, 1
correlation height: 0.41514489177183955, 1
height filter: 0.41514489177183955, 1
filter action: 0.41514489177183955, 1
development embodied: 0.41514489177183955, 1
embodied six: 0.41514489177183955, 1
six lessons: 0.41514489177183955, 1
lessons babies: 0.41514489177183955, 1
real obstacle: 0.41514489177183955, 1
obstacle avoidance: 0.41514489177183955, 1
avoidance manipulators: 0.41514489177183955, 1
manipulators mobile: 0.41514489177183955, 1
movement produced: 0.41514489177183955, 1
produced stimulation: 0.41514489177183955, 1
stimulation development: 0.41514489177183955, 1
development visually: 0.41514489177183955, 1
unified approach: 0.41514489177183955, 1
approach motion: 0.41514489177183955, 1
motion force: 0.41514489177183955, 1
force control: 0.41514489177183955, 1
control robot: 0.41514489177183955, 1
robot operational: 0.41514489177183955, 1
operational space: 0.41514489177183955, 1
space formulation: 0.41514489177183955, 1
vision meets: 0.41514489177183955, 1
meets kitti: 0.41514489177183955, 1
kitti dataset: 0.41514489177183955, 1
autonomous helicopter: 0.41514489177183955, 1
helicopter aerobatics: 0.41514489177183955, 1
aerobatics apprenticeship: 0.41514489177183955, 1
apprenticeship learning: 0.41514489177183955, 1
learning realistic: 0.41514489177183955, 1
realistic human: 0.41514489177183955, 1
action movies: 0.41514489177183955, 1
modeling control: 0.41514489177183955, 1
control formations: 0.41514489177183955, 1
formations nonholonomic: 0.41514489177183955, 1
nonholonomic mobile: 0.41514489177183955, 1
arcade learning: 0.41514489177183955, 1
evaluation platform: 0.41514489177183955, 1
platform general: 0.41514489177183955, 1
general agent: 0.41514489177183955, 1
adaptation structural: 0.41514489177183955, 1
structural correspondence: 0.41514489177183955, 1
correspondence learning: 0.41514489177183955, 1
autonomous land: 0.41514489177183955, 1
land vehicle: 0.41514489177183955, 1
vehicle neural: 0.41514489177183955, 1
application reinforcement: 0.41514489177183955, 1
learning aerobatic: 0.41514489177183955, 1
aerobatic helicopter: 0.41514489177183955, 1
helicopter flight: 0.41514489177183955, 1
depth map: 0.41514489177183955, 1
map predict: 0.41514489177183955, 1
predict single: 0.41514489177183955, 1
image multi: 0.41514489177183955, 1
scale deep: 0.41514489177183955, 1
approach selective: 0.41514489177183955, 1
selective image: 0.41514489177183955, 1
rendering superpixels: 0.41514489177183955, 1
perceiving physical: 0.41514489177183955, 1
properties integrating: 0.41514489177183955, 1
integrating physics: 0.41514489177183955, 1
physics engine: 0.41514489177183955, 1
engine deep: 0.41514489177183955, 1
information rich: 0.41514489177183955, 1
model repository: 0.41514489177183955, 1
grasp 50k: 0.41514489177183955, 1
50k tries: 0.41514489177183955, 1
tries 700: 0.41514489177183955, 1
700 robot: 0.41514489177183955, 1
robot hours: 0.41514489177183955, 1
kernel sample: 0.41514489177183955, 1
sample test: 0.41514489177183955, 1
ldi tree: 0.41514489177183955, 1
view morphing: 0.41514489177183955, 1
regenerative morphing: 0.41514489177183955, 1
perception losses: 0.41514489177183955, 1
losses real: 0.41514489177183955, 1
real style: 0.41514489177183955, 1
style transfer: 0.41514489177183955, 1
transfer super: 0.41514489177183955, 1
super resolution: 0.41514489177183955, 1
view synthesis: 0.41514489177183955, 1
synthesis appearance: 0.41514489177183955, 1
appearance flow: 0.41514489177183955, 1
intelligence without: 0.41514489177183955, 1
without representation: 0.41514489177183955, 1
virtualworlds proxy: 0.41514489177183955, 1
synthia large: 0.41514489177183955, 1
collection synthetic: 0.41514489177183955, 1
synthetic image: 0.41514489177183955, 1
image semantic: 0.41514489177183955, 1
segmentation urban: 0.41514489177183955, 1
empagliflozin type: 0.41514489177183955, 1
type 2: 0.41514489177183955, 1
2 diabetes: 0.41514489177183955, 1
diabetes overview: 0.41514489177183955, 1
overview phase: 0.41514489177183955, 1
phase 3: 0.41514489177183955, 1
3 clinical: 0.41514489177183955, 1
clinical trials: 0.41514489177183955, 1
malmo platform: 0.41514489177183955, 1
platform artificial: 0.41514489177183955, 1
intelligence experimentation: 0.41514489177183955, 1
model single: 0.41514489177183955, 1
image convolutional: 0.41514489177183955, 1
learning transferrable: 0.41514489177183955, 1
transferrable representation: 0.41514489177183955, 1
representation unsupervised: 0.41514489177183955, 1
unsupervised domain: 0.41514489177183955, 1
scalable inside: 0.41514489177183955, 1
inside image: 0.41514489177183955, 1
semantic scene: 0.41514489177183955, 1
scene completion: 0.41514489177183955, 1
completion single: 0.41514489177183955, 1
single depth: 0.41514489177183955, 1
real single: 0.41514489177183955, 1
image flight: 0.41514489177183955, 1
flight without: 0.41514489177183955, 1
without single: 0.41514489177183955, 1
single real: 0.41514489177183955, 1
action without: 0.41514489177183955, 1
without human: 0.41514489177183955, 1
human supervision: 0.41514489177183955, 1
joint 2d: 0.41514489177183955, 1
3d semantic: 0.41514489177183955, 1
semantic data: 0.41514489177183955, 1
dataset developing: 0.41514489177183955, 1
developing benchmarking: 0.41514489177183955, 1
benchmarking active: 0.41514489177183955, 1
addressing appearance: 0.41514489177183955, 1
appearance change: 0.41514489177183955, 1
change outdoor: 0.41514489177183955, 1
outdoor robot: 0.41514489177183955, 1
robot adversarial: 0.41514489177183955, 1
adversarial domain: 0.41514489177183955, 1
photorealistic image: 0.41514489177183955, 1
rendering ground: 0.41514489177183955, 1
truth synthesis: 0.41514489177183955, 1
synthesis sampling: 0.41514489177183955, 1
sampling stochastic: 0.41514489177183955, 1
grammars representing: 0.41514489177183955, 1
representing indoor: 0.41514489177183955, 1
physics learning: 0.41514489177183955, 1
properties unlabeled: 0.41514489177183955, 1
unlabeled video: 0.41514489177183955, 1
high fidelity: 0.41514489177183955, 1
fidelity visual: 0.41514489177183955, 1
visual physical: 0.41514489177183955, 1
physical simulate: 0.41514489177183955, 1
simulate autonomous: 0.41514489177183955, 1
autonomous vehicles: 0.41514489177183955, 1
emergence locomotion: 0.41514489177183955, 1
locomotion behaviours: 0.41514489177183955, 1
behaviours rich: 0.41514489177183955, 1
rich environment: 0.41514489177183955, 1
open racing: 0.41514489177183955, 1
racing car: 0.41514489177183955, 1
car simulator: 0.41514489177183955, 1
10 million: 0.41514489177183955, 1
million image: 0.41514489177183955, 1
database scene: 0.41514489177183955, 1
scene recognition: 0.41514489177183955, 1
proximal policy: 0.41514489177183955, 1
optimization algorithm: 0.41514489177183955, 1
photographic image: 0.41514489177183955, 1
synthesis cascaded: 0.41514489177183955, 1
cascaded refinement: 0.41514489177183955, 1
refinement network: 0.41514489177183955, 1
d data: 0.41514489177183955, 1
indoor environment: 0.41514489177183955, 1
multimodal indoor: 0.41514489177183955, 1
indoor simulator: 0.41514489177183955, 1
simulator navigation: 0.41514489177183955, 1
complex environment: 0.41514489177183955, 1
building generalizable: 0.41514489177183955, 1
generalizable agent: 0.41514489177183955, 1
agent realistic: 0.41514489177183955, 1
realistic rich: 0.41514489177183955, 1
approach traversability: 0.41514489177183955, 1
traversability estimation: 0.41514489177183955, 1
virtual worlds: 0.41514489177183955, 1
worlds proxy: 0.41514489177183955, 1
cognitive mapping: 0.41514489177183955, 1
mapping planning: 0.41514489177183955, 1
planning visual: 0.41514489177183955, 1
learning hand: 0.41514489177183955, 1
hand eye: 0.41514489177183955, 1
eye coordination: 0.41514489177183955, 1
coordination robot: 0.41514489177183955, 1
grasp deep: 0.41514489177183955, 1
scale data: 0.41514489177183955, 1
data collection: 0.41514489177183955, 1
open urban: 0.41514489177183955, 1
urban driving: 0.41514489177183955, 1
driving simulator: 0.41514489177183955, 1
target driven: 0.41514489177183955, 1
navigation indoor: 0.41514489177183955, 1
return frustratingly: 0.41514489177183955, 1
fast image: 0.41514489177183955, 1
processing fully: 0.41514489177183955, 1
deep spatial: 0.41514489177183955, 1
spatial autoencoders: 0.41514489177183955, 1
autoencoders visuomotor: 0.41514489177183955, 1
visuomotor learning: 0.41514489177183955, 1
language interpreting: 0.41514489177183955, 1
interpreting visually: 0.41514489177183955, 1
visually grounded: 0.41514489177183955, 1
grounded navigation: 0.41514489177183955, 1
navigation instructions: 0.41514489177183955, 1
instructions real: 0.41514489177183955, 1
real environment: 0.41514489177183955, 1
scale context: 0.41514489177183955, 1
context aggregation: 0.41514489177183955, 1
aggregation dilated: 0.41514489177183955, 1
dilated convolutions: 0.41514489177183955, 1
newtonian image: 0.41514489177183955, 1
image unfolding: 0.41514489177183955, 1
unfolding dynamics: 0.41514489177183955, 1
dynamics object: 0.41514489177183955, 1
object static: 0.41514489177183955, 1
deep correlation: 0.41514489177183955, 1
correlation alignment: 0.41514489177183955, 1
alignment deep: 0.41514489177183955, 1
deep domain: 0.41514489177183955, 1
let large: 0.41514489177183955, 1
scale texturing: 0.41514489177183955, 1
texturing 3d: 0.41514489177183955, 1
3d reconstructions: 0.41514489177183955, 1
configurable 3d: 0.41514489177183955, 1
scene synthesis: 0.41514489177183955, 1
synthesis 2d: 0.41514489177183955, 1
rendering per: 0.41514489177183955, 1
pixel ground: 0.41514489177183955, 1
truth stochastic: 0.41514489177183955, 1
linear matrix: 0.41514489177183955, 1
matrix inequalities: 0.41514489177183955, 1
inequalities system: 0.41514489177183955, 1
system control: 0.41514489177183955, 1
control theory: 0.41514489177183955, 1
learning activities: 0.41514489177183955, 1
activities enactments: 0.41514489177183955, 1
enactments learning: 0.41514489177183955, 1
affordance review: 0.41514489177183955, 1
review based: 0.41514489177183955, 1
based classification: 0.41514489177183955, 1
motivational effect: 0.41514489177183955, 1
effect educational: 0.41514489177183955, 1
affordance serious: 0.41514489177183955, 1
serious games: 0.41514489177183955, 1
learning cantonese: 0.41514489177183955, 1
cantonese opera: 0.41514489177183955, 1
opera movements: 0.41514489177183955, 1
estimation via: 0.41514489177183955, 1
via spatial: 0.41514489177183955, 1
spatial temporal: 0.41514489177183955, 1
temporal attention: 0.41514489177183955, 1
attention scene: 0.41514489177183955, 1
scene point: 0.41514489177183955, 1
affordance wearable: 0.41514489177183955, 1
based holistic: 0.41514489177183955, 1
holistic scene: 0.41514489177183955, 1
understanding towards: 0.41514489177183955, 1
towards proactive: 0.41514489177183955, 1
proactive collaboration: 0.41514489177183955, 1
parallel vision: 0.41514489177183955, 1
vision perception: 0.41514489177183955, 1
perception understanding: 0.41514489177183955, 1
understanding complex: 0.41514489177183955, 1
complex perspectives: 0.41514489177183955, 1
mixed method: 0.41514489177183955, 1
method assessment: 0.41514489177183955, 1
assessment flow: 0.41514489177183955, 1
flow experiences: 0.41514489177183955, 1
experiences during: 0.41514489177183955, 1
during mobile: 0.41514489177183955, 1
mobile augmented: 0.41514489177183955, 1
augmented reality: 0.41514489177183955, 1
reality science: 0.41514489177183955, 1
science game: 0.41514489177183955, 1
influence e: 0.41514489177183955, 1
e commerce: 0.41514489177183955, 1
commerce live: 0.41514489177183955, 1
live streaming: 0.41514489177183955, 1
streaming affordance: 0.41514489177183955, 1
affordance gift: 0.41514489177183955, 1
gift giving: 0.41514489177183955, 1
giving purchase: 0.41514489177183955, 1
purchase intention: 0.41514489177183955, 1
3d slicer: 0.41514489177183955, 1
slicer image: 0.41514489177183955, 1
image computing: 0.41514489177183955, 1
computing platform: 0.41514489177183955, 1
platform quantitative: 0.41514489177183955, 1
quantitative imaging: 0.41514489177183955, 1
imaging network: 0.41514489177183955, 1
future simultaneous: 0.41514489177183955, 1
simultaneous localization: 0.41514489177183955, 1
localization toward: 0.41514489177183955, 1
toward robust: 0.41514489177183955, 1
robust perception: 0.41514489177183955, 1
perception age: 0.41514489177183955, 1
survey autonomous: 0.41514489177183955, 1
autonomous practices: 0.41514489177183955, 1
practices emerging: 0.41514489177183955, 1
content videogames: 0.41514489177183955, 1
videogames designed: 0.41514489177183955, 1
designed experience: 0.41514489177183955, 1
self organizing: 0.41514489177183955, 1
organizing approach: 0.41514489177183955, 1
approach background: 0.41514489177183955, 1
background subtraction: 0.41514489177183955, 1
subtraction visual: 0.41514489177183955, 1
visual surveillance: 0.41514489177183955, 1
surveillance applications: 0.41514489177183955, 1
driven grasp: 0.41514489177183955, 1
grasp survey: 0.41514489177183955, 1
wealth social: 0.41514489177183955, 1
social production: 0.41514489177183955, 1
production transforms: 0.41514489177183955, 1
transforms markets: 0.41514489177183955, 1
markets freedom: 0.41514489177183955, 1
metaverse beyond: 0.41514489177183955, 1
beyond multidisciplinary: 0.41514489177183955, 1
multidisciplinary perspectives: 0.41514489177183955, 1
perspectives emerging: 0.41514489177183955, 1
emerging agenda: 0.41514489177183955, 1
agenda practice: 0.41514489177183955, 1
practice policy: 0.41514489177183955, 1
socializing affordance: 0.41514489177183955, 1
affordance pretend: 0.41514489177183955, 1
pretend object: 0.41514489177183955, 1
object play: 0.41514489177183955, 1
perception emotional: 0.41514489177183955, 1
emotional affordance: 0.41514489177183955, 1
learning disabled: 0.41514489177183955, 1
disabled students: 0.41514489177183955, 1
students without: 0.41514489177183955, 1
without visual: 0.41514489177183955, 1
spatial deficits: 0.41514489177183955, 1
early objectification: 0.41514489177183955, 1
objectification self: 0.41514489177183955, 1
review auditory: 0.41514489177183955, 1
auditory perception: 0.41514489177183955, 1
theories prospects: 0.41514489177183955, 1
prospects ecological: 0.41514489177183955, 1
ecological account: 0.41514489177183955, 1
emergent origins: 0.41514489177183955, 1
origins early: 0.41514489177183955, 1
early development: 0.41514489177183955, 1
development human: 0.41514489177183955, 1
occupation parent: 0.41514489177183955, 1
parent child: 0.41514489177183955, 1
child interaction: 0.41514489177183955, 1
interaction service: 0.41514489177183955, 1
service social: 0.41514489177183955, 1
social competence: 0.41514489177183955, 1
centered integrating: 0.41514489177183955, 1
integrating goal: 0.41514489177183955, 1
ecological task: 0.41514489177183955, 1
analysis approach: 0.41514489177183955, 1
approach understanding: 0.41514489177183955, 1
understanding motor: 0.41514489177183955, 1
motor development: 0.41514489177183955, 1
development mental: 0.41514489177183955, 1
mental philosophical: 0.41514489177183955, 1
philosophical theoretical: 0.41514489177183955, 1
theoretical underpinnings: 0.41514489177183955, 1
design systems: 0.41514489177183955, 1
systems learning: 0.41514489177183955, 1
learning working: 0.41514489177183955, 1
working librarianship: 0.41514489177183955, 1
encountering toward: 0.41514489177183955, 1
media lived: 0.41514489177183955, 1
lived ecological: 0.41514489177183955, 1
psychology educational: 0.41514489177183955, 1
educational technology: 0.41514489177183955, 1
affordance constraints: 0.41514489177183955, 1
constraints internet: 0.41514489177183955, 1
internet learning: 0.41514489177183955, 1
epistemic technology: 0.41514489177183955, 1
technology relevance: 0.41514489177183955, 1
relevance rethinking: 0.41514489177183955, 1
rethinking cognitive: 0.41514489177183955, 1
cognitive technology: 0.41514489177183955, 1
reality ecological: 0.41514489177183955, 1
approach cognitive: 0.41514489177183955, 1
cognitive film: 0.41514489177183955, 1
film theory: 0.41514489177183955, 1
autonomous grounding: 0.41514489177183955, 1
grounding mechanism: 0.41514489177183955, 1
mechanism sensory: 0.41514489177183955, 1
sensory input: 0.41514489177183955, 1
explorations perceptional: 0.41514489177183955, 1
perceptional mechanism: 0.41514489177183955, 1
mechanism adaptive: 0.41514489177183955, 1
adaptive agent: 0.41514489177183955, 1
perception cognition: 0.41514489177183955, 1
visual self: 0.41514489177183955, 1
self recognition: 0.41514489177183955, 1
recognition infancy: 0.41514489177183955, 1
object designing: 0.41514489177183955, 1
designing product: 0.41514489177183955, 1
product semantics: 0.41514489177183955, 1
semantics experiential: 0.41514489177183955, 1
experiential semantics: 0.41514489177183955, 1
ontology affordance: 0.41514489177183955, 1
relevance ecological: 0.41514489177183955, 1
perception environment: 0.41514489177183955, 1
environment behavior: 0.41514489177183955, 1
behavior study: 0.41514489177183955, 1
ecological schema: 0.41514489177183955, 1
schema theoretic: 0.41514489177183955, 1
theoretic approach: 0.41514489177183955, 1
approach stimulus: 0.41514489177183955, 1
stimulus response: 0.41514489177183955, 1
response compatibility: 0.41514489177183955, 1
three centuries: 0.41514489177183955, 1
centuries category: 0.41514489177183955, 1
category errors: 0.41514489177183955, 1
errors study: 0.41514489177183955, 1
study neural: 0.41514489177183955, 1
neural basis: 0.41514489177183955, 1
basis consciousness: 0.41514489177183955, 1
consciousness intentionality: 0.41514489177183955, 1
ecological information: 0.41514489177183955, 1
systems support: 0.41514489177183955, 1
support coupling: 0.41514489177183955, 1
coupling domain: 0.41514489177183955, 1
domain information: 0.41514489177183955, 1
information user: 0.41514489177183955, 1
user characteristics: 0.41514489177183955, 1
skill acquisition: 0.41514489177183955, 1
acquisition applications: 0.41514489177183955, 1
applications evolving: 0.41514489177183955, 1
evolving practice: 0.41514489177183955, 1
practice ecology: 0.41514489177183955, 1
perception material: 0.41514489177183955, 1
material historical: 0.41514489177183955, 1
historical cross: 0.41514489177183955, 1
cross cultural: 0.41514489177183955, 1
cultural perspectives: 0.41514489177183955, 1
perception threshhold: 0.41514489177183955, 1
threshhold thinking: 0.41514489177183955, 1
thinking machine: 0.41514489177183955, 1
ecology physical: 0.41514489177183955, 1
physical activity: 0.41514489177183955, 1
activity merging: 0.41514489177183955, 1
merging science: 0.41514489177183955, 1
science practice: 0.41514489177183955, 1
principles method: 0.41514489177183955, 1
method landscape: 0.41514489177183955, 1
landscape ecology: 0.41514489177183955, 1
affordance acting: 0.41514489177183955, 1
acting direct: 0.41514489177183955, 1
direct manipulation: 0.41514489177183955, 1
manipulation interfaces: 0.41514489177183955, 1
visually controlled: 0.41514489177183955, 1
controlled 40: 0.41514489177183955, 1
40 years: 0.41514489177183955, 1
years later: 0.41514489177183955, 1
ecological theory: 0.41514489177183955, 1
theory expertise: 0.41514489177183955, 1
expertise effect: 0.41514489177183955, 1
effect memory: 0.41514489177183955, 1
street extended: 0.41514489177183955, 1
extended concept: 0.41514489177183955, 1
affordance means: 0.41514489177183955, 1
means shaping: 0.41514489177183955, 1
shaping environment: 0.41514489177183955, 1
dimensions perception: 0.41514489177183955, 1
perception scaling: 0.41514489177183955, 1
scaling passability: 0.41514489177183955, 1
planning user: 0.41514489177183955, 1
ecological foundations: 0.41514489177183955, 1
foundations symmetry: 0.41514489177183955, 1
symmetry specificity: 0.41514489177183955, 1
specificity animal: 0.41514489177183955, 1
environment systems: 0.41514489177183955, 1
common structure: 0.41514489177183955, 1
structure affordance: 0.41514489177183955, 1
situated ecological: 0.41514489177183955, 1
ecological novel: 0.41514489177183955, 1
novel perspective: 0.41514489177183955, 1
perspective cognitive: 0.41514489177183955, 1
affordance another: 0.41514489177183955, 1
planning representation: 0.41514489177183955, 1
design policy: 0.41514489177183955, 1
policy intelligent: 0.41514489177183955, 1
intelligent space: 0.41514489177183955, 1
perception raison: 0.41514489177183955, 1
raison anchored: 0.41514489177183955, 1
anchored ecological: 0.41514489177183955, 1
psychology perspective: 0.41514489177183955, 1
principles self: 0.41514489177183955, 1
learning participation: 0.41514489177183955, 1
participation autocatakinetic: 0.41514489177183955, 1
autocatakinetic systems: 0.41514489177183955, 1
world collide: 0.41514489177183955, 1
affordance events: 0.41514489177183955, 1
ecological field: 0.41514489177183955, 1
theory perception: 0.41514489177183955, 1
perception control: 0.41514489177183955, 1
control locomotion: 0.41514489177183955, 1
events affordance: 0.41514489177183955, 1
clarifying evolving: 0.41514489177183955, 1
evolving concept: 0.41514489177183955, 1
foundations designing: 0.41514489177183955, 1
designing ecological: 0.41514489177183955, 1
ecological interface: 0.41514489177183955, 1
interface mobile: 0.41514489177183955, 1
robot teleoperation: 0.41514489177183955, 1
method language: 0.41514489177183955, 1
language constructing: 0.41514489177183955, 1
constructing multiagent: 0.41514489177183955, 1
multiagent systems: 0.41514489177183955, 1
failings three: 0.41514489177183955, 1
three event: 0.41514489177183955, 1
event perception: 0.41514489177183955, 1
kant essays: 0.41514489177183955, 1
essays language: 0.41514489177183955, 1
language cognition: 0.41514489177183955, 1
natural environment: 0.41514489177183955, 1
environment playground: 0.41514489177183955, 1
playground landscape: 0.41514489177183955, 1
landscape description: 0.41514489177183955, 1
description analyses: 0.41514489177183955, 1
analyses natural: 0.41514489177183955, 1
natural playscape: 0.41514489177183955, 1
affordance activity: 0.41514489177183955, 1
theory cognitive: 0.41514489177183955, 1
cognitive systems: 0.41514489177183955, 1
systems engineering: 0.41514489177183955, 1
affordance inertial: 0.41514489177183955, 1
inertial constraints: 0.41514489177183955, 1
constraints tool: 0.41514489177183955, 1
action schemas: 0.41514489177183955, 1
influence object: 0.41514489177183955, 1
object orientation: 0.41514489177183955, 1
orientation speed: 0.41514489177183955, 1
speed object: 0.41514489177183955, 1
affordance facilitation: 0.41514489177183955, 1
facilitation cognitive: 0.41514489177183955, 1
cognitive coding: 0.41514489177183955, 1
simulate human: 0.41514489177183955, 1
perception model: 0.41514489177183955, 1
model unfamiliar: 0.41514489177183955, 1
unfamiliar buildings: 0.41514489177183955, 1
ontology epistemology: 0.41514489177183955, 1
epistemology agent: 0.41514489177183955, 1
based wayfinding: 0.41514489177183955, 1
wayfinding simulate: 0.41514489177183955, 1
ecological physics: 0.41514489177183955, 1
physics physical: 0.41514489177183955, 1
physical psychology: 0.41514489177183955, 1
psychology james: 0.41514489177183955, 1
james roger: 0.41514489177183955, 1
roger legacy: 0.41514489177183955, 1
legacy william: 0.41514489177183955, 1
william radical: 0.41514489177183955, 1
radical empiricism: 0.41514489177183955, 1
texts affordance: 0.41514489177183955, 1
human wayfinding: 0.41514489177183955, 1
wayfinding unfamiliar: 0.41514489177183955, 1
unfamiliar simulate: 0.41514489177183955, 1
simulate cognizing: 0.41514489177183955, 1
cognizing agent: 0.41514489177183955, 1
approach embodiment: 0.41514489177183955, 1
embodiment cognition: 0.41514489177183955, 1
introduction ecology: 0.41514489177183955, 1
ecology spatio: 0.41514489177183955, 1
temporal affordance: 0.41514489177183955, 1
affordance airspace: 0.41514489177183955, 1
psychology environmentally: 0.41514489177183955, 1
environmentally sustainable: 0.41514489177183955, 1
sustainable fitting: 0.41514489177183955, 1
fitting together: 0.41514489177183955, 1
together pieces: 0.41514489177183955, 1
pieces puzzle: 0.41514489177183955, 1
theory human: 0.41514489177183955, 1
human ecological: 0.41514489177183955, 1
ecological control: 0.41514489177183955, 1
control network: 0.41514489177183955, 1
network role: 0.41514489177183955, 1
role emotion: 0.41514489177183955, 1
intuition design: 0.41514489177183955, 1
smart smart: 0.41514489177183955, 1
smart talent: 0.41514489177183955, 1
talent development: 0.41514489177183955, 1
development age: 0.41514489177183955, 1
age situated: 0.41514489177183955, 1
situated approach: 0.41514489177183955, 1
approach knowing: 0.41514489177183955, 1
knowing learning: 0.41514489177183955, 1
combinatory grammar: 0.41514489177183955, 1
autonomy proactive: 0.41514489177183955, 1
proactive perception: 0.41514489177183955, 1
perception virtual: 0.41514489177183955, 1
virtual actors: 0.41514489177183955, 1
ecological model: 0.41514489177183955, 1
human performance: 0.41514489177183955, 1
performance based: 0.41514489177183955, 1
based emotion: 0.41514489177183955, 1
four points: 0.41514489177183955, 1
points debate: 0.41514489177183955, 1
beyond lens: 0.41514489177183955, 1
lens model: 0.41514489177183955, 1
model direct: 0.41514489177183955, 1
direct toward: 0.41514489177183955, 1
toward broader: 0.41514489177183955, 1
broader ecological: 0.41514489177183955, 1
mobile applications: 0.41514489177183955, 1
effective design: 0.41514489177183955, 1
design principles: 0.41514489177183955, 1
principles activity: 0.41514489177183955, 1
activity based: 0.41514489177183955, 1
based crucial: 0.41514489177183955, 1
role science: 0.41514489177183955, 1
science engineering: 0.41514489177183955, 1
engineering education: 0.41514489177183955, 1
agent environment: 0.41514489177183955, 1
environment indirect: 0.41514489177183955, 1
indirect direct: 0.41514489177183955, 1
language acquisition: 0.41514489177183955, 1
acquisition language: 0.41514489177183955, 1
language ecological: 0.41514489177183955, 1
ecological perspectives: 0.41514489177183955, 1
ecological philosophy: 0.41514489177183955, 1
study direct: 0.41514489177183955, 1
perception collision: 0.41514489177183955, 1
collision avoidance: 0.41514489177183955, 1
avoidance mediated: 0.41514489177183955, 1
mediated brightness: 0.41514489177183955, 1
brightness differences: 0.41514489177183955, 1
functional affordance: 0.41514489177183955, 1
sociable cscl: 0.41514489177183955, 1
cscl social: 0.41514489177183955, 1
social social: 0.41514489177183955, 1
pickup nonspecifying: 0.41514489177183955, 1
nonspecifying variables: 0.41514489177183955, 1
variables does: 0.41514489177183955, 1
does entail: 0.41514489177183955, 1
entail indirect: 0.41514489177183955, 1
indirect perception: 0.41514489177183955, 1
affordance garden: 0.41514489177183955, 1
garden towards: 0.41514489177183955, 1
towards restorative: 0.41514489177183955, 1
restorative process: 0.41514489177183955, 1
process hospitalized: 0.41514489177183955, 1
hospitalized children: 0.41514489177183955, 1
understanding technological: 0.41514489177183955, 1
technological commentary: 0.41514489177183955, 1
commentary conole: 0.41514489177183955, 1
conole dyke: 0.41514489177183955, 1
art perception: 0.41514489177183955, 1
perception robot: 0.41514489177183955, 1
perception bayesian: 0.41514489177183955, 1
geography services: 0.41514489177183955, 1
ecology semiotics: 0.41514489177183955, 1
semiotics language: 0.41514489177183955, 1
language sociocultural: 0.41514489177183955, 1
sociocultural perspective: 0.41514489177183955, 1
ecological integration: 0.41514489177183955, 1
integration affordance: 0.41514489177183955, 1
affordance drives: 0.41514489177183955, 1
drives behaviour: 0.41514489177183955, 1
behaviour selection: 0.41514489177183955, 1
affordance aperture: 0.41514489177183955, 1
aperture crossing: 0.41514489177183955, 1
crossing person: 0.41514489177183955, 1
person plus: 0.41514489177183955, 1
plus object: 0.41514489177183955, 1
object system: 0.41514489177183955, 1
ways ecological: 0.41514489177183955, 1
perception musical: 0.41514489177183955, 1
musical meaning: 0.41514489177183955, 1
opportunity pedagogical: 0.41514489177183955, 1
pedagogical implications: 0.41514489177183955, 1
implications ict: 0.41514489177183955, 1
evoking affordance: 0.41514489177183955, 1
affordance virtual: 0.41514489177183955, 1
virtual environment: 0.41514489177183955, 1
via sensory: 0.41514489177183955, 1
stimuli substitution: 0.41514489177183955, 1
concept action: 0.41514489177183955, 1
ecological conceptualizations: 0.41514489177183955, 1
conceptualizations perception: 0.41514489177183955, 1
systems action: 0.41514489177183955, 1
action systems: 0.41514489177183955, 1
reinventing intelligence: 0.41514489177183955, 1
intelligence invented: 0.41514489177183955, 1
invented world: 0.41514489177183955, 1
06231 abstracts: 0.41514489177183955, 1
abstracts collection: 0.41514489177183955, 1
collection towards: 0.41514489177183955, 1
perception traversibility: 0.41514489177183955, 1
traversibility affordance: 0.41514489177183955, 1
image learning: 0.41514489177183955, 1
learning mobile: 0.41514489177183955, 1
environmental annotations: 0.41514489177183955, 1
annotations affordance: 0.41514489177183955, 1
model culture: 0.41514489177183955, 1
affordance level: 0.41514489177183955, 1
level detail: 0.41514489177183955, 1
detail ai: 0.41514489177183955, 1
ai virtual: 0.41514489177183955, 1
virtual human: 0.41514489177183955, 1
affordance hockey: 0.41514489177183955, 1
hockey sticks: 0.41514489177183955, 1
sticks dynamic: 0.41514489177183955, 1
dynamic touch: 0.41514489177183955, 1
distributed affordance: 0.41514489177183955, 1
petri nets: 0.41514489177183955, 1
nets steps: 0.41514489177183955, 1
steps perception: 0.41514489177183955, 1
based task: 0.41514489177183955, 1
dynamics decision: 0.41514489177183955, 1
making sport: 0.41514489177183955, 1
control visually: 0.41514489177183955, 1
guided action: 0.41514489177183955, 1
reconciling evolutionary: 0.41514489177183955, 1
evolutionary psychology: 0.41514489177183955, 1
psychology ecological: 0.41514489177183955, 1
perception fitness: 0.41514489177183955, 1
fitness affordance: 0.41514489177183955, 1
driven learning: 0.41514489177183955, 1
action sport: 0.41514489177183955, 1
functional model: 0.41514489177183955, 1
ecological six: 0.41514489177183955, 1
six principles: 0.41514489177183955, 1
principles approach: 0.41514489177183955, 1
approach behavior: 0.41514489177183955, 1
ecology physically: 0.41514489177183955, 1
physically embedded: 0.41514489177183955, 1
embedded intelligent: 0.41514489177183955, 1
intelligent systems: 0.41514489177183955, 1
evolution pivotal: 0.41514489177183955, 1
pivotal concept: 0.41514489177183955, 1
sharing representation: 0.41514489177183955, 1
representation creating: 0.41514489177183955, 1
creating chances: 0.41514489177183955, 1
chances cognitive: 0.41514489177183955, 1
niche role: 0.41514489177183955, 1
affordance abduction: 0.41514489177183955, 1
intrinsic reward: 0.41514489177183955, 1
reward affordance: 0.41514489177183955, 1
affordance exploration: 0.41514489177183955, 1
object states: 0.41514489177183955, 1
states learned: 0.41514489177183955, 1
autonomous exploration: 0.41514489177183955, 1
based relational: 0.41514489177183955, 1
relational theory: 0.41514489177183955, 1
theory design: 0.41514489177183955, 1
granular 12th: 0.41514489177183955, 1
12th international: 0.41514489177183955, 1
international rsfdgrc: 0.41514489177183955, 1
rsfdgrc december: 0.41514489177183955, 1
december 16: 0.41514489177183955, 1
granular computing: 0.41514489177183955, 1
approach architectural: 0.41514489177183955, 1
architectural practice: 0.41514489177183955, 1
based word: 0.41514489177183955, 1
word meaning: 0.41514489177183955, 1
meaning association: 0.41514489177183955, 1
affordance help: 0.41514489177183955, 1
help understand: 0.41514489177183955, 1
understand ict: 0.41514489177183955, 1
affordance qualities: 0.41514489177183955, 1
niche plasticity: 0.41514489177183955, 1
plasticity environmental: 0.41514489177183955, 1
environmental situated: 0.41514489177183955, 1
speech perception: 0.41514489177183955, 1
framework learning: 0.41514489177183955, 1
grounded semantic: 0.41514489177183955, 1
semantic reasoning: 0.41514489177183955, 1
reasoning robot: 0.41514489177183955, 1
perception manipulation: 0.41514489177183955, 1
affordance evolutionary: 0.41514489177183955, 1
evolutionary process: 0.41514489177183955, 1
process niche: 0.41514489177183955, 1
niche construction: 0.41514489177183955, 1
construction perspective: 0.41514489177183955, 1
predict via: 0.41514489177183955, 1
via learned: 0.41514489177183955, 1
learned object: 0.41514489177183955, 1
object attributes: 0.41514489177183955, 1
going beyond: 0.41514489177183955, 1
beyond perception: 0.41514489177183955, 1
learning actualize: 0.41514489177183955, 1
actualize behavioral: 0.41514489177183955, 1
behavioral parameters: 0.41514489177183955, 1
learning generalizable: 0.41514489177183955, 1
generalizable control: 0.41514489177183955, 1
control programs: 0.41514489177183955, 1
affordance language: 0.41514489177183955, 1
language learning: 0.41514489177183955, 1
learning beyond: 0.41514489177183955, 1
beyond classroom: 0.41514489177183955, 1
comparative study: 0.41514489177183955, 1
navigation model: 0.41514489177183955, 1
based experimental: 0.41514489177183955, 1
experimental analysis: 0.41514489177183955, 1
analysis learning: 0.41514489177183955, 1
learning ability: 0.41514489177183955, 1
ability mobile: 0.41514489177183955, 1
robot taps: 0.41514489177183955, 1
taps object: 0.41514489177183955, 1
planning mobile: 0.41514489177183955, 1
mobile manipulation: 0.41514489177183955, 1
manipulation platform: 0.41514489177183955, 1
awareness multilinguals: 0.41514489177183955, 1
multilinguals versus: 0.41514489177183955, 1
versus bilinguals: 0.41514489177183955, 1
bilinguals perception: 0.41514489177183955, 1
perception cognates: 0.41514489177183955, 1
materiality affordance: 0.41514489177183955, 1
affordance perspective: 0.41514489177183955, 1
affordance invite: 0.41514489177183955, 1
invite reconsidering: 0.41514489177183955, 1
reconsidering relationship: 0.41514489177183955, 1
relationship affordance: 0.41514489177183955, 1
affordance agency: 0.41514489177183955, 1
based framework: 0.41514489177183955, 1
framework human: 0.41514489177183955, 1
human computation: 0.41514489177183955, 1
computation human: 0.41514489177183955, 1
computer collaboration: 0.41514489177183955, 1
music affordance: 0.41514489177183955, 1
affordance significance: 0.41514489177183955, 1
significance sidebar: 0.41514489177183955, 1
sidebar james: 0.41514489177183955, 1
james last: 0.41514489177183955, 1
last book: 0.41514489177183955, 1
toward mediated: 0.41514489177183955, 1
mediated action: 0.41514489177183955, 1
action perspective: 0.41514489177183955, 1
why gamers: 0.41514489177183955, 1
gamers learn: 0.41514489177183955, 1
learn ecological: 0.41514489177183955, 1
approach games: 0.41514489177183955, 1
affordance ai: 0.41514489177183955, 1
landscape gis: 0.41514489177183955, 1
gis role: 0.41514489177183955, 1
terrorism introduction: 0.41514489177183955, 1
parallel deep: 0.41514489177183955, 1
learning suggestive: 0.41514489177183955, 1
suggestive activation: 0.41514489177183955, 1
activation object: 0.41514489177183955, 1
category recognition: 0.41514489177183955, 1
ecosystem wide: 0.41514489177183955, 1
wide characteristics: 0.41514489177183955, 1
characteristics esl: 0.41514489177183955, 1
esl environment: 0.41514489177183955, 1
affordance semiotics: 0.41514489177183955, 1
semiotics perspective: 0.41514489177183955, 1
simulate affordance: 0.41514489177183955, 1
based human: 0.41514489177183955, 1
human behavior: 0.41514489177183955, 1
behavior emergency: 0.41514489177183955, 1
conceptualizing perception: 0.41514489177183955, 1
media interaction: 0.41514489177183955, 1
intention estimation: 0.41514489177183955, 1
estimation recommendation: 0.41514489177183955, 1
recommendation system: 0.41514489177183955, 1
system based: 0.41514489177183955, 1
based attention: 0.41514489177183955, 1
attention sharing: 0.41514489177183955, 1
dynamics approach: 0.41514489177183955, 1
approach skill: 0.41514489177183955, 1
skill implications: 0.41514489177183955, 1
implications development: 0.41514489177183955, 1
development talent: 0.41514489177183955, 1
talent sport: 0.41514489177183955, 1
perception kind: 0.41514489177183955, 1
kind science: 0.41514489177183955, 1
science does: 0.41514489177183955, 1
recognizing object: 0.41514489177183955, 1
terms spatio: 0.41514489177183955, 1
object relationships: 0.41514489177183955, 1
significance ecological: 0.41514489177183955, 1
ecological methodology: 0.41514489177183955, 1
methodology affordance: 0.41514489177183955, 1
based design: 0.41514489177183955, 1
does concept: 0.41514489177183955, 1
affordance add: 0.41514489177183955, 1
add anything: 0.41514489177183955, 1
anything explanations: 0.41514489177183955, 1
explanations compatibility: 0.41514489177183955, 1
self free: 0.41514489177183955, 1
energy optimal: 0.41514489177183955, 1
optimal grip: 0.41514489177183955, 1
grip field: 0.41514489177183955, 1
discovering inner: 0.41514489177183955, 1
inner reconciling: 0.41514489177183955, 1
reconciling action: 0.41514489177183955, 1
action specific: 0.41514489177183955, 1
specific ecological: 0.41514489177183955, 1
social cognition: 0.41514489177183955, 1
mining semantic: 0.41514489177183955, 1
semantic affordance: 0.41514489177183955, 1
intention understanding: 0.41514489177183955, 1
action classification: 0.41514489177183955, 1
imagined reconstructing: 0.41514489177183955, 1
reconstructing keyword: 0.41514489177183955, 1
keyword communication: 0.41514489177183955, 1
communication theory: 0.41514489177183955, 1
simulate model: 0.41514489177183955, 1
concept model: 0.41514489177183955, 1
model design: 0.41514489177183955, 1
selection interaction: 0.41514489177183955, 1
interaction system: 0.41514489177183955, 1
system cognitive: 0.41514489177183955, 1
cognitive interpretations: 0.41514489177183955, 1
affordance animate: 0.41514489177183955, 1
animate social: 0.41514489177183955, 1
social science: 0.41514489177183955, 1
science ecological: 0.41514489177183955, 1
ecological point: 0.41514489177183955, 1
point view: 0.41514489177183955, 1
theorizing request: 0.41514489177183955, 1
request refuse: 0.41514489177183955, 1
theory engineering: 0.41514489177183955, 1
engineering design: 0.41514489177183955, 1
directed affordance: 0.41514489177183955, 1
predict subtask: 0.41514489177183955, 1
subtask level: 0.41514489177183955, 1
i self: 0.41514489177183955, 1
learning tool: 0.41514489177183955, 1
affordance 3: 0.41514489177183955, 1
d geometry: 0.41514489177183955, 1
single multiple: 0.41514489177183955, 1
multiple instance: 0.41514489177183955, 1
instance object: 0.41514489177183955, 1
why object: 0.41514489177183955, 1
affordance prehension: 0.41514489177183955, 1
prehension architecture: 0.41514489177183955, 1
affording adopting: 0.41514489177183955, 1
adopting theory: 0.41514489177183955, 1
affordance guiding: 0.41514489177183955, 1
guiding heuristic: 0.41514489177183955, 1
heuristic environmental: 0.41514489177183955, 1
environmental policy: 0.41514489177183955, 1
weighted affordance: 0.41514489177183955, 1
agent modeling: 0.41514489177183955, 1
modeling simulate: 0.41514489177183955, 1
simulate emergency: 0.41514489177183955, 1
sense fast: 0.41514489177183955, 1
fast potential: 0.41514489177183955, 1
potential contributions: 0.41514489177183955, 1
contributions affordance: 0.41514489177183955, 1
theory sense: 0.41514489177183955, 1
sense place: 0.41514489177183955, 1
active inference: 0.41514489177183955, 1
inference approach: 0.41514489177183955, 1
approach ecological: 0.41514489177183955, 1
ecological general: 0.41514489177183955, 1
general information: 0.41514489177183955, 1
information dynamics: 0.41514489177183955, 1
dynamics natural: 0.41514489177183955, 1
natural artificial: 0.41514489177183955, 1
artificial embodied: 0.41514489177183955, 1
exercising affordance: 0.41514489177183955, 1
affordance part: 0.41514489177183955, 1
part based: 0.41514489177183955, 1
computational mechanism: 0.41514489177183955, 1
mechanism underlying: 0.41514489177183955, 1
underlying cortical: 0.41514489177183955, 1
cortical responses: 0.41514489177183955, 1
responses affordance: 0.41514489177183955, 1
properties visual: 0.41514489177183955, 1
visual scene: 0.41514489177183955, 1
toward theoretical: 0.41514489177183955, 1
theoretical base: 0.41514489177183955, 1
representation design: 0.41514489177183955, 1
design computer: 0.41514489177183955, 1
computer ecological: 0.41514489177183955, 1
perception aiding: 0.41514489177183955, 1
aiding human: 0.41514489177183955, 1
human cognition: 0.41514489177183955, 1
oh places: 0.41514489177183955, 1
places affordance: 0.41514489177183955, 1
model social: 0.41514489177183955, 1
media posting: 0.41514489177183955, 1
posting behavior: 0.41514489177183955, 1
driven inverse: 0.41514489177183955, 1
learning conceptual: 0.41514489177183955, 1
conceptual abstraction: 0.41514489177183955, 1
abstraction advice: 0.41514489177183955, 1
modal feedback: 0.41514489177183955, 1
feedback affordance: 0.41514489177183955, 1
driven interactive: 0.41514489177183955, 1
review intelligent: 0.41514489177183955, 1
intelligent object: 0.41514489177183955, 1
perception method: 0.41514489177183955, 1
method combining: 0.41514489177183955, 1
combining knowledge: 0.41514489177183955, 1
knowledge based: 0.41514489177183955, 1
based reasoning: 0.41514489177183955, 1
reasoning machine: 0.41514489177183955, 1
perception perceiving: 0.41514489177183955, 1
perceiving learning: 0.41514489177183955, 1
bootstrapping robot: 0.41514489177183955, 1
robot ecological: 0.41514489177183955, 1
perception exploration: 0.41514489177183955, 1
exploration interaction: 0.41514489177183955, 1
james ecological: 0.41514489177183955, 1
approach locomotion: 0.41514489177183955, 1
locomotion development: 0.41514489177183955, 1
development changing: 0.41514489177183955, 1
changing affordance: 0.41514489177183955, 1
affordance space: 0.41514489177183955, 1
space physical: 0.41514489177183955, 1
world vision: 0.41514489177183955, 1
reasoning smart: 0.41514489177183955, 1
smart affordance: 0.41514489177183955, 1
affordance personalized: 0.41514489177183955, 1
personalized behavior: 0.41514489177183955, 1
behavior monitoring: 0.41514489177183955, 1
monitoring pervasive: 0.41514489177183955, 1
pervasive information: 0.41514489177183955, 1
learning move: 0.41514489177183955, 1
move affordance: 0.41514489177183955, 1
incorporating object: 0.41514489177183955, 1
object intrinsic: 0.41514489177183955, 1
intrinsic feature: 0.41514489177183955, 1
within deep: 0.41514489177183955, 1
deep grasp: 0.41514489177183955, 1
affordance innovative: 0.41514489177183955, 1
innovative learning: 0.41514489177183955, 1
environment deep: 0.41514489177183955, 1
deep perception: 0.41514489177183955, 1
uav environmental: 0.41514489177183955, 1
autonomous obstacle: 0.41514489177183955, 1
obstacle deep: 0.41514489177183955, 1
depth camera: 0.41514489177183955, 1
camera combined: 0.41514489177183955, 1
combined solution: 0.41514489177183955, 1
3d benchmark: 0.41514489177183955, 1
benchmark visual: 0.41514489177183955, 1
affordance understanding: 0.41514489177183955, 1
affordance target: 0.41514489177183955, 1
target orientated: 0.41514489177183955, 1
orientated deep: 0.41514489177183955, 1
deep q: 0.41514489177183955, 1
q network: 0.41514489177183955, 1
network grasp: 0.41514489177183955, 1
object harnessing: 0.41514489177183955, 1
harnessing environmental: 0.41514489177183955, 1
environmental fixtures: 0.41514489177183955, 1
affordance guided: 0.41514489177183955, 1
guided tactile: 0.41514489177183955, 1
tactile material: 0.41514489177183955, 1
material recognition: 0.41514489177183955, 1
recognition waste: 0.41514489177183955, 1
waste recycling: 0.41514489177183955, 1
point detection: 0.41514489177183955, 1
detection graph: 0.41514489177183955, 1
graph convolutional: 0.41514489177183955, 1
network industrial: 0.41514489177183955, 1
industrial bin: 0.41514489177183955, 1
bin picking: 0.41514489177183955, 1
picking applications: 0.41514489177183955, 1
improve robot: 0.41514489177183955, 1
robot understanding: 0.41514489177183955, 1
detection efficient: 0.41514489177183955, 1
efficient attention: 0.41514489177183955, 1
attention convolutional: 0.41514489177183955, 1
policy based: 0.41514489177183955, 1
learning train: 0.41514489177183955, 1
train autonomous: 0.41514489177183955, 1
driving agent: 0.41514489177183955, 1
agent urban: 0.41514489177183955, 1
urban areas: 0.41514489177183955, 1
areas affordance: 0.41514489177183955, 1
detection wild: 0.41514489177183955, 1
detection boundary: 0.41514489177183955, 1
boundary preserving: 0.41514489177183955, 1
preserving network: 0.41514489177183955, 1
network learning: 0.41514489177183955, 1
learning context: 0.41514489177183955, 1
context dependent: 0.41514489177183955, 1
dependent affordance: 0.41514489177183955, 1
affordance embeddings: 0.41514489177183955, 1
embeddings situated: 0.41514489177183955, 1
situated language: 0.41514489177183955, 1
curing eco: 0.41514489177183955, 1
eco cognitive: 0.41514489177183955, 1
cognitive abduction: 0.41514489177183955, 1
grounding human: 0.41514489177183955, 1
behavior multimodal: 0.41514489177183955, 1
multimodal datasets: 0.41514489177183955, 1
affordance grounding: 0.41514489177183955, 1
grounding demonstration: 0.41514489177183955, 1
demonstration video: 0.41514489177183955, 1
ecological predict: 0.41514489177183955, 1
predict processing: 0.41514489177183955, 1
processing limits: 0.41514489177183955, 1
limits direct: 0.41514489177183955, 1
multi label: 0.41514489177183955, 1
label affordance: 0.41514489177183955, 1
affordance mapping: 0.41514489177183955, 1
mapping egocentric: 0.41514489177183955, 1
egocentric vision: 0.41514489177183955, 1
affordance shape: 0.41514489177183955, 1
shape visual: 0.41514489177183955, 1
years does: 0.41514489177183955, 1
does affordance: 0.41514489177183955, 1
affordance afford: 0.41514489177183955, 1
beyond object: 0.41514489177183955, 1
object benchmark: 0.41514489177183955, 1
benchmark towards: 0.41514489177183955, 1
towards object: 0.41514489177183955, 1
concept learning: 0.41514489177183955, 1
predict hand: 0.41514489177183955, 1
hand occluded: 0.41514489177183955, 1
perception knowledge: 0.41514489177183955, 1
guided vision: 0.41514489177183955, 1
model efficient: 0.41514489177183955, 1
efficient error: 0.41514489177183955, 1
error correction: 0.41514489177183955, 1
affordance labeling: 0.41514489177183955, 1
labeling manifold: 0.41514489177183955, 1
manifold based: 0.41514489177183955, 1
intention predict: 0.41514489177183955, 1
development semantic: 0.41514489177183955, 1
semantic model: 0.41514489177183955, 1
model synthetic: 0.41514489177183955, 1
synthetic dataset: 0.41514489177183955, 1
dataset multi: 0.41514489177183955, 1
multi grasp: 0.41514489177183955, 1
detection application: 0.41514489177183955, 1
application vision: 0.41514489177183955, 1
based upper: 0.41514489177183955, 1
upper limb: 0.41514489177183955, 1
limb prosthetic: 0.41514489177183955, 1
prosthetic grasp: 0.41514489177183955, 1
theory ecological: 0.41514489177183955, 1
ecological moral: 0.41514489177183955, 1
moral learning: 0.41514489177183955, 1
learning moral: 0.41514489177183955, 1
moral affordance: 0.41514489177183955, 1
distinct representation: 0.41514489177183955, 1
representation locomotive: 0.41514489177183955, 1
locomotive action: 0.41514489177183955, 1
human brains: 0.41514489177183955, 1
brains deep: 0.41514489177183955, 1
field scene: 0.41514489177183955, 1
behavior change: 0.41514489177183955, 1
change based: 0.41514489177183955, 1
visual risk: 0.41514489177183955, 1
risk object: 0.41514489177183955, 1
object identification: 0.41514489177183955, 1
body size: 0.41514489177183955, 1
size metric: 0.41514489177183955, 1
metric affordable: 0.41514489177183955, 1
affordable world: 0.41514489177183955, 1
robo affordance: 0.41514489177183955, 1
affordance generalization: 0.41514489177183955, 1
generalization beyond: 0.41514489177183955, 1
beyond categories: 0.41514489177183955, 1
categories via: 0.41514489177183955, 1
via semantic: 0.41514489177183955, 1
semantic correspondence: 0.41514489177183955, 1
correspondence robot: 0.41514489177183955, 1
embodied vision: 0.41514489177183955, 1
language programmer: 0.41514489177183955, 1
programmer environmental: 0.41514489177183955, 1
environmental feedback: 0.41514489177183955, 1
based vehicle: 0.41514489177183955, 1
vehicle detection: 0.41514489177183955, 1
detection tracking: 0.41514489177183955, 1
based headlight: 0.41514489177183955, 1
headlight extraction: 0.41514489177183955, 1
extraction gmm: 0.41514489177183955, 1
gmm clustering: 0.41514489177183955, 1
clustering under: 0.41514489177183955, 1
under low: 0.41514489177183955, 1
low illumination: 0.41514489177183955, 1
illumination conditions: 0.41514489177183955, 1
reducing social: 0.41514489177183955, 1
social inequity: 0.41514489177183955, 1
inequity neighborhood: 0.41514489177183955, 1
neighborhood visual: 0.41514489177183955, 1
visual environment: 0.41514489177183955, 1
environment los: 0.41514489177183955, 1
los angeles: 0.41514489177183955, 1
angeles computer: 0.41514489177183955, 1
vision multi: 0.41514489177183955, 1
model machine: 0.41514489177183955, 1
method differentiating: 0.41514489177183955, 1
differentiating safflower: 0.41514489177183955, 1
safflower germplasm: 0.41514489177183955, 1
germplasm optimal: 0.41514489177183955, 1
optimal leaf: 0.41514489177183955, 1
leaf structure: 0.41514489177183955, 1
structure feature: 0.41514489177183955, 1
price fairness: 0.41514489177183955, 1
fairness perception: 0.41514489177183955, 1
perception online: 0.41514489177183955, 1
online food: 0.41514489177183955, 1
food service: 0.41514489177183955, 1
service data: 0.41514489177183955, 1
driven approach: 0.41514489177183955, 1
approach fsqca: 0.41514489177183955, 1
fsqca machine: 0.41514489177183955, 1
review deep: 0.41514489177183955, 1
based stereo: 0.41514489177183955, 1
stereo vision: 0.41514489177183955, 1
vision techniques: 0.41514489177183955, 1
techniques phenotype: 0.41514489177183955, 1
phenotype feature: 0.41514489177183955, 1
feature behavioral: 0.41514489177183955, 1
behavioral analysis: 0.41514489177183955, 1
analysis fish: 0.41514489177183955, 1
fish aquaculture: 0.41514489177183955, 1
quantum foliage: 0.41514489177183955, 1
foliage finder: 0.41514489177183955, 1
finder revolutionizing: 0.41514489177183955, 1
revolutionizing environmental: 0.41514489177183955, 1
environmental monitoring: 0.41514489177183955, 1
monitoring conservation: 0.41514489177183955, 1
conservation quantum: 0.41514489177183955, 1
quantum computing: 0.41514489177183955, 1
computing deep: 0.41514489177183955, 1
affordance interactive: 0.41514489177183955, 1
interactive exploration: 0.41514489177183955, 1
exploration object: 0.41514489177183955, 1
level map: 0.41514489177183955, 1
multi class: 0.41514489177183955, 1
class lane: 0.41514489177183955, 1
lane marking: 0.41514489177183955, 1
marking segmentation: 0.41514489177183955, 1
segmentation dataset: 0.41514489177183955, 1
dataset vision: 0.41514489177183955, 1
based environmental: 0.41514489177183955, 1
motivated intuitive: 0.41514489177183955, 1
intuitive understanding: 0.41514489177183955, 1
human space: 0.41514489177183955, 1
space possible: 0.41514489177183955, 1
possible ease: 0.41514489177183955, 1
ease generalize: 0.41514489177183955, 1
generalize understanding: 0.41514489177183955, 1
understanding previously: 0.41514489177183955, 1
previously unseen: 0.41514489177183955, 1
unseen develop: 0.41514489177183955, 1
develop approach: 0.41514489177183955, 1
learning given: 0.41514489177183955, 1
given input: 0.41514489177183955, 1
image infer: 0.41514489177183955, 1
infer distribution: 0.41514489177183955, 1
distribution plausible: 0.41514489177183955, 1
plausible future: 0.41514489177183955, 1
future states: 0.41514489177183955, 1
states achieved: 0.41514489177183955, 1
achieved via: 0.41514489177183955, 1
via interaction: 0.41514489177183955, 1
interaction allow: 0.41514489177183955, 1
predict diverse: 0.41514489177183955, 1
diverse plausible: 0.41514489177183955, 1
plausible discretize: 0.41514489177183955, 1
discretize space: 0.41514489177183955, 1
space continuous: 0.41514489177183955, 1
continuous image: 0.41514489177183955, 1
image vq: 0.41514489177183955, 1
vq vae: 0.41514489177183955, 1
vae transformer: 0.41514489177183955, 1
transformer based: 0.41514489177183955, 1
learn conditional: 0.41514489177183955, 1
conditional distribution: 0.41514489177183955, 1
distribution latent: 0.41514489177183955, 1
latent embedding: 0.41514489177183955, 1
embedding model: 0.41514489177183955, 1
train large: 0.41514489177183955, 1
scale diverse: 0.41514489177183955, 1
diverse passive: 0.41514489177183955, 1
passive learned: 0.41514489177183955, 1
learned model: 0.41514489177183955, 1
model exhibit: 0.41514489177183955, 1
exhibit compositional: 0.41514489177183955, 1
compositional generalization: 0.41514489177183955, 1
generalization diverse: 0.41514489177183955, 1
diverse object: 0.41514489177183955, 1
object beyond: 0.41514489177183955, 1
beyond train: 0.41514489177183955, 1
train evaluate: 0.41514489177183955, 1
evaluate quality: 0.41514489177183955, 1
quality diversity: 0.41514489177183955, 1
diversity demonstrate: 0.41514489177183955, 1
model guiding: 0.41514489177183955, 1
guiding exploration: 0.41514489177183955, 1
exploration during: 0.41514489177183955, 1
during visual: 0.41514489177183955, 1
visual goal: 0.41514489177183955, 1
imagine robot: 0.41514489177183955, 1
robot emerging: 0.41514489177183955, 1
emerging concept: 0.41514489177183955, 1
concept involves: 0.41514489177183955, 1
involves capability: 0.41514489177183955, 1
capability automatically: 0.41514489177183955, 1
automatically generate: 0.41514489177183955, 1
generate realistic: 0.41514489177183955, 1
realistic requires: 0.41514489177183955, 1
requires assessment: 0.41514489177183955, 1
assessment feasibility: 0.41514489177183955, 1
feasibility transitioning: 0.41514489177183955, 1
transitioning current: 0.41514489177183955, 1
current conditions: 0.41514489177183955, 1
conditions initial: 0.41514489177183955, 1
initial scene: 0.41514489177183955, 1
scene desired: 0.41514489177183955, 1
desired goal: 0.41514489177183955, 1
goal existing: 0.41514489177183955, 1
research explored: 0.41514489177183955, 1
explored utilization: 0.41514489177183955, 1
utilization diverse: 0.41514489177183955, 1
diverse image: 0.41514489177183955, 1
model create: 0.41514489177183955, 1
create image: 0.41514489177183955, 1
image depicting: 0.41514489177183955, 1
depicting potential: 0.41514489177183955, 1
potential goal: 0.41514489177183955, 1
goal states: 0.41514489177183955, 1
states based: 0.41514489177183955, 1
state illustrate: 0.41514489177183955, 1
illustrate limitations: 0.41514489177183955, 1
limitations current: 0.41514489177183955, 1
art image: 0.41514489177183955, 1
model accurately: 0.41514489177183955, 1
accurately assessing: 0.41514489177183955, 1
assessing feasibility: 0.41514489177183955, 1
feasibility specific: 0.41514489177183955, 1
specific action: 0.41514489177183955, 1
action particular: 0.41514489177183955, 1
particular present: 0.41514489177183955, 1
present integrating: 0.41514489177183955, 1
integrating large: 0.41514489177183955, 1
language possess: 0.41514489177183955, 1
possess profound: 0.41514489177183955, 1
profound knowledge: 0.41514489177183955, 1
knowledge real: 0.41514489177183955, 1
object enhance: 0.41514489177183955, 1
enhance performance: 0.41514489177183955, 1
performance image: 0.41514489177183955, 1
model discerning: 0.41514489177183955, 1
discerning plausible: 0.41514489177183955, 1
plausible implausible: 0.41514489177183955, 1
implausible action: 0.41514489177183955, 1
action simulating: 0.41514489177183955, 1
simulating outcome: 0.41514489177183955, 1
given step: 0.41514489177183955, 1
towards achieving: 0.41514489177183955, 1
achieving pragmatic: 0.41514489177183955, 1
pragmatic goal: 0.41514489177183955, 1
paper explore: 0.41514489177183955, 1
space vision: 0.41514489177183955, 1
based generative: 0.41514489177183955, 1
model combines: 0.41514489177183955, 1
combines unsupervised: 0.41514489177183955, 1
unsupervised generative: 0.41514489177183955, 1
generative learning: 0.41514489177183955, 1
predictor learn: 0.41514489177183955, 1
learn exploit: 0.41514489177183955, 1
exploit task: 0.41514489177183955, 1
given visual: 0.41514489177183955, 1
observations reaching: 0.41514489177183955, 1
reaching involving: 0.41514489177183955, 1
involving scenario: 0.41514489177183955, 1
scenario stick: 0.41514489177183955, 1
stick while: 0.41514489177183955, 1
while learned: 0.41514489177183955, 1
learned embedding: 0.41514489177183955, 1
embedding generative: 0.41514489177183955, 1
capture factors: 0.41514489177183955, 1
variation 3d: 0.41514489177183955, 1
tool geometry: 0.41514489177183955, 1
geometry performance: 0.41514489177183955, 1
predictor identifies: 0.41514489177183955, 1
identifies sub: 0.41514489177183955, 1
sub manifolds: 0.41514489177183955, 1
manifolds embedding: 0.41514489177183955, 1
embedding correlate: 0.41514489177183955, 1
correlate task: 0.41514489177183955, 1
task within: 0.41514489177183955, 1
within variety: 0.41514489177183955, 1
variety demonstrate: 0.41514489177183955, 1
demonstrate traversing: 0.41514489177183955, 1
traversing latent: 0.41514489177183955, 1
space via: 0.41514489177183955, 1
via backpropagation: 0.41514489177183955, 1
backpropagation performance: 0.41514489177183955, 1
predictor allows: 0.41514489177183955, 1
allows imagine: 0.41514489177183955, 1
tool appropriate: 0.41514489177183955, 1
task result: 0.41514489177183955, 1
result indicate: 0.41514489177183955, 1
indicate affordance: 0.41514489177183955, 1
affordance utility: 0.41514489177183955, 1
utility reaching: 0.41514489177183955, 1
reaching encoded: 0.41514489177183955, 1
encoded along: 0.41514489177183955, 1
along smooth: 0.41514489177183955, 1
smooth trajectories: 0.41514489177183955, 1
trajectories latent: 0.41514489177183955, 1
latent accessing: 0.41514489177183955, 1
accessing emergent: 0.41514489177183955, 1
affordance considering: 0.41514489177183955, 1
considering high: 0.41514489177183955, 1
level performance: 0.41514489177183955, 1
performance criteria: 0.41514489177183955, 1
criteria task: 0.41514489177183955, 1
task enables: 0.41514489177183955, 1
enables agent: 0.41514489177183955, 1
agent manipulate: 0.41514489177183955, 1
manipulate tool: 0.41514489177183955, 1
geometries targeted: 0.41514489177183955, 1
generalist robot: 0.41514489177183955, 1
equipped learned: 0.41514489177183955, 1
skills able: 0.41514489177183955, 1
perform many: 0.41514489177183955, 1
many task: 0.41514489177183955, 1
task many: 0.41514489177183955, 1
different zero: 0.41514489177183955, 1
zero shot: 0.41514489177183955, 1
shot generalization: 0.41514489177183955, 1
generalization settings: 0.41514489177183955, 1
settings always: 0.41514489177183955, 1
always robot: 0.41514489177183955, 1
encounters environment: 0.41514489177183955, 1
environment may: 0.41514489177183955, 1
may finetune: 0.41514489177183955, 1
finetune previously: 0.41514489177183955, 1
skills accommodate: 0.41514489177183955, 1
accommodate previously: 0.41514489177183955, 1
learned behavior: 0.41514489177183955, 1
behavior model: 0.41514489177183955, 1
still suitable: 0.41514489177183955, 1
suitable accelerate: 0.41514489177183955, 1
accelerate aim: 0.41514489177183955, 1
aim study: 0.41514489177183955, 1
study generative: 0.41514489177183955, 1
model possible: 0.41514489177183955, 1
outcome allow: 0.41514489177183955, 1
allow robot: 0.41514489177183955, 1
learn visual: 0.41514489177183955, 1
robot sample: 0.41514489177183955, 1
sample potentially: 0.41514489177183955, 1
potentially possible: 0.41514489177183955, 1
outcome further: 0.41514489177183955, 1
further train: 0.41514489177183955, 1
train policy: 0.41514489177183955, 1
policy achieve: 0.41514489177183955, 1
achieve prior: 0.41514489177183955, 1
prior data: 0.41514489177183955, 1
data learn: 0.41514489177183955, 1
learn kinds: 0.41514489177183955, 1
kinds outcome: 0.41514489177183955, 1
outcome may: 0.41514489177183955, 1
may robot: 0.41514489177183955, 1
encounters unfamiliar: 0.41514489177183955, 1
unfamiliar sample: 0.41514489177183955, 1
sample potential: 0.41514489177183955, 1
potential outcome: 0.41514489177183955, 1
outcome attempt: 0.41514489177183955, 1
attempt reach: 0.41514489177183955, 1
reach thereby: 0.41514489177183955, 1
thereby update: 0.41514489177183955, 1
update skills: 0.41514489177183955, 1
skills outcome: 0.41514489177183955, 1
outcome approach: 0.41514489177183955, 1
approach train: 0.41514489177183955, 1
train goal: 0.41514489177183955, 1
policy operate: 0.41514489177183955, 1
operate raw: 0.41514489177183955, 1
image rapidly: 0.41514489177183955, 1
rapidly learn: 0.41514489177183955, 1
learn manipulate: 0.41514489177183955, 1
manipulate object: 0.41514489177183955, 1
via propose: 0.41514489177183955, 1
propose affordance: 0.41514489177183955, 1
affordance directed: 0.41514489177183955, 1
directed exploration: 0.41514489177183955, 1
paper investigate: 0.41514489177183955, 1
investigate artificial: 0.41514489177183955, 1
artificial ability: 0.41514489177183955, 1
ability perform: 0.41514489177183955, 1
perform task: 0.41514489177183955, 1
focused tool: 0.41514489177183955, 1
synthesis via: 0.41514489177183955, 1
via motivation: 0.41514489177183955, 1
motivation explore: 0.41514489177183955, 1
space object: 0.41514489177183955, 1
model exploit: 0.41514489177183955, 1
exploit approach: 0.41514489177183955, 1
approach employs: 0.41514489177183955, 1
employs activation: 0.41514489177183955, 1
activation maximisation: 0.41514489177183955, 1
maximisation task: 0.41514489177183955, 1
predictor optimise: 0.41514489177183955, 1
optimise latent: 0.41514489177183955, 1
latent variable: 0.41514489177183955, 1
variable structured: 0.41514489177183955, 1
space model: 0.41514489177183955, 1
model order: 0.41514489177183955, 1
order generate: 0.41514489177183955, 1
generate tool: 0.41514489177183955, 1
geometries appropriate: 0.41514489177183955, 1
task evaluate: 0.41514489177183955, 1
evaluate model: 0.41514489177183955, 1
model novel: 0.41514489177183955, 1
dataset synthetic: 0.41514489177183955, 1
synthetic reaching: 0.41514489177183955, 1
task inspired: 0.41514489177183955, 1
inspired cognitive: 0.41514489177183955, 1
cognitive sciences: 0.41514489177183955, 1
sciences behavioural: 0.41514489177183955, 1
behavioural doing: 0.41514489177183955, 1
doing examine: 0.41514489177183955, 1
examine ability: 0.41514489177183955, 1
ability imagine: 0.41514489177183955, 1
tool increasingly: 0.41514489177183955, 1
increasingly complex: 0.41514489177183955, 1
complex scenario: 0.41514489177183955, 1
scenario beyond: 0.41514489177183955, 1
beyond seen: 0.41514489177183955, 1
during experiments: 0.41514489177183955, 1
experiments demonstrate: 0.41514489177183955, 1
demonstrate synthesis: 0.41514489177183955, 1
synthesis process: 0.41514489177183955, 1
process modifies: 0.41514489177183955, 1
modifies task: 0.41514489177183955, 1
affordance targeted: 0.41514489177183955, 1
deliberate agent: 0.41514489177183955, 1
agent often: 0.41514489177183955, 1
often specifically: 0.41514489177183955, 1
specifically modify: 0.41514489177183955, 1
modify aspects: 0.41514489177183955, 1
aspects tool: 0.41514489177183955, 1
tool relate: 0.41514489177183955, 1
relate meaningful: 0.41514489177183955, 1
meaningful implicitly: 0.41514489177183955, 1
implicitly concept: 0.41514489177183955, 1
concept width: 0.41514489177183955, 1
width result: 0.41514489177183955, 1
result therefore: 0.41514489177183955, 1
therefore task: 0.41514489177183955, 1
affordance implicitly: 0.41514489177183955, 1
implicitly encoded: 0.41514489177183955, 1
encoded directions: 0.41514489177183955, 1
directions structured: 0.41514489177183955, 1
space shaped: 0.41514489177183955, 1
investigate knowledge: 0.41514489177183955, 1
affordance pre: 0.41514489177183955, 1
train language: 0.41514489177183955, 1
model pre: 0.41514489177183955, 1
train vision: 0.41514489177183955, 1
model growing: 0.41514489177183955, 1
growing body: 0.41514489177183955, 1
body literature: 0.41514489177183955, 1
literature shows: 0.41514489177183955, 1
shows ptlms: 0.41514489177183955, 1
ptlms fail: 0.41514489177183955, 1
fail inconsistently: 0.41514489177183955, 1
inconsistently non: 0.41514489177183955, 1
non demonstrating: 0.41514489177183955, 1
demonstrating lack: 0.41514489177183955, 1
lack reasoning: 0.41514489177183955, 1
reasoning step: 0.41514489177183955, 1
step toward: 0.41514489177183955, 1
toward quantifying: 0.41514489177183955, 1
quantifying effect: 0.41514489177183955, 1
effect grounding: 0.41514489177183955, 1
grounding lack: 0.41514489177183955, 1
lack curate: 0.41514489177183955, 1
curate novel: 0.41514489177183955, 1
novel comprehensive: 0.41514489177183955, 1
comprehensive dataset: 0.41514489177183955, 1
dataset object: 0.41514489177183955, 1
affordance characterized: 0.41514489177183955, 1
characterized 15: 0.41514489177183955, 1
15 affordance: 0.41514489177183955, 1
affordance unlike: 0.41514489177183955, 1
unlike affordance: 0.41514489177183955, 1
affordance datasets: 0.41514489177183955, 1
datasets collected: 0.41514489177183955, 1
collected vision: 0.41514489177183955, 1
language annotate: 0.41514489177183955, 1
annotate wild: 0.41514489177183955, 1
wild sentences: 0.41514489177183955, 1
sentences object: 0.41514489177183955, 1
object experimental: 0.41514489177183955, 1
result reveal: 0.41514489177183955, 1
reveal ptlms: 0.41514489177183955, 1
ptlms exhibit: 0.41514489177183955, 1
exhibit limited: 0.41514489177183955, 1
limited reasoning: 0.41514489177183955, 1
reasoning abilities: 0.41514489177183955, 1
abilities comes: 0.41514489177183955, 1
comes uncommon: 0.41514489177183955, 1
uncommon object: 0.41514489177183955, 1
object observe: 0.41514489177183955, 1
observe pre: 0.41514489177183955, 1
train vlms: 0.41514489177183955, 1
vlms necessarily: 0.41514489177183955, 1
necessarily capture: 0.41514489177183955, 1
capture object: 0.41514489177183955, 1
affordance few: 0.41514489177183955, 1
shot fine: 0.41514489177183955, 1
fine demonstrate: 0.41514489177183955, 1
demonstrate improvement: 0.41514489177183955, 1
improvement affordance: 0.41514489177183955, 1
knowledge ptlms: 0.41514489177183955, 1
ptlms research: 0.41514489177183955, 1
research contributes: 0.41514489177183955, 1
contributes novel: 0.41514489177183955, 1
dataset language: 0.41514489177183955, 1
language grounding: 0.41514489177183955, 1
grounding present: 0.41514489177183955, 1
present insights: 0.41514489177183955, 1
insights lm: 0.41514489177183955, 1
lm advancing: 0.41514489177183955, 1
advancing understanding: 0.41514489177183955, 1
understanding object: 0.41514489177183955, 1
language interfaces: 0.41514489177183955, 1
interfaces many: 0.41514489177183955, 1
many cognitive: 0.41514489177183955, 1
cognitive paper: 0.41514489177183955, 1
paper explores: 0.41514489177183955, 1
explores interaction: 0.41514489177183955, 1
interaction interfaces: 0.41514489177183955, 1
interfaces studied: 0.41514489177183955, 1
studied deep: 0.41514489177183955, 1
learning focusing: 0.41514489177183955, 1
focusing relation: 0.41514489177183955, 1
relation language: 0.41514489177183955, 1
emergence visual: 0.41514489177183955, 1
model emergence: 0.41514489177183955, 1
emergence sender: 0.41514489177183955, 1
sender receiver: 0.41514489177183955, 1
receiver agent: 0.41514489177183955, 1
agent train: 0.41514489177183955, 1
train reference: 0.41514489177183955, 1
reference agent: 0.41514489177183955, 1
agent implemented: 0.41514489177183955, 1
implemented deep: 0.41514489177183955, 1
neural dedicated: 0.41514489177183955, 1
dedicated vision: 0.41514489177183955, 1
language motivated: 0.41514489177183955, 1
motivated mutual: 0.41514489177183955, 1
perception apply: 0.41514489177183955, 1
apply systematic: 0.41514489177183955, 1
systematic manipulations: 0.41514489177183955, 1
manipulations visual: 0.41514489177183955, 1
visual analyze: 0.41514489177183955, 1
effect emergent: 0.41514489177183955, 1
communication analyze: 0.41514489177183955, 1
effect visual: 0.41514489177183955, 1
visual analyses: 0.41514489177183955, 1
analyses perception: 0.41514489177183955, 1
perception biases: 0.41514489177183955, 1
biases shape: 0.41514489177183955, 1
shape semantic: 0.41514489177183955, 1
semantic categorization: 0.41514489177183955, 1
categorization communicative: 0.41514489177183955, 1
communicative communication: 0.41514489177183955, 1
communication protocol: 0.41514489177183955, 1
protocol partitions: 0.41514489177183955, 1
partitions object: 0.41514489177183955, 1
object space: 0.41514489177183955, 1
space along: 0.41514489177183955, 1
along certain: 0.41514489177183955, 1
certain agent: 0.41514489177183955, 1
learn represent: 0.41514489177183955, 1
represent visual: 0.41514489177183955, 1
information attributes: 0.41514489177183955, 1
attributes more: 0.41514489177183955, 1
more representation: 0.41514489177183955, 1
representation communication: 0.41514489177183955, 1
communication partners: 0.41514489177183955, 1
partners evolutionary: 0.41514489177183955, 1
evolutionary analysis: 0.41514489177183955, 1
analysis suggests: 0.41514489177183955, 1
suggests visual: 0.41514489177183955, 1
may shaped: 0.41514489177183955, 1
shaped part: 0.41514489177183955, 1
part facilitate: 0.41514489177183955, 1
facilitate communication: 0.41514489177183955, 1
communication environmentally: 0.41514489177183955, 1
environmentally relevant: 0.41514489177183955, 1
relevant aside: 0.41514489177183955, 1
aside accounting: 0.41514489177183955, 1
accounting co: 0.41514489177183955, 1
co adaptation: 0.41514489177183955, 1
adaptation effect: 0.41514489177183955, 1
effect language: 0.41514489177183955, 1
language result: 0.41514489177183955, 1
result point: 0.41514489177183955, 1
point ways: 0.41514489177183955, 1
ways modulate: 0.41514489177183955, 1
modulate improve: 0.41514489177183955, 1
improve visual: 0.41514489177183955, 1
learning emergent: 0.41514489177183955, 1
communication artificial: 0.41514489177183955, 1
goal interactive: 0.41514489177183955, 1
interactive machine: 0.41514489177183955, 1
learning enable: 0.41514489177183955, 1
enable specialized: 0.41514489177183955, 1
specialized train: 0.41514489177183955, 1
train intuitively: 0.41514489177183955, 1
intuitively teach: 0.41514489177183955, 1
teach intelligent: 0.41514489177183955, 1
intelligent agent: 0.41514489177183955, 1
perform toward: 0.41514489177183955, 1
achieving studying: 0.41514489177183955, 1
studying design: 0.41514489177183955, 1
design interaction: 0.41514489177183955, 1
interaction method: 0.41514489177183955, 1
method bayesian: 0.41514489177183955, 1
bayesian q: 0.41514489177183955, 1
q learning: 0.41514489177183955, 1
algorithm impacts: 0.41514489177183955, 1
impacts aspects: 0.41514489177183955, 1
aspects experience: 0.41514489177183955, 1
experience teaching: 0.41514489177183955, 1
teaching agent: 0.41514489177183955, 1
agent human: 0.41514489177183955, 1
centric metrics: 0.41514489177183955, 1
metrics frustration: 0.41514489177183955, 1
frustration addition: 0.41514489177183955, 1
addition traditional: 0.41514489177183955, 1
traditional ml: 0.41514489177183955, 1
ml performance: 0.41514489177183955, 1
performance study: 0.41514489177183955, 1
study investigated: 0.41514489177183955, 1
investigated method: 0.41514489177183955, 1
method natural: 0.41514489177183955, 1
language critique: 0.41514489177183955, 1
critique action: 0.41514489177183955, 1
action conducted: 0.41514489177183955, 1
conducted human: 0.41514489177183955, 1
human loop: 0.41514489177183955, 1
loop experiment: 0.41514489177183955, 1
experiment train: 0.41514489177183955, 1
agent different: 0.41514489177183955, 1
different teaching: 0.41514489177183955, 1
teaching method: 0.41514489177183955, 1
method unknown: 0.41514489177183955, 1
unknown each: 0.41514489177183955, 1
each same: 0.41514489177183955, 1
same underlying: 0.41514489177183955, 1
underlying reinforcement: 0.41514489177183955, 1
learning result: 0.41514489177183955, 1
result agent: 0.41514489177183955, 1
learn action: 0.41514489177183955, 1
action advice: 0.41514489177183955, 1
advice creates: 0.41514489177183955, 1
creates better: 0.41514489177183955, 1
better user: 0.41514489177183955, 1
user experience: 0.41514489177183955, 1
experience compared: 0.41514489177183955, 1
compared agent: 0.41514489177183955, 1
learn binary: 0.41514489177183955, 1
binary critique: 0.41514489177183955, 1
critique terms: 0.41514489177183955, 1
terms perception: 0.41514489177183955, 1
perception perception: 0.41514489177183955, 1
perception identified: 0.41514489177183955, 1
identified nine: 0.41514489177183955, 1
nine main: 0.41514489177183955, 1
main characteristics: 0.41514489177183955, 1
characteristics iml: 0.41514489177183955, 1
iml design: 0.41514489177183955, 1
design impact: 0.41514489177183955, 1
impact experience: 0.41514489177183955, 1
experience including: 0.41514489177183955, 1
including human: 0.41514489177183955, 1
instructions compliance: 0.41514489177183955, 1
compliance deterministic: 0.41514489177183955, 1
deterministic complexity: 0.41514489177183955, 1
complexity accuracy: 0.41514489177183955, 1
accuracy speech: 0.41514489177183955, 1
speech recognition: 0.41514489177183955, 1
recognition robust: 0.41514489177183955, 1
robust flexible: 0.41514489177183955, 1
flexible nature: 0.41514489177183955, 1
nature interaction: 0.41514489177183955, 1
learning useful: 0.41514489177183955, 1
useful representation: 0.41514489177183955, 1
representation without: 0.41514489177183955, 1
without supervision: 0.41514489177183955, 1
supervision remains: 0.41514489177183955, 1
remains key: 0.41514489177183955, 1
key challenge: 0.41514489177183955, 1
challenge machine: 0.41514489177183955, 1
machine simple: 0.41514489177183955, 1
powerful generative: 0.41514489177183955, 1
discrete vector: 0.41514489177183955, 1
vector quantised: 0.41514489177183955, 1
quantised variational: 0.41514489177183955, 1
variational autoencoder: 0.41514489177183955, 1
autoencoder differs: 0.41514489177183955, 1
differs vaes: 0.41514489177183955, 1
vaes key: 0.41514489177183955, 1
key encoder: 0.41514489177183955, 1
encoder network: 0.41514489177183955, 1
network output: 0.41514489177183955, 1
output rather: 0.41514489177183955, 1
rather prior: 0.41514489177183955, 1
prior learnt: 0.41514489177183955, 1
learnt rather: 0.41514489177183955, 1
rather order: 0.41514489177183955, 1
order learn: 0.41514489177183955, 1
discrete latent: 0.41514489177183955, 1
latent incorporate: 0.41514489177183955, 1
incorporate ideas: 0.41514489177183955, 1
ideas vector: 0.41514489177183955, 1
vector quantisation: 0.41514489177183955, 1
quantisation vq: 0.41514489177183955, 1
vq method: 0.41514489177183955, 1
allows model: 0.41514489177183955, 1
model circumvent: 0.41514489177183955, 1
circumvent issues: 0.41514489177183955, 1
issues latents: 0.41514489177183955, 1
latents ignored: 0.41514489177183955, 1
ignored paired: 0.41514489177183955, 1
paired powerful: 0.41514489177183955, 1
powerful autoregressive: 0.41514489177183955, 1
autoregressive decoder: 0.41514489177183955, 1
decoder typically: 0.41514489177183955, 1
typically observed: 0.41514489177183955, 1
observed vae: 0.41514489177183955, 1
vae pairing: 0.41514489177183955, 1
pairing representation: 0.41514489177183955, 1
representation autoregressive: 0.41514489177183955, 1
autoregressive model: 0.41514489177183955, 1
model generate: 0.41514489177183955, 1
generate high: 0.41514489177183955, 1
quality speech: 0.41514489177183955, 1
speech doing: 0.41514489177183955, 1
doing high: 0.41514489177183955, 1
quality speaker: 0.41514489177183955, 1
speaker conversion: 0.41514489177183955, 1
conversion unsupervised: 0.41514489177183955, 1
learning providing: 0.41514489177183955, 1
providing further: 0.41514489177183955, 1
further evidence: 0.41514489177183955, 1
evidence utility: 0.41514489177183955, 1
utility learnt: 0.41514489177183955, 1
while reinforcement: 0.41514489177183955, 1
learning provides: 0.41514489177183955, 1
provides appealing: 0.41514489177183955, 1
appealing formalism: 0.41514489177183955, 1
formalism learning: 0.41514489177183955, 1
learning individual: 0.41514489177183955, 1
individual general: 0.41514489177183955, 1
purpose robot: 0.41514489177183955, 1
able master: 0.41514489177183955, 1
master extensive: 0.41514489177183955, 1
extensive repertoire: 0.41514489177183955, 1
repertoire instead: 0.41514489177183955, 1
instead learning: 0.41514489177183955, 1
collection skills: 0.41514489177183955, 1
skills instead: 0.41514489177183955, 1
instead enable: 0.41514489177183955, 1
enable robot: 0.41514489177183955, 1
robot practice: 0.41514489177183955, 1
practice own: 0.41514489177183955, 1
own behavior: 0.41514489177183955, 1
behavior learning: 0.41514489177183955, 1
behavior perform: 0.41514489177183955, 1
perform repurpose: 0.41514489177183955, 1
repurpose knowledge: 0.41514489177183955, 1
knowledge once: 0.41514489177183955, 1
once task: 0.41514489177183955, 1
task commanded: 0.41514489177183955, 1
commanded study: 0.41514489177183955, 1
study question: 0.41514489177183955, 1
question context: 0.41514489177183955, 1
context self: 0.41514489177183955, 1
conditioned reinforcement: 0.41514489177183955, 1
reinforcement central: 0.41514489177183955, 1
central challenge: 0.41514489177183955, 1
challenge learning: 0.41514489177183955, 1
learning regime: 0.41514489177183955, 1
regime problem: 0.41514489177183955, 1
problem goal: 0.41514489177183955, 1
goal order: 0.41514489177183955, 1
order practice: 0.41514489177183955, 1
practice useful: 0.41514489177183955, 1
useful robot: 0.41514489177183955, 1
able autonomously: 0.41514489177183955, 1
autonomously set: 0.41514489177183955, 1
set goal: 0.41514489177183955, 1
feasible environment: 0.41514489177183955, 1
available object: 0.41514489177183955, 1
object open: 0.41514489177183955, 1
open world: 0.41514489177183955, 1
world robot: 0.41514489177183955, 1
robot itself: 0.41514489177183955, 1
itself goal: 0.41514489177183955, 1
goal accomplish: 0.41514489177183955, 1
accomplish present: 0.41514489177183955, 1
present setting: 0.41514489177183955, 1
setting object: 0.41514489177183955, 1
object previous: 0.41514489177183955, 1
previous study: 0.41514489177183955, 1
study self: 0.41514489177183955, 1
conditioned rl: 0.41514489177183955, 1
rl single: 0.41514489177183955, 1
single environment: 0.41514489177183955, 1
environment goal: 0.41514489177183955, 1
goal proposals: 0.41514489177183955, 1
proposals past: 0.41514489177183955, 1
past experience: 0.41514489177183955, 1
experience generative: 0.41514489177183955, 1
model more: 0.41514489177183955, 1
more diverse: 0.41514489177183955, 1
diverse frequently: 0.41514489177183955, 1
frequently leads: 0.41514489177183955, 1
leads impossible: 0.41514489177183955, 1
impossible goal: 0.41514489177183955, 1
goal prevents: 0.41514489177183955, 1
prevents effective: 0.41514489177183955, 1
effective conditional: 0.41514489177183955, 1
conditional goal: 0.41514489177183955, 1
goal setting: 0.41514489177183955, 1
setting model: 0.41514489177183955, 1
model aims: 0.41514489177183955, 1
aims goal: 0.41514489177183955, 1
feasible current: 0.41514489177183955, 1
current demonstrate: 0.41514489177183955, 1
demonstrate enables: 0.41514489177183955, 1
enables self: 0.41514489177183955, 1
conditioned off: 0.41514489177183955, 1
learning raw: 0.41514489177183955, 1
image observations: 0.41514489177183955, 1
observations real: 0.41514489177183955, 1
real enabling: 0.41514489177183955, 1
robot manipulate: 0.41514489177183955, 1
manipulate variety: 0.41514489177183955, 1
variety object: 0.41514489177183955, 1
object generalize: 0.41514489177183955, 1
object seen: 0.41514489177183955, 1
recognition important: 0.41514489177183955, 1
important research: 0.41514489177183955, 1
research topic: 0.41514489177183955, 1
topic human: 0.41514489177183955, 1
computer computer: 0.41514489177183955, 1
vision recent: 0.41514489177183955, 1
method achieved: 0.41514489177183955, 1
achieved remarkable: 0.41514489177183955, 1
remarkable unified: 0.41514489177183955, 1
unified intensive: 0.41514489177183955, 1
intensive survey: 0.41514489177183955, 1
survey method: 0.41514489177183955, 1
method article: 0.41514489177183955, 1
article reviews: 0.41514489177183955, 1
reviews investigates: 0.41514489177183955, 1
investigates existing: 0.41514489177183955, 1
existing deep: 0.41514489177183955, 1
method comprehensive: 0.41514489177183955, 1
comprehensive hoping: 0.41514489177183955, 1
hoping pursue: 0.41514489177183955, 1
pursue greater: 0.41514489177183955, 1
greater acceleration: 0.41514489177183955, 1
acceleration research: 0.41514489177183955, 1
research article: 0.41514489177183955, 1
article classifies: 0.41514489177183955, 1
classifies affordance: 0.41514489177183955, 1
recognition five: 0.41514489177183955, 1
five delves: 0.41514489177183955, 1
delves methodologies: 0.41514489177183955, 1
methodologies each: 0.41514489177183955, 1
each explores: 0.41514489177183955, 1
explores rationales: 0.41514489177183955, 1
rationales essential: 0.41514489177183955, 1
essential several: 0.41514489177183955, 1
several representative: 0.41514489177183955, 1
representative affordance: 0.41514489177183955, 1
recognition datasets: 0.41514489177183955, 1
datasets investigated: 0.41514489177183955, 1
investigated based: 0.41514489177183955, 1
based article: 0.41514489177183955, 1
provides comprehensive: 0.41514489177183955, 1
comprehensive performance: 0.41514489177183955, 1
performance comparison: 0.41514489177183955, 1
comparison analysis: 0.41514489177183955, 1
analysis current: 0.41514489177183955, 1
current affordance: 0.41514489177183955, 1
recognition reporting: 0.41514489177183955, 1
reporting result: 0.41514489177183955, 1
different method: 0.41514489177183955, 1
method same: 0.41514489177183955, 1
same datasets: 0.41514489177183955, 1
datasets result: 0.41514489177183955, 1
result each: 0.41514489177183955, 1
each method: 0.41514489177183955, 1
method different: 0.41514489177183955, 1
different article: 0.41514489177183955, 1
article summarizes: 0.41514489177183955, 1
summarizes progress: 0.41514489177183955, 1
progress affordance: 0.41514489177183955, 1
affordance outlines: 0.41514489177183955, 1
outlines existing: 0.41514489177183955, 1
existing difficulties: 0.41514489177183955, 1
difficulties provides: 0.41514489177183955, 1
provides corresponding: 0.41514489177183955, 1
corresponding discusses: 0.41514489177183955, 1
discusses future: 0.41514489177183955, 1
future application: 0.41514489177183955, 1
established cognitive: 0.41514489177183955, 1
neuroscience human: 0.41514489177183955, 1
perception object: 0.41514489177183955, 1
object constitutes: 0.41514489177183955, 1
constitutes complex: 0.41514489177183955, 1
complex object: 0.41514489177183955, 1
appearance information: 0.41514489177183955, 1
information combined: 0.41514489177183955, 1
combined evidence: 0.41514489177183955, 1
evidence called: 0.41514489177183955, 1
called object: 0.41514489177183955, 1
object namely: 0.41514489177183955, 1
types action: 0.41514489177183955, 1
action human: 0.41514489177183955, 1
human typically: 0.41514489177183955, 1
typically perform: 0.41514489177183955, 1
perform interacting: 0.41514489177183955, 1
interacting fact: 0.41514489177183955, 1
fact recently: 0.41514489177183955, 1
recently motivated: 0.41514489177183955, 1
motivated approach: 0.41514489177183955, 1
approach challenging: 0.41514489177183955, 1
challenging task: 0.41514489177183955, 1
task automatic: 0.41514489177183955, 1
automatic object: 0.41514489177183955, 1
object information: 0.41514489177183955, 1
sources fused: 0.41514489177183955, 1
fused improve: 0.41514489177183955, 1
improve aforementioned: 0.41514489177183955, 1
aforementioned paradigm: 0.41514489177183955, 1
paradigm surpassing: 0.41514489177183955, 1
current limitations: 0.41514489177183955, 1
limitations sensorimotor: 0.41514489177183955, 1
recognition deep: 0.41514489177183955, 1
learning paradigm: 0.41514489177183955, 1
paradigm introduced: 0.41514489177183955, 1
introduced problem: 0.41514489177183955, 1
problem developing: 0.41514489177183955, 1
developing number: 0.41514489177183955, 1
number novel: 0.41514489177183955, 1
novel neuro: 0.41514489177183955, 1
neuro biologically: 0.41514489177183955, 1
biologically neuro: 0.41514489177183955, 1
neuro physiologically: 0.41514489177183955, 1
physiologically inspired: 0.41514489177183955, 1
inspired architectures: 0.41514489177183955, 1
architectures utilize: 0.41514489177183955, 1
utilize state: 0.41514489177183955, 1
art neural: 0.41514489177183955, 1
network fusing: 0.41514489177183955, 1
fusing available: 0.41514489177183955, 1
available information: 0.41514489177183955, 1
sources multiple: 0.41514489177183955, 1
multiple propose: 0.41514489177183955, 1
evaluated large: 0.41514489177183955, 1
large rgb: 0.41514489177183955, 1
d specifically: 0.41514489177183955, 1
specifically collected: 0.41514489177183955, 1
collected task: 0.41514489177183955, 1
task sensorimotor: 0.41514489177183955, 1
recognition made: 0.41514489177183955, 1
made publicly: 0.41514489177183955, 1
publicly experimental: 0.41514489177183955, 1
demonstrate utility: 0.41514489177183955, 1
utility affordance: 0.41514489177183955, 1
object achieving: 0.41514489177183955, 1
achieving relative: 0.41514489177183955, 1
relative error: 0.41514489177183955, 1
error reduction: 0.41514489177183955, 1
popular concept: 0.41514489177183955, 1
concept propose: 0.41514489177183955, 1
propose field: 0.41514489177183955, 1
affordance regarded: 0.41514489177183955, 1
regarded important: 0.41514489177183955, 1
important abilities: 0.41514489177183955, 1
abilities enable: 0.41514489177183955, 1
enable human: 0.41514489177183955, 1
human understand: 0.41514489177183955, 1
interact capture: 0.41514489177183955, 1
capture possibilities: 0.41514489177183955, 1
possibilities effect: 0.41514489177183955, 1
agent applied: 0.41514489177183955, 1
applied specific: 0.41514489177183955, 1
more part: 0.41514489177183955, 1
part article: 0.41514489177183955, 1
provides short: 0.41514489177183955, 1
short review: 0.41514489177183955, 1
review recent: 0.41514489177183955, 1
recent developments: 0.41514489177183955, 1
developments deep: 0.41514489177183955, 1
learning aims: 0.41514489177183955, 1
aims develop: 0.41514489177183955, 1
develop data: 0.41514489177183955, 1
driven method: 0.41514489177183955, 1
method concept: 0.41514489177183955, 1
affordance aid: 0.41514489177183955, 1
aid robot: 0.41514489177183955, 1
robot classify: 0.41514489177183955, 1
classify papers: 0.41514489177183955, 1
papers reinforcement: 0.41514489177183955, 1
perspective draw: 0.41514489177183955, 1
draw connections: 0.41514489177183955, 1
connections rl: 0.41514489177183955, 1
rl technical: 0.41514489177183955, 1
technical details: 0.41514489177183955, 1
details each: 0.41514489177183955, 1
category discussed: 0.41514489177183955, 1
discussed limitations: 0.41514489177183955, 1
limitations further: 0.41514489177183955, 1
further summarize: 0.41514489177183955, 1
summarize identify: 0.41514489177183955, 1
identify future: 0.41514489177183955, 1
challenge aspects: 0.41514489177183955, 1
affordance data: 0.41514489177183955, 1
data real: 0.41514489177183955, 1
world final: 0.41514489177183955, 1
final remark: 0.41514489177183955, 1
remark given: 0.41514489177183955, 1
given end: 0.41514489177183955, 1
end promising: 0.41514489177183955, 1
promising future: 0.41514489177183955, 1
future direction: 0.41514489177183955, 1
direction rl: 0.41514489177183955, 1
rl based: 0.41514489177183955, 1
affordance definition: 0.41514489177183955, 1
definition include: 0.41514489177183955, 1
include predict: 0.41514489177183955, 1
predict arbitrary: 0.41514489177183955, 1
arbitrary action: 0.41514489177183955, 1
novel real: 0.41514489177183955, 1
real method: 0.41514489177183955, 1
d method: 0.41514489177183955, 1
method trains: 0.41514489177183955, 1
trains deep: 0.41514489177183955, 1
learn deep: 0.41514489177183955, 1
feature input: 0.41514489177183955, 1
data end: 0.41514489177183955, 1
end cnn: 0.41514489177183955, 1
cnn encoder: 0.41514489177183955, 1
decoder architecture: 0.41514489177183955, 1
architecture order: 0.41514489177183955, 1
obtain smooth: 0.41514489177183955, 1
smooth label: 0.41514489177183955, 1
label input: 0.41514489177183955, 1
data represented: 0.41514489177183955, 1
represented multiple: 0.41514489177183955, 1
multiple modalities: 0.41514489177183955, 1
modalities let: 0.41514489177183955, 1
let network: 0.41514489177183955, 1
feature more: 0.41514489177183955, 1
more method: 0.41514489177183955, 1
method sets: 0.41514489177183955, 1
sets benchmark: 0.41514489177183955, 1
benchmark detecting: 0.41514489177183955, 1
object improving: 0.41514489177183955, 1
accuracy comparison: 0.41514489177183955, 1
comparison state: 0.41514489177183955, 1
method hand: 0.41514489177183955, 1
hand designed: 0.41514489177183955, 1
designed geometric: 0.41514489177183955, 1
geometric apply: 0.41514489177183955, 1
apply detection: 0.41514489177183955, 1
detection method: 0.41514489177183955, 1
method full: 0.41514489177183955, 1
demonstrate robot: 0.41514489177183955, 1
perform grasps: 0.41514489177183955, 1
grasps efficiently: 0.41514489177183955, 1
efficiently detecting: 0.41514489177183955, 1
relevance concept: 0.41514489177183955, 1
perception interactive: 0.41514489177183955, 1
interactive autonomous: 0.41514489177183955, 1
robot extension: 0.41514489177183955, 1
feature identify: 0.41514489177183955, 1
identify importance: 0.41514489177183955, 1
robot investigate: 0.41514489177183955, 1
investigate originally: 0.41514489177183955, 1
generalized arbitrary: 0.41514489177183955, 1
cues predictable: 0.41514489177183955, 1
predictable emphasize: 0.41514489177183955, 1
emphasize novel: 0.41514489177183955, 1
novel framework: 0.41514489177183955, 1
cueing hypothesis: 0.41514489177183955, 1
hypothesis verification: 0.41514489177183955, 1
verification affordance: 0.41514489177183955, 1
control argue: 0.41514489177183955, 1
argue affordance: 0.41514489177183955, 1
based perception: 0.41514489177183955, 1
efficient provide: 0.41514489177183955, 1
basis responses: 0.41514489177183955, 1
implementation applying: 0.41514489177183955, 1
interest within: 0.41514489177183955, 1
within simulate: 0.41514489177183955, 1
selected predict: 0.41514489177183955, 1
affordance real: 0.41514489177183955, 1
world scene: 0.41514489177183955, 1
network object: 0.41514489177183955, 1
detector dense: 0.41514489177183955, 1
field system: 0.41514489177183955, 1
system trains: 0.41514489177183955, 1
trains object: 0.41514489177183955, 1
detector generate: 0.41514489177183955, 1
generate bounding: 0.41514489177183955, 1
bounding box: 0.41514489177183955, 1
box candidates: 0.41514489177183955, 1
candidates deep: 0.41514489177183955, 1
deep cnn: 0.41514489177183955, 1
cnn learn: 0.41514489177183955, 1
learn depth: 0.41514489177183955, 1
depth feature: 0.41514489177183955, 1
feature bounding: 0.41514489177183955, 1
bounding feature: 0.41514489177183955, 1
feature maps: 0.41514489177183955, 1
maps post: 0.41514489177183955, 1
post processed: 0.41514489177183955, 1
processed dense: 0.41514489177183955, 1
dense crf: 0.41514489177183955, 1
crf improve: 0.41514489177183955, 1
improve predict: 0.41514489177183955, 1
predict along: 0.41514489177183955, 1
along class: 0.41514489177183955, 1
class experimental: 0.41514489177183955, 1
result challenging: 0.41514489177183955, 1
challenging dataset: 0.41514489177183955, 1
dataset propose: 0.41514489177183955, 1
method substantial: 0.41514489177183955, 1
substantial detected: 0.41514489177183955, 1
detected affordance: 0.41514489177183955, 1
affordance introduce: 0.41514489177183955, 1
introduce grasp: 0.41514489177183955, 1
grasp method: 0.41514489177183955, 1
method robust: 0.41514489177183955, 1
robust noisy: 0.41514489177183955, 1
noisy demonstrate: 0.41514489177183955, 1
effectiveness framework: 0.41514489177183955, 1
framework full: 0.41514489177183955, 1
robot walk: 0.41514489177183955, 1
walk man: 0.41514489177183955, 1
man different: 0.41514489177183955, 1
different object: 0.41514489177183955, 1
object real: 0.41514489177183955, 1
algorithm had: 0.41514489177183955, 1
had profound: 0.41514489177183955, 1
profound impact: 0.41514489177183955, 1
impact field: 0.41514489177183955, 1
computer science: 0.41514489177183955, 1
science past: 0.41514489177183955, 1
past few: 0.41514489177183955, 1
few performance: 0.41514489177183955, 1
performance algorithm: 0.41514489177183955, 1
algorithm heavily: 0.41514489177183955, 1
heavily depends: 0.41514489177183955, 1
depends representation: 0.41514489177183955, 1
representation derived: 0.41514489177183955, 1
derived data: 0.41514489177183955, 1
during learning: 0.41514489177183955, 1
learning successful: 0.41514489177183955, 1
successful learning: 0.41514489177183955, 1
learning processes: 0.41514489177183955, 1
processes aim: 0.41514489177183955, 1
aim produce: 0.41514489177183955, 1
produce meaningful: 0.41514489177183955, 1
meaningful representation: 0.41514489177183955, 1
representation effectively: 0.41514489177183955, 1
effectively applied: 0.41514489177183955, 1
applied various: 0.41514489177183955, 1
various recent: 0.41514489177183955, 1
recent advancements: 0.41514489177183955, 1
advancements deep: 0.41514489177183955, 1
model proven: 0.41514489177183955, 1
proven highly: 0.41514489177183955, 1
highly effective: 0.41514489177183955, 1
effective capturing: 0.41514489177183955, 1
capturing high: 0.41514489177183955, 1
high non: 0.41514489177183955, 1
non multi: 0.41514489177183955, 1
modal provide: 0.41514489177183955, 1
comprehensive overview: 0.41514489177183955, 1
overview current: 0.41514489177183955, 1
art deep: 0.41514489177183955, 1
learning principles: 0.41514489177183955, 1
principles developments: 0.41514489177183955, 1
developments made: 0.41514489177183955, 1
made process: 0.41514489177183955, 1
process representation: 0.41514489177183955, 1
representation study: 0.41514489177183955, 1
study encompasses: 0.41514489177183955, 1
encompasses supervised: 0.41514489177183955, 1
supervised unsupervised: 0.41514489177183955, 1
unsupervised including: 0.41514489177183955, 1
including popular: 0.41514489177183955, 1
popular techniques: 0.41514489177183955, 1
techniques self: 0.41514489177183955, 1
neural explore: 0.41514489177183955, 1
explore wide: 0.41514489177183955, 1
range including: 0.41514489177183955, 1
including image: 0.41514489177183955, 1
recognition natural: 0.41514489177183955, 1
language discuss: 0.41514489177183955, 1
discuss recent: 0.41514489177183955, 1
recent key: 0.41514489177183955, 1
key open: 0.41514489177183955, 1
challenge survey: 0.41514489177183955, 1
survey endeavors: 0.41514489177183955, 1
endeavors significant: 0.41514489177183955, 1
significant contribution: 0.41514489177183955, 1
contribution field: 0.41514489177183955, 1
field deep: 0.41514489177183955, 1
representation fostering: 0.41514489177183955, 1
fostering understanding: 0.41514489177183955, 1
understanding facilitating: 0.41514489177183955, 1
facilitating further: 0.41514489177183955, 1
systems obtained: 0.41514489177183955, 1
obtained significant: 0.41514489177183955, 1
significant performance: 0.41514489177183955, 1
performance deployed: 0.41514489177183955, 1
deployed widely: 0.41514489177183955, 1
widely daily: 0.41514489177183955, 1
daily lives: 0.41514489177183955, 1
lives human: 0.41514489177183955, 1
human enjoy: 0.41514489177183955, 1
enjoy benefits: 0.41514489177183955, 1
benefits brought: 0.41514489177183955, 1
brought technologies: 0.41514489177183955, 1
technologies suffer: 0.41514489177183955, 1
suffer many: 0.41514489177183955, 1
many social: 0.41514489177183955, 1
social issues: 0.41514489177183955, 1
issues induced: 0.41514489177183955, 1
induced ai: 0.41514489177183955, 1
systems enough: 0.41514489177183955, 1
enough plenty: 0.41514489177183955, 1
plenty researches: 0.41514489177183955, 1
researches done: 0.41514489177183955, 1
done build: 0.41514489177183955, 1
build guidelines: 0.41514489177183955, 1
guidelines trustworthy: 0.41514489177183955, 1
trustworthy ai: 0.41514489177183955, 1
ai machine: 0.41514489177183955, 1
learning important: 0.41514489177183955, 1
important parts: 0.41514489177183955, 1
parts ai: 0.41514489177183955, 1
ai representation: 0.41514489177183955, 1
learning fundamental: 0.41514489177183955, 1
fundamental technology: 0.41514489177183955, 1
technology machine: 0.41514489177183955, 1
machine representation: 0.41514489177183955, 1
learning trustworthy: 0.41514489177183955, 1
trustworthy real: 0.41514489177183955, 1
world cross: 0.41514489177183955, 1
cross domain: 0.41514489177183955, 1
domain valuable: 0.41514489177183955, 1
valuable necessary: 0.41514489177183955, 1
necessary machine: 0.41514489177183955, 1
learning ai: 0.41514489177183955, 1
ai system: 0.41514489177183955, 1
system inspired: 0.41514489177183955, 1
inspired concept: 0.41514489177183955, 1
concept trustworthy: 0.41514489177183955, 1
trustworthy propose: 0.41514489177183955, 1
domains includes: 0.41514489177183955, 1
includes four: 0.41514489177183955, 1
four comprehensive: 0.41514489177183955, 1
comprehensive literature: 0.41514489177183955, 1
literature review: 0.41514489177183955, 1
review research: 0.41514489177183955, 1
research introduce: 0.41514489177183955, 1
introduce details: 0.41514489177183955, 1
details propose: 0.41514489177183955, 1
framework representation: 0.41514489177183955, 1
across provide: 0.41514489177183955, 1
provide basic: 0.41514489177183955, 1
basic notions: 0.41514489177183955, 1
notions comprehensively: 0.41514489177183955, 1
comprehensively summarize: 0.41514489177183955, 1
summarize existing: 0.41514489177183955, 1
method trustworthy: 0.41514489177183955, 1
framework four: 0.41514489177183955, 1
four conclude: 0.41514489177183955, 1
conclude survey: 0.41514489177183955, 1
survey insights: 0.41514489177183955, 1
insights discussions: 0.41514489177183955, 1
discussions future: 0.41514489177183955, 1
future research: 0.41514489177183955, 1
central theme: 0.41514489177183955, 1
theme review: 0.41514489177183955, 1
review dynamic: 0.41514489177183955, 1
interaction information: 0.41514489177183955, 1
information selection: 0.41514489177183955, 1
selection pose: 0.41514489177183955, 1
pose fundamental: 0.41514489177183955, 1
fundamental question: 0.41514489177183955, 1
question learn: 0.41514489177183955, 1
feature experiences: 0.41514489177183955, 1
experiences worth: 0.41514489177183955, 1
worth learning: 0.41514489177183955, 1
learning process: 0.41514489177183955, 1
process depends: 0.41514489177183955, 1
depends attention: 0.41514489177183955, 1
attention cognitive: 0.41514489177183955, 1
cognitive functions: 0.41514489177183955, 1
functions together: 0.41514489177183955, 1
together constrain: 0.41514489177183955, 1
constrain representation: 0.41514489177183955, 1
representation world: 0.41514489177183955, 1
world feature: 0.41514489177183955, 1
feature relevant: 0.41514489177183955, 1
relevant goal: 0.41514489177183955, 1
goal recent: 0.41514489177183955, 1
recent evidence: 0.41514489177183955, 1
evidence suggests: 0.41514489177183955, 1
suggests representation: 0.41514489177183955, 1
representation shaped: 0.41514489177183955, 1
shaped attention: 0.41514489177183955, 1
attention memory: 0.41514489177183955, 1
memory themselves: 0.41514489177183955, 1
themselves inferred: 0.41514489177183955, 1
inferred experience: 0.41514489177183955, 1
experience each: 0.41514489177183955, 1
each review: 0.41514489177183955, 1
review evidence: 0.41514489177183955, 1
evidence place: 0.41514489177183955, 1
place context: 0.41514489177183955, 1
context explicitly: 0.41514489177183955, 1
explicitly characterized: 0.41514489177183955, 1
characterized representation: 0.41514489177183955, 1
learning statistical: 0.41514489177183955, 1
statistical discuss: 0.41514489177183955, 1
discuss inference: 0.41514489177183955, 1
inference scaled: 0.41514489177183955, 1
scaled real: 0.41514489177183955, 1
world decisions: 0.41514489177183955, 1
decisions approximating: 0.41514489177183955, 1
approximating beliefs: 0.41514489177183955, 1
beliefs based: 0.41514489177183955, 1
based small: 0.41514489177183955, 1
number highlight: 0.41514489177183955, 1
highlight implications: 0.41514489177183955, 1
implications inference: 0.41514489177183955, 1
inference process: 0.41514489177183955, 1
process human: 0.41514489177183955, 1
human decision: 0.41514489177183955, 1
making social: 0.41514489177183955, 1
address problem: 0.41514489177183955, 1
reasoning diverse: 0.41514489177183955, 1
diverse scene: 0.41514489177183955, 1
scene appear: 0.41514489177183955, 1
appear real: 0.41514489177183955, 1
real affordance: 0.41514489177183955, 1
affordance relate: 0.41514489177183955, 1
relate action: 0.41514489177183955, 1
effect taken: 0.41514489177183955, 1
taken surrounding: 0.41514489177183955, 1
surrounding egocentric: 0.41514489177183955, 1
egocentric view: 0.41514489177183955, 1
view aim: 0.41514489177183955, 1
aim reason: 0.41514489177183955, 1
reason action: 0.41514489177183955, 1
affordance respect: 0.41514489177183955, 1
respect physical: 0.41514489177183955, 1
world social: 0.41514489177183955, 1
social norms: 0.41514489177183955, 1
norms imposed: 0.41514489177183955, 1
imposed aim: 0.41514489177183955, 1
aim teach: 0.41514489177183955, 1
teach artificial: 0.41514489177183955, 1
agent why: 0.41514489177183955, 1
why action: 0.41514489177183955, 1
action taken: 0.41514489177183955, 1
taken certain: 0.41514489177183955, 1
certain likely: 0.41514489177183955, 1
likely happen: 0.41514489177183955, 1
happen action: 0.41514489177183955, 1
action collect: 0.41514489177183955, 1
collect dataset: 0.41514489177183955, 1
dataset builds: 0.41514489177183955, 1
builds upon: 0.41514489177183955, 1
upon referred: 0.41514489177183955, 1
referred ade: 0.41514489177183955, 1
ade containing: 0.41514489177183955, 1
containing annotations: 0.41514489177183955, 1
annotations enabling: 0.41514489177183955, 1
enabling rich: 0.41514489177183955, 1
rich visual: 0.41514489177183955, 1
model exploits: 0.41514489177183955, 1
exploits graph: 0.41514489177183955, 1
graph neural: 0.41514489177183955, 1
network propagate: 0.41514489177183955, 1
propagate contextual: 0.41514489177183955, 1
contextual information: 0.41514489177183955, 1
information scene: 0.41514489177183955, 1
scene order: 0.41514489177183955, 1
order perform: 0.41514489177183955, 1
perform detailed: 0.41514489177183955, 1
detailed affordance: 0.41514489177183955, 1
reasoning each: 0.41514489177183955, 1
each model: 0.41514489177183955, 1
model showcased: 0.41514489177183955, 1
showcased various: 0.41514489177183955, 1
various ablation: 0.41514489177183955, 1
ablation pointing: 0.41514489177183955, 1
pointing successes: 0.41514489177183955, 1
successes challenge: 0.41514489177183955, 1
challenge complex: 0.41514489177183955, 1
driving fall: 0.41514489177183955, 1
fall modular: 0.41514489177183955, 1
modular build: 0.41514489177183955, 1
build extensive: 0.41514489177183955, 1
extensive model: 0.41514489177183955, 1
model imitation: 0.41514489177183955, 1
learning map: 0.41514489177183955, 1
map image: 0.41514489177183955, 1
image directly: 0.41514489177183955, 1
directly control: 0.41514489177183955, 1
control recently: 0.41514489177183955, 1
propose third: 0.41514489177183955, 1
direct aims: 0.41514489177183955, 1
aims combine: 0.41514489177183955, 1
combine advantages: 0.41514489177183955, 1
advantages neural: 0.41514489177183955, 1
learn appropriate: 0.41514489177183955, 1
appropriate low: 0.41514489177183955, 1
dimensional intermediate: 0.41514489177183955, 1
intermediate existing: 0.41514489177183955, 1
existing direct: 0.41514489177183955, 1
approach restricted: 0.41514489177183955, 1
restricted simple: 0.41514489177183955, 1
simple highway: 0.41514489177183955, 1
highway lacking: 0.41514489177183955, 1
lacking ability: 0.41514489177183955, 1
ability navigate: 0.41514489177183955, 1
navigate stop: 0.41514489177183955, 1
stop traffic: 0.41514489177183955, 1
lights respect: 0.41514489177183955, 1
respect speed: 0.41514489177183955, 1
speed direct: 0.41514489177183955, 1
approach maps: 0.41514489177183955, 1
maps video: 0.41514489177183955, 1
video input: 0.41514489177183955, 1
input intermediate: 0.41514489177183955, 1
intermediate representation: 0.41514489177183955, 1
representation suitable: 0.41514489177183955, 1
suitable autonomous: 0.41514489177183955, 1
autonomous navigation: 0.41514489177183955, 1
complex urban: 0.41514489177183955, 1
environment given: 0.41514489177183955, 1
given high: 0.41514489177183955, 1
level directional: 0.41514489177183955, 1
directional compared: 0.41514489177183955, 1
compared state: 0.41514489177183955, 1
art reinforcement: 0.41514489177183955, 1
reinforcement conditional: 0.41514489177183955, 1
learning achieve: 0.41514489177183955, 1
achieve improvement: 0.41514489177183955, 1
improvement 68: 0.41514489177183955, 1
68 goal: 0.41514489177183955, 1
directed navigation: 0.41514489177183955, 1
navigation challenging: 0.41514489177183955, 1
challenging carla: 0.41514489177183955, 1
carla simulate: 0.41514489177183955, 1
simulate approach: 0.41514489177183955, 1
approach handle: 0.41514489177183955, 1
handle traffic: 0.41514489177183955, 1
lights speed: 0.41514489177183955, 1
speed signs: 0.41514489177183955, 1
signs image: 0.41514489177183955, 1
image level: 0.41514489177183955, 1
level label: 0.41514489177183955, 1
label smooth: 0.41514489177183955, 1
smooth car: 0.41514489177183955, 1
car resulting: 0.41514489177183955, 1
resulting significant: 0.41514489177183955, 1
significant reduction: 0.41514489177183955, 1
reduction traffic: 0.41514489177183955, 1
traffic accidents: 0.41514489177183955, 1
affordance allows: 0.41514489177183955, 1
allows autonomous: 0.41514489177183955, 1
autonomous agent: 0.41514489177183955, 1
perform generalised: 0.41514489177183955, 1
generalised manipulation: 0.41514489177183955, 1
task among: 0.41514489177183955, 1
among object: 0.41514489177183955, 1
object while: 0.41514489177183955, 1
while current: 0.41514489177183955, 1
current approach: 0.41514489177183955, 1
approach grasp: 0.41514489177183955, 1
affordance estimation: 0.41514489177183955, 1
estimation limited: 0.41514489177183955, 1
limited single: 0.41514489177183955, 1
single present: 0.41514489177183955, 1
approach detection: 0.41514489177183955, 1
detection extraction: 0.41514489177183955, 1
extraction multiple: 0.41514489177183955, 1
multiple grasp: 0.41514489177183955, 1
via visual: 0.41514489177183955, 1
visual define: 0.41514489177183955, 1
define semantics: 0.41514489177183955, 1
semantics combination: 0.41514489177183955, 1
combination multiple: 0.41514489177183955, 1
multiple yields: 0.41514489177183955, 1
yields benefits: 0.41514489177183955, 1
benefits terms: 0.41514489177183955, 1
terms generalisation: 0.41514489177183955, 1
generalisation grasp: 0.41514489177183955, 1
affordance markov: 0.41514489177183955, 1
markov logic: 0.41514489177183955, 1
logic network: 0.41514489177183955, 1
network build: 0.41514489177183955, 1
build knowledge: 0.41514489177183955, 1
base graph: 0.41514489177183955, 1
representation obtain: 0.41514489177183955, 1
obtain probability: 0.41514489177183955, 1
probability distribution: 0.41514489177183955, 1
distribution grasp: 0.41514489177183955, 1
affordance harvest: 0.41514489177183955, 1
harvest knowledge: 0.41514489177183955, 1
knowledge collect: 0.41514489177183955, 1
collect available: 0.41514489177183955, 1
available novel: 0.41514489177183955, 1
dataset relates: 0.41514489177183955, 1
relates different: 0.41514489177183955, 1
different semantic: 0.41514489177183955, 1
semantic achieve: 0.41514489177183955, 1
achieve reliable: 0.41514489177183955, 1
reliable mappings: 0.41514489177183955, 1
mappings predicted: 0.41514489177183955, 1
predicted grasp: 0.41514489177183955, 1
learning prototypical: 0.41514489177183955, 1
prototypical grasp: 0.41514489177183955, 1
grasp patches: 0.41514489177183955, 1
patches several: 0.41514489177183955, 1
several generalisation: 0.41514489177183955, 1
generalisation capabilities: 0.41514489177183955, 1
capabilities grasp: 0.41514489177183955, 1
predict novel: 0.41514489177183955, 1
novel instances: 0.41514489177183955, 1
instances compare: 0.41514489177183955, 1
compare similar: 0.41514489177183955, 1
similar method: 0.41514489177183955, 1
method robot: 0.41514489177183955, 1
robot simulate: 0.41514489177183955, 1
real evaluate: 0.41514489177183955, 1
evaluate success: 0.41514489177183955, 1
success grasp: 0.41514489177183955, 1
grasp task: 0.41514489177183955, 1
task conditioned: 0.41514489177183955, 1
conditioned grasp: 0.41514489177183955, 1
application deep: 0.41514489177183955, 1
robot platforms: 0.41514489177183955, 1
platforms successfully: 0.41514489177183955, 1
successfully advanced: 0.41514489177183955, 1
advanced robot: 0.41514489177183955, 1
perception task: 0.41514489177183955, 1
task related: 0.41514489177183955, 1
related human: 0.41514489177183955, 1
collaboration task: 0.41514489177183955, 1
task scene: 0.41514489177183955, 1
scene object: 0.41514489177183955, 1
interaction facilitated: 0.41514489177183955, 1
facilitated acquisition: 0.41514489177183955, 1
acquisition knowledge: 0.41514489177183955, 1
taking place: 0.41514489177183955, 1
place contributions: 0.41514489177183955, 1
contributions thesis: 0.41514489177183955, 1
thesis shows: 0.41514489177183955, 1
shows representation: 0.41514489177183955, 1
unsupervised predict: 0.41514489177183955, 1
predict categories: 0.41514489177183955, 1
categories object: 0.41514489177183955, 1
object depending: 0.41514489177183955, 1
depending shows: 0.41514489177183955, 1
shows future: 0.41514489177183955, 1
future frame: 0.41514489177183955, 1
frame independent: 0.41514489177183955, 1
independent interaction: 0.41514489177183955, 1
learned self: 0.41514489177183955, 1
supervised exploiting: 0.41514489177183955, 1
exploiting high: 0.41514489177183955, 1
level graph: 0.41514489177183955, 1
object aim: 0.41514489177183955, 1
aim research: 0.41514489177183955, 1
research create: 0.41514489177183955, 1
create representation: 0.41514489177183955, 1
representation perform: 0.41514489177183955, 1
perform predict: 0.41514489177183955, 1
predict interaction: 0.41514489177183955, 1
interaction abstract: 0.41514489177183955, 1
abstract image: 0.41514489177183955, 1
image space: 0.41514489177183955, 1
space attain: 0.41514489177183955, 1
attain generalization: 0.41514489177183955, 1
generalization across: 0.41514489177183955, 1
across various: 0.41514489177183955, 1
various scene: 0.41514489177183955, 1
interaction holding: 0.41514489177183955, 1
holding playing: 0.41514489177183955, 1
playing temporal: 0.41514489177183955, 1
aspect sequence: 0.41514489177183955, 1
sequence several: 0.41514489177183955, 1
several static: 0.41514489177183955, 1
static interaction: 0.41514489177183955, 1
interaction importance: 0.41514489177183955, 1
importance dynamic: 0.41514489177183955, 1
interaction occlusion: 0.41514489177183955, 1
occlusion object: 0.41514489177183955, 1
object 2d: 0.41514489177183955, 1
2d domain: 0.41514489177183955, 1
domain handled: 0.41514489177183955, 1
handled avoid: 0.41514489177183955, 1
avoid false: 0.41514489177183955, 1
false positive: 0.41514489177183955, 1
positive interaction: 0.41514489177183955, 1
interaction rgb: 0.41514489177183955, 1
video data: 0.41514489177183955, 1
data exploited: 0.41514489177183955, 1
exploited human: 0.41514489177183955, 1
human tend: 0.41514489177183955, 1
tend object: 0.41514489177183955, 1
object many: 0.41514489177183955, 1
different ways: 0.41514489177183955, 1
ways depending: 0.41514489177183955, 1
depending scene: 0.41514489177183955, 1
scene learning: 0.41514489177183955, 1
affordance everyday: 0.41514489177183955, 1
life scenarios: 0.41514489177183955, 1
scenarios challenging: 0.41514489177183955, 1
challenging particularly: 0.41514489177183955, 1
particularly presence: 0.41514489177183955, 1
presence open: 0.41514489177183955, 1
open set: 0.41514489177183955, 1
set interaction: 0.41514489177183955, 1
interaction class: 0.41514489177183955, 1
class agnostic: 0.41514489177183955, 1
agnostic order: 0.41514489177183955, 1
order abstract: 0.41514489177183955, 1
abstract continuous: 0.41514489177183955, 1
continuous representation: 0.41514489177183955, 1
representation spatio: 0.41514489177183955, 1
interaction video: 0.41514489177183955, 1
video novel: 0.41514489177183955, 1
novel set: 0.41514489177183955, 1
set high: 0.41514489177183955, 1
qualitative depth: 0.41514489177183955, 1
depth informed: 0.41514489177183955, 1
informed spatial: 0.41514489177183955, 1
relations learning: 0.41514489177183955, 1
learning similarities: 0.41514489177183955, 1
similarities via: 0.41514489177183955, 1
via unsupervised: 0.41514489177183955, 1
unsupervised method: 0.41514489177183955, 1
method exploiting: 0.41514489177183955, 1
exploiting graph: 0.41514489177183955, 1
interaction induces: 0.41514489177183955, 1
induces hierarchy: 0.41514489177183955, 1
hierarchy clusters: 0.41514489177183955, 1
clusters object: 0.41514489177183955, 1
object similar: 0.41514489177183955, 1
similar propose: 0.41514489177183955, 1
method handles: 0.41514489177183955, 1
handles object: 0.41514489177183955, 1
object occlusions: 0.41514489177183955, 1
occlusions capturing: 0.41514489177183955, 1
capturing effectively: 0.41514489177183955, 1
effectively possible: 0.41514489177183955, 1
possible interaction: 0.41514489177183955, 1
without imposing: 0.41514489177183955, 1
imposing object: 0.41514489177183955, 1
interaction action: 0.41514489177183955, 1
action anticipation: 0.41514489177183955, 1
anticipation remains: 0.41514489177183955, 1
remains challenging: 0.41514489177183955, 1
challenging especially: 0.41514489177183955, 1
especially considering: 0.41514489177183955, 1
considering generalizability: 0.41514489177183955, 1
generalizability constraints: 0.41514489177183955, 1
constraints train: 0.41514489177183955, 1
data exploiting: 0.41514489177183955, 1
exploiting visual: 0.41514489177183955, 1
visual video: 0.41514489177183955, 1
video state: 0.41514489177183955, 1
method allow: 0.41514489177183955, 1
predict approximately: 0.41514489177183955, 1
approximately three: 0.41514489177183955, 1
three seconds: 0.41514489177183955, 1
seconds everyday: 0.41514489177183955, 1
life consist: 0.41514489177183955, 1
consist action: 0.41514489177183955, 1
action more: 0.41514489177183955, 1
more five: 0.41514489177183955, 1
five seconds: 0.41514489177183955, 1
seconds thesis: 0.41514489177183955, 1
thesis present: 0.41514489177183955, 1
solving task: 0.41514489177183955, 1
task interaction: 0.41514489177183955, 1
interaction anticipation: 0.41514489177183955, 1
anticipation object: 0.41514489177183955, 1
object video: 0.41514489177183955, 1
video scene: 0.41514489177183955, 1
scene utilizing: 0.41514489177183955, 1
utilizing high: 0.41514489177183955, 1
qualitative frame: 0.41514489177183955, 1
frame number: 0.41514489177183955, 1
number independent: 0.41514489177183955, 1
independent spatial: 0.41514489177183955, 1
spatial graphs: 0.41514489177183955, 1
graphs represent: 0.41514489177183955, 1
represent object: 0.41514489177183955, 1
object deep: 0.41514489177183955, 1
learn self: 0.41514489177183955, 1
predict graph: 0.41514489177183955, 1
graph structures: 0.41514489177183955, 1
structures future: 0.41514489177183955, 1
object whilst: 0.41514489177183955, 1
whilst being: 0.41514489177183955, 1
being decoupled: 0.41514489177183955, 1
decoupled visual: 0.41514489177183955, 1
visual underlying: 0.41514489177183955, 1
underlying duration: 0.41514489177183955, 1
duration each: 0.41514489177183955, 1
each interaction: 0.41514489177183955, 1
taking propose: 0.41514489177183955, 1
evaluated rgb: 0.41514489177183955, 1
datasets capturing: 0.41514489177183955, 1
capturing everyday: 0.41514489177183955, 1
life activities: 0.41514489177183955, 1
activities human: 0.41514489177183955, 1
human compared: 0.41514489177183955, 1
compared against: 0.41514489177183955, 1
against closely: 0.41514489177183955, 1
closely related: 0.41514489177183955, 1
related state: 0.41514489177183955, 1
learning known: 0.41514489177183955, 1
known suffer: 0.41514489177183955, 1
suffer poor: 0.41514489177183955, 1
poor sample: 0.41514489177183955, 1
efficiency generalisation: 0.41514489177183955, 1
generalisation unseen: 0.41514489177183955, 1
unseen visuals: 0.41514489177183955, 1
visuals distractors: 0.41514489177183955, 1
distractors independent: 0.41514489177183955, 1
independent aspects: 0.41514489177183955, 1
aspects observation: 0.41514489177183955, 1
observation visual: 0.41514489177183955, 1
visual domain: 0.41514489177183955, 1
domain randomisation: 0.41514489177183955, 1
randomisation encourages: 0.41514489177183955, 1
encourages transfer: 0.41514489177183955, 1
transfer train: 0.41514489177183955, 1
train visual: 0.41514489177183955, 1
visual factors: 0.41514489177183955, 1
variation may: 0.41514489177183955, 1
may encountered: 0.41514489177183955, 1
encountered target: 0.41514489177183955, 1
target increases: 0.41514489177183955, 1
increases learning: 0.41514489177183955, 1
learning negatively: 0.41514489177183955, 1
negatively impact: 0.41514489177183955, 1
impact learning: 0.41514489177183955, 1
learning rate: 0.41514489177183955, 1
rate requires: 0.41514489177183955, 1
requires knowledge: 0.41514489177183955, 1
knowledge potential: 0.41514489177183955, 1
potential variations: 0.41514489177183955, 1
variations during: 0.41514489177183955, 1
during introduce: 0.41514489177183955, 1
introduce attention: 0.41514489177183955, 1
learning uses: 0.41514489177183955, 1
supervised attention: 0.41514489177183955, 1
mechanism significantly: 0.41514489177183955, 1
significantly alleviate: 0.41514489177183955, 1
alleviate focusing: 0.41514489177183955, 1
focusing task: 0.41514489177183955, 1
relevant aspects: 0.41514489177183955, 1
aspects attention: 0.41514489177183955, 1
attention provides: 0.41514489177183955, 1
provides robustness: 0.41514489177183955, 1
robustness distractors: 0.41514489177183955, 1
distractors significantly: 0.41514489177183955, 1
significantly increased: 0.41514489177183955, 1
increased learning: 0.41514489177183955, 1
learning april: 0.41514489177183955, 1
april trains: 0.41514489177183955, 1
trains attention: 0.41514489177183955, 1
attention augmented: 0.41514489177183955, 1
augmented actor: 0.41514489177183955, 1
critic purely: 0.41514489177183955, 1
purely based: 0.41514489177183955, 1
image available: 0.41514489177183955, 1
available across: 0.41514489177183955, 1
across train: 0.41514489177183955, 1
train transfer: 0.41514489177183955, 1
transfer access: 0.41514489177183955, 1
privileged information: 0.41514489177183955, 1
information environment: 0.41514489177183955, 1
available during: 0.41514489177183955, 1
during experience: 0.41514489177183955, 1
experience shared: 0.41514489177183955, 1
shared agent: 0.41514489177183955, 1
agent attention: 0.41514489177183955, 1
mechanism image: 0.41514489177183955, 1
based policy: 0.41514489177183955, 1
policy deployed: 0.41514489177183955, 1
deployed without: 0.41514489177183955, 1
without access: 0.41514489177183955, 1
privileged experimentally: 0.41514489177183955, 1
experimentally demonstrate: 0.41514489177183955, 1
demonstrate accelerated: 0.41514489177183955, 1
accelerated more: 0.41514489177183955, 1
more robust: 0.41514489177183955, 1
robust learning: 0.41514489177183955, 1
learning diverse: 0.41514489177183955, 1
set leading: 0.41514489177183955, 1
leading improved: 0.41514489177183955, 1
improved final: 0.41514489177183955, 1
final performance: 0.41514489177183955, 1
performance environment: 0.41514489177183955, 1
environment within: 0.41514489177183955, 1
within outside: 0.41514489177183955, 1
outside train: 0.41514489177183955, 1
biologically motivated: 0.41514489177183955, 1
motivated computational: 0.41514489177183955, 1
control interactive: 0.41514489177183955, 1
interactive top: 0.41514489177183955, 1
learned interactively: 0.41514489177183955, 1
interactively search: 0.41514489177183955, 1
search desired: 0.41514489177183955, 1
desired object: 0.41514489177183955, 1
biasing bottom: 0.41514489177183955, 1
bottom attention: 0.41514489177183955, 1
attention order: 0.41514489177183955, 1
order form: 0.41514489177183955, 1
form based: 0.41514489177183955, 1
object driven: 0.41514489177183955, 1
driven state: 0.41514489177183955, 1
consists three: 0.41514489177183955, 1
three early: 0.41514489177183955, 1
early visual: 0.41514489177183955, 1
processing salient: 0.41514489177183955, 1
salient location: 0.41514489177183955, 1
location scene: 0.41514489177183955, 1
scene derived: 0.41514489177183955, 1
derived biased: 0.41514489177183955, 1
biased saliency: 0.41514489177183955, 1
saliency based: 0.41514489177183955, 1
based bottom: 0.41514489177183955, 1
bottom model: 0.41514489177183955, 1
cognitive component: 0.41514489177183955, 1
component higher: 0.41514489177183955, 1
higher visual: 0.41514489177183955, 1
processing layer: 0.41514489177183955, 1
layer performs: 0.41514489177183955, 1
performs application: 0.41514489177183955, 1
application specific: 0.41514489177183955, 1
specific operation: 0.41514489177183955, 1
operation object: 0.41514489177183955, 1
recognition focus: 0.41514489177183955, 1
focus state: 0.41514489177183955, 1
state derived: 0.41514489177183955, 1
derived decision: 0.41514489177183955, 1
making learning: 0.41514489177183955, 1
learned u: 0.41514489177183955, 1
u tree: 0.41514489177183955, 1
tree algorithm: 0.41514489177183955, 1
algorithm successively: 0.41514489177183955, 1
successively grows: 0.41514489177183955, 1
grows object: 0.41514489177183955, 1
based binary: 0.41514489177183955, 1
binary internal: 0.41514489177183955, 1
internal nodes: 0.41514489177183955, 1
nodes tree: 0.41514489177183955, 1
tree check: 0.41514489177183955, 1
check existence: 0.41514489177183955, 1
existence specific: 0.41514489177183955, 1
biasing early: 0.41514489177183955, 1
early vision: 0.41514489177183955, 1
vision object: 0.41514489177183955, 1
recognition leaves: 0.41514489177183955, 1
leaves point: 0.41514489177183955, 1
point states: 0.41514489177183955, 1
states action: 0.41514489177183955, 1
action value: 0.41514489177183955, 1
value motor: 0.41514489177183955, 1
motor action: 0.41514489177183955, 1
action associated: 0.41514489177183955, 1
associated performing: 0.41514489177183955, 1
performing motor: 0.41514489177183955, 1
motor agent: 0.41514489177183955, 1
agent receives: 0.41514489177183955, 1
receives reinforcement: 0.41514489177183955, 1
reinforcement signal: 0.41514489177183955, 1
signal signal: 0.41514489177183955, 1
signal alternately: 0.41514489177183955, 1
alternately modifying: 0.41514489177183955, 1
modifying tree: 0.41514489177183955, 1
tree updating: 0.41514489177183955, 1
updating action: 0.41514489177183955, 1
selection propose: 0.41514489177183955, 1
model evaluated: 0.41514489177183955, 1
evaluated visual: 0.41514489177183955, 1
navigation obtained: 0.41514489177183955, 1
obtained result: 0.41514489177183955, 1
result lend: 0.41514489177183955, 1
lend support: 0.41514489177183955, 1
support applicability: 0.41514489177183955, 1
applicability usefulness: 0.41514489177183955, 1
usefulness developed: 0.41514489177183955, 1
developed method: 0.41514489177183955, 1
feature previously: 0.41514489177183955, 1
previously associated: 0.41514489177183955, 1
associated reward: 0.41514489177183955, 1
reward capture: 0.41514489177183955, 1
capture attention: 0.41514489177183955, 1
attention task: 0.41514489177183955, 1
task phenomenon: 0.41514489177183955, 1
phenomenon known: 0.41514489177183955, 1
known value: 0.41514489177183955, 1
capture vdac: 0.41514489177183955, 1
vdac persists: 0.41514489177183955, 1
persists without: 0.41514489177183955, 1
without unlike: 0.41514489177183955, 1
unlike forms: 0.41514489177183955, 1
forms removing: 0.41514489177183955, 1
removing reinforcement: 0.41514489177183955, 1
reinforcement typically: 0.41514489177183955, 1
typically leads: 0.41514489177183955, 1
leads five: 0.41514489177183955, 1
five factors: 0.41514489177183955, 1
factors common: 0.41514489177183955, 1
common many: 0.41514489177183955, 1
many study: 0.41514489177183955, 1
study manipulated: 0.41514489177183955, 1
manipulated examine: 0.41514489177183955, 1
examine impact: 0.41514489177183955, 1
impact vdac: 0.41514489177183955, 1
experiments included: 0.41514489177183955, 1
included learning: 0.41514489177183955, 1
learning test: 0.41514489177183955, 1
test during: 0.41514489177183955, 1
during participants: 0.41514489177183955, 1
completed visual: 0.41514489177183955, 1
task during: 0.41514489177183955, 1
during target: 0.41514489177183955, 1
target colors: 0.41514489177183955, 1
colors associated: 0.41514489177183955, 1
associated during: 0.41514489177183955, 1
during 1: 0.41514489177183955, 1
1 week: 0.41514489177183955, 1
week participants: 0.41514489177183955, 1
completed another: 0.41514489177183955, 1
another visual: 0.41514489177183955, 1
task reward: 0.41514489177183955, 1
reward association: 0.41514489177183955, 1
association rewarded: 0.41514489177183955, 1
feature remained: 0.41514489177183955, 1
remained task: 0.41514489177183955, 1
relevant vdac: 0.41514489177183955, 1
vdac rewarded: 0.41514489177183955, 1
feature made: 0.41514489177183955, 1
made task: 0.41514489177183955, 1
task irrelevant: 0.41514489177183955, 1
irrelevant evidence: 0.41514489177183955, 1
vdac except: 0.41514489177183955, 1
except target: 0.41514489177183955, 1
target feature: 0.41514489177183955, 1
feature physically: 0.41514489177183955, 1
physically salient: 0.41514489177183955, 1
salient reduction: 0.41514489177183955, 1
reduction frequency: 0.41514489177183955, 1
frequency exposure: 0.41514489177183955, 1
exposure reward: 0.41514489177183955, 1
reward associated: 0.41514489177183955, 1
associated feature: 0.41514489177183955, 1
feature failed: 0.41514489177183955, 1
failed find: 0.41514489177183955, 1
find evidence: 0.41514489177183955, 1
experiments suggesting: 0.41514489177183955, 1
suggesting vdac: 0.41514489177183955, 1
vdac may: 0.41514489177183955, 1
may depend: 0.41514489177183955, 1
depend demands: 0.41514489177183955, 1
demands task: 0.41514489177183955, 1
task resulting: 0.41514489177183955, 1
resulting vulnerability: 0.41514489177183955, 1
vulnerability vdac: 0.41514489177183955, 1
vdac extinction: 0.41514489177183955, 1
extinction indicates: 0.41514489177183955, 1
indicates vdac: 0.41514489177183955, 1
vdac subject: 0.41514489177183955, 1
subject extinction: 0.41514489177183955, 1
extinction expected: 0.41514489177183955, 1
expected effect: 0.41514489177183955, 1
effect driven: 0.41514489177183955, 1
several recent: 0.41514489177183955, 1
recent study: 0.41514489177183955, 1
study demonstrate: 0.41514489177183955, 1
demonstrate promise: 0.41514489177183955, 1
promise deep: 0.41514489177183955, 1
policy robot: 0.41514489177183955, 1
robot manipulator: 0.41514489177183955, 1
manipulator despite: 0.41514489177183955, 1
despite impressive: 0.41514489177183955, 1
impressive systems: 0.41514489177183955, 1
systems known: 0.41514489177183955, 1
known vulnerable: 0.41514489177183955, 1
vulnerable physical: 0.41514489177183955, 1
physical accidental: 0.41514489177183955, 1
accidental adversarial: 0.41514489177183955, 1
adversarial bumps: 0.41514489177183955, 1
bumps drop: 0.41514489177183955, 1
drop manipulated: 0.41514489177183955, 1
manipulated tend: 0.41514489177183955, 1
tend distracted: 0.41514489177183955, 1
distracted visual: 0.41514489177183955, 1
visual disturbances: 0.41514489177183955, 1
disturbances object: 0.41514489177183955, 1
object moving: 0.41514489177183955, 1
moving field: 0.41514489177183955, 1
field disturbance: 0.41514489177183955, 1
disturbance does: 0.41514489177183955, 1
does physically: 0.41514489177183955, 1
physically prevent: 0.41514489177183955, 1
prevent execution: 0.41514489177183955, 1
execution approach: 0.41514489177183955, 1
approach augmenting: 0.41514489177183955, 1
augmenting deep: 0.41514489177183955, 1
train demonstrations: 0.41514489177183955, 1
demonstrations task: 0.41514489177183955, 1
attention manipulation: 0.41514489177183955, 1
task specified: 0.41514489177183955, 1
specified natural: 0.41514489177183955, 1
language text: 0.41514489177183955, 1
text red: 0.41514489177183955, 1
red bowl: 0.41514489177183955, 1
bowl allows: 0.41514489177183955, 1
allows visual: 0.41514489177183955, 1
attention component: 0.41514489177183955, 1
component concentrate: 0.41514489177183955, 1
concentrate current: 0.41514489177183955, 1
current object: 0.41514489177183955, 1
needs benign: 0.41514489177183955, 1
benign tfa: 0.41514489177183955, 1
tfa allows: 0.41514489177183955, 1
allows policy: 0.41514489177183955, 1
policy consistently: 0.41514489177183955, 1
consistently outperform: 0.41514489177183955, 1
outperform variant: 0.41514489177183955, 1
variant attention: 0.41514489177183955, 1
attention more: 0.41514489177183955, 1
more policy: 0.41514489177183955, 1
policy significantly: 0.41514489177183955, 1
significantly more: 0.41514489177183955, 1
more regularly: 0.41514489177183955, 1
regularly recovers: 0.41514489177183955, 1
recovers severe: 0.41514489177183955, 1
severe physical: 0.41514489177183955, 1
physical disturbances: 0.41514489177183955, 1
disturbances bumps: 0.41514489177183955, 1
bumps causing: 0.41514489177183955, 1
causing drop: 0.41514489177183955, 1
drop baseline: 0.41514489177183955, 1
baseline visual: 0.41514489177183955, 1
visual almost: 0.41514489177183955, 1
almost never: 0.41514489177183955, 1
never propose: 0.41514489177183955, 1
propose policy: 0.41514489177183955, 1
policy performs: 0.41514489177183955, 1
performs correctly: 0.41514489177183955, 1
correctly presence: 0.41514489177183955, 1
presence wide: 0.41514489177183955, 1
wide class: 0.41514489177183955, 1
class visual: 0.41514489177183955, 1
visual exhibiting: 0.41514489177183955, 1
exhibiting behavior: 0.41514489177183955, 1
behavior reminiscent: 0.41514489177183955, 1
reminiscent human: 0.41514489177183955, 1
human selective: 0.41514489177183955, 1
selective visual: 0.41514489177183955, 1
anchor bolt: 0.41514489177183955, 1
bolt insertion: 0.41514489177183955, 1
insertion peg: 0.41514489177183955, 1
task performed: 0.41514489177183955, 1
performed construction: 0.41514489177183955, 1
construction field: 0.41514489177183955, 1
field holes: 0.41514489177183955, 1
holes efforts: 0.41514489177183955, 1
efforts made: 0.41514489177183955, 1
made automate: 0.41514489177183955, 1
automate variable: 0.41514489177183955, 1
variable lighting: 0.41514489177183955, 1
surface requirements: 0.41514489177183955, 1
requirements short: 0.41514489177183955, 1
short setup: 0.41514489177183955, 1
setup task: 0.41514489177183955, 1
execution automation: 0.41514489177183955, 1
automation introduce: 0.41514489177183955, 1
introduce vision: 0.41514489177183955, 1
vision proprioceptive: 0.41514489177183955, 1
model task: 0.41514489177183955, 1
task robust: 0.41514489177183955, 1
robust challenging: 0.41514489177183955, 1
surface model: 0.41514489177183955, 1
consists spatial: 0.41514489177183955, 1
attention point: 0.41514489177183955, 1
point network: 0.41514489177183955, 1
network deep: 0.41514489177183955, 1
learning policy: 0.41514489177183955, 1
train jointly: 0.41514489177183955, 1
jointly end: 0.41514489177183955, 1
end control: 0.41514489177183955, 1
train offline: 0.41514489177183955, 1
offline sample: 0.41514489177183955, 1
sample efficient: 0.41514489177183955, 1
efficient framework: 0.41514489177183955, 1
framework designed: 0.41514489177183955, 1
designed reduce: 0.41514489177183955, 1
reduce train: 0.41514489177183955, 1
train minimize: 0.41514489177183955, 1
minimize reality: 0.41514489177183955, 1
reality gap: 0.41514489177183955, 1
gap transferring: 0.41514489177183955, 1
transferring model: 0.41514489177183955, 1
physical evaluations: 0.41514489177183955, 1
evaluations industrial: 0.41514489177183955, 1
industrial robot: 0.41514489177183955, 1
performing task: 0.41514489177183955, 1
task 12: 0.41514489177183955, 1
12 unknown: 0.41514489177183955, 1
unknown starting: 0.41514489177183955, 1
starting 16: 0.41514489177183955, 1
16 different: 0.41514489177183955, 1
different initial: 0.41514489177183955, 1
initial under: 0.41514489177183955, 1
under three: 0.41514489177183955, 1
different lighting: 0.41514489177183955, 1
lighting conditions: 0.41514489177183955, 1
conditions misleading: 0.41514489177183955, 1
misleading demonstrate: 0.41514489177183955, 1
demonstrate sap: 0.41514489177183955, 1
sap generate: 0.41514489177183955, 1
generate relevant: 0.41514489177183955, 1
relevant attention: 0.41514489177183955, 1
attention points: 0.41514489177183955, 1
points image: 0.41514489177183955, 1
image challenging: 0.41514489177183955, 1
lighting propose: 0.41514489177183955, 1
model enables: 0.41514489177183955, 1
enables task: 0.41514489177183955, 1
execution higher: 0.41514489177183955, 1
higher success: 0.41514489177183955, 1
success rate: 0.41514489177183955, 1
rate shorter: 0.41514489177183955, 1
shorter task: 0.41514489177183955, 1
task completion: 0.41514489177183955, 1
completion various: 0.41514489177183955, 1
various due: 0.41514489177183955, 1
due propose: 0.41514489177183955, 1
propose high: 0.41514489177183955, 1
high effectiveness: 0.41514489177183955, 1
effectiveness severe: 0.41514489177183955, 1
severe initial: 0.41514489177183955, 1
initial hole: 0.41514489177183955, 1
hole offline: 0.41514489177183955, 1
offline train: 0.41514489177183955, 1
train high: 0.41514489177183955, 1
high sample: 0.41514489177183955, 1
efficiency short: 0.41514489177183955, 1
short train: 0.41514489177183955, 1
train approach: 0.41514489177183955, 1
approach easily: 0.41514489177183955, 1
easily applied: 0.41514489177183955, 1
present computational: 0.41514489177183955, 1
model gaze: 0.41514489177183955, 1
predict egocentric: 0.41514489177183955, 1
video exploring: 0.41514489177183955, 1
exploring patterns: 0.41514489177183955, 1
patterns temporal: 0.41514489177183955, 1
temporal shift: 0.41514489177183955, 1
shift gaze: 0.41514489177183955, 1
gaze fixations: 0.41514489177183955, 1
fixations dependent: 0.41514489177183955, 1
dependent egocentric: 0.41514489177183955, 1
egocentric manipulation: 0.41514489177183955, 1
manipulation assumption: 0.41514489177183955, 1
assumption high: 0.41514489177183955, 1
level context: 0.41514489177183955, 1
context task: 0.41514489177183955, 1
task completed: 0.41514489177183955, 1
completed certain: 0.41514489177183955, 1
certain strong: 0.41514489177183955, 1
strong influence: 0.41514489177183955, 1
influence attention: 0.41514489177183955, 1
transition modeled: 0.41514489177183955, 1
modeled gaze: 0.41514489177183955, 1
predict natural: 0.41514489177183955, 1
natural dynamic: 0.41514489177183955, 1
dynamic hybrid: 0.41514489177183955, 1
hybrid model: 0.41514489177183955, 1
network integrates: 0.41514489177183955, 1
integrates task: 0.41514489177183955, 1
transition bottom: 0.41514489177183955, 1
saliency task: 0.41514489177183955, 1
transition learned: 0.41514489177183955, 1
learned recurrent: 0.41514489177183955, 1
network exploit: 0.41514489177183955, 1
exploit temporal: 0.41514489177183955, 1
temporal context: 0.41514489177183955, 1
context gaze: 0.41514489177183955, 1
gaze looking: 0.41514489177183955, 1
looking cup: 0.41514489177183955, 1
cup moving: 0.41514489177183955, 1
moving gaze: 0.41514489177183955, 1
gaze away: 0.41514489177183955, 1
away grasp: 0.41514489177183955, 1
grasp experiments: 0.41514489177183955, 1
experiments public: 0.41514489177183955, 1
public egocentric: 0.41514489177183955, 1
egocentric activity: 0.41514489177183955, 1
activity datasets: 0.41514489177183955, 1
datasets model: 0.41514489177183955, 1
model significantly: 0.41514489177183955, 1
significantly outperforms: 0.41514489177183955, 1
outperforms state: 0.41514489177183955, 1
art gaze: 0.41514489177183955, 1
predict method: 0.41514489177183955, 1
meaningful transition: 0.41514489177183955, 1
transition human: 0.41514489177183955, 1
approach combines: 0.41514489177183955, 1
combines asynchronous: 0.41514489177183955, 1
asynchronous actor: 0.41514489177183955, 1
critic model: 0.41514489177183955, 1
model recurrent: 0.41514489177183955, 1
recurrent model: 0.41514489177183955, 1
visual instead: 0.41514489177183955, 1
instead full: 0.41514489177183955, 1
full visual: 0.41514489177183955, 1
information resulting: 0.41514489177183955, 1
resulting model: 0.41514489177183955, 1
model accumulates: 0.41514489177183955, 1
accumulates foveal: 0.41514489177183955, 1
foveal information: 0.41514489177183955, 1
information controlled: 0.41514489177183955, 1
controlled glimpses: 0.41514489177183955, 1
glimpses thus: 0.41514489177183955, 1
thus able: 0.41514489177183955, 1
able reduce: 0.41514489177183955, 1
reduce complexity: 0.41514489177183955, 1
complexity designed: 0.41514489177183955, 1
designed artificial: 0.41514489177183955, 1
agent able: 0.41514489177183955, 1
solve challenging: 0.41514489177183955, 1
challenging desired: 0.41514489177183955, 1
effect cannot: 0.41514489177183955, 1
cannot created: 0.41514489177183955, 1
created direct: 0.41514489177183955, 1
direct instead: 0.41514489177183955, 1
instead require: 0.41514489177183955, 1
require learner: 0.41514489177183955, 1
learner discover: 0.41514489177183955, 1
discover exert: 0.41514489177183955, 1
exert suitable: 0.41514489177183955, 1
suitable effect: 0.41514489177183955, 1
effect target: 0.41514489177183955, 1
object involving: 0.41514489177183955, 1
involving learn: 0.41514489177183955, 1
learn given: 0.41514489177183955, 1
given mediated: 0.41514489177183955, 1
mediated interaction: 0.41514489177183955, 1
interaction agent: 0.41514489177183955, 1
agent searching: 0.41514489177183955, 1
searching salient: 0.41514489177183955, 1
salient points: 0.41514489177183955, 1
points within: 0.41514489177183955, 1
within environment: 0.41514489177183955, 1
environment taking: 0.41514489177183955, 1
taking limited: 0.41514489177183955, 1
limited number: 0.41514489177183955, 1
number fovea: 0.41514489177183955, 1
fovea uses: 0.41514489177183955, 1
uses accumulated: 0.41514489177183955, 1
accumulated information: 0.41514489177183955, 1
information decide: 0.41514489177183955, 1
decide action: 0.41514489177183955, 1
despite considerable: 0.41514489177183955, 1
considerable amount: 0.41514489177183955, 1
amount previous: 0.41514489177183955, 1
previous bottom: 0.41514489177183955, 1
saliency modeling: 0.41514489177183955, 1
modeling predict: 0.41514489177183955, 1
human fixations: 0.41514489177183955, 1
fixations static: 0.41514489177183955, 1
static dynamic: 0.41514489177183955, 1
dynamic few: 0.41514489177183955, 1
few study: 0.41514489177183955, 1
study thus: 0.41514489177183955, 1
thus far: 0.41514489177183955, 1
far attempted: 0.41514489177183955, 1
attempted model: 0.41514489177183955, 1
model top: 0.41514489177183955, 1
down task: 0.41514489177183955, 1
driven influences: 0.41514489177183955, 1
influences visual: 0.41514489177183955, 1
visual taking: 0.41514489177183955, 1
taking advantage: 0.41514489177183955, 1
advantage sequential: 0.41514489177183955, 1
sequential nature: 0.41514489177183955, 1
nature real: 0.41514489177183955, 1
world unified: 0.41514489177183955, 1
unified bayesian: 0.41514489177183955, 1
approach modeling: 0.41514489177183955, 1
modeling task: 0.41514489177183955, 1
visual several: 0.41514489177183955, 1
several sources: 0.41514489177183955, 1
sources including: 0.41514489177183955, 1
including global: 0.41514489177183955, 1
global context: 0.41514489177183955, 1
context previous: 0.41514489177183955, 1
previous attended: 0.41514489177183955, 1
attended previous: 0.41514489177183955, 1
previous motor: 0.41514489177183955, 1
motor integrated: 0.41514489177183955, 1
integrated predict: 0.41514489177183955, 1
predict next: 0.41514489177183955, 1
next attended: 0.41514489177183955, 1
attended recording: 0.41514489177183955, 1
recording eye: 0.41514489177183955, 1
eye movements: 0.41514489177183955, 1
movements while: 0.41514489177183955, 1
while subjects: 0.41514489177183955, 1
subjects engage: 0.41514489177183955, 1
engage 5: 0.41514489177183955, 1
5 contemporary: 0.41514489177183955, 1
contemporary 2d: 0.41514489177183955, 1
3d video: 0.41514489177183955, 1
video modest: 0.41514489177183955, 1
modest counterparts: 0.41514489177183955, 1
counterparts everyday: 0.41514489177183955, 1
everyday approach: 0.41514489177183955, 1
human attention: 0.41514489177183955, 1
attention gaze: 0.41514489177183955, 1
gaze better: 0.41514489177183955, 1
better state: 0.41514489177183955, 1
state large: 0.41514489177183955, 1
large margin: 0.41514489177183955, 1
margin increase: 0.41514489177183955, 1
increase predict: 0.41514489177183955, 1
predict advantage: 0.41514489177183955, 1
advantage approach: 0.41514489177183955, 1
approach automatic: 0.41514489177183955, 1
automatic applicable: 0.41514489177183955, 1
applicable arbitrary: 0.41514489177183955, 1
affordance important: 0.41514489177183955, 1
important autonomous: 0.41514489177183955, 1
robot necessary: 0.41514489177183955, 1
necessary action: 0.41514489177183955, 1
performed complete: 0.41514489177183955, 1
complete given: 0.41514489177183955, 1
given task: 0.41514489177183955, 1
task itpsilas: 0.41514489177183955, 1
itpsilas difficult: 0.41514489177183955, 1
difficult identify: 0.41514489177183955, 1
identify apply: 0.41514489177183955, 1
apply affordance: 0.41514489177183955, 1
affordance practice: 0.41514489177183955, 1
practice order: 0.41514489177183955, 1
order robot: 0.41514489177183955, 1
robot benefits: 0.41514489177183955, 1
benefits paper: 0.41514489177183955, 1
paper proposes: 0.41514489177183955, 1
proposes ontology: 0.41514489177183955, 1
concept ubiquitous: 0.41514489177183955, 1
ubiquitous goal: 0.41514489177183955, 1
goal affordance: 0.41514489177183955, 1
based ontology: 0.41514489177183955, 1
ontology effort: 0.41514489177183955, 1
effort develop: 0.41514489177183955, 1
develop begin: 0.41514489177183955, 1
begin populate: 0.41514489177183955, 1
populate neutral: 0.41514489177183955, 1
neutral knowledge: 0.41514489177183955, 1
knowledge representation: 0.41514489177183955, 1
representation capturing: 0.41514489177183955, 1
capturing relevant: 0.41514489177183955, 1
relevant information: 0.41514489177183955, 1
information robot: 0.41514489177183955, 1
capabilities environment: 0.41514489177183955, 1
environment afford: 0.41514489177183955, 1
afford assist: 0.41514489177183955, 1
assist human: 0.41514489177183955, 1
human life: 0.41514489177183955, 1
life ubiquitous: 0.41514489177183955, 1
robot method: 0.41514489177183955, 1
method dasiateachpsila: 0.41514489177183955, 1
dasiateachpsila robot: 0.41514489177183955, 1
context affordance: 0.41514489177183955, 1
affordance ontology: 0.41514489177183955, 1
approach semantic: 0.41514489177183955, 1
robot ubiquitous: 0.41514489177183955, 1
invariances conservation: 0.41514489177183955, 1
conservation equations: 0.41514489177183955, 1
equations always: 0.41514489177183955, 1
always invaluable: 0.41514489177183955, 1
invaluable guide: 0.41514489177183955, 1
guide science: 0.41514489177183955, 1
science model: 0.41514489177183955, 1
model natural: 0.41514489177183955, 1
natural phenomena: 0.41514489177183955, 1
phenomena simple: 0.41514489177183955, 1
yet effective: 0.41514489177183955, 1
effective computer: 0.41514489177183955, 1
computer translation: 0.41514489177183955, 1
translation equivariance: 0.41514489177183955, 1
equivariance typically: 0.41514489177183955, 1
typically built: 0.41514489177183955, 1
built property: 0.41514489177183955, 1
property neural: 0.41514489177183955, 1
architectures solve: 0.41514489177183955, 1
solve visual: 0.41514489177183955, 1
visual network: 0.41514489177183955, 1
network computational: 0.41514489177183955, 1
computational layers: 0.41514489177183955, 1
layers implementing: 0.41514489177183955, 1
implementing property: 0.41514489177183955, 1
property known: 0.41514489177183955, 1
known convolutional: 0.41514489177183955, 1
network kind: 0.41514489177183955, 1
kind mathematical: 0.41514489177183955, 1
mathematical many: 0.41514489177183955, 1
many others: 0.41514489177183955, 1
others recently: 0.41514489177183955, 1
recently typically: 0.41514489177183955, 1
typically generated: 0.41514489177183955, 1
generated underlying: 0.41514489177183955, 1
underlying group: 0.41514489177183955, 1
transformations case: 0.41514489177183955, 1
case particularly: 0.41514489177183955, 1
particularly suitable: 0.41514489177183955, 1
suitable process: 0.41514489177183955, 1
process highly: 0.41514489177183955, 1
highly structured: 0.41514489177183955, 1
structured data: 0.41514489177183955, 1
data molecules: 0.41514489177183955, 1
molecules chemical: 0.41514489177183955, 1
chemical compounds: 0.41514489177183955, 1
compounds known: 0.41514489177183955, 1
known possess: 0.41514489177183955, 1
possess specific: 0.41514489177183955, 1
specific dealing: 0.41514489177183955, 1
dealing video: 0.41514489177183955, 1
video common: 0.41514489177183955, 1
common built: 0.41514489177183955, 1
built equivariances: 0.41514489177183955, 1
equivariances able: 0.41514489177183955, 1
handle small: 0.41514489177183955, 1
small fraction: 0.41514489177183955, 1
fraction broad: 0.41514489177183955, 1
broad spectrum: 0.41514489177183955, 1
spectrum transformations: 0.41514489177183955, 1
transformations encoded: 0.41514489177183955, 1
encoded visual: 0.41514489177183955, 1
visual stimulus: 0.41514489177183955, 1
stimulus corresponding: 0.41514489177183955, 1
corresponding neural: 0.41514489177183955, 1
architectures resort: 0.41514489177183955, 1
resort huge: 0.41514489177183955, 1
huge amount: 0.41514489177183955, 1
amount supervision: 0.41514489177183955, 1
supervision order: 0.41514489177183955, 1
order achieve: 0.41514489177183955, 1
achieve generalization: 0.41514489177183955, 1
generalization paper: 0.41514489177183955, 1
paper formulate: 0.41514489177183955, 1
formulate theory: 0.41514489177183955, 1
theory development: 0.41514489177183955, 1
feature based: 0.41514489177183955, 1
based idea: 0.41514489177183955, 1
idea movement: 0.41514489177183955, 1
movement itself: 0.41514489177183955, 1
itself provides: 0.41514489177183955, 1
provides trajectories: 0.41514489177183955, 1
trajectories impose: 0.41514489177183955, 1
impose introduce: 0.41514489177183955, 1
introduce principle: 0.41514489177183955, 1
principle material: 0.41514489177183955, 1
material point: 0.41514489177183955, 1
point invariance: 0.41514489177183955, 1
invariance states: 0.41514489177183955, 1
states each: 0.41514489177183955, 1
each visual: 0.41514489177183955, 1
feature invariant: 0.41514489177183955, 1
invariant respect: 0.41514489177183955, 1
respect associated: 0.41514489177183955, 1
optical feature: 0.41514489177183955, 1
feature corresponding: 0.41514489177183955, 1
corresponding velocities: 0.41514489177183955, 1
velocities indissoluble: 0.41514489177183955, 1
indissoluble discuss: 0.41514489177183955, 1
discuss interaction: 0.41514489177183955, 1
interaction feature: 0.41514489177183955, 1
feature velocities: 0.41514489177183955, 1
velocities certain: 0.41514489177183955, 1
certain motion: 0.41514489177183955, 1
invariance traits: 0.41514489177183955, 1
traits regarded: 0.41514489177183955, 1
regarded generalization: 0.41514489177183955, 1
generalization classical: 0.41514489177183955, 1
classical concept: 0.41514489177183955, 1
concept analyses: 0.41514489177183955, 1
analyses feature: 0.41514489177183955, 1
feature velocity: 0.41514489177183955, 1
velocity interaction: 0.41514489177183955, 1
interaction invariance: 0.41514489177183955, 1
invariance properties: 0.41514489177183955, 1
properties leads: 0.41514489177183955, 1
leads visual: 0.41514489177183955, 1
visual field: 0.41514489177183955, 1
theory expresses: 0.41514489177183955, 1
expresses dynamical: 0.41514489177183955, 1
dynamical constraints: 0.41514489177183955, 1
constraints motion: 0.41514489177183955, 1
motion coherence: 0.41514489177183955, 1
coherence might: 0.41514489177183955, 1
might lead: 0.41514489177183955, 1
lead discover: 0.41514489177183955, 1
discover joint: 0.41514489177183955, 1
joint evolution: 0.41514489177183955, 1
evolution visual: 0.41514489177183955, 1
feature along: 0.41514489177183955, 1
along associated: 0.41514489177183955, 1
deep particular: 0.41514489177183955, 1
particular convolutional: 0.41514489177183955, 1
network widely: 0.41514489177183955, 1
widely robot: 0.41514489177183955, 1
object classification: 0.41514489177183955, 1
classification action: 0.41514489177183955, 1
action among: 0.41514489177183955, 1
among high: 0.41514489177183955, 1
high high: 0.41514489177183955, 1
high mostly: 0.41514489177183955, 1
mostly classification: 0.41514489177183955, 1
classification rarely: 0.41514489177183955, 1
rarely accompanied: 0.41514489177183955, 1
accompanied reasoning: 0.41514489177183955, 1
reasoning processes: 0.41514489177183955, 1
processes consider: 0.41514489177183955, 1
consider relationships: 0.41514489177183955, 1
relationships three: 0.41514489177183955, 1
three cnns: 0.41514489177183955, 1
cnns classify: 0.41514489177183955, 1
classify effect: 0.41514489177183955, 1
effect train: 0.41514489177183955, 1
train certh: 0.41514489177183955, 1
certh sor3d: 0.41514489177183955, 1
sor3d dataset: 0.41514489177183955, 1
dataset more: 0.41514489177183955, 1
more rgb: 0.41514489177183955, 1
d dataset: 0.41514489177183955, 1
dataset involves: 0.41514489177183955, 1
involves 14: 0.41514489177183955, 1
14 13: 0.41514489177183955, 1
13 article: 0.41514489177183955, 1
article augmented: 0.41514489177183955, 1
augmented seven: 0.41514489177183955, 1
seven probabilistic: 0.41514489177183955, 1
probabilistic vector: 0.41514489177183955, 1
vector output: 0.41514489177183955, 1
output each: 0.41514489177183955, 1
each train: 0.41514489177183955, 1
train cnn: 0.41514489177183955, 1
cnn combined: 0.41514489177183955, 1
combined bayesian: 0.41514489177183955, 1
network capture: 0.41514489177183955, 1
capture relationships: 0.41514489177183955, 1
relationships shown: 0.41514489177183955, 1
shown probabilistically: 0.41514489177183955, 1
probabilistically combining: 0.41514489177183955, 1
combining information: 0.41514489177183955, 1
information three: 0.41514489177183955, 1
three possible: 0.41514489177183955, 1
possible improve: 0.41514489177183955, 1
improve classification: 0.41514489177183955, 1
classification performance: 0.41514489177183955, 1
performance each: 0.41514489177183955, 1
each cnn: 0.41514489177183955, 1
cnn level: 0.41514489177183955, 1
level same: 0.41514489177183955, 1
same performance: 0.41514489177183955, 1
performance less: 0.41514489177183955, 1
less train: 0.41514489177183955, 1
train recognition: 0.41514489177183955, 1
recognition performance: 0.41514489177183955, 1
performance improved: 0.41514489177183955, 1
improved shown: 0.41514489177183955, 1
shown missing: 0.41514489177183955, 1
missing model: 0.41514489177183955, 1
still produce: 0.41514489177183955, 1
produce reasonable: 0.41514489177183955, 1
reasonable classification: 0.41514489177183955, 1
classification system: 0.41514489177183955, 1
system reasoning: 0.41514489177183955, 1
reasoning purposes: 0.41514489177183955, 1
purposes action: 0.41514489177183955, 1
action planning: 0.41514489177183955, 1
planning information: 0.41514489177183955, 1
object effect: 0.41514489177183955, 1
effect information: 0.41514489177183955, 1
aspect visual: 0.41514489177183955, 1
perception explored: 0.41514489177183955, 1
explored context: 0.41514489177183955, 1
context concept: 0.41514489177183955, 1
various ways: 0.41514489177183955, 1
ways extension: 0.41514489177183955, 1
feature focus: 0.41514489177183955, 1
focus importance: 0.41514489177183955, 1
robot originally: 0.41514489177183955, 1
generalized towards: 0.41514489177183955, 1
towards arbitrary: 0.41514489177183955, 1
cues associated: 0.41514489177183955, 1
associated anticipated: 0.41514489177183955, 1
anticipated visual: 0.41514489177183955, 1
information within: 0.41514489177183955, 1
within framework: 0.41514489177183955, 1
framework markov: 0.41514489177183955, 1
decision processes: 0.41514489177183955, 1
processes emphasize: 0.41514489177183955, 1
emphasize framework: 0.41514489177183955, 1
cueing recognition: 0.41514489177183955, 1
recognition affordance: 0.41514489177183955, 1
visual entities: 0.41514489177183955, 1
entities play: 0.41514489177183955, 1
control affordance: 0.41514489177183955, 1
more efficiently: 0.41514489177183955, 1
efficiently provide: 0.41514489177183955, 1
basis relevant: 0.41514489177183955, 1
relevant responses: 0.41514489177183955, 1
implementation learning: 0.41514489177183955, 1
cues applying: 0.41514489177183955, 1
interest extracted: 0.41514489177183955, 1
extracted simulate: 0.41514489177183955, 1
selected relevance: 0.41514489177183955, 1
relevance predict: 0.41514489177183955, 1
interaction fundamental: 0.41514489177183955, 1
challenge computer: 0.41514489177183955, 1
vision crucial: 0.41514489177183955, 1
crucial ability: 0.41514489177183955, 1
ability infer: 0.41514489177183955, 1
infer visual: 0.41514489177183955, 1
visual namely: 0.41514489177183955, 1
types interaction: 0.41514489177183955, 1
interaction supported: 0.41514489177183955, 1
supported object: 0.41514489177183955, 1
object interest: 0.41514489177183955, 1
interest object: 0.41514489177183955, 1
parts inference: 0.41514489177183955, 1
inference approached: 0.41514489177183955, 1
approached object: 0.41514489177183955, 1
recognized localized: 0.41514489177183955, 1
localized image: 0.41514489177183955, 1
image affordance: 0.41514489177183955, 1
label obtained: 0.41514489177183955, 1
obtained more: 0.41514489177183955, 1
more image: 0.41514489177183955, 1
image pixel: 0.41514489177183955, 1
pixel tackle: 0.41514489177183955, 1
tackle existing: 0.41514489177183955, 1
method treat: 0.41514489177183955, 1
treat adopt: 0.41514489177183955, 1
adopt static: 0.41514489177183955, 1
based ignoring: 0.41514489177183955, 1
ignoring temporal: 0.41514489177183955, 1
aspect human: 0.41514489177183955, 1
object require: 0.41514489177183955, 1
require additional: 0.41514489177183955, 1
additional strong: 0.41514489177183955, 1
supervision concerning: 0.41514489177183955, 1
concerning object: 0.41514489177183955, 1
class focus: 0.41514489177183955, 1
focus while: 0.41514489177183955, 1
while addressing: 0.41514489177183955, 1
addressing three: 0.41514489177183955, 1
three aforementioned: 0.41514489177183955, 1
aforementioned deep: 0.41514489177183955, 1
based dual: 0.41514489177183955, 1
dual encoder: 0.41514489177183955, 1
decoder model: 0.41514489177183955, 1
model joint: 0.41514489177183955, 1
joint affordance: 0.41514489177183955, 1
reasoning learn: 0.41514489177183955, 1
learn recently: 0.41514489177183955, 1
recently introduced: 0.41514489177183955, 1
introduced sor3d: 0.41514489177183955, 1
aff corpus: 0.41514489177183955, 1
corpus rgb: 0.41514489177183955, 1
d human: 0.41514489177183955, 1
without relying: 0.41514489177183955, 1
relying object: 0.41514489177183955, 1
object localization: 0.41514489177183955, 1
localization basic: 0.41514489177183955, 1
basic components: 0.41514489177183955, 1
components model: 0.41514489177183955, 1
model parallel: 0.41514489177183955, 1
parallel encoders: 0.41514489177183955, 1
encoders capture: 0.41514489177183955, 1
capture spatio: 0.41514489177183955, 1
interaction reasoning: 0.41514489177183955, 1
reasoning decoder: 0.41514489177183955, 1
decoder predicts: 0.41514489177183955, 1
predicts affordance: 0.41514489177183955, 1
affordance assisted: 0.41514489177183955, 1
assisted affordance: 0.41514489177183955, 1
affordance classifier: 0.41514489177183955, 1
classifier attention: 0.41514489177183955, 1
attention segmentation: 0.41514489177183955, 1
segmentation decoder: 0.41514489177183955, 1
decoder exploits: 0.41514489177183955, 1
exploits predicted: 0.41514489177183955, 1
predicted heatmap: 0.41514489177183955, 1
heatmap yield: 0.41514489177183955, 1
yield pixel: 0.41514489177183955, 1
pixel level: 0.41514489177183955, 1
affordance modules: 0.41514489177183955, 1
modules jointly: 0.41514489177183955, 1
jointly while: 0.41514489177183955, 1
while system: 0.41514489177183955, 1
system operate: 0.41514489177183955, 1
operate static: 0.41514489177183955, 1
image approach: 0.41514489177183955, 1
approach evaluated: 0.41514489177183955, 1
evaluated four: 0.41514489177183955, 1
four surpassing: 0.41514489177183955, 1
art affordance: 0.41514489177183955, 1
affordance fundamental: 0.41514489177183955, 1
fundamental concept: 0.41514489177183955, 1
concept robot: 0.41514489177183955, 1
since relate: 0.41514489177183955, 1
relate available: 0.41514489177183955, 1
available action: 0.41514489177183955, 1
agent depending: 0.41514489177183955, 1
depending sensory: 0.41514489177183955, 1
motor capabilities: 0.41514489177183955, 1
capabilities present: 0.41514489177183955, 1
novel bayesian: 0.41514489177183955, 1
network detect: 0.41514489177183955, 1
detect affordance: 0.41514489177183955, 1
affordance same: 0.41514489177183955, 1
same quantify: 0.41514489177183955, 1
quantify distribution: 0.41514489177183955, 1
distribution aleatoric: 0.41514489177183955, 1
aleatoric epistemic: 0.41514489177183955, 1
variance spatial: 0.41514489177183955, 1
spatial adapt: 0.41514489177183955, 1
adapt mask: 0.41514489177183955, 1
mask rcnn: 0.41514489177183955, 1
rcnn architecture: 0.41514489177183955, 1
architecture learn: 0.41514489177183955, 1
learn probabilistic: 0.41514489177183955, 1
probabilistic representation: 0.41514489177183955, 1
representation monte: 0.41514489177183955, 1
monte carlo: 0.41514489177183955, 1
carlo result: 0.41514489177183955, 1
result outperform: 0.41514489177183955, 1
outperform state: 0.41514489177183955, 1
art deterministic: 0.41514489177183955, 1
deterministic attribute: 0.41514489177183955, 1
attribute improvement: 0.41514489177183955, 1
improvement better: 0.41514489177183955, 1
better probabilistic: 0.41514489177183955, 1
probabilistic feature: 0.41514489177183955, 1
feature space: 0.41514489177183955, 1
space representation: 0.41514489177183955, 1
representation encoder: 0.41514489177183955, 1
encoder bayesian: 0.41514489177183955, 1
bayesian variability: 0.41514489177183955, 1
variability induced: 0.41514489177183955, 1
induced mask: 0.41514489177183955, 1
mask adapts: 0.41514489177183955, 1
adapts better: 0.41514489177183955, 1
better object: 0.41514489177183955, 1
object introduce: 0.41514489177183955, 1
introduce probability: 0.41514489177183955, 1
probability based: 0.41514489177183955, 1
based mask: 0.41514489177183955, 1
mask quality: 0.41514489177183955, 1
quality measure: 0.41514489177183955, 1
measure reveals: 0.41514489177183955, 1
reveals semantic: 0.41514489177183955, 1
semantic spatial: 0.41514489177183955, 1
spatial differences: 0.41514489177183955, 1
differences probabilistic: 0.41514489177183955, 1
probabilistic instance: 0.41514489177183955, 1
instance segmentation: 0.41514489177183955, 1
segmentation modify: 0.41514489177183955, 1
modify existing: 0.41514489177183955, 1
existing probabilistic: 0.41514489177183955, 1
probabilistic detection: 0.41514489177183955, 1
detection quality: 0.41514489177183955, 1
quality metric: 0.41514489177183955, 1
metric comparing: 0.41514489177183955, 1
comparing binary: 0.41514489177183955, 1
binary masks: 0.41514489177183955, 1
masks rather: 0.41514489177183955, 1
rather predicted: 0.41514489177183955, 1
predicted bounding: 0.41514489177183955, 1
bounding achieving: 0.41514489177183955, 1
achieving finer: 0.41514489177183955, 1
finer grained: 0.41514489177183955, 1
grained evaluation: 0.41514489177183955, 1
evaluation probabilistic: 0.41514489177183955, 1
probabilistic find: 0.41514489177183955, 1
find aleatoric: 0.41514489177183955, 1
aleatoric variance: 0.41514489177183955, 1
variance contours: 0.41514489177183955, 1
contours object: 0.41514489177183955, 1
object due: 0.41514489177183955, 1
due camera: 0.41514489177183955, 1
camera while: 0.41514489177183955, 1
while epistemic: 0.41514489177183955, 1
variance appears: 0.41514489177183955, 1
appears visual: 0.41514489177183955, 1
visual challenging: 0.41514489177183955, 1
play crucial: 0.41514489177183955, 1
role robot: 0.41514489177183955, 1
since allow: 0.41514489177183955, 1
allow developing: 0.41514489177183955, 1
developing truly: 0.41514489177183955, 1
truly autonomous: 0.41514489177183955, 1
autonomous freely: 0.41514489177183955, 1
freely explore: 0.41514489177183955, 1
explore interact: 0.41514489177183955, 1
interact existing: 0.41514489177183955, 1
approach analyzing: 0.41514489177183955, 1
analyzing affordance: 0.41514489177183955, 1
affordance scene: 0.41514489177183955, 1
scene consider: 0.41514489177183955, 1
consider few: 0.41514489177183955, 1
few types: 0.41514489177183955, 1
types grasp: 0.41514489177183955, 1
manipulation many: 0.41514489177183955, 1
many cases: 0.41514489177183955, 1
cases whole: 0.41514489177183955, 1
whole object: 0.41514489177183955, 1
object study: 0.41514489177183955, 1
study include: 0.41514489177183955, 1
include total: 0.41514489177183955, 1
total 12: 0.41514489177183955, 1
12 affordance: 0.41514489177183955, 1
manipulation locomotion: 0.41514489177183955, 1
locomotion considering: 0.41514489177183955, 1
considering affordance: 0.41514489177183955, 1
object design: 0.41514489177183955, 1
design system: 0.41514489177183955, 1
system densely: 0.41514489177183955, 1
single 2d: 0.41514489177183955, 1
2d rgb: 0.41514489177183955, 1
rgb method: 0.41514489177183955, 1
method transfers: 0.41514489177183955, 1
transfers object: 0.41514489177183955, 1
class label: 0.41514489177183955, 1
label enables: 0.41514489177183955, 1
enables train: 0.41514489177183955, 1
neural pspnet: 0.41514489177183955, 1
pspnet based: 0.41514489177183955, 1
based network: 0.41514489177183955, 1
network u: 0.41514489177183955, 1
u net: 0.41514489177183955, 1
net style: 0.41514489177183955, 1
style directly: 0.41514489177183955, 1
directly predict: 0.41514489177183955, 1
image selective: 0.41514489177183955, 1
selective binary: 0.41514489177183955, 1
binary cross: 0.41514489177183955, 1
cross entropy: 0.41514489177183955, 1
entropy loss: 0.41514489177183955, 1
loss method: 0.41514489177183955, 1
handle affordance: 0.41514489177183955, 1
parts pixel: 0.41514489177183955, 1
wise manner: 0.41514489177183955, 1
manner case: 0.41514489177183955, 1
case incomplete: 0.41514489177183955, 1
incomplete perform: 0.41514489177183955, 1
perform qualitative: 0.41514489177183955, 1
qualitative quantitative: 0.41514489177183955, 1
quantitative evaluations: 0.41514489177183955, 1
evaluations simulate: 0.41514489177183955, 1
data including: 0.41514489177183955, 1
including robot: 0.41514489177183955, 1
robot find: 0.41514489177183955, 1
find frequent: 0.41514489177183955, 1
frequent affordance: 0.41514489177183955, 1
recognized substantial: 0.41514489177183955, 1
substantial fraction: 0.41514489177183955, 1
fraction correctly: 0.41514489177183955, 1
correctly assigned: 0.41514489177183955, 1
assigned while: 0.41514489177183955, 1
while harder: 0.41514489177183955, 1
harder infrequent: 0.41514489177183955, 1
infrequent affordance: 0.41514489177183955, 1
affordance small: 0.41514489177183955, 1
small demonstrate: 0.41514489177183955, 1
demonstrate method: 0.41514489177183955, 1
method performs: 0.41514489177183955, 1
performs better: 0.41514489177183955, 1
better recent: 0.41514489177183955, 1
recent competitive: 0.41514489177183955, 1
competitive propose: 0.41514489177183955, 1
method operates: 0.41514489177183955, 1
operates 2d: 0.41514489177183955, 1
2d easier: 0.41514489177183955, 1
easier implement: 0.41514489177183955, 1
implement competing: 0.41514489177183955, 1
competing 3d: 0.41514489177183955, 1
3d method: 0.41514489177183955, 1
method therefore: 0.41514489177183955, 1
therefore more: 0.41514489177183955, 1
more easily: 0.41514489177183955, 1
easily provide: 0.41514489177183955, 1
provide useful: 0.41514489177183955, 1
useful affordance: 0.41514489177183955, 1
affordance estimates: 0.41514489177183955, 1
estimates robot: 0.41514489177183955, 1
action demonstrate: 0.41514489177183955, 1
network recently: 0.41514489177183955, 1
recently successful: 0.41514489177183955, 1
successful variety: 0.41514489177183955, 1
variety computer: 0.41514489177183955, 1
vision especially: 0.41514489177183955, 1
especially linked: 0.41514489177183955, 1
linked optical: 0.41514489177183955, 1
estimation among: 0.41514489177183955, 1
among task: 0.41514489177183955, 1
task cnns: 0.41514489177183955, 1
cnns succeeded: 0.41514489177183955, 1
succeeded paper: 0.41514489177183955, 1
paper construct: 0.41514489177183955, 1
construct cnns: 0.41514489177183955, 1
cnns capable: 0.41514489177183955, 1
capable solving: 0.41514489177183955, 1
solving optical: 0.41514489177183955, 1
estimation problem: 0.41514489177183955, 1
problem supervised: 0.41514489177183955, 1
learning compare: 0.41514489177183955, 1
compare generic: 0.41514489177183955, 1
generic architecture: 0.41514489177183955, 1
architecture another: 0.41514489177183955, 1
another including: 0.41514489177183955, 1
including layer: 0.41514489177183955, 1
layer correlates: 0.41514489177183955, 1
correlates feature: 0.41514489177183955, 1
feature vectors: 0.41514489177183955, 1
vectors different: 0.41514489177183955, 1
different image: 0.41514489177183955, 1
image since: 0.41514489177183955, 1
since existing: 0.41514489177183955, 1
existing ground: 0.41514489177183955, 1
truth data: 0.41514489177183955, 1
data sets: 0.41514489177183955, 1
sets sufficiently: 0.41514489177183955, 1
sufficiently large: 0.41514489177183955, 1
large train: 0.41514489177183955, 1
train generate: 0.41514489177183955, 1
generate large: 0.41514489177183955, 1
large synthetic: 0.41514489177183955, 1
synthetic flying: 0.41514489177183955, 1
flying chairs: 0.41514489177183955, 1
chairs network: 0.41514489177183955, 1
train unrealistic: 0.41514489177183955, 1
unrealistic data: 0.41514489177183955, 1
data still: 0.41514489177183955, 1
still generalize: 0.41514489177183955, 1
generalize existing: 0.41514489177183955, 1
existing datasets: 0.41514489177183955, 1
datasets sintel: 0.41514489177183955, 1
sintel achieving: 0.41514489177183955, 1
achieving competitive: 0.41514489177183955, 1
competitive accuracy: 0.41514489177183955, 1
accuracy frame: 0.41514489177183955, 1
frame rates: 0.41514489177183955, 1
rates 5: 0.41514489177183955, 1
5 10: 0.41514489177183955, 1
range problem: 0.41514489177183955, 1
problem applied: 0.41514489177183955, 1
applied physics: 0.41514489177183955, 1
physics engineering: 0.41514489177183955, 1
engineering involve: 0.41514489177183955, 1
involve learning: 0.41514489177183955, 1
physical displacement: 0.41514489177183955, 1
field paper: 0.41514489177183955, 1
paper deep: 0.41514489177183955, 1
learning displacement: 0.41514489177183955, 1
field end: 0.41514489177183955, 1
end focusing: 0.41514489177183955, 1
focusing specific: 0.41514489177183955, 1
specific case: 0.41514489177183955, 1
case particle: 0.41514489177183955, 1
velocimetry key: 0.41514489177183955, 1
key approach: 0.41514489177183955, 1
approach experimental: 0.41514489177183955, 1
experimental fluid: 0.41514489177183955, 1
fluid dynamics: 0.41514489177183955, 1
dynamics crucial: 0.41514489177183955, 1
crucial importance: 0.41514489177183955, 1
importance diverse: 0.41514489177183955, 1
diverse applications: 0.41514489177183955, 1
applications aerospace: 0.41514489177183955, 1
aerospace biomedical: 0.41514489177183955, 1
biomedical current: 0.41514489177183955, 1
art piv: 0.41514489177183955, 1
piv data: 0.41514489177183955, 1
data processing: 0.41514489177183955, 1
processing involves: 0.41514489177183955, 1
involves traditional: 0.41514489177183955, 1
traditional handcrafted: 0.41514489177183955, 1
handcrafted model: 0.41514489177183955, 1
model subject: 0.41514489177183955, 1
subject limitations: 0.41514489177183955, 1
limitations including: 0.41514489177183955, 1
including substantial: 0.41514489177183955, 1
substantial manual: 0.41514489177183955, 1
manual effort: 0.41514489177183955, 1
effort required: 0.41514489177183955, 1
required difficulties: 0.41514489177183955, 1
difficulties generalizing: 0.41514489177183955, 1
generalizing across: 0.41514489177183955, 1
across deep: 0.41514489177183955, 1
approach introduced: 0.41514489177183955, 1
introduced based: 0.41514489177183955, 1
based recent: 0.41514489177183955, 1
recent optical: 0.41514489177183955, 1
learning architecture: 0.41514489177183955, 1
architecture known: 0.41514489177183955, 1
known recurrent: 0.41514489177183955, 1
recurrent pairs: 0.41514489177183955, 1
pairs field: 0.41514489177183955, 1
field largely: 0.41514489177183955, 1
largely automated: 0.41514489177183955, 1
automated provides: 0.41514489177183955, 1
provides high: 0.41514489177183955, 1
high spatial: 0.41514489177183955, 1
spatial extensive: 0.41514489177183955, 1
extensive including: 0.41514489177183955, 1
including benchmark: 0.41514489177183955, 1
benchmark examples: 0.41514489177183955, 1
examples true: 0.41514489177183955, 1
true gold: 0.41514489177183955, 1
gold standards: 0.41514489177183955, 1
standards available: 0.41514489177183955, 1
available demonstrate: 0.41514489177183955, 1
demonstrate propose: 0.41514489177183955, 1
approach achieves: 0.41514489177183955, 1
achieves state: 0.41514489177183955, 1
art accuracy: 0.41514489177183955, 1
accuracy generalization: 0.41514489177183955, 1
generalization relative: 0.41514489177183955, 1
relative classical: 0.41514489177183955, 1
classical approach: 0.41514489177183955, 1
approach previously: 0.41514489177183955, 1
previously propose: 0.41514489177183955, 1
propose optical: 0.41514489177183955, 1
present supervised: 0.41514489177183955, 1
based method: 0.41514489177183955, 1
method estimate: 0.41514489177183955, 1
estimate per: 0.41514489177183955, 1
pixel confidence: 0.41514489177183955, 1
confidence optical: 0.41514489177183955, 1
flow regions: 0.41514489177183955, 1
regions low: 0.41514489177183955, 1
low texture: 0.41514489177183955, 1
texture pixels: 0.41514489177183955, 1
pixels close: 0.41514489177183955, 1
close occlusion: 0.41514489177183955, 1
occlusion boundaries: 0.41514489177183955, 1
boundaries known: 0.41514489177183955, 1
known difficult: 0.41514489177183955, 1
difficult optical: 0.41514489177183955, 1
flow spatiotemporal: 0.41514489177183955, 1
spatiotemporal feature: 0.41514489177183955, 1
feature estimate: 0.41514489177183955, 1
estimate flow: 0.41514489177183955, 1
algorithm likely: 0.41514489177183955, 1
likely fail: 0.41514489177183955, 1
fail given: 0.41514489177183955, 1
given method: 0.41514489177183955, 1
method restricted: 0.41514489177183955, 1
restricted specific: 0.41514489177183955, 1
specific class: 0.41514489177183955, 1
class flow: 0.41514489177183955, 1
algorithm does: 0.41514489177183955, 1
does scene: 0.41514489177183955, 1
scene specific: 0.41514489177183955, 1
specific automatically: 0.41514489177183955, 1
automatically learning: 0.41514489177183955, 1
learning combine: 0.41514489177183955, 1
combine output: 0.41514489177183955, 1
output several: 0.41514489177183955, 1
several computed: 0.41514489177183955, 1
computed flow: 0.41514489177183955, 1
flow field: 0.41514489177183955, 1
field different: 0.41514489177183955, 1
different algorithm: 0.41514489177183955, 1
algorithm select: 0.41514489177183955, 1
select best: 0.41514489177183955, 1
best performing: 0.41514489177183955, 1
performing algorithm: 0.41514489177183955, 1
algorithm per: 0.41514489177183955, 1
per optical: 0.41514489177183955, 1
flow confidence: 0.41514489177183955, 1
measure allows: 0.41514489177183955, 1
allows achieve: 0.41514489177183955, 1
achieve better: 0.41514489177183955, 1
better overall: 0.41514489177183955, 1
result discarding: 0.41514489177183955, 1
discarding troublesome: 0.41514489177183955, 1
troublesome illustrate: 0.41514489177183955, 1
illustrate effectiveness: 0.41514489177183955, 1
effectiveness method: 0.41514489177183955, 1
method four: 0.41514489177183955, 1
four different: 0.41514489177183955, 1
different optical: 0.41514489177183955, 1
algorithm variety: 0.41514489177183955, 1
variety real: 0.41514489177183955, 1
real synthetic: 0.41514489177183955, 1
synthetic algorithm: 0.41514489177183955, 1
algorithm achieve: 0.41514489177183955, 1
achieve top: 0.41514489177183955, 1
top overall: 0.41514489177183955, 1
result large: 0.41514489177183955, 1
large test: 0.41514489177183955, 1
test times: 0.41514489177183955, 1
times surpass: 0.41514489177183955, 1
surpass result: 0.41514489177183955, 1
result best: 0.41514489177183955, 1
best algorithm: 0.41514489177183955, 1
algorithm among: 0.41514489177183955, 1
flownet demonstrate: 0.41514489177183955, 1
demonstrate optical: 0.41514489177183955, 1
estimation cast: 0.41514489177183955, 1
cast learning: 0.41514489177183955, 1
learning state: 0.41514489177183955, 1
art regard: 0.41514489177183955, 1
regard quality: 0.41514489177183955, 1
quality flow: 0.41514489177183955, 1
flow still: 0.41514489177183955, 1
still defined: 0.41514489177183955, 1
defined traditional: 0.41514489177183955, 1
traditional particularly: 0.41514489177183955, 1
particularly small: 0.41514489177183955, 1
displacements real: 0.41514489177183955, 1
world flownet: 0.41514489177183955, 1
flownet cannot: 0.41514489177183955, 1
cannot compete: 0.41514489177183955, 1
compete variational: 0.41514489177183955, 1
variational advance: 0.41514489177183955, 1
advance concept: 0.41514489177183955, 1
concept end: 0.41514489177183955, 1
flow really: 0.41514489177183955, 1
really large: 0.41514489177183955, 1
large improvements: 0.41514489177183955, 1
improvements quality: 0.41514489177183955, 1
quality speed: 0.41514489177183955, 1
speed caused: 0.41514489177183955, 1
caused three: 0.41514489177183955, 1
three major: 0.41514489177183955, 1
major focus: 0.41514489177183955, 1
focus train: 0.41514489177183955, 1
data schedule: 0.41514489177183955, 1
schedule presenting: 0.41514489177183955, 1
presenting data: 0.41514489177183955, 1
during train: 0.41514489177183955, 1
train develop: 0.41514489177183955, 1
develop stacked: 0.41514489177183955, 1
stacked architecture: 0.41514489177183955, 1
architecture includes: 0.41514489177183955, 1
includes warping: 0.41514489177183955, 1
warping second: 0.41514489177183955, 1
second image: 0.41514489177183955, 1
image intermediate: 0.41514489177183955, 1
intermediate optical: 0.41514489177183955, 1
optical elaborate: 0.41514489177183955, 1
elaborate small: 0.41514489177183955, 1
displacements introducing: 0.41514489177183955, 1
introducing subnetwork: 0.41514489177183955, 1
subnetwork specializing: 0.41514489177183955, 1
specializing small: 0.41514489177183955, 1
small flownet: 0.41514489177183955, 1
flownet marginally: 0.41514489177183955, 1
marginally slower: 0.41514489177183955, 1
slower original: 0.41514489177183955, 1
original flownet: 0.41514489177183955, 1
flownet decreases: 0.41514489177183955, 1
decreases estimation: 0.41514489177183955, 1
estimation error: 0.41514489177183955, 1
error more: 0.41514489177183955, 1
more performs: 0.41514489177183955, 1
performs par: 0.41514489177183955, 1
par state: 0.41514489177183955, 1
art while: 0.41514489177183955, 1
while running: 0.41514489177183955, 1
running interactive: 0.41514489177183955, 1
interactive frame: 0.41514489177183955, 1
frame present: 0.41514489177183955, 1
present faster: 0.41514489177183955, 1
faster variants: 0.41514489177183955, 1
variants allow: 0.41514489177183955, 1
allow optical: 0.41514489177183955, 1
flow computation: 0.41514489177183955, 1
computation 140fps: 0.41514489177183955, 1
140fps accuracy: 0.41514489177183955, 1
accuracy matching: 0.41514489177183955, 1
matching original: 0.41514489177183955, 1
jol deep: 0.41514489177183955, 1
deep machine: 0.41514489177183955, 1
single multi: 0.41514489177183955, 1
multi channel: 0.41514489177183955, 1
channel video: 0.41514489177183955, 1
installation among: 0.41514489177183955, 1
among playful: 0.41514489177183955, 1
playful investigation: 0.41514489177183955, 1
investigation old: 0.41514489177183955, 1
old mechanical: 0.41514489177183955, 1
device technological: 0.41514489177183955, 1
technological apparatus: 0.41514489177183955, 1
apparatus role: 0.41514489177183955, 1
role old: 0.41514489177183955, 1
old filled: 0.41514489177183955, 1
filled fully: 0.41514489177183955, 1
fully functional: 0.41514489177183955, 1
functional 4: 0.41514489177183955, 1
4 stage: 0.41514489177183955, 1
stage hand: 0.41514489177183955, 1
hand cranked: 0.41514489177183955, 1
cranked calculator: 0.41514489177183955, 1
calculator conceptualized: 0.41514489177183955, 1
conceptualized built: 0.41514489177183955, 1
built german: 0.41514489177183955, 1
german astronomer: 0.41514489177183955, 1
astronomer inventor: 0.41514489177183955, 1
inventor phillipp: 0.41514489177183955, 1
phillipp hahn: 0.41514489177183955, 1
hahn calculator: 0.41514489177183955, 1
calculator wondrously: 0.41514489177183955, 1
wondrously intricate: 0.41514489177183955, 1
intricate mechanical: 0.41514489177183955, 1
device capable: 0.41514489177183955, 1
capable phillipp: 0.41514489177183955, 1
phillipp amongst: 0.41514489177183955, 1
amongst build: 0.41514489177183955, 1
build functional: 0.41514489177183955, 1
functional machine: 0.41514489177183955, 1
machine capable: 0.41514489177183955, 1
capable four: 0.41514489177183955, 1
four basic: 0.41514489177183955, 1
basic arithmetical: 0.41514489177183955, 1
arithmetical initiating: 0.41514489177183955, 1
initiating precision: 0.41514489177183955, 1
precision industry: 0.41514489177183955, 1
industry video: 0.41514489177183955, 1
installation capture: 0.41514489177183955, 1
capture device: 0.41514489177183955, 1
device scrutinized: 0.41514489177183955, 1
scrutinized equally: 0.41514489177183955, 1
equally wondrous: 0.41514489177183955, 1
wondrous next: 0.41514489177183955, 1
next generation: 0.41514489177183955, 1
generation six: 0.41514489177183955, 1
six axis: 0.41514489177183955, 1
axis robot: 0.41514489177183955, 1
robot designed: 0.41514489177183955, 1
designed bosch: 0.41514489177183955, 1
bosch gmbh: 0.41514489177183955, 1
gmbh engineers: 0.41514489177183955, 1
engineers specialising: 0.41514489177183955, 1
specialising human: 0.41514489177183955, 1
human apas: 0.41514489177183955, 1
arm deceptively: 0.41514489177183955, 1
deceptively simple: 0.41514489177183955, 1
simple looking: 0.41514489177183955, 1
looking machine: 0.41514489177183955, 1
machine equipped: 0.41514489177183955, 1
equipped wide: 0.41514489177183955, 1
range advanced: 0.41514489177183955, 1
advanced imaging: 0.41514489177183955, 1
imaging sheathed: 0.41514489177183955, 1
sheathed proximity: 0.41514489177183955, 1
proximity sensing: 0.41514489177183955, 1
sensing allows: 0.41514489177183955, 1
arm operate: 0.41514489177183955, 1
operate high: 0.41514489177183955, 1
high speeds: 0.41514489177183955, 1
speeds close: 0.41514489177183955, 1
close proximity: 0.41514489177183955, 1
proximity apas: 0.41514489177183955, 1
robot subjects: 0.41514489177183955, 1
subjects mechanical: 0.41514489177183955, 1
mechanical calculator: 0.41514489177183955, 1
calculator variety: 0.41514489177183955, 1
variety different: 0.41514489177183955, 1
different sensor: 0.41514489177183955, 1
sensor based: 0.41514489177183955, 1
computational range: 0.41514489177183955, 1
range regular: 0.41514489177183955, 1
regular video: 0.41514489177183955, 1
video capture: 0.41514489177183955, 1
capture laser: 0.41514489177183955, 1
laser guided: 0.41514489177183955, 1
guided 3d: 0.41514489177183955, 1
3d measurement: 0.41514489177183955, 1
measurement recording: 0.41514489177183955, 1
recording optical: 0.41514489177183955, 1
data invisible: 0.41514489177183955, 1
invisible human: 0.41514489177183955, 1
human allows: 0.41514489177183955, 1
allows device: 0.41514489177183955, 1
device observe: 0.41514489177183955, 1
observe object: 0.41514489177183955, 1
object before: 0.41514489177183955, 1
before perception: 0.41514489177183955, 1
perception apparatus: 0.41514489177183955, 1
apparatus far: 0.41514489177183955, 1
far surpasses: 0.41514489177183955, 1
surpasses human: 0.41514489177183955, 1
human agent: 0.41514489177183955, 1
agent generally: 0.41514489177183955, 1
generally mean: 0.41514489177183955, 1
mean thomson: 0.41514489177183955, 1
thomson reveals: 0.41514489177183955, 1
reveals viewer: 0.41514489177183955, 1
viewer pairing: 0.41514489177183955, 1
pairing video: 0.41514489177183955, 1
video documentation: 0.41514489177183955, 1
documentation interactive: 0.41514489177183955, 1
interactive environment: 0.41514489177183955, 1
environment entirety: 0.41514489177183955, 1
entirety alongside: 0.41514489177183955, 1
alongside visualizations: 0.41514489177183955, 1
visualizations different: 0.41514489177183955, 1
different forms: 0.41514489177183955, 1
forms visual: 0.41514489177183955, 1
visual non: 0.41514489177183955, 1
non visual: 0.41514489177183955, 1
data captured: 0.41514489177183955, 1
captured during: 0.41514489177183955, 1
during project: 0.41514489177183955, 1
project punctuated: 0.41514489177183955, 1
punctuated textual: 0.41514489177183955, 1
textual excerpts: 0.41514489177183955, 1
excerpts drawn: 0.41514489177183955, 1
drawn european: 0.41514489177183955, 1
european parliament: 0.41514489177183955, 1
parliament report: 0.41514489177183955, 1
report civil: 0.41514489177183955, 1
civil law: 0.41514489177183955, 1
law rules: 0.41514489177183955, 1
rules robot: 0.41514489177183955, 1
robot call: 0.41514489177183955, 1
call consideration: 0.41514489177183955, 1
consideration liabilities: 0.41514489177183955, 1
liabilities intelligent: 0.41514489177183955, 1
algorithm usually: 0.41514489177183955, 1
usually assume: 0.41514489177183955, 1
assume action: 0.41514489177183955, 1
action always: 0.41514489177183955, 1
always available: 0.41514489177183955, 1
available animals: 0.41514489177183955, 1
animals understand: 0.41514489177183955, 1
understand general: 0.41514489177183955, 1
general link: 0.41514489177183955, 1
link feature: 0.41514489177183955, 1
feature environment: 0.41514489177183955, 1
environment action: 0.41514489177183955, 1
action gibson: 0.41514489177183955, 1
gibson coined: 0.41514489177183955, 1
coined term: 0.41514489177183955, 1
term describe: 0.41514489177183955, 1
describe fact: 0.41514489177183955, 1
fact certain: 0.41514489177183955, 1
certain states: 0.41514489177183955, 1
states enable: 0.41514489177183955, 1
enable agent: 0.41514489177183955, 1
agent certain: 0.41514489177183955, 1
certain context: 0.41514489177183955, 1
context embodied: 0.41514489177183955, 1
embodied develop: 0.41514489177183955, 1
develop theory: 0.41514489177183955, 1
affordance agent: 0.41514489177183955, 1
learn plan: 0.41514489177183955, 1
plan markov: 0.41514489177183955, 1
play dual: 0.41514489177183955, 1
dual role: 0.41514489177183955, 1
role allow: 0.41514489177183955, 1
allow faster: 0.41514489177183955, 1
faster reducing: 0.41514489177183955, 1
reducing number: 0.41514489177183955, 1
number action: 0.41514489177183955, 1
action available: 0.41514489177183955, 1
available given: 0.41514489177183955, 1
given facilitate: 0.41514489177183955, 1
facilitate more: 0.41514489177183955, 1
efficient precise: 0.41514489177183955, 1
precise learning: 0.41514489177183955, 1
learning transition: 0.41514489177183955, 1
model especially: 0.41514489177183955, 1
especially model: 0.41514489177183955, 1
model require: 0.41514489177183955, 1
require function: 0.41514489177183955, 1
function establish: 0.41514489177183955, 1
establish properties: 0.41514489177183955, 1
properties theoretical: 0.41514489177183955, 1
theoretical result: 0.41514489177183955, 1
result illustrative: 0.41514489177183955, 1
illustrative approach: 0.41514489177183955, 1
affordance estimate: 0.41514489177183955, 1
estimate transition: 0.41514489177183955, 1
model simpler: 0.41514489177183955, 1
simpler generalize: 0.41514489177183955, 1
relation effect: 0.41514489177183955, 1
given environmental: 0.41514489177183955, 1
environmental key: 0.41514489177183955, 1
key benefit: 0.41514489177183955, 1
benefit concept: 0.41514489177183955, 1
affordance provides: 0.41514489177183955, 1
information consequence: 0.41514489177183955, 1
consequence action: 0.41514489177183955, 1
action stored: 0.41514489177183955, 1
stored reused: 0.41514489177183955, 1
reused range: 0.41514489177183955, 1
range task: 0.41514489177183955, 1
needs learn: 0.41514489177183955, 1
learn address: 0.41514489177183955, 1
address challenge: 0.41514489177183955, 1
challenge line: 0.41514489177183955, 1
affordance simultaneously: 0.41514489177183955, 1
simultaneously while: 0.41514489177183955, 1
while performing: 0.41514489177183955, 1
performing goal: 0.41514489177183955, 1
directed requires: 0.41514489177183955, 1
requires efficient: 0.41514489177183955, 1
efficient online: 0.41514489177183955, 1
online performance: 0.41514489177183955, 1
performance ensure: 0.41514489177183955, 1
ensure robot: 0.41514489177183955, 1
able achieve: 0.41514489177183955, 1
achieve goal: 0.41514489177183955, 1
goal providing: 0.41514489177183955, 1
providing conceptual: 0.41514489177183955, 1
conceptual knowledge: 0.41514489177183955, 1
knowledge action: 0.41514489177183955, 1
possibilities desired: 0.41514489177183955, 1
desired humanoid: 0.41514489177183955, 1
robot nao: 0.41514489177183955, 1
nao learn: 0.41514489177183955, 1
task demonstrate: 0.41514489177183955, 1
approach integrating: 0.41514489177183955, 1
integrating affordance: 0.41514489177183955, 1
affordance extended: 0.41514489177183955, 1
extended classifier: 0.41514489177183955, 1
classifier system: 0.41514489177183955, 1
system learning: 0.41514489177183955, 1
learning general: 0.41514489177183955, 1
general rules: 0.41514489177183955, 1
rules reinforcement: 0.41514489177183955, 1
learning experimental: 0.41514489177183955, 1
result significant: 0.41514489177183955, 1
significant speedups: 0.41514489177183955, 1
speedups learning: 0.41514489177183955, 1
robot solves: 0.41514489177183955, 1
solves given: 0.41514489177183955, 1
affordance define: 0.41514489177183955, 1
define action: 0.41514489177183955, 1
possibilities object: 0.41514489177183955, 1
object environment: 0.41514489177183955, 1
environment robot: 0.41514489177183955, 1
robot play: 0.41514489177183955, 1
play role: 0.41514489177183955, 1
role basic: 0.41514489177183955, 1
basic cognitive: 0.41514489177183955, 1
cognitive previous: 0.41514489177183955, 1
previous works: 0.41514489177183955, 1
works focused: 0.41514489177183955, 1
focused affordance: 0.41514489177183955, 1
though many: 0.41514489177183955, 1
many scenarios: 0.41514489177183955, 1
scenarios defined: 0.41514489177183955, 1
defined configurations: 0.41514489177183955, 1
configurations multiple: 0.41514489177183955, 1
object interact: 0.41514489177183955, 1
each employ: 0.41514489177183955, 1
employ recent: 0.41514489177183955, 1
advances statistical: 0.41514489177183955, 1
statistical relational: 0.41514489177183955, 1
relational learning: 0.41514489177183955, 1
learning learn: 0.41514489177183955, 1
model model: 0.41514489177183955, 1
object deal: 0.41514489177183955, 1
deal effectively: 0.41514489177183955, 1
effectively object: 0.41514489177183955, 1
interaction model: 0.41514489177183955, 1
model learned: 0.41514489177183955, 1
learned robot: 0.41514489177183955, 1
object world: 0.41514489177183955, 1
world employed: 0.41514489177183955, 1
employed situations: 0.41514489177183955, 1
situations arbitrary: 0.41514489177183955, 1
arbitrary numbers: 0.41514489177183955, 1
numbers illustrate: 0.41514489177183955, 1
illustrate ideas: 0.41514489177183955, 1
ideas experimental: 0.41514489177183955, 1
result action: 0.41514489177183955, 1
robot manipulates: 0.41514489177183955, 1
manipulates object: 0.41514489177183955, 1
article present: 0.41514489177183955, 1
affordance spatiotemporally: 0.41514489177183955, 1
spatiotemporally correlated: 0.41514489177183955, 1
correlated haptic: 0.41514489177183955, 1
depth method: 0.41514489177183955, 1
robot incrementally: 0.41514489177183955, 1
incrementally learn: 0.41514489177183955, 1
present environment: 0.41514489177183955, 1
environment actually: 0.41514489177183955, 1
actually critical: 0.41514489177183955, 1
critical requirement: 0.41514489177183955, 1
requirement wheeled: 0.41514489177183955, 1
performing natural: 0.41514489177183955, 1
natural inability: 0.41514489177183955, 1
inability discern: 0.41514489177183955, 1
discern vegetation: 0.41514489177183955, 1
vegetation non: 0.41514489177183955, 1
non traversable: 0.41514489177183955, 1
traversable obstacles: 0.41514489177183955, 1
obstacles frequently: 0.41514489177183955, 1
frequently hampers: 0.41514489177183955, 1
hampers terrain: 0.41514489177183955, 1
terrain wheeled: 0.41514489177183955, 1
prototype developed: 0.41514489177183955, 1
developed order: 0.41514489177183955, 1
order experimentally: 0.41514489177183955, 1
experimentally validate: 0.41514489177183955, 1
propose robot: 0.41514489177183955, 1
prototype obtains: 0.41514489177183955, 1
obtains haptic: 0.41514489177183955, 1
sensory feedback: 0.41514489177183955, 1
feedback pan: 0.41514489177183955, 1
antenna structured: 0.41514489177183955, 1
structured light: 0.41514489177183955, 1
light presented: 0.41514489177183955, 1
presented robot: 0.41514489177183955, 1
learn mapping: 0.41514489177183955, 1
mapping given: 0.41514489177183955, 1
given range: 0.41514489177183955, 1
range data: 0.41514489177183955, 1
data provided: 0.41514489177183955, 1
provided estimated: 0.41514489177183955, 1
estimated interaction: 0.41514489177183955, 1
interaction antenna: 0.41514489177183955, 1
antenna learning: 0.41514489177183955, 1
confidence estimation: 0.41514489177183955, 1
estimation considered: 0.41514489177183955, 1
considered order: 0.41514489177183955, 1
order progressively: 0.41514489177183955, 1
progressively reduce: 0.41514489177183955, 1
reduce number: 0.41514489177183955, 1
number required: 0.41514489177183955, 1
required physical: 0.41514489177183955, 1
physical interaction: 0.41514489177183955, 1
interaction acquainted: 0.41514489177183955, 1
acquainted raise: 0.41514489177183955, 1
raise number: 0.41514489177183955, 1
number meaningful: 0.41514489177183955, 1
meaningful interaction: 0.41514489177183955, 1
interaction per: 0.41514489177183955, 1
per object: 0.41514489177183955, 1
under several: 0.41514489177183955, 1
several segments: 0.41514489177183955, 1
segments object: 0.41514489177183955, 1
under analysis: 0.41514489177183955, 1
analysis prioritised: 0.41514489177183955, 1
prioritised according: 0.41514489177183955, 1
according set: 0.41514489177183955, 1
set morphological: 0.41514489177183955, 1
morphological field: 0.41514489177183955, 1
major paradigms: 0.41514489177183955, 1
paradigms vision: 0.41514489177183955, 1
driving mediated: 0.41514489177183955, 1
approach parse: 0.41514489177183955, 1
parse entire: 0.41514489177183955, 1
entire scene: 0.41514489177183955, 1
scene driving: 0.41514489177183955, 1
driving behavior: 0.41514489177183955, 1
behavior reflex: 0.41514489177183955, 1
reflex approach: 0.41514489177183955, 1
approach directly: 0.41514489177183955, 1
directly map: 0.41514489177183955, 1
image driving: 0.41514489177183955, 1
driving action: 0.41514489177183955, 1
action third: 0.41514489177183955, 1
approach estimate: 0.41514489177183955, 1
estimate affordance: 0.41514489177183955, 1
image small: 0.41514489177183955, 1
number key: 0.41514489177183955, 1
key perception: 0.41514489177183955, 1
perception indicators: 0.41514489177183955, 1
indicators directly: 0.41514489177183955, 1
directly relate: 0.41514489177183955, 1
relate affordance: 0.41514489177183955, 1
affordance state: 0.41514489177183955, 1
provides set: 0.41514489177183955, 1
set compact: 0.41514489177183955, 1
compact yet: 0.41514489177183955, 1
yet complete: 0.41514489177183955, 1
complete descriptions: 0.41514489177183955, 1
descriptions scene: 0.41514489177183955, 1
scene enable: 0.41514489177183955, 1
enable simple: 0.41514489177183955, 1
simple controller: 0.41514489177183955, 1
controller drive: 0.41514489177183955, 1
drive falling: 0.41514489177183955, 1
falling extremes: 0.41514489177183955, 1
extremes mediated: 0.41514489177183955, 1
perception behavior: 0.41514489177183955, 1
behavior argue: 0.41514489177183955, 1
argue direct: 0.41514489177183955, 1
perception representation: 0.41514489177183955, 1
provides right: 0.41514489177183955, 1
right level: 0.41514489177183955, 1
level demonstrate: 0.41514489177183955, 1
network recording: 0.41514489177183955, 1
recording 12: 0.41514489177183955, 1
12 hours: 0.41514489177183955, 1
hours human: 0.41514489177183955, 1
human driving: 0.41514489177183955, 1
driving video: 0.41514489177183955, 1
video game: 0.41514489177183955, 1
game model: 0.41514489177183955, 1
model drive: 0.41514489177183955, 1
drive car: 0.41514489177183955, 1
car diverse: 0.41514489177183955, 1
set virtual: 0.41514489177183955, 1
virtual train: 0.41514489177183955, 1
model car: 0.41514489177183955, 1
car distance: 0.41514489177183955, 1
distance estimation: 0.41514489177183955, 1
estimation kitti: 0.41514489177183955, 1
kitti result: 0.41514489177183955, 1
result direct: 0.41514489177183955, 1
approach generalize: 0.41514489177183955, 1
generalize real: 0.41514489177183955, 1
real driving: 0.41514489177183955, 1
driving source: 0.41514489177183955, 1
code data: 0.41514489177183955, 1
data available: 0.41514489177183955, 1
available project: 0.41514489177183955, 1
gibson developed: 0.41514489177183955, 1
developed affordance: 0.41514489177183955, 1
concept complement: 0.41514489177183955, 1
complement theory: 0.41514489177183955, 1
theory direct: 0.41514489177183955, 1
perception stands: 0.41514489177183955, 1
stands sharp: 0.41514489177183955, 1
sharp contrast: 0.41514489177183955, 1
contrast prevalent: 0.41514489177183955, 1
prevalent inferential: 0.41514489177183955, 1
inferential theories: 0.41514489177183955, 1
theories comparison: 0.41514489177183955, 1
comparison approach: 0.41514489177183955, 1
approach shows: 0.41514489177183955, 1
shows distinction: 0.41514489177183955, 1
distinction ontological: 0.41514489177183955, 1
ontological trace: 0.41514489177183955, 1
trace history: 0.41514489177183955, 1
history newer: 0.41514489177183955, 1
newer formalizations: 0.41514489177183955, 1
formalizations notion: 0.41514489177183955, 1
affordance discuss: 0.41514489177183955, 1
discuss competing: 0.41514489177183955, 1
competing opinions: 0.41514489177183955, 1
opinions empirical: 0.41514489177183955, 1
empirical affordance: 0.41514489177183955, 1
concept reviewed: 0.41514489177183955, 1
reviewed brief: 0.41514489177183955, 1
brief relevance: 0.41514489177183955, 1
relevance dynamical: 0.41514489177183955, 1
systems theory: 0.41514489177183955, 1
research striking: 0.41514489177183955, 1
striking often: 0.41514489177183955, 1
often neglected: 0.41514489177183955, 1
neglected convergence: 0.41514489177183955, 1
convergence ideas: 0.41514489177183955, 1
ideas gibson: 0.41514489177183955, 1
gibson certain: 0.41514489177183955, 1
certain continental: 0.41514489177183955, 1
continental philosophers: 0.41514489177183955, 1
learning mechanism: 0.41514489177183955, 1
mechanism facilitate: 0.41514489177183955, 1
facilitate autonomous: 0.41514489177183955, 1
autonomous discovery: 0.41514489177183955, 1
discovery effective: 0.41514489177183955, 1
effective affordance: 0.41514489177183955, 1
predict structure: 0.41514489177183955, 1
structure multiple: 0.41514489177183955, 1
multiple action: 0.41514489177183955, 1
action different: 0.41514489177183955, 1
levels robot: 0.41514489177183955, 1
robot benefit: 0.41514489177183955, 1
benefit hierarchical: 0.41514489177183955, 1
structure pre: 0.41514489177183955, 1
pre learned: 0.41514489177183955, 1
learned basic: 0.41514489177183955, 1
affordance input: 0.41514489177183955, 1
input bootstrap: 0.41514489177183955, 1
bootstrap learning: 0.41514489177183955, 1
learning complex: 0.41514489177183955, 1
complex developmental: 0.41514489177183955, 1
developmental links: 0.41514489177183955, 1
links basic: 0.41514489177183955, 1
affordance related: 0.41514489177183955, 1
related complex: 0.41514489177183955, 1
complex affordance: 0.41514489177183955, 1
self discovered: 0.41514489177183955, 1
discovered along: 0.41514489177183955, 1
along suitable: 0.41514489177183955, 1
suitable learning: 0.41514489177183955, 1
learning order: 0.41514489177183955, 1
order discover: 0.41514489177183955, 1
discover developmental: 0.41514489177183955, 1
developmental intrinsic: 0.41514489177183955, 1
motivation approach: 0.41514489177183955, 1
approach guide: 0.41514489177183955, 1
guide robot: 0.41514489177183955, 1
robot explore: 0.41514489177183955, 1
explore action: 0.41514489177183955, 1
action execute: 0.41514489177183955, 1
execute order: 0.41514489177183955, 1
order maximize: 0.41514489177183955, 1
maximize learning: 0.41514489177183955, 1
learning during: 0.41514489177183955, 1
discovers structure: 0.41514489177183955, 1
structure discovering: 0.41514489177183955, 1
discovering distinctive: 0.41514489177183955, 1
distinctive object: 0.41514489177183955, 1
feature predict: 0.41514489177183955, 1
predict implemented: 0.41514489177183955, 1
implemented method: 0.41514489177183955, 1
learning tested: 0.41514489177183955, 1
real dataset: 0.41514489177183955, 1
83 object: 0.41514489177183955, 1
object discrete: 0.41514489177183955, 1
discrete effect: 0.41514489177183955, 1
effect created: 0.41514489177183955, 1
created three: 0.41514489177183955, 1
three poke: 0.41514489177183955, 1
stack result: 0.41514489177183955, 1
result hierarchical: 0.41514489177183955, 1
structure development: 0.41514489177183955, 1
development order: 0.41514489177183955, 1
order emerged: 0.41514489177183955, 1
emerged learning: 0.41514489177183955, 1
learning dynamics: 0.41514489177183955, 1
dynamics guided: 0.41514489177183955, 1
motivation mechanism: 0.41514489177183955, 1
mechanism distinctive: 0.41514489177183955, 1
distinctive feature: 0.41514489177183955, 1
concept central: 0.41514489177183955, 1
central pillars: 0.41514489177183955, 1
pillars ecological: 0.41514489177183955, 1
ecological truly: 0.41514489177183955, 1
truly remarkable: 0.41514489177183955, 1
remarkable idea: 0.41514489177183955, 1
idea provides: 0.41514489177183955, 1
provides concise: 0.41514489177183955, 1
concise theory: 0.41514489177183955, 1
theory animal: 0.41514489177183955, 1
animal perception: 0.41514489177183955, 1
perception predicated: 0.41514489177183955, 1
predicated environmental: 0.41514489177183955, 1
environmental thus: 0.41514489177183955, 1
thus surprising: 0.41514489177183955, 1
surprising idea: 0.41514489177183955, 1
idea found: 0.41514489177183955, 1
found robot: 0.41514489177183955, 1
robot research: 0.41514489177183955, 1
research underlying: 0.41514489177183955, 1
underlying theories: 0.41514489177183955, 1
theories action: 0.41514489177183955, 1
action success: 0.41514489177183955, 1
success theory: 0.41514489177183955, 1
theory regard: 0.41514489177183955, 1
regard meant: 0.41514489177183955, 1
meant existing: 0.41514489177183955, 1
research abundant: 0.41514489177183955, 1
abundant diffuse: 0.41514489177183955, 1
diffuse virtue: 0.41514489177183955, 1
virtue pursuit: 0.41514489177183955, 1
pursuit multiple: 0.41514489177183955, 1
multiple different: 0.41514489177183955, 1
different paths: 0.41514489177183955, 1
paths techniques: 0.41514489177183955, 1
techniques common: 0.41514489177183955, 1
common goal: 0.41514489177183955, 1
goal enabling: 0.41514489177183955, 1
robot act: 0.41514489177183955, 1
act upon: 0.41514489177183955, 1
upon until: 0.41514489177183955, 1
until existed: 0.41514489177183955, 1
existed systematic: 0.41514489177183955, 1
systematic investigation: 0.41514489177183955, 1
investigation existing: 0.41514489177183955, 1
existing motivated: 0.41514489177183955, 1
motivated begin: 0.41514489177183955, 1
begin defining: 0.41514489177183955, 1
defining taxonomy: 0.41514489177183955, 1
taxonomy computational: 0.41514489177183955, 1
affordance rooted: 0.41514489177183955, 1
rooted comprehensive: 0.41514489177183955, 1
comprehensive analysis: 0.41514489177183955, 1
analysis prominent: 0.41514489177183955, 1
prominent theoretical: 0.41514489177183955, 1
theoretical ideas: 0.41514489177183955, 1
ideas import: 0.41514489177183955, 1
import performing: 0.41514489177183955, 1
performing systematic: 0.41514489177183955, 1
systematic literature: 0.41514489177183955, 1
literature provide: 0.41514489177183955, 1
provide classification: 0.41514489177183955, 1
classification existing: 0.41514489177183955, 1
research within: 0.41514489177183955, 1
within propose: 0.41514489177183955, 1
propose quantitatively: 0.41514489177183955, 1
quantitatively qualitatively: 0.41514489177183955, 1
qualitatively assessing: 0.41514489177183955, 1
assessing data: 0.41514489177183955, 1
data resulting: 0.41514489177183955, 1
resulting classification: 0.41514489177183955, 1
classification highlight: 0.41514489177183955, 1
highlight gaps: 0.41514489177183955, 1
gaps research: 0.41514489177183955, 1
research terrain: 0.41514489177183955, 1
terrain outline: 0.41514489177183955, 1
outline open: 0.41514489177183955, 1
open questions: 0.41514489177183955, 1
questions investigation: 0.41514489177183955, 1
investigation affordance: 0.41514489177183955, 1
robot believe: 0.41514489177183955, 1
believe help: 0.41514489177183955, 1
help inform: 0.41514489177183955, 1
inform future: 0.41514489177183955, 1
future prioritize: 0.41514489177183955, 1
prioritize research: 0.41514489177183955, 1
research potentially: 0.41514489177183955, 1
potentially advance: 0.41514489177183955, 1
advance field: 0.41514489177183955, 1
field toward: 0.41514489177183955, 1
toward greater: 0.41514489177183955, 1
greater robot: 0.41514489177183955, 1
goal densely: 0.41514489177183955, 1
predict comparatively: 0.41514489177183955, 1
comparatively large: 0.41514489177183955, 1
set affordance: 0.41514489177183955, 1
rgb approach: 0.41514489177183955, 1
approach task: 0.41514489177183955, 1
task convolutional: 0.41514489177183955, 1
based known: 0.41514489177183955, 1
known resnet: 0.41514489177183955, 1
resnet blend: 0.41514489177183955, 1
blend refinement: 0.41514489177183955, 1
refinement modules: 0.41514489177183955, 1
modules recently: 0.41514489177183955, 1
propose semantic: 0.41514489177183955, 1
segmentation novel: 0.41514489177183955, 1
novel cost: 0.41514489177183955, 1
cost capable: 0.41514489177183955, 1
capable handling: 0.41514489177183955, 1
handling incomplete: 0.41514489177183955, 1
incomplete necessary: 0.41514489177183955, 1
necessary segmentations: 0.41514489177183955, 1
segmentations object: 0.41514489177183955, 1
parts generate: 0.41514489177183955, 1
generate affordance: 0.41514489177183955, 1
affordance demonstrate: 0.41514489177183955, 1
demonstrate quantitatively: 0.41514489177183955, 1
quantitatively learning: 0.41514489177183955, 1
dense predictor: 0.41514489177183955, 1
predictor affordance: 0.41514489177183955, 1
object part: 0.41514489177183955, 1
part dataset: 0.41514489177183955, 1
dataset indeed: 0.41514489177183955, 1
indeed possible: 0.41514489177183955, 1
possible model: 0.41514489177183955, 1
model outperforms: 0.41514489177183955, 1
outperforms several: 0.41514489177183955, 1
finding large: 0.41514489177183955, 1
large network: 0.41514489177183955, 1
train efficiently: 0.41514489177183955, 1
efficiently reliably: 0.41514489177183955, 1
reliably led: 0.41514489177183955, 1
led paradigm: 0.41514489177183955, 1
paradigm shift: 0.41514489177183955, 1
shift computer: 0.41514489177183955, 1
vision engineered: 0.41514489177183955, 1
engineered solutions: 0.41514489177183955, 1
solutions learning: 0.41514489177183955, 1
learning research: 0.41514489177183955, 1
research challenge: 0.41514489177183955, 1
challenge shifts: 0.41514489177183955, 1
shifts devising: 0.41514489177183955, 1
devising algorithm: 0.41514489177183955, 1
algorithm creating: 0.41514489177183955, 1
creating suitable: 0.41514489177183955, 1
suitable abundant: 0.41514489177183955, 1
abundant train: 0.41514489177183955, 1
data supervised: 0.41514489177183955, 1
supervised efficiently: 0.41514489177183955, 1
efficiently create: 0.41514489177183955, 1
create train: 0.41514489177183955, 1
train dominant: 0.41514489177183955, 1
dominant data: 0.41514489177183955, 1
data acquisition: 0.41514489177183955, 1
acquisition method: 0.41514489177183955, 1
method visual: 0.41514489177183955, 1
based web: 0.41514489177183955, 1
web data: 0.41514489177183955, 1
data manual: 0.41514489177183955, 1
manual many: 0.41514489177183955, 1
many computer: 0.41514489177183955, 1
vision stereo: 0.41514489177183955, 1
stereo optical: 0.41514489177183955, 1
approach feasible: 0.41514489177183955, 1
feasible human: 0.41514489177183955, 1
human cannot: 0.41514489177183955, 1
cannot manually: 0.41514489177183955, 1
manually enter: 0.41514489177183955, 1
enter pixel: 0.41514489177183955, 1
pixel accurate: 0.41514489177183955, 1
accurate flow: 0.41514489177183955, 1
flow promote: 0.41514489177183955, 1
promote synthetically: 0.41514489177183955, 1
synthetically generated: 0.41514489177183955, 1
generated data: 0.41514489177183955, 1
data purpose: 0.41514489177183955, 1
purpose train: 0.41514489177183955, 1
network suggest: 0.41514489177183955, 1
suggest multiple: 0.41514489177183955, 1
multiple ways: 0.41514489177183955, 1
ways generate: 0.41514489177183955, 1
generate data: 0.41514489177183955, 1
data evaluate: 0.41514489177183955, 1
evaluate influence: 0.41514489177183955, 1
influence dataset: 0.41514489177183955, 1
dataset properties: 0.41514489177183955, 1
properties performance: 0.41514489177183955, 1
performance generalization: 0.41514489177183955, 1
generalization properties: 0.41514489177183955, 1
properties resulting: 0.41514489177183955, 1
resulting demonstrate: 0.41514489177183955, 1
demonstrate benefit: 0.41514489177183955, 1
benefit learning: 0.41514489177183955, 1
learning schedules: 0.41514489177183955, 1
schedules different: 0.41514489177183955, 1
different types: 0.41514489177183955, 1
types data: 0.41514489177183955, 1
data selected: 0.41514489177183955, 1
selected stages: 0.41514489177183955, 1
stages train: 0.41514489177183955, 1
while annotating: 0.41514489177183955, 1
annotating object: 0.41514489177183955, 1
object image: 0.41514489177183955, 1
image already: 0.41514489177183955, 1
already annotating: 0.41514489177183955, 1
annotating finer: 0.41514489177183955, 1
finer details: 0.41514489177183955, 1
details object: 0.41514489177183955, 1
parts affordance: 0.41514489177183955, 1
more given: 0.41514489177183955, 1
given fact: 0.41514489177183955, 1
fact large: 0.41514489177183955, 1
large datasets: 0.41514489177183955, 1
datasets object: 0.41514489177183955, 1
object annotations: 0.41514489177183955, 1
annotations already: 0.41514489177183955, 1
already address: 0.41514489177183955, 1
address question: 0.41514489177183955, 1
question whether: 0.41514489177183955, 1
whether leverage: 0.41514489177183955, 1
leverage information: 0.41514489177183955, 1
information train: 0.41514489177183955, 1
network segmenting: 0.41514489177183955, 1
segmenting affordance: 0.41514489177183955, 1
parts few: 0.41514489177183955, 1
examples finer: 0.41514489177183955, 1
finer achieve: 0.41514489177183955, 1
achieve semantic: 0.41514489177183955, 1
semantic alignment: 0.41514489177183955, 1
alignment network: 0.41514489177183955, 1
network transfer: 0.41514489177183955, 1
transfer annotations: 0.41514489177183955, 1
annotations small: 0.41514489177183955, 1
small set: 0.41514489177183955, 1
set annotated: 0.41514489177183955, 1
annotated examples: 0.41514489177183955, 1
examples large: 0.41514489177183955, 1
set image: 0.41514489177183955, 1
image coarse: 0.41514489177183955, 1
coarse annotations: 0.41514489177183955, 1
annotations object: 0.41514489177183955, 1
object train: 0.41514489177183955, 1
network weakly: 0.41514489177183955, 1
supervised small: 0.41514489177183955, 1
small annotated: 0.41514489177183955, 1
annotated train: 0.41514489177183955, 1
train set: 0.41514489177183955, 1
set additional: 0.41514489177183955, 1
additional image: 0.41514489177183955, 1
image transferred: 0.41514489177183955, 1
transferred evaluate: 0.41514489177183955, 1
evaluate approach: 0.41514489177183955, 1
approach iit: 0.41514489177183955, 1
iit aff: 0.41514489177183955, 1
aff pascal: 0.41514489177183955, 1
pascal parts: 0.41514489177183955, 1
parts dataset: 0.41514489177183955, 1
dataset approach: 0.41514489177183955, 1
outperforms weakly: 0.41514489177183955, 1
present framework: 0.41514489177183955, 1
framework task: 0.41514489177183955, 1
oriented learning: 0.41514489177183955, 1
recognition aims: 0.41514489177183955, 1
aims understanding: 0.41514489177183955, 1
understanding underlying: 0.41514489177183955, 1
underlying physics: 0.41514489177183955, 1
physics causality: 0.41514489177183955, 1
causality object: 0.41514489177183955, 1
object given: 0.41514489177183955, 1
given cracking: 0.41514489177183955, 1
cracking nut: 0.41514489177183955, 1
nut painting: 0.41514489177183955, 1
painting represent: 0.41514489177183955, 1
represent each: 0.41514489177183955, 1
each hammer: 0.41514489177183955, 1
hammer generative: 0.41514489177183955, 1
generative spatio: 0.41514489177183955, 1
temporal representation: 0.41514489177183955, 1
representation consisting: 0.41514489177183955, 1
consisting four: 0.41514489177183955, 1
four affordance: 0.41514489177183955, 1
affordance basis: 0.41514489177183955, 1
basis grasp: 0.41514489177183955, 1
grasp functional: 0.41514489177183955, 1
functional basis: 0.41514489177183955, 1
basis act: 0.41514489177183955, 1
act target: 0.41514489177183955, 1
object imagined: 0.41514489177183955, 1
imagined action: 0.41514489177183955, 1
action typical: 0.41514489177183955, 1
typical motion: 0.41514489177183955, 1
motion underlying: 0.41514489177183955, 1
underlying physical: 0.41514489177183955, 1
physical learning: 0.41514489177183955, 1
algorithm observes: 0.41514489177183955, 1
observes rgb: 0.41514489177183955, 1
d rational: 0.41514489177183955, 1
rational human: 0.41514489177183955, 1
human picks: 0.41514489177183955, 1
picks object: 0.41514489177183955, 1
object among: 0.41514489177183955, 1
among number: 0.41514489177183955, 1
number candidates: 0.41514489177183955, 1
candidates accomplish: 0.41514489177183955, 1
accomplish algorithm: 0.41514489177183955, 1
algorithm learn: 0.41514489177183955, 1
learn essential: 0.41514489177183955, 1
essential physical: 0.41514489177183955, 1
physical concept: 0.41514489177183955, 1
concept task: 0.41514489177183955, 1
task forces: 0.41514489177183955, 1
forces cracking: 0.41514489177183955, 1
cracking inference: 0.41514489177183955, 1
inference algorithm: 0.41514489177183955, 1
algorithm given: 0.41514489177183955, 1
given set: 0.41514489177183955, 1
set object: 0.41514489177183955, 1
object picks: 0.41514489177183955, 1
picks best: 0.41514489177183955, 1
best choice: 0.41514489177183955, 1
choice available: 0.41514489177183955, 1
available together: 0.41514489177183955, 1
together inferred: 0.41514489177183955, 1
inferred affordance: 0.41514489177183955, 1
affordance functional: 0.41514489177183955, 1
functional imagined: 0.41514489177183955, 1
imagined human: 0.41514489177183955, 1
action expected: 0.41514489177183955, 1
expected physical: 0.41514489177183955, 1
physical quantity: 0.41514489177183955, 1
quantity object: 0.41514489177183955, 1
object viewed: 0.41514489177183955, 1
viewed hammer: 0.41514489177183955, 1
hammer object: 0.41514489177183955, 1
recognition merely: 0.41514489177183955, 1
merely memorizing: 0.41514489177183955, 1
memorizing typical: 0.41514489177183955, 1
typical appearance: 0.41514489177183955, 1
appearance examples: 0.41514489177183955, 1
examples each: 0.41514489177183955, 1
category reasoning: 0.41514489177183955, 1
reasoning physical: 0.41514489177183955, 1
physical mechanism: 0.41514489177183955, 1
mechanism various: 0.41514489177183955, 1
various task: 0.41514489177183955, 1
task achieve: 0.41514489177183955, 1
person video: 0.41514489177183955, 1
video naturally: 0.41514489177183955, 1
naturally brings: 0.41514489177183955, 1
brings physical: 0.41514489177183955, 1
physical environment: 0.41514489177183955, 1
environment since: 0.41514489177183955, 1
since shows: 0.41514489177183955, 1
shows camera: 0.41514489177183955, 1
camera wearer: 0.41514489177183955, 1
wearer interacting: 0.41514489177183955, 1
interacting fluidly: 0.41514489177183955, 1
fluidly space: 0.41514489177183955, 1
space based: 0.41514489177183955, 1
current method: 0.41514489177183955, 1
method largely: 0.41514489177183955, 1
largely separate: 0.41514489177183955, 1
separate observed: 0.41514489177183955, 1
observed action: 0.41514489177183955, 1
action persistent: 0.41514489177183955, 1
persistent space: 0.41514489177183955, 1
space introduce: 0.41514489177183955, 1
introduce model: 0.41514489177183955, 1
model environment: 0.41514489177183955, 1
learned directly: 0.41514489177183955, 1
directly egocentric: 0.41514489177183955, 1
egocentric main: 0.41514489177183955, 1
main idea: 0.41514489177183955, 1
idea gain: 0.41514489177183955, 1
gain human: 0.41514489177183955, 1
centric model: 0.41514489177183955, 1
physical space: 0.41514489177183955, 1
space capture: 0.41514489177183955, 1
capture primary: 0.41514489177183955, 1
primary spatial: 0.41514489177183955, 1
spatial zones: 0.41514489177183955, 1
zones interaction: 0.41514489177183955, 1
interaction likely: 0.41514489177183955, 1
likely activities: 0.41514489177183955, 1
activities approach: 0.41514489177183955, 1
decomposes space: 0.41514489177183955, 1
space topological: 0.41514489177183955, 1
topological map: 0.41514489177183955, 1
map derived: 0.41514489177183955, 1
derived person: 0.41514489177183955, 1
person organizing: 0.41514489177183955, 1
organizing ego: 0.41514489177183955, 1
ego video: 0.41514489177183955, 1
video series: 0.41514489177183955, 1
series visits: 0.41514489177183955, 1
visits different: 0.41514489177183955, 1
different link: 0.41514489177183955, 1
link zones: 0.41514489177183955, 1
zones across: 0.41514489177183955, 1
across multiple: 0.41514489177183955, 1
multiple related: 0.41514489177183955, 1
related environment: 0.41514489177183955, 1
environment video: 0.41514489177183955, 1
video multiple: 0.41514489177183955, 1
multiple obtain: 0.41514489177183955, 1
obtain consolidated: 0.41514489177183955, 1
consolidated representation: 0.41514489177183955, 1
representation environment: 0.41514489177183955, 1
environment epic: 0.41514489177183955, 1
epic kitchens: 0.41514489177183955, 1
kitchens demonstrate: 0.41514489177183955, 1
learning scene: 0.41514489177183955, 1
affordance anticipating: 0.41514489177183955, 1
anticipating future: 0.41514489177183955, 1
future action: 0.41514489177183955, 1
action long: 0.41514489177183955, 1
long form: 0.41514489177183955, 1
rise deep: 0.41514489177183955, 1
learning brought: 0.41514489177183955, 1
brought remarkable: 0.41514489177183955, 1
progress estimating: 0.41514489177183955, 1
estimating hand: 0.41514489177183955, 1
hand geometry: 0.41514489177183955, 1
geometry image: 0.41514489177183955, 1
image hand: 0.41514489177183955, 1
hand part: 0.41514489177183955, 1
part paper: 0.41514489177183955, 1
paper focuses: 0.41514489177183955, 1
focuses problem: 0.41514489177183955, 1
problem explored: 0.41514489177183955, 1
explored consisting: 0.41514489177183955, 1
consisting predict: 0.41514489177183955, 1
grasp several: 0.41514489177183955, 1
several given: 0.41514489177183955, 1
image problem: 0.41514489177183955, 1
problem enormous: 0.41514489177183955, 1
enormous potential: 0.41514489177183955, 1
potential augmented: 0.41514489177183955, 1
augmented robot: 0.41514489177183955, 1
robot prosthetic: 0.41514489177183955, 1
prosthetic order: 0.41514489177183955, 1
order predict: 0.41514489177183955, 1
predict feasible: 0.41514489177183955, 1
feasible understand: 0.41514489177183955, 1
understand semantic: 0.41514489177183955, 1
semantic content: 0.41514489177183955, 1
content geometric: 0.41514489177183955, 1
geometric structure: 0.41514489177183955, 1
structure potential: 0.41514489177183955, 1
potential interaction: 0.41514489177183955, 1
interaction hand: 0.41514489177183955, 1
hand physical: 0.41514489177183955, 1
physical introduce: 0.41514489177183955, 1
introduce generative: 0.41514489177183955, 1
model jointly: 0.41514489177183955, 1
jointly reasons: 0.41514489177183955, 1
reasons levels: 0.41514489177183955, 1
levels regresses: 0.41514489177183955, 1
regresses 3d: 0.41514489177183955, 1
3d shape: 0.41514489177183955, 1
shape pose: 0.41514489177183955, 1
pose object: 0.41514489177183955, 1
object estimates: 0.41514489177183955, 1
estimates grasp: 0.41514489177183955, 1
grasp refines: 0.41514489177183955, 1
refines 51: 0.41514489177183955, 1
51 dof: 0.41514489177183955, 1
dof 3d: 0.41514489177183955, 1
3d hand: 0.41514489177183955, 1
hand model: 0.41514489177183955, 1
model minimize: 0.41514489177183955, 1
minimize graspability: 0.41514489177183955, 1
graspability train: 0.41514489177183955, 1
model build: 0.41514489177183955, 1
build ycb: 0.41514489177183955, 1
ycb affordance: 0.41514489177183955, 1
affordance contains: 0.41514489177183955, 1
contains more: 0.41514489177183955, 1
more 133k: 0.41514489177183955, 1
133k image: 0.41514489177183955, 1
image 21: 0.41514489177183955, 1
21 object: 0.41514489177183955, 1
object ycb: 0.41514489177183955, 1
ycb video: 0.41514489177183955, 1
video annotated: 0.41514489177183955, 1
annotated image: 0.41514489177183955, 1
image more: 0.41514489177183955, 1
more 28m: 0.41514489177183955, 1
28m plausible: 0.41514489177183955, 1
plausible 3d: 0.41514489177183955, 1
3d human: 0.41514489177183955, 1
human grasps: 0.41514489177183955, 1
grasps according: 0.41514489177183955, 1
according 33: 0.41514489177183955, 1
33 class: 0.41514489177183955, 1
class thorough: 0.41514489177183955, 1
thorough evaluation: 0.41514489177183955, 1
evaluation synthetic: 0.41514489177183955, 1
synthetic real: 0.41514489177183955, 1
image shows: 0.41514489177183955, 1
shows model: 0.41514489177183955, 1
model robustly: 0.41514489177183955, 1
robustly predict: 0.41514489177183955, 1
predict realistic: 0.41514489177183955, 1
realistic cluttered: 0.41514489177183955, 1
cluttered scene: 0.41514489177183955, 1
scene multiple: 0.41514489177183955, 1
object close: 0.41514489177183955, 1
service working: 0.41514489177183955, 1
working evolving: 0.41514489177183955, 1
evolving human: 0.41514489177183955, 1
human ability: 0.41514489177183955, 1
ability continuously: 0.41514489177183955, 1
continuously learn: 0.41514489177183955, 1
learn recognize: 0.41514489177183955, 1
recognize act: 0.41514489177183955, 1
act human: 0.41514489177183955, 1
human observing: 0.41514489177183955, 1
observing environment: 0.41514489177183955, 1
environment interacting: 0.41514489177183955, 1
interacting without: 0.41514489177183955, 1
without specific: 0.41514489177183955, 1
specific taking: 0.41514489177183955, 1
taking inspiration: 0.41514489177183955, 1
inspiration infant: 0.41514489177183955, 1
infant developmental: 0.41514489177183955, 1
approach enables: 0.41514489177183955, 1
enables robot: 0.41514489177183955, 1
appearance social: 0.41514489177183955, 1
social active: 0.41514489177183955, 1
active object: 0.41514489177183955, 1
focus unsupervised: 0.41514489177183955, 1
learning does: 0.41514489177183955, 1
does require: 0.41514489177183955, 1
require prior: 0.41514489177183955, 1
knowledge environment: 0.41514489177183955, 1
environment analyse: 0.41514489177183955, 1
analyse visual: 0.41514489177183955, 1
visual space: 0.41514489177183955, 1
space detect: 0.41514489177183955, 1
detect proto: 0.41514489177183955, 1
proto object: 0.41514489177183955, 1
object units: 0.41514489177183955, 1
units attention: 0.41514489177183955, 1
learned recognized: 0.41514489177183955, 1
recognized possible: 0.41514489177183955, 1
possible physical: 0.41514489177183955, 1
physical appearance: 0.41514489177183955, 1
appearance each: 0.41514489177183955, 1
each entity: 0.41514489177183955, 1
entity represented: 0.41514489177183955, 1
represented multi: 0.41514489177183955, 1
view model: 0.41514489177183955, 1
based complementary: 0.41514489177183955, 1
complementary visual: 0.41514489177183955, 1
visual second: 0.41514489177183955, 1
second entities: 0.41514489177183955, 1
entities classified: 0.41514489177183955, 1
classified three: 0.41514489177183955, 1
three parts: 0.41514489177183955, 1
parts body: 0.41514489177183955, 1
body parts: 0.41514489177183955, 1
parts human: 0.41514489177183955, 1
human manipulable: 0.41514489177183955, 1
manipulable categorization: 0.41514489177183955, 1
categorization approach: 0.41514489177183955, 1
based mutual: 0.41514489177183955, 1
mutual information: 0.41514489177183955, 1
information visual: 0.41514489177183955, 1
visual proprioceptive: 0.41514489177183955, 1
proprioceptive motion: 0.41514489177183955, 1
motion behaviour: 0.41514489177183955, 1
behaviour ability: 0.41514489177183955, 1
ability categorize: 0.41514489177183955, 1
categorize entities: 0.41514489177183955, 1
entities during: 0.41514489177183955, 1
during interactive: 0.41514489177183955, 1
object exploration: 0.41514489177183955, 1
exploration improve: 0.41514489177183955, 1
improve previously: 0.41514489177183955, 1
acquired object: 0.41514489177183955, 1
object propose: 0.41514489177183955, 1
propose system: 0.41514489177183955, 1
system implemented: 0.41514489177183955, 1
implemented evaluated: 0.41514489177183955, 1
evaluated icub: 0.41514489177183955, 1
icub meka: 0.41514489177183955, 1
meka robot: 0.41514489177183955, 1
learning 20: 0.41514489177183955, 1
20 system: 0.41514489177183955, 1
recognize object: 0.41514489177183955, 1
object success: 0.41514489177183955, 1
success create: 0.41514489177183955, 1
create coherent: 0.41514489177183955, 1
coherent representation: 0.41514489177183955, 1
model further: 0.41514489177183955, 1
further improved: 0.41514489177183955, 1
improved interactive: 0.41514489177183955, 1
address challenging: 0.41514489177183955, 1
problem unsupervised: 0.41514489177183955, 1
learning existing: 0.41514489177183955, 1
method utilize: 0.41514489177183955, 1
utilize spatio: 0.41514489177183955, 1
temporal continuity: 0.41514489177183955, 1
continuity contiguous: 0.41514489177183955, 1
contiguous video: 0.41514489177183955, 1
video frames: 0.41514489177183955, 1
frames regularization: 0.41514489177183955, 1
regularization learning: 0.41514489177183955, 1
coherence close: 0.41514489177183955, 1
close frames: 0.41514489177183955, 1
frames free: 0.41514489177183955, 1
free form: 0.41514489177183955, 1
form encouraging: 0.41514489177183955, 1
encouraging learned: 0.41514489177183955, 1
learned representation: 0.41514489177183955, 1
representation exhibit: 0.41514489177183955, 1
exhibit small: 0.41514489177183955, 1
small differences: 0.41514489177183955, 1
differences type: 0.41514489177183955, 1
type approach: 0.41514489177183955, 1
approach fails: 0.41514489177183955, 1
fails capture: 0.41514489177183955, 1
capture dissimilarity: 0.41514489177183955, 1
dissimilarity video: 0.41514489177183955, 1
video different: 0.41514489177183955, 1
different hence: 0.41514489177183955, 1
hence learning: 0.41514489177183955, 1
learning less: 0.41514489177183955, 1
less discriminative: 0.41514489177183955, 1
discriminative here: 0.41514489177183955, 1
here siamese: 0.41514489177183955, 1
siamese architectures: 0.41514489177183955, 1
architectures convolutional: 0.41514489177183955, 1
neural corresponding: 0.41514489177183955, 1
corresponding novel: 0.41514489177183955, 1
novel loss: 0.41514489177183955, 1
loss learn: 0.41514489177183955, 1
learn unlabeled: 0.41514489177183955, 1
unlabeled jointly: 0.41514489177183955, 1
jointly exploit: 0.41514489177183955, 1
exploit local: 0.41514489177183955, 1
local temporal: 0.41514489177183955, 1
coherence contiguous: 0.41514489177183955, 1
contiguous global: 0.41514489177183955, 1
global discriminative: 0.41514489177183955, 1
discriminative margin: 0.41514489177183955, 1
margin separate: 0.41514489177183955, 1
separate representation: 0.41514489177183955, 1
representation different: 0.41514489177183955, 1
different extensive: 0.41514489177183955, 1
extensive experimental: 0.41514489177183955, 1
experimental evaluation: 0.41514489177183955, 1
evaluation validate: 0.41514489177183955, 1
model various: 0.41514489177183955, 1
various learned: 0.41514489177183955, 1
learned feature: 0.41514489177183955, 1
feature discover: 0.41514489177183955, 1
discover action: 0.41514489177183955, 1
scene video: 0.41514489177183955, 1
video benefits: 0.41514489177183955, 1
benefits unsupervised: 0.41514489177183955, 1
learning unlabeled: 0.41514489177183955, 1
unlabeled directly: 0.41514489177183955, 1
directly prior: 0.41514489177183955, 1
prior supervised: 0.41514489177183955, 1
supervised recognition: 0.41514489177183955, 1
task action: 0.41514489177183955, 1
object result: 0.41514489177183955, 1
result further: 0.41514489177183955, 1
further feature: 0.41514489177183955, 1
feature surpass: 0.41514489177183955, 1
surpass traditional: 0.41514489177183955, 1
traditional heavily: 0.41514489177183955, 1
heavily supervised: 0.41514489177183955, 1
supervised pre: 0.41514489177183955, 1
train plus: 0.41514489177183955, 1
plus fine: 0.41514489177183955, 1
fine tuning: 0.41514489177183955, 1
affordance key: 0.41514489177183955, 1
key attributes: 0.41514489177183955, 1
attributes perception: 0.41514489177183955, 1
agent order: 0.41514489177183955, 1
order effectively: 0.41514489177183955, 1
effectively interact: 0.41514489177183955, 1
interact novel: 0.41514489177183955, 1
novel concept: 0.41514489177183955, 1
concept derives: 0.41514489177183955, 1
derives literature: 0.41514489177183955, 1
literature psychology: 0.41514489177183955, 1
psychology cognitive: 0.41514489177183955, 1
affordance discussed: 0.41514489177183955, 1
discussed makes: 0.41514489177183955, 1
makes hard: 0.41514489177183955, 1
hard definition: 0.41514489177183955, 1
definition directly: 0.41514489177183955, 1
directly transferred: 0.41514489177183955, 1
transferred computational: 0.41514489177183955, 1
computational specifications: 0.41514489177183955, 1
specifications useful: 0.41514489177183955, 1
useful review: 0.41514489177183955, 1
review article: 0.41514489177183955, 1
article focused: 0.41514489177183955, 1
focused specifically: 0.41514489177183955, 1
specifically discuss: 0.41514489177183955, 1
discuss related: 0.41514489177183955, 1
related literature: 0.41514489177183955, 1
literature classify: 0.41514489177183955, 1
classify literature: 0.41514489177183955, 1
literature try: 0.41514489177183955, 1
try find: 0.41514489177183955, 1
find common: 0.41514489177183955, 1
common ground: 0.41514489177183955, 1
ground amongst: 0.41514489177183955, 1
amongst different: 0.41514489177183955, 1
different approach: 0.41514489177183955, 1
approach view: 0.41514489177183955, 1
view application: 0.41514489177183955, 1
application categorisation: 0.41514489177183955, 1
categorisation based: 0.41514489177183955, 1
based level: 0.41514489177183955, 1
level prior: 0.41514489177183955, 1
knowledge assumed: 0.41514489177183955, 1
assumed build: 0.41514489177183955, 1
build relationship: 0.41514489177183955, 1
relationship among: 0.41514489177183955, 1
among different: 0.41514489177183955, 1
different affordance: 0.41514489177183955, 1
affordance components: 0.41514489177183955, 1
components matter: 0.41514489177183955, 1
matter particular: 0.41514489177183955, 1
particular robot: 0.41514489177183955, 1
robot identify: 0.41514489177183955, 1
identify areas: 0.41514489177183955, 1
areas future: 0.41514489177183955, 1
future improvement: 0.41514489177183955, 1
improvement discuss: 0.41514489177183955, 1
discuss possible: 0.41514489177183955, 1
possible directions: 0.41514489177183955, 1
directions likely: 0.41514489177183955, 1
likely fruitful: 0.41514489177183955, 1
fruitful terms: 0.41514489177183955, 1
terms impact: 0.41514489177183955, 1
impact robot: 0.41514489177183955, 1
motion analysis: 0.41514489177183955, 1
analysis fundamental: 0.41514489177183955, 1
fundamental challenging: 0.41514489177183955, 1
problem field: 0.41514489177183955, 1
computer widely: 0.41514489177183955, 1
widely applied: 0.41514489177183955, 1
applied many: 0.41514489177183955, 1
many autonomous: 0.41514489177183955, 1
autonomous action: 0.41514489177183955, 1
scene displacement: 0.41514489177183955, 1
field subsequent: 0.41514489177183955, 1
subsequent frames: 0.41514489177183955, 1
frames divided: 0.41514489177183955, 1
divided optical: 0.41514489177183955, 1
scene optical: 0.41514489177183955, 1
flow represents: 0.41514489177183955, 1
represents pixel: 0.41514489177183955, 1
pixel motion: 0.41514489177183955, 1
motion adjacent: 0.41514489177183955, 1
adjacent scene: 0.41514489177183955, 1
flow 3d: 0.41514489177183955, 1
3d motion: 0.41514489177183955, 1
field dynamic: 0.41514489177183955, 1
dynamic scene: 0.41514489177183955, 1
scene traditional: 0.41514489177183955, 1
traditional approach: 0.41514489177183955, 1
approach estimation: 0.41514489177183955, 1
estimation optical: 0.41514489177183955, 1
flow usually: 0.41514489177183955, 1
usually leverage: 0.41514489177183955, 1
leverage variational: 0.41514489177183955, 1
variational solved: 0.41514489177183955, 1
solved energy: 0.41514489177183955, 1
energy minimization: 0.41514489177183955, 1
minimization recent: 0.41514489177183955, 1
learning emerged: 0.41514489177183955, 1
emerged powerful: 0.41514489177183955, 1
powerful technique: 0.41514489177183955, 1
technique learning: 0.41514489177183955, 1
feature representation: 0.41514489177183955, 1
representation directly: 0.41514489177183955, 1
directly led: 0.41514489177183955, 1
led remarkable: 0.41514489177183955, 1
progress field: 0.41514489177183955, 1
flow provide: 0.41514489177183955, 1
comprehensive survey: 0.41514489177183955, 1
survey optical: 0.41514489177183955, 1
flow briefly: 0.41514489177183955, 1
briefly review: 0.41514489177183955, 1
review pioneering: 0.41514489177183955, 1
pioneering approach: 0.41514489177183955, 1
approach variational: 0.41514489177183955, 1
variational technique: 0.41514489177183955, 1
technique delve: 0.41514489177183955, 1
delve detail: 0.41514489177183955, 1
detail deep: 0.41514489177183955, 1
based present: 0.41514489177183955, 1
present insightful: 0.41514489177183955, 1
insightful observations: 0.41514489177183955, 1
observations evaluation: 0.41514489177183955, 1
evaluation specifically: 0.41514489177183955, 1
specifically benchmark: 0.41514489177183955, 1
benchmark evaluation: 0.41514489177183955, 1
evaluation state: 0.41514489177183955, 1
art promising: 0.41514489177183955, 1
promising directions: 0.41514489177183955, 1
directions future: 0.41514489177183955, 1
future best: 0.41514489177183955, 1
best review: 0.41514489177183955, 1
review optical: 0.41514489177183955, 1
flow cover: 0.41514489177183955, 1
cover traditional: 0.41514489177183955, 1
traditional deep: 0.41514489177183955, 1
present phase: 0.41514489177183955, 1
phase machine: 0.41514489177183955, 1
learning characterized: 0.41514489177183955, 1
characterized supervised: 0.41514489177183955, 1
algorithm relying: 0.41514489177183955, 1
relying large: 0.41514489177183955, 1
large sets: 0.41514489177183955, 1
sets labeled: 0.41514489177183955, 1
examples next: 0.41514489177183955, 1
next phase: 0.41514489177183955, 1
phase likely: 0.41514489177183955, 1
likely focus: 0.41514489177183955, 1
focus algorithm: 0.41514489177183955, 1
algorithm capable: 0.41514489177183955, 1
learning few: 0.41514489177183955, 1
few labeled: 0.41514489177183955, 1
human seem: 0.41514489177183955, 1
seem able: 0.41514489177183955, 1
able approach: 0.41514489177183955, 1
approach problem: 0.41514489177183955, 1
problem describe: 0.41514489177183955, 1
describe underlying: 0.41514489177183955, 1
underlying based: 0.41514489177183955, 1
based automatic: 0.41514489177183955, 1
automatic learning: 0.41514489177183955, 1
learning representation: 0.41514489177183955, 1
representation supervised: 0.41514489177183955, 1
supervised characterized: 0.41514489177183955, 1
characterized small: 0.41514489177183955, 1
small sample: 0.41514489177183955, 1
sample consider: 0.41514489177183955, 1
consider case: 0.41514489177183955, 1
case visual: 0.41514489177183955, 1
though theory: 0.41514489177183955, 1
theory applies: 0.41514489177183955, 1
applies domains: 0.41514489177183955, 1
domains starting: 0.41514489177183955, 1
starting point: 0.41514489177183955, 1
point proved: 0.41514489177183955, 1
proved specific: 0.41514489177183955, 1
specific image: 0.41514489177183955, 1
image representation: 0.41514489177183955, 1
invariant scaling: 0.41514489177183955, 1
scaling transformations: 0.41514489177183955, 1
transformations considerably: 0.41514489177183955, 1
considerably reduce: 0.41514489177183955, 1
reduce sample: 0.41514489177183955, 1
sample complexity: 0.41514489177183955, 1
complexity prove: 0.41514489177183955, 1
prove invariant: 0.41514489177183955, 1
selective signature: 0.41514489177183955, 1
signature computed: 0.41514489177183955, 1
computed each: 0.41514489177183955, 1
each image: 0.41514489177183955, 1
image image: 0.41514489177183955, 1
image invariance: 0.41514489177183955, 1
invariance exact: 0.41514489177183955, 1
exact case: 0.41514489177183955, 1
case group: 0.41514489177183955, 1
transformations approximate: 0.41514489177183955, 1
approximate under: 0.41514489177183955, 1
under non: 0.41514489177183955, 1
non group: 0.41514489177183955, 1
group module: 0.41514489177183955, 1
module performing: 0.41514489177183955, 1
performing filtering: 0.41514489177183955, 1
filtering simple: 0.41514489177183955, 1
complex cells: 0.41514489177183955, 1
cells described: 0.41514489177183955, 1
described hubel: 0.41514489177183955, 1
hubel compute: 0.41514489177183955, 1
compute theory: 0.41514489177183955, 1
theory offers: 0.41514489177183955, 1
offers novel: 0.41514489177183955, 1
novel unsupervised: 0.41514489177183955, 1
algorithm architectures: 0.41514489177183955, 1
architectures image: 0.41514489177183955, 1
image speech: 0.41514489177183955, 1
speech conjecture: 0.41514489177183955, 1
conjecture main: 0.41514489177183955, 1
main computational: 0.41514489177183955, 1
computational goal: 0.41514489177183955, 1
goal ventral: 0.41514489177183955, 1
ventral stream: 0.41514489177183955, 1
stream visual: 0.41514489177183955, 1
visual cortex: 0.41514489177183955, 1
cortex provide: 0.41514489177183955, 1
provide hierarchical: 0.41514489177183955, 1
selective representation: 0.41514489177183955, 1
may continuously: 0.41514489177183955, 1
continuously learned: 0.41514489177183955, 1
unsupervised during: 0.41514489177183955, 1
during development: 0.41514489177183955, 1
robot coordinate: 0.41514489177183955, 1
coordinate action: 0.41514489177183955, 1
action considering: 0.41514489177183955, 1
considering spatial: 0.41514489177183955, 1
spatial requirements: 0.41514489177183955, 1
requirements human: 0.41514489177183955, 1
human whom: 0.41514489177183955, 1
whom general: 0.41514489177183955, 1
affordance generalizes: 0.41514489177183955, 1
generalizes geometrical: 0.41514489177183955, 1
geometrical accounts: 0.41514489177183955, 1
accounts problem: 0.41514489177183955, 1
problem human: 0.41514489177183955, 1
human aware: 0.41514489177183955, 1
aware placement: 0.41514489177183955, 1
placement robot: 0.41514489177183955, 1
robot framework: 0.41514489177183955, 1
framework provides: 0.41514489177183955, 1
provides conceptual: 0.41514489177183955, 1
conceptual instrument: 0.41514489177183955, 1
instrument account: 0.41514489177183955, 1
account heterogeneous: 0.41514489177183955, 1
heterogeneous abilities: 0.41514489177183955, 1
abilities affordance: 0.41514489177183955, 1
affordance environmental: 0.41514489177183955, 1
environmental discuss: 0.41514489177183955, 1
discuss knowledge: 0.41514489177183955, 1
knowledge aspects: 0.41514489177183955, 1
various reasoning: 0.41514489177183955, 1
reasoning task: 0.41514489177183955, 1
relevant human: 0.41514489177183955, 1
robot applying: 0.41514489177183955, 1
applying notion: 0.41514489177183955, 1
notion practical: 0.41514489177183955, 1
practical socially: 0.41514489177183955, 1
solve social: 0.41514489177183955, 1
social activity: 0.41514489177183955, 1
activity placement: 0.41514489177183955, 1
ability learn: 0.41514489177183955, 1
learn efficiently: 0.41514489177183955, 1
efficiently tool: 0.41514489177183955, 1
tool constitutes: 0.41514489177183955, 1
constitutes desirable: 0.41514489177183955, 1
desirable property: 0.41514489177183955, 1
property general: 0.41514489177183955, 1
purpose humanoid: 0.41514489177183955, 1
humanoid allows: 0.41514489177183955, 1
allows extend: 0.41514489177183955, 1
extend capabilities: 0.41514489177183955, 1
capabilities beyond: 0.41514489177183955, 1
beyond limitations: 0.41514489177183955, 1
limitations own: 0.41514489177183955, 1
own topic: 0.41514489177183955, 1
topic recently: 0.41514489177183955, 1
recently tackled: 0.41514489177183955, 1
tackled robot: 0.41514489177183955, 1
study published: 0.41514489177183955, 1
published far: 0.41514489177183955, 1
far tool: 0.41514489177183955, 1
tool representation: 0.41514489177183955, 1
representation allow: 0.41514489177183955, 1
allow model: 0.41514489177183955, 1
generalize knowledge: 0.41514489177183955, 1
knowledge among: 0.41514489177183955, 1
among similar: 0.41514489177183955, 1
similar tool: 0.41514489177183955, 1
tool limited: 0.41514489177183955, 1
limited study: 0.41514489177183955, 1
study assume: 0.41514489177183955, 1
assume tool: 0.41514489177183955, 1
tool always: 0.41514489177183955, 1
always grasp: 0.41514489177183955, 1
grasp common: 0.41514489177183955, 1
common canonical: 0.41514489177183955, 1
canonical grasp: 0.41514489177183955, 1
grasp thus: 0.41514489177183955, 1
thus considering: 0.41514489177183955, 1
considering influence: 0.41514489177183955, 1
influence grasp: 0.41514489177183955, 1
configuration outcome: 0.41514489177183955, 1
method tackles: 0.41514489177183955, 1
tackles issues: 0.41514489177183955, 1
issues simultaneously: 0.41514489177183955, 1
simultaneously extended: 0.41514489177183955, 1
extended set: 0.41514489177183955, 1
set functional: 0.41514489177183955, 1
feature novel: 0.41514489177183955, 1
novel representation: 0.41514489177183955, 1
representation effect: 0.41514489177183955, 1
effect tool: 0.41514489177183955, 1
tool implicitly: 0.41514489177183955, 1
implicitly account: 0.41514489177183955, 1
account grasp: 0.41514489177183955, 1
configuration allow: 0.41514489177183955, 1
allow icub: 0.41514489177183955, 1
icub generalize: 0.41514489177183955, 1
generalize among: 0.41514489177183955, 1
among tool: 0.41514489177183955, 1
based learning: 0.41514489177183955, 1
learning happens: 0.41514489177183955, 1
happens self: 0.41514489177183955, 1
robot autonomously: 0.41514489177183955, 1
autonomously discovers: 0.41514489177183955, 1
discovers affordance: 0.41514489177183955, 1
affordance categories: 0.41514489177183955, 1
categories tool: 0.41514489177183955, 1
tool clustering: 0.41514489177183955, 1
clustering effect: 0.41514489177183955, 1
effect categories: 0.41514489177183955, 1
categories subsequently: 0.41514489177183955, 1
subsequently teaching: 0.41514489177183955, 1
teaching signal: 0.41514489177183955, 1
signal associate: 0.41514489177183955, 1
associate visually: 0.41514489177183955, 1
visually obtained: 0.41514489177183955, 1
obtained functional: 0.41514489177183955, 1
feature expected: 0.41514489177183955, 1
expected technique: 0.41514489177183955, 1
technique effectively: 0.41514489177183955, 1
effectively given: 0.41514489177183955, 1
given best: 0.41514489177183955, 1
action achieve: 0.41514489177183955, 1
novel behavior: 0.41514489177183955, 1
behavior representation: 0.41514489177183955, 1
representation introduced: 0.41514489177183955, 1
introduced permits: 0.41514489177183955, 1
permits robot: 0.41514489177183955, 1
systematically explore: 0.41514489177183955, 1
explore best: 0.41514489177183955, 1
best method: 0.41514489177183955, 1
method successfully: 0.41514489177183955, 1
successfully execute: 0.41514489177183955, 1
execute affordance: 0.41514489177183955, 1
behavior particular: 0.41514489177183955, 1
particular approach: 0.41514489177183955, 1
decomposes affordance: 0.41514489177183955, 1
behavior three: 0.41514489177183955, 1
three define: 0.41514489177183955, 1
define controller: 0.41514489177183955, 1
controller specify: 0.41514489177183955, 1
specify achieve: 0.41514489177183955, 1
desired change: 0.41514489177183955, 1
change object: 0.41514489177183955, 1
object state: 0.41514489177183955, 1
state changes: 0.41514489177183955, 1
changes each: 0.41514489177183955, 1
each controller: 0.41514489177183955, 1
controller develop: 0.41514489177183955, 1
develop least: 0.41514489177183955, 1
least behavior: 0.41514489177183955, 1
behavior primitive: 0.41514489177183955, 1
primitive determines: 0.41514489177183955, 1
determines controller: 0.41514489177183955, 1
controller output: 0.41514489177183955, 1
output translate: 0.41514489177183955, 1
translate specific: 0.41514489177183955, 1
specific movements: 0.41514489177183955, 1
movements additionally: 0.41514489177183955, 1
additionally provide: 0.41514489177183955, 1
provide multiple: 0.41514489177183955, 1
multiple perception: 0.41514489177183955, 1
perception proxies: 0.41514489177183955, 1
proxies define: 0.41514489177183955, 1
define representation: 0.41514489177183955, 1
object computed: 0.41514489177183955, 1
computed input: 0.41514489177183955, 1
input controller: 0.41514489177183955, 1
controller during: 0.41514489177183955, 1
during variety: 0.41514489177183955, 1
variety proxies: 0.41514489177183955, 1
proxies may: 0.41514489177183955, 1
may selected: 0.41514489177183955, 1
selected given: 0.41514489177183955, 1
given controller: 0.41514489177183955, 1
controller given: 0.41514489177183955, 1
given proxy: 0.41514489177183955, 1
proxy may: 0.41514489177183955, 1
may provide: 0.41514489177183955, 1
provide input: 0.41514489177183955, 1
input more: 0.41514489177183955, 1
more developing: 0.41514489177183955, 1
developing appropriate: 0.41514489177183955, 1
appropriate affordance: 0.41514489177183955, 1
behavior strategy: 0.41514489177183955, 1
strategy given: 0.41514489177183955, 1
systematically vary: 0.41514489177183955, 1
vary elements: 0.41514489177183955, 1
elements note: 0.41514489177183955, 1
note impact: 0.41514489177183955, 1
impact additional: 0.41514489177183955, 1
additional task: 0.41514489177183955, 1
task variables: 0.41514489177183955, 1
variables location: 0.41514489177183955, 1
location demonstrate: 0.41514489177183955, 1
approach pr2: 0.41514489177183955, 1
pr2 robot: 0.41514489177183955, 1
robot explores: 0.41514489177183955, 1
explores different: 0.41514489177183955, 1
different combinations: 0.41514489177183955, 1
combinations behavior: 0.41514489177183955, 1
behavior proxy: 0.41514489177183955, 1
proxy perform: 0.41514489177183955, 1
perform push: 0.41514489177183955, 1
push pull: 0.41514489177183955, 1
pull positioning: 0.41514489177183955, 1
positioning behavior: 0.41514489177183955, 1
behavior selection: 0.41514489177183955, 1
selection household: 0.41514489177183955, 1
household learning: 0.41514489177183955, 1
method best: 0.41514489177183955, 1
best each: 0.41514489177183955, 1
endowing artificial: 0.41514489177183955, 1
agent ability: 0.41514489177183955, 1
ability predict: 0.41514489177183955, 1
predict consequences: 0.41514489177183955, 1
consequences own: 0.41514489177183955, 1
own action: 0.41514489177183955, 1
action efficiently: 0.41514489177183955, 1
efficiently planning: 0.41514489177183955, 1
planning behavior: 0.41514489177183955, 1
based predict: 0.41514489177183955, 1
predict fundamental: 0.41514489177183955, 1
challenge artificial: 0.41514489177183955, 1
intelligence computationally: 0.41514489177183955, 1
computationally practical: 0.41514489177183955, 1
practical yet: 0.41514489177183955, 1
powerful model: 0.41514489177183955, 1
model referred: 0.41514489177183955, 1
referred object: 0.41514489177183955, 1
object probabilistic: 0.41514489177183955, 1
probabilistic dependencies: 0.41514489177183955, 1
dependencies object: 0.41514489177183955, 1
object allows: 0.41514489177183955, 1
allows inferences: 0.41514489177183955, 1
inferences across: 0.41514489177183955, 1
across predict: 0.41514489177183955, 1
action selecting: 0.41514489177183955, 1
selecting best: 0.41514489177183955, 1
action repertoire: 0.41514489177183955, 1
repertoire order: 0.41514489177183955, 1
obtain desired: 0.41514489177183955, 1
effect probabilistic: 0.41514489177183955, 1
probabilistic model: 0.41514489177183955, 1
model capable: 0.41514489177183955, 1
learning mutual: 0.41514489177183955, 1
mutual interaction: 0.41514489177183955, 1
object complex: 0.41514489177183955, 1
complex task: 0.41514489177183955, 1
task involve: 0.41514489177183955, 1
involve object: 0.41514489177183955, 1
object plays: 0.41514489177183955, 1
plays active: 0.41514489177183955, 1
active tool: 0.41514489177183955, 1
tool role: 0.41514489177183955, 1
role while: 0.41514489177183955, 1
being grasp: 0.41514489177183955, 1
grasp while: 0.41514489177183955, 1
while another: 0.41514489177183955, 1
another item: 0.41514489177183955, 1
item passively: 0.41514489177183955, 1
passively acted: 0.41514489177183955, 1
acted upon: 0.41514489177183955, 1
upon consider: 0.41514489177183955, 1
consider visual: 0.41514489177183955, 1
visual meaning: 0.41514489177183955, 1
meaning model: 0.41514489177183955, 1
label compute: 0.41514489177183955, 1
compute set: 0.41514489177183955, 1
set visual: 0.41514489177183955, 1
feature represent: 0.41514489177183955, 1
represent geometrical: 0.41514489177183955, 1
properties allows: 0.41514489177183955, 1
allows generalize: 0.41514489177183955, 1
generalize previously: 0.41514489177183955, 1
knowledge describe: 0.41514489177183955, 1
describe experiment: 0.41514489177183955, 1
experiment simulate: 0.41514489177183955, 1
simulate humanoid: 0.41514489177183955, 1
model autonomously: 0.41514489177183955, 1
autonomously exploring: 0.41514489177183955, 1
exploring different: 0.41514489177183955, 1
different action: 0.41514489177183955, 1
present playground: 0.41514489177183955, 1
playground report: 0.41514489177183955, 1
report result: 0.41514489177183955, 1
result showing: 0.41514489177183955, 1
showing robot: 0.41514489177183955, 1
meaningful relationships: 0.41514489177183955, 1
relationships object: 0.41514489177183955, 1
object exploit: 0.41514489177183955, 1
exploit acquired: 0.41514489177183955, 1
knowledge predict: 0.41514489177183955, 1
predict optimal: 0.41514489177183955, 1
based estimation: 0.41514489177183955, 1
estimation grasp: 0.41514489177183955, 1
affordance desirable: 0.41514489177183955, 1
desirable 3: 0.41514489177183955, 1
d scans: 0.41514489177183955, 1
scans become: 0.41514489177183955, 1
become unreliable: 0.41514489177183955, 1
unreliable due: 0.41514489177183955, 1
due clutter: 0.41514489177183955, 1
clutter material: 0.41514489177183955, 1
material develop: 0.41514489177183955, 1
develop general: 0.41514489177183955, 1
framework estimating: 0.41514489177183955, 1
affordance 2: 0.41514489177183955, 1
2 d: 0.41514489177183955, 1
d including: 0.41514489177183955, 1
including local: 0.41514489177183955, 1
local texture: 0.41514489177183955, 1
texture measures: 0.41514489177183955, 1
measures object: 0.41514489177183955, 1
category measures: 0.41514489177183955, 1
measures capture: 0.41514489177183955, 1
capture previously: 0.41514489177183955, 1
learned grasp: 0.41514489177183955, 1
grasp local: 0.41514489177183955, 1
local approach: 0.41514489177183955, 1
approach estimating: 0.41514489177183955, 1
grasp positions: 0.41514489177183955, 1
positions shown: 0.41514489177183955, 1
shown effective: 0.41514489177183955, 1
effective real: 0.41514489177183955, 1
world unable: 0.41514489177183955, 1
unable impart: 0.41514489177183955, 1
impart object: 0.41514489177183955, 1
level biases: 0.41514489177183955, 1
biases prone: 0.41514489177183955, 1
prone false: 0.41514489177183955, 1
false describe: 0.41514489177183955, 1
describe global: 0.41514489177183955, 1
global cues: 0.41514489177183955, 1
cues compute: 0.41514489177183955, 1
compute continuous: 0.41514489177183955, 1
pose estimates: 0.41514489177183955, 1
estimates corresponding: 0.41514489177183955, 1
corresponding grasp: 0.41514489177183955, 1
point max: 0.41514489177183955, 1
max margin: 0.41514489177183955, 1
margin optimization: 0.41514489177183955, 1
optimization category: 0.41514489177183955, 1
category level: 0.41514489177183955, 1
level continuous: 0.41514489177183955, 1
pose provide: 0.41514489177183955, 1
provide novel: 0.41514489177183955, 1
dataset evaluate: 0.41514489177183955, 1
evaluate visual: 0.41514489177183955, 1
affordance dataset: 0.41514489177183955, 1
dataset fused: 0.41514489177183955, 1
fused method: 0.41514489177183955, 1
method outperforms: 0.41514489177183955, 1
outperforms either: 0.41514489177183955, 1
either local: 0.41514489177183955, 1
local global: 0.41514489177183955, 1
global method: 0.41514489177183955, 1
method continuous: 0.41514489177183955, 1
estimation improves: 0.41514489177183955, 1
improves discrete: 0.41514489177183955, 1
discrete output: 0.41514489177183955, 1
output demonstrate: 0.41514489177183955, 1
demonstrate autonomous: 0.41514489177183955, 1
autonomous object: 0.41514489177183955, 1
detection grasp: 0.41514489177183955, 1
system willow: 0.41514489177183955, 1
willow garage: 0.41514489177183955, 1
garage pr2: 0.41514489177183955, 1
inspired infant: 0.41514489177183955, 1
infant three: 0.41514489177183955, 1
three staged: 0.41514489177183955, 1
staged developmental: 0.41514489177183955, 1
framework anthropomorphic: 0.41514489177183955, 1
anthropomorphic robot: 0.41514489177183955, 1
robot robot: 0.41514489177183955, 1
robot initialized: 0.41514489177183955, 1
initialized basic: 0.41514489177183955, 1
basic reach: 0.41514489177183955, 1
reach enclose: 0.41514489177183955, 1
enclose contact: 0.41514489177183955, 1
contact movement: 0.41514489177183955, 1
movement discovers: 0.41514489177183955, 1
discovers set: 0.41514489177183955, 1
set behavior: 0.41514489177183955, 1
behavior primitives: 0.41514489177183955, 1
primitives exploring: 0.41514489177183955, 1
exploring movement: 0.41514489177183955, 1
movement parameter: 0.41514489177183955, 1
parameter next: 0.41514489177183955, 1
next robot: 0.41514489177183955, 1
robot exercises: 0.41514489177183955, 1
exercises discovered: 0.41514489177183955, 1
discovered behavior: 0.41514489177183955, 1
behavior different: 0.41514489177183955, 1
different learn: 0.41514489177183955, 1
learn caused: 0.41514489177183955, 1
caused effectively: 0.41514489177183955, 1
effectively building: 0.41514489177183955, 1
building library: 0.41514489177183955, 1
library affordance: 0.41514489177183955, 1
affordance associated: 0.41514489177183955, 1
associated third: 0.41514489177183955, 1
third learned: 0.41514489177183955, 1
learned structures: 0.41514489177183955, 1
structures predictors: 0.41514489177183955, 1
predictors bootstrap: 0.41514489177183955, 1
bootstrap complex: 0.41514489177183955, 1
complex imitation: 0.41514489177183955, 1
imitation action: 0.41514489177183955, 1
action learning: 0.41514489177183955, 1
learning help: 0.41514489177183955, 1
help cooperative: 0.41514489177183955, 1
cooperative main: 0.41514489177183955, 1
main contribution: 0.41514489177183955, 1
contribution paper: 0.41514489177183955, 1
paper realization: 0.41514489177183955, 1
realization integrated: 0.41514489177183955, 1
integrated developmental: 0.41514489177183955, 1
developmental system: 0.41514489177183955, 1
system structures: 0.41514489177183955, 1
structures emerging: 0.41514489177183955, 1
emerging sensorimotor: 0.41514489177183955, 1
sensorimotor experience: 0.41514489177183955, 1
experience interacting: 0.41514489177183955, 1
interacting real: 0.41514489177183955, 1
robot sole: 0.41514489177183955, 1
sole building: 0.41514489177183955, 1
building blocks: 0.41514489177183955, 1
blocks subsequent: 0.41514489177183955, 1
subsequent stages: 0.41514489177183955, 1
stages generate: 0.41514489177183955, 1
generate increasingly: 0.41514489177183955, 1
increasingly more: 0.41514489177183955, 1
complex cognitive: 0.41514489177183955, 1
cognitive propose: 0.41514489177183955, 1
framework includes: 0.41514489177183955, 1
includes number: 0.41514489177183955, 1
number common: 0.41514489177183955, 1
common feature: 0.41514489177183955, 1
feature infant: 0.41514489177183955, 1
infant sensorimotor: 0.41514489177183955, 1
sensorimotor findings: 0.41514489177183955, 1
findings obtained: 0.41514489177183955, 1
obtained self: 0.41514489177183955, 1
exploration motionese: 0.41514489177183955, 1
motionese guided: 0.41514489177183955, 1
guided human: 0.41514489177183955, 1
interaction experiments: 0.41514489177183955, 1
experiments allow: 0.41514489177183955, 1
allow reason: 0.41514489177183955, 1
reason underlying: 0.41514489177183955, 1
underlying mechanism: 0.41514489177183955, 1
mechanism simple: 0.41514489177183955, 1
complex sensorimotor: 0.41514489177183955, 1
sensorimotor skill: 0.41514489177183955, 1
skill progression: 0.41514489177183955, 1
progression human: 0.41514489177183955, 1
present ground: 0.41514489177183955, 1
ground vehicle: 0.41514489177183955, 1
vehicle capable: 0.41514489177183955, 1
capable exploiting: 0.41514489177183955, 1
cues learn: 0.41514489177183955, 1
learn navigation: 0.41514489177183955, 1
affordance depth: 0.41514489177183955, 1
depth simple: 0.41514489177183955, 1
simple pan: 0.41514489177183955, 1
antenna kinect: 0.41514489177183955, 1
kinect fitted: 0.41514489177183955, 1
fitted body: 0.41514489177183955, 1
body provide: 0.41514489177183955, 1
provide required: 0.41514489177183955, 1
required haptic: 0.41514489177183955, 1
sensory robot: 0.41514489177183955, 1
robot determines: 0.41514489177183955, 1
determines whether: 0.41514489177183955, 1
whether object: 0.41514489177183955, 1
traversable interaction: 0.41514489177183955, 1
interaction outcome: 0.41514489177183955, 1
outcome associated: 0.41514489177183955, 1
associated depth: 0.41514489177183955, 1
based later: 0.41514489177183955, 1
later robot: 0.41514489177183955, 1
robot predict: 0.41514489177183955, 1
predict newly: 0.41514489177183955, 1
newly observed: 0.41514489177183955, 1
observed object: 0.41514489177183955, 1
traversable inspecting: 0.41514489177183955, 1
inspecting depth: 0.41514489177183955, 1
based appearance: 0.41514489177183955, 1
appearance uses: 0.41514489177183955, 1
uses acquired: 0.41514489177183955, 1
acquired set: 0.41514489177183955, 1
set field: 0.41514489177183955, 1
present affordance: 0.41514489177183955, 1
system robot: 0.41514489177183955, 1
system involves: 0.41514489177183955, 1
involves three: 0.41514489177183955, 1
three important: 0.41514489177183955, 1
important affordance: 0.41514489177183955, 1
affordance synergy: 0.41514489177183955, 1
control strategy: 0.41514489177183955, 1
strategy local: 0.41514489177183955, 1
local sensor: 0.41514489177183955, 1
sensor affordance: 0.41514489177183955, 1
memory modeled: 0.41514489177183955, 1
modeled modified: 0.41514489177183955, 1
modified growing: 0.41514489177183955, 1
growing neural: 0.41514489177183955, 1
neural gas: 0.41514489177183955, 1
gas network: 0.41514489177183955, 1
network allows: 0.41514489177183955, 1
allows affordance: 0.41514489177183955, 1
learned quickly: 0.41514489177183955, 1
quickly small: 0.41514489177183955, 1
small dataset: 0.41514489177183955, 1
dataset human: 0.41514489177183955, 1
object being: 0.41514489177183955, 1
being train: 0.41514489177183955, 1
memory system: 0.41514489177183955, 1
system generate: 0.41514489177183955, 1
generate online: 0.41514489177183955, 1
online motor: 0.41514489177183955, 1
motor commands: 0.41514489177183955, 1
commands reaching: 0.41514489177183955, 1
system explore: 0.41514489177183955, 1
explore various: 0.41514489177183955, 1
various grasp: 0.41514489177183955, 1
grasp postures: 0.41514489177183955, 1
postures efficiently: 0.41514489177183955, 1
efficiently low: 0.41514489177183955, 1
dimensional synergy: 0.41514489177183955, 1
synergy space: 0.41514489177183955, 1
space synergies: 0.41514489177183955, 1
synergies automatically: 0.41514489177183955, 1
automatically avoid: 0.41514489177183955, 1
avoid abnormal: 0.41514489177183955, 1
abnormal postures: 0.41514489177183955, 1
postures more: 0.41514489177183955, 1
more likely: 0.41514489177183955, 1
likely lead: 0.41514489177183955, 1
lead failed: 0.41514489177183955, 1
failed experimental: 0.41514489177183955, 1
demonstrate affordance: 0.41514489177183955, 1
memory generalize: 0.41514489177183955, 1
generalize grasp: 0.41514489177183955, 1
object predict: 0.41514489177183955, 1
effect grasp: 0.41514489177183955, 1
grasp tactile: 0.41514489177183955, 1
study embodiment: 0.41514489177183955, 1
specific robot: 0.41514489177183955, 1
grasp represented: 0.41514489177183955, 1
represented probabilistic: 0.41514489177183955, 1
probabilistic framework: 0.41514489177183955, 1
framework consists: 0.41514489177183955, 1
consists bayesian: 0.41514489177183955, 1
network integrated: 0.41514489177183955, 1
integrated novel: 0.41514489177183955, 1
novel multi: 0.41514489177183955, 1
multi variate: 0.41514489177183955, 1
variate discretization: 0.41514489177183955, 1
discretization bn: 0.41514489177183955, 1
bn model: 0.41514489177183955, 1
model probabilistic: 0.41514489177183955, 1
probabilistic relationships: 0.41514489177183955, 1
relationships among: 0.41514489177183955, 1
among grasp: 0.41514489177183955, 1
grasp action: 0.41514489177183955, 1
action discretization: 0.41514489177183955, 1
discretization model: 0.41514489177183955, 1
model provides: 0.41514489177183955, 1
provides compact: 0.41514489177183955, 1
compact data: 0.41514489177183955, 1
data representation: 0.41514489177183955, 1
representation allows: 0.41514489177183955, 1
allows efficient: 0.41514489177183955, 1
learning conditional: 0.41514489177183955, 1
conditional structures: 0.41514489177183955, 1
structures evaluate: 0.41514489177183955, 1
evaluate database: 0.41514489177183955, 1
database generated: 0.41514489177183955, 1
generated simulate: 0.41514489177183955, 1
simulate environment: 0.41514489177183955, 1
environment including: 0.41514489177183955, 1
including examples: 0.41514489177183955, 1
robot hand: 0.41514489177183955, 1
hand interacting: 0.41514489177183955, 1
interacting result: 0.41514489177183955, 1
different kinematic: 0.41514489177183955, 1
kinematic structures: 0.41514489177183955, 1
structures hand: 0.41514489177183955, 1
hand affect: 0.41514489177183955, 1
affect bn: 0.41514489177183955, 1
bn structure: 0.41514489177183955, 1
structure conditional: 0.41514489177183955, 1
conditional distributions: 0.41514489177183955, 1
distributions modeled: 0.41514489177183955, 1
modeled model: 0.41514489177183955, 1
model achieve: 0.41514489177183955, 1
achieve accurate: 0.41514489177183955, 1
accurate task: 0.41514489177183955, 1
task successfully: 0.41514489177183955, 1
successfully encode: 0.41514489177183955, 1
encode semantic: 0.41514489177183955, 1
semantic task: 0.41514489177183955, 1
task requirements: 0.41514489177183955, 1
requirements continuous: 0.41514489177183955, 1
continuous observation: 0.41514489177183955, 1
observation imitation: 0.41514489177183955, 1
imitation demonstrate: 0.41514489177183955, 1
demonstrate representation: 0.41514489177183955, 1
representation framework: 0.41514489177183955, 1
framework transfer: 0.41514489177183955, 1
transfer task: 0.41514489177183955, 1
task knowledge: 0.41514489177183955, 1
knowledge different: 0.41514489177183955, 1
different therefore: 0.41514489177183955, 1
therefore suitable: 0.41514489177183955, 1
suitable model: 0.41514489177183955, 1
planning imitation: 0.41514489177183955, 1
imitation goal: 0.41514489177183955, 1
affordance encodes: 0.41514489177183955, 1
encodes latent: 0.41514489177183955, 1
latent given: 0.41514489177183955, 1
robot interact: 0.41514489177183955, 1
interact present: 0.41514489177183955, 1
present 4: 0.41514489177183955, 1
4 tuple: 0.41514489177183955, 1
tuple formalization: 0.41514489177183955, 1
formalization describe: 0.41514489177183955, 1
describe robot: 0.41514489177183955, 1
robot environment: 0.41514489177183955, 1
environment precondition: 0.41514489177183955, 1
precondition postcondition: 0.41514489177183955, 1
postcondition enable: 0.41514489177183955, 1
enable each: 0.41514489177183955, 1
each action: 0.41514489177183955, 1
action place: 0.41514489177183955, 1
place measureable: 0.41514489177183955, 1
measureable analysis: 0.41514489177183955, 1
analysis functions: 0.41514489177183955, 1
functions extract: 0.41514489177183955, 1
extract functional: 0.41514489177183955, 1
functional information: 0.41514489177183955, 1
information basis: 0.41514489177183955, 1
basis key: 0.41514489177183955, 1
key problem: 0.41514489177183955, 1
learning addressed: 0.41514489177183955, 1
addressed based: 0.41514489177183955, 1
based analysis: 0.41514489177183955, 1
analysis robot: 0.41514489177183955, 1
architecture simulate: 0.41514489177183955, 1
robot performed: 0.41514489177183955, 1
performed task: 0.41514489177183955, 1
task effectively: 0.41514489177183955, 1
effectively under: 0.41514489177183955, 1
continuous learning: 0.41514489177183955, 1
affordance cognitive: 0.41514489177183955, 1
robot challenging: 0.41514489177183955, 1
challenging solution: 0.41514489177183955, 1
solution arguably: 0.41514489177183955, 1
arguably requires: 0.41514489177183955, 1
requires developmental: 0.41514489177183955, 1
developmental describe: 0.41514489177183955, 1
describe scenarios: 0.41514489177183955, 1
scenarios robot: 0.41514489177183955, 1
systems interact: 0.41514489177183955, 1
interact household: 0.41514489177183955, 1
household object: 0.41514489177183955, 1
object pushing: 0.41514489177183955, 1
pushing robot: 0.41514489177183955, 1
robot arms: 0.41514489177183955, 1
arms while: 0.41514489177183955, 1
while observing: 0.41514489177183955, 1
observing scene: 0.41514489177183955, 1
scene incrementally: 0.41514489177183955, 1
incrementally without: 0.41514489177183955, 1
without external: 0.41514489177183955, 1
external effect: 0.41514489177183955, 1
effect classes: 0.41514489177183955, 1
classes emerge: 0.41514489177183955, 1
emerge interaction: 0.41514489177183955, 1
interaction discriminative: 0.41514489177183955, 1
model predict: 0.41514489177183955, 1
predict object: 0.41514489177183955, 1
object formalize: 0.41514489177183955, 1
formalize scenario: 0.41514489177183955, 1
scenario multi: 0.41514489177183955, 1
view learning: 0.41514489177183955, 1
learning problem: 0.41514489177183955, 1
problem data: 0.41514489177183955, 1
data co: 0.41514489177183955, 1
co occur: 0.41514489177183955, 1
occur separate: 0.41514489177183955, 1
separate data: 0.41514489177183955, 1
data views: 0.41514489177183955, 1
views present: 0.41514489177183955, 1
present online: 0.41514489177183955, 1
framework uses: 0.41514489177183955, 1
supervised form: 0.41514489177183955, 1
form learning: 0.41514489177183955, 1
learning vector: 0.41514489177183955, 1
vector quantization: 0.41514489177183955, 1
quantization build: 0.41514489177183955, 1
build discriminative: 0.41514489177183955, 1
discriminative various: 0.41514489177183955, 1
various demonstrate: 0.41514489177183955, 1
approach comparison: 0.41514489177183955, 1
comparison related: 0.41514489177183955, 1
related supervised: 0.41514489177183955, 1
method data: 0.41514489177183955, 1
data experiments: 0.41514489177183955, 1
experiments performed: 0.41514489177183955, 1
performed different: 0.41514489177183955, 1
different robot: 0.41514489177183955, 1
aspects brain: 0.41514489177183955, 1
brain processing: 0.41514489177183955, 1
processing visual: 0.41514489177183955, 1
cognitive value: 0.41514489177183955, 1
value decision: 0.41514489177183955, 1
action melded: 0.41514489177183955, 1
melded together: 0.41514489177183955, 1
together coherent: 0.41514489177183955, 1
coherent manner: 0.41514489177183955, 1
manner cognitive: 0.41514489177183955, 1
cycle visually: 0.41514489177183955, 1
guided reaching: 0.41514489177183955, 1
robot based: 0.41514489177183955, 1
notion separate: 0.41514489177183955, 1
separate visuomotor: 0.41514489177183955, 1
visuomotor channels: 0.41514489177183955, 1
channels activated: 0.41514489177183955, 1
activated parallel: 0.41514489177183955, 1
parallel specific: 0.41514489177183955, 1
visual input: 0.41514489177183955, 1
input continuously: 0.41514489177183955, 1
continuously modulated: 0.41514489177183955, 1
modulated attention: 0.41514489177183955, 1
action suggested: 0.41514489177183955, 1
suggested visual: 0.41514489177183955, 1
visual apparatus: 0.41514489177183955, 1
apparatus allows: 0.41514489177183955, 1
allows recognize: 0.41514489177183955, 1
recognize shape: 0.41514489177183955, 1
shape extract: 0.41514489177183955, 1
extract affordance: 0.41514489177183955, 1
affordance formulate: 0.41514489177183955, 1
formulate motor: 0.41514489177183955, 1
motor plans: 0.41514489177183955, 1
plans reaching: 0.41514489177183955, 1
reaching focus: 0.41514489177183955, 1
attention signal: 0.41514489177183955, 1
signal plays: 0.41514489177183955, 1
plays instrumental: 0.41514489177183955, 1
instrumental role: 0.41514489177183955, 1
role selecting: 0.41514489177183955, 1
selecting correct: 0.41514489177183955, 1
correct object: 0.41514489177183955, 1
object corresponding: 0.41514489177183955, 1
corresponding location: 0.41514489177183955, 1
location selects: 0.41514489177183955, 1
selects appropriate: 0.41514489177183955, 1
appropriate arm: 0.41514489177183955, 1
arm reaching: 0.41514489177183955, 1
reaching hand: 0.41514489177183955, 1
hand grasp: 0.41514489177183955, 1
configuration list: 0.41514489177183955, 1
list configurations: 0.41514489177183955, 1
configurations based: 0.41514489177183955, 1
based success: 0.41514489177183955, 1
success previous: 0.41514489177183955, 1
previous cognitive: 0.41514489177183955, 1
architecture consists: 0.41514489177183955, 1
consists number: 0.41514489177183955, 1
number neurocomputational: 0.41514489177183955, 1
neurocomputational mechanism: 0.41514489177183955, 1
mechanism heavily: 0.41514489177183955, 1
heavily supported: 0.41514489177183955, 1
supported experimental: 0.41514489177183955, 1
experimental brain: 0.41514489177183955, 1
brain spatial: 0.41514489177183955, 1
spatial object: 0.41514489177183955, 1
object invariance: 0.41514489177183955, 1
invariance object: 0.41514489177183955, 1
focus motor: 0.41514489177183955, 1
motor spatial: 0.41514489177183955, 1
spatial joint: 0.41514489177183955, 1
joint direction: 0.41514489177183955, 1
direction transformation: 0.41514489177183955, 1
transformation volitional: 0.41514489177183955, 1
volitional scaling: 0.41514489177183955, 1
inspired recent: 0.41514489177183955, 1
advances propose: 0.41514489177183955, 1
propose ecological: 0.41514489177183955, 1
psychology many: 0.41514489177183955, 1
many developmental: 0.41514489177183955, 1
study started: 0.41514489177183955, 1
started investigate: 0.41514489177183955, 1
investigate modeling: 0.41514489177183955, 1
modeling learning: 0.41514489177183955, 1
affordance humanoid: 0.41514489177183955, 1
humanoid paper: 0.41514489177183955, 1
paper leverage: 0.41514489177183955, 1
leverage probabilistic: 0.41514489177183955, 1
model place: 0.41514489177183955, 1
place least: 0.41514489177183955, 1
least square: 0.41514489177183955, 1
square support: 0.41514489177183955, 1
support vector: 0.41514489177183955, 1
vector machine: 0.41514489177183955, 1
machine previous: 0.41514489177183955, 1
previous testing: 0.41514489177183955, 1
testing bayesian: 0.41514489177183955, 1
learning icub: 0.41514489177183955, 1
icub present: 0.41514489177183955, 1
present experiments: 0.41514489177183955, 1
experiments related: 0.41514489177183955, 1
related learning: 0.41514489177183955, 1
effect consequent: 0.41514489177183955, 1
consequent tapping: 0.41514489177183955, 1
tapping object: 0.41514489177183955, 1
object several: 0.41514489177183955, 1
several directions: 0.41514489177183955, 1
directions pulling: 0.41514489177183955, 1
pulling reach: 0.41514489177183955, 1
reach object: 0.41514489177183955, 1
object choosing: 0.41514489177183955, 1
choosing appropriate: 0.41514489177183955, 1
appropriate propose: 0.41514489177183955, 1
propose probabilistic: 0.41514489177183955, 1
model lssvm: 0.41514489177183955, 1
lssvm identifies: 0.41514489177183955, 1
identifies regression: 0.41514489177183955, 1
regression function: 0.41514489177183955, 1
function predict: 0.41514489177183955, 1
action provides: 0.41514489177183955, 1
information reliability: 0.41514489177183955, 1
reliability predicted: 0.41514489177183955, 1
predicted values: 0.41514489177183955, 1
tool afford: 0.41514489177183955, 1
afford similar: 0.41514489177183955, 1
similar functionality: 0.41514489177183955, 1
functionality share: 0.41514489177183955, 1
share common: 0.41514489177183955, 1
common geometrical: 0.41514489177183955, 1
geometrical effect: 0.41514489177183955, 1
effect achieved: 0.41514489177183955, 1
achieved tool: 0.41514489177183955, 1
tool depends: 0.41514489177183955, 1
depends much: 0.41514489177183955, 1
much action: 0.41514489177183955, 1
present step: 0.41514489177183955, 1
step model: 0.41514489177183955, 1
specifically tackles: 0.41514489177183955, 1
tackles introduce: 0.41514489177183955, 1
introduce oriented: 0.41514489177183955, 1
oriented multi: 0.41514489177183955, 1
scale extended: 0.41514489177183955, 1
extended gaussian: 0.41514489177183955, 1
gaussian image: 0.41514489177183955, 1
image set: 0.41514489177183955, 1
set 3d: 0.41514489177183955, 1
feature devised: 0.41514489177183955, 1
devised describe: 0.41514489177183955, 1
describe tool: 0.41514489177183955, 1
tool interaction: 0.41514489177183955, 1
interaction able: 0.41514489177183955, 1
able encapsulate: 0.41514489177183955, 1
encapsulate general: 0.41514489177183955, 1
general compact: 0.41514489177183955, 1
compact geometrical: 0.41514489177183955, 1
tool relative: 0.41514489177183955, 1
relative based: 0.41514489177183955, 1
learn predict: 0.41514489177183955, 1
discovers available: 0.41514489177183955, 1
available tool: 0.41514489177183955, 1
pose categories: 0.41514489177183955, 1
categories set: 0.41514489177183955, 1
set hand: 0.41514489177183955, 1
hand held: 0.41514489177183955, 1
held learn: 0.41514489177183955, 1
learn distinct: 0.41514489177183955, 1
distinct affordance: 0.41514489177183955, 1
model each: 0.41514489177183955, 1
each discovered: 0.41514489177183955, 1
discovered tool: 0.41514489177183955, 1
pose result: 0.41514489177183955, 1
result combination: 0.41514489177183955, 1
combination oms: 0.41514489177183955, 1
oms egi: 0.41514489177183955, 1
egi 3d: 0.41514489177183955, 1
feature multi: 0.41514489177183955, 1
able produce: 0.41514489177183955, 1
produce quite: 0.41514489177183955, 1
quite accurate: 0.41514489177183955, 1
accurate predict: 0.41514489177183955, 1
performed tool: 0.41514489177183955, 1
grasp particular: 0.41514489177183955, 1
particular unseen: 0.41514489177183955, 1
unseen tool: 0.41514489177183955, 1
affordance facilitates: 0.41514489177183955, 1
facilitates encoding: 0.41514489177183955, 1
encoding relations: 0.41514489177183955, 1
relations action: 0.41514489177183955, 1
effect environment: 0.41514489177183955, 1
environment centered: 0.41514489177183955, 1
centered around: 0.41514489177183955, 1
around interpretation: 0.41514489177183955, 1
interpretation important: 0.41514489177183955, 1
important impacts: 0.41514489177183955, 1
impacts several: 0.41514489177183955, 1
several cognitive: 0.41514489177183955, 1
cognitive capabilities: 0.41514489177183955, 1
capabilities manifestations: 0.41514489177183955, 1
manifestations predict: 0.41514489177183955, 1
predict framework: 0.41514489177183955, 1
based denoising: 0.41514489177183955, 1
encoders propose: 0.41514489177183955, 1
propose allows: 0.41514489177183955, 1
allows agent: 0.41514489177183955, 1
agent explore: 0.41514489177183955, 1
explore environment: 0.41514489177183955, 1
environment actively: 0.41514489177183955, 1
actively learn: 0.41514489177183955, 1
tool observing: 0.41514489177183955, 1
observing consequences: 0.41514489177183955, 1
consequences acting: 0.41514489177183955, 1
acting da: 0.41514489177183955, 1
da serves: 0.41514489177183955, 1
serves unified: 0.41514489177183955, 1
unified framework: 0.41514489177183955, 1
framework fuse: 0.41514489177183955, 1
fuse multi: 0.41514489177183955, 1
modal data: 0.41514489177183955, 1
data retrieve: 0.41514489177183955, 1
retrieve entire: 0.41514489177183955, 1
entire missing: 0.41514489177183955, 1
missing modality: 0.41514489177183955, 1
modality feature: 0.41514489177183955, 1
within modality: 0.41514489177183955, 1
modality given: 0.41514489177183955, 1
given information: 0.41514489177183955, 1
information major: 0.41514489177183955, 1
major since: 0.41514489177183955, 1
since train: 0.41514489177183955, 1
train da: 0.41514489177183955, 1
da done: 0.41514489177183955, 1
done continuous: 0.41514489177183955, 1
continuous discretize: 0.41514489177183955, 1
discretize dataset: 0.41514489177183955, 1
dataset higher: 0.41514489177183955, 1
higher accuracies: 0.41514489177183955, 1
accuracies inference: 0.41514489177183955, 1
inference achieved: 0.41514489177183955, 1
achieved respect: 0.41514489177183955, 1
respect approach: 0.41514489177183955, 1
approach data: 0.41514489177183955, 1
data discretization: 0.41514489177183955, 1
discretization required: 0.41514489177183955, 1
required bayesian: 0.41514489177183955, 1
bayesian fixing: 0.41514489177183955, 1
fixing structure: 0.41514489177183955, 1
structure knowledge: 0.41514489177183955, 1
knowledge added: 0.41514489177183955, 1
added incrementally: 0.41514489177183955, 1
incrementally making: 0.41514489177183955, 1
making architecture: 0.41514489177183955, 1
architecture particularly: 0.41514489177183955, 1
particularly useful: 0.41514489177183955, 1
useful online: 0.41514489177183955, 1
evaluation scores: 0.41514489177183955, 1
scores real: 0.41514489177183955, 1
real simulate: 0.41514489177183955, 1
robot experiments: 0.41514489177183955, 1
experiments improvements: 0.41514489177183955, 1
improvements previous: 0.41514489177183955, 1
previous approach: 0.41514489177183955, 1
approach while: 0.41514489177183955, 1
while model: 0.41514489177183955, 1
model applied: 0.41514489177183955, 1
applied wider: 0.41514489177183955, 1
wider range: 0.41514489177183955, 1
paper study: 0.41514489177183955, 1
study mechanism: 0.41514489177183955, 1
mechanism produce: 0.41514489177183955, 1
produce hierarchical: 0.41514489177183955, 1
hierarchical structuring: 0.41514489177183955, 1
structuring affordance: 0.41514489177183955, 1
task different: 0.41514489177183955, 1
levels guided: 0.41514489177183955, 1
intrinsic system: 0.41514489177183955, 1
system detects: 0.41514489177183955, 1
detects easy: 0.41514489177183955, 1
task learn: 0.41514489177183955, 1
learn selected: 0.41514489177183955, 1
selected environment: 0.41514489177183955, 1
environment maximally: 0.41514489177183955, 1
maximally different: 0.41514489177183955, 1
different previously: 0.41514489177183955, 1
previously encountered: 0.41514489177183955, 1
encountered easy: 0.41514489177183955, 1
learned observed: 0.41514489177183955, 1
observed low: 0.41514489177183955, 1
attributes provide: 0.41514489177183955, 1
provide abstractions: 0.41514489177183955, 1
abstractions learning: 0.41514489177183955, 1
system shifts: 0.41514489177183955, 1
shifts focus: 0.41514489177183955, 1
focus starts: 0.41514489177183955, 1
starts learning: 0.41514489177183955, 1
learning harder: 0.41514489177183955, 1
harder task: 0.41514489177183955, 1
task low: 0.41514489177183955, 1
attributes previously: 0.41514489177183955, 1
learned abstract: 0.41514489177183955, 1
abstract hard: 0.41514489177183955, 1
task autonomously: 0.41514489177183955, 1
autonomously placed: 0.41514489177183955, 1
placed higher: 0.41514489177183955, 1
higher hierarchy: 0.41514489177183955, 1
hierarchy easy: 0.41514489177183955, 1
task concept: 0.41514489177183955, 1
concept identified: 0.41514489177183955, 1
identified distinctive: 0.41514489177183955, 1
distinctive input: 0.41514489177183955, 1
input attributes: 0.41514489177183955, 1
attributes hard: 0.41514489177183955, 1
hard abstract: 0.41514489177183955, 1
abstract concept: 0.41514489177183955, 1
concept allows: 0.41514489177183955, 1
allows hard: 0.41514489177183955, 1
learned faster: 0.41514489177183955, 1
faster learning: 0.41514489177183955, 1
learning low: 0.41514489177183955, 1
level perception: 0.41514489177183955, 1
perception tested: 0.41514489177183955, 1
tested system: 0.41514489177183955, 1
system task: 0.41514489177183955, 1
task learning: 0.41514489177183955, 1
predict poke: 0.41514489177183955, 1
action dataset: 0.41514489177183955, 1
83 real: 0.41514489177183955, 1
world basis: 0.41514489177183955, 1
basis large: 0.41514489177183955, 1
large number: 0.41514489177183955, 1
number runs: 0.41514489177183955, 1
runs analysis: 0.41514489177183955, 1
analysis shows: 0.41514489177183955, 1
shows hierarchical: 0.41514489177183955, 1
hierarchical task: 0.41514489177183955, 1
task structure: 0.41514489177183955, 1
structure emerged: 0.41514489177183955, 1
emerged along: 0.41514489177183955, 1
along consistent: 0.41514489177183955, 1
consistent learning: 0.41514489177183955, 1
learning significant: 0.41514489177183955, 1
significant bootstrapping: 0.41514489177183955, 1
bootstrapping effect: 0.41514489177183955, 1
effect learning: 0.41514489177183955, 1
learning speed: 0.41514489177183955, 1
speed stack: 0.41514489177183955, 1
action observed: 0.41514489177183955, 1
observed discovered: 0.41514489177183955, 1
discovered albeit: 0.41514489177183955, 1
albeit fully: 0.41514489177183955, 1
fully learned: 0.41514489177183955, 1
learned poke: 0.41514489177183955, 1
poke action: 0.41514489177183955, 1
learn interact: 0.41514489177183955, 1
interact object: 0.41514489177183955, 1
object developing: 0.41514489177183955, 1
developing computational: 0.41514489177183955, 1
model paper: 0.41514489177183955, 1
learning operation: 0.41514489177183955, 1
operation occur: 0.41514489177183955, 1
occur toward: 0.41514489177183955, 1
achieving lifelong: 0.41514489177183955, 1
affordance regime: 0.41514489177183955, 1
regime robot: 0.41514489177183955, 1
learn without: 0.41514489177183955, 1
without general: 0.41514489177183955, 1
general rule: 0.41514489177183955, 1
rule robot: 0.41514489177183955, 1
learn everything: 0.41514489177183955, 1
everything environment: 0.41514489177183955, 1
environment determine: 0.41514489177183955, 1
determine sensorimotor: 0.41514489177183955, 1
sensorimotor coordination: 0.41514489177183955, 1
coordination modeled: 0.41514489177183955, 1
modeled distributed: 0.41514489177183955, 1
distributed semi: 0.41514489177183955, 1
semi markov: 0.41514489177183955, 1
decision created: 0.41514489177183955, 1
created online: 0.41514489177183955, 1
online during: 0.41514489177183955, 1
robot performs: 0.41514489177183955, 1
performs continual: 0.41514489177183955, 1
continual action: 0.41514489177183955, 1
selection reach: 0.41514489177183955, 1
reach goal: 0.41514489177183955, 1
goal initial: 0.41514489177183955, 1
initial experiment: 0.41514489177183955, 1
experiment model: 0.41514489177183955, 1
capture exploited: 0.41514489177183955, 1
exploited perform: 0.41514489177183955, 1
perform several: 0.41514489177183955, 1
several different: 0.41514489177183955, 1
task mobile: 0.41514489177183955, 1
equipped gripper: 0.41514489177183955, 1
gripper infrared: 0.41514489177183955, 1
infrared secondary: 0.41514489177183955, 1
secondary robot: 0.41514489177183955, 1
learn marker: 0.41514489177183955, 1
marker visual: 0.41514489177183955, 1
feature gripped: 0.41514489177183955, 1
gripped walls: 0.41514489177183955, 1
walls floor: 0.41514489177183955, 1
floor affordance: 0.41514489177183955, 1
affordance being: 0.41514489177183955, 1
being distributed: 0.41514489177183955, 1
distributed mechanism: 0.41514489177183955, 1
mechanism necessary: 0.41514489177183955, 1
necessary modeling: 0.41514489177183955, 1
modeling multiple: 0.41514489177183955, 1
multiple sensory: 0.41514489177183955, 1
stimuli selection: 0.41514489177183955, 1
selection object: 0.41514489177183955, 1
object necessary: 0.41514489177183955, 1
necessary affordance: 0.41514489177183955, 1
affordance task: 0.41514489177183955, 1
task emergent: 0.41514489177183955, 1
emergent while: 0.41514489177183955, 1
while parts: 0.41514489177183955, 1
parts environment: 0.41514489177183955, 1
environment walls: 0.41514489177183955, 1
focuses robot: 0.41514489177183955, 1
robot deployed: 0.41514489177183955, 1
deployed human: 0.41514489177183955, 1
human specialized: 0.41514489177183955, 1
specialized object: 0.41514489177183955, 1
manipulation leverage: 0.41514489177183955, 1
leverage end: 0.41514489177183955, 1
end users: 0.41514489177183955, 1
users efficiently: 0.41514489177183955, 1
efficiently learn: 0.41514489177183955, 1
object approach: 0.41514489177183955, 1
approach promising: 0.41514489177183955, 1
promising naturally: 0.41514489177183955, 1
naturally focus: 0.41514489177183955, 1
focus showing: 0.41514489177183955, 1
showing salient: 0.41514489177183955, 1
salient aspects: 0.41514489177183955, 1
aspects object: 0.41514489177183955, 1
object replicate: 0.41514489177183955, 1
replicate prior: 0.41514489177183955, 1
prior result: 0.41514489177183955, 1
result build: 0.41514489177183955, 1
build create: 0.41514489177183955, 1
create combination: 0.41514489177183955, 1
combination self: 0.41514489177183955, 1
supervised present: 0.41514489177183955, 1
present experimental: 0.41514489177183955, 1
result robot: 0.41514489177183955, 1
learning 5: 0.41514489177183955, 1
5 affordance: 0.41514489177183955, 1
affordance 4: 0.41514489177183955, 1
4 object: 0.41514489177183955, 1
object 1219: 0.41514489177183955, 1
1219 compare: 0.41514489177183955, 1
three learning: 0.41514489177183955, 1
learning self: 0.41514489177183955, 1
learning supervised: 0.41514489177183955, 1
supervised examples: 0.41514489177183955, 1
examples provided: 0.41514489177183955, 1
provided 10: 0.41514489177183955, 1
10 self: 0.41514489177183955, 1
exploration biased: 0.41514489177183955, 1
biased user: 0.41514489177183955, 1
user result: 0.41514489177183955, 1
result characterize: 0.41514489177183955, 1
characterize benefits: 0.41514489177183955, 1
benefits self: 0.41514489177183955, 1
learning combined: 0.41514489177183955, 1
combined approach: 0.41514489177183955, 1
approach efficient: 0.41514489177183955, 1
present visual: 0.41514489177183955, 1
visual robot: 0.41514489177183955, 1
robot whose: 0.41514489177183955, 1
whose associated: 0.41514489177183955, 1
associated neural: 0.41514489177183955, 1
neural controller: 0.41514489177183955, 1
controller develops: 0.41514489177183955, 1
develops realistic: 0.41514489177183955, 1
realistic perception: 0.41514489177183955, 1
perception controller: 0.41514489177183955, 1
controller uses: 0.41514489177183955, 1
uses known: 0.41514489177183955, 1
known insect: 0.41514489177183955, 1
insect brain: 0.41514489177183955, 1
brain particularly: 0.41514489177183955, 1
particularly stabilized: 0.41514489177183955, 1
sparse code: 0.41514489177183955, 1
code communication: 0.41514489177183955, 1
communication antennal: 0.41514489177183955, 1
antennal lobe: 0.41514489177183955, 1
lobe mushroom: 0.41514489177183955, 1
mushroom robot: 0.41514489177183955, 1
robot perceives: 0.41514489177183955, 1
perceives world: 0.41514489177183955, 1
world webcam: 0.41514489177183955, 1
webcam canny: 0.41514489177183955, 1
canny border: 0.41514489177183955, 1
border opencv: 0.41514489177183955, 1
opencv self: 0.41514489177183955, 1
self controlled: 0.41514489177183955, 1
controlled neural: 0.41514489177183955, 1
agent process: 0.41514489177183955, 1
process massive: 0.41514489177183955, 1
massive raw: 0.41514489177183955, 1
raw data: 0.41514489177183955, 1
data produce: 0.41514489177183955, 1
produce stabilized: 0.41514489177183955, 1
sparse implicit: 0.41514489177183955, 1
implicit space: 0.41514489177183955, 1
space information: 0.41514489177183955, 1
information preprocessed: 0.41514489177183955, 1
preprocessed information: 0.41514489177183955, 1
information relayed: 0.41514489177183955, 1
relayed population: 0.41514489177183955, 1
population neural: 0.41514489177183955, 1
agent specialized: 0.41514489177183955, 1
specialized cognitive: 0.41514489177183955, 1
cognitive activities: 0.41514489177183955, 1
activities train: 0.41514489177183955, 1
train under: 0.41514489177183955, 1
under self: 0.41514489177183955, 1
self critical: 0.41514489177183955, 1
critical isolated: 0.41514489177183955, 1
isolated isolation: 0.41514489177183955, 1
isolation induces: 0.41514489177183955, 1
induces emergent: 0.41514489177183955, 1
emergent behavior: 0.41514489177183955, 1
behavior makes: 0.41514489177183955, 1
makes possible: 0.41514489177183955, 1
possible invariant: 0.41514489177183955, 1
invariant visual: 0.41514489177183955, 1
recognition later: 0.41514489177183955, 1
later capacity: 0.41514489177183955, 1
capacity assembled: 0.41514489177183955, 1
assembled cognitive: 0.41514489177183955, 1
cognitive strings: 0.41514489177183955, 1
strings incorporate: 0.41514489177183955, 1
incorporate elapse: 0.41514489177183955, 1
elapse learning: 0.41514489177183955, 1
learning resources: 0.41514489177183955, 1
resources assembled: 0.41514489177183955, 1
assembled capacity: 0.41514489177183955, 1
capacity during: 0.41514489177183955, 1
during extended: 0.41514489177183955, 1
extended learning: 0.41514489177183955, 1
learning period: 0.41514489177183955, 1
period robot: 0.41514489177183955, 1
robot finally: 0.41514489177183955, 1
finally achieves: 0.41514489177183955, 1
achieves perception: 0.41514489177183955, 1
system tested: 0.41514489177183955, 1
real real: 0.41514489177183955, 1
describe technique: 0.41514489177183955, 1
technique build: 0.41514489177183955, 1
build affordance: 0.41514489177183955, 1
map interactively: 0.41514489177183955, 1
interactively robot: 0.41514489177183955, 1
affordance predicted: 0.41514489177183955, 1
predicted train: 0.41514489177183955, 1
train classifier: 0.41514489177183955, 1
classifier geometric: 0.41514489177183955, 1
feature extracted: 0.41514489177183955, 1
extracted based: 0.41514489177183955, 1
based 2d: 0.41514489177183955, 1
2d occupancy: 0.41514489177183955, 1
occupancy markov: 0.41514489177183955, 1
markov random: 0.41514489177183955, 1
field model: 0.41514489177183955, 1
model builds: 0.41514489177183955, 1
builds affordance: 0.41514489177183955, 1
map relational: 0.41514489177183955, 1
affordance neighboring: 0.41514489177183955, 1
neighboring quality: 0.41514489177183955, 1
quality affordance: 0.41514489177183955, 1
map refined: 0.41514489177183955, 1
refined sequences: 0.41514489177183955, 1
sequences interactive: 0.41514489177183955, 1
interactive manipulations: 0.41514489177183955, 1
manipulations selected: 0.41514489177183955, 1
selected model: 0.41514489177183955, 1
model yield: 0.41514489177183955, 1
yield highest: 0.41514489177183955, 1
highest reduction: 0.41514489177183955, 1
imagining: 0.4151156232374507, 2
explaining: 0.4151156232374507, 2
text: 0.4151156232374507, 2
influencing: 0.4151156232374507, 2
beliefs: 0.4151156232374507, 2
increase: 0.4151156232374507, 2
strategies: 0.4151156232374507, 2
cultural: 0.4151156232374507, 2
internet: 0.4151156232374507, 2
advancements: 0.4151156232374507, 2
right: 0.4151156232374507, 2
replay: 0.4151156232374507, 2
regular: 0.4151156232374507, 2
span: 0.4151156232374507, 2
playing: 0.4151156232374507, 2
encoding: 0.4151156232374507, 2
count: 0.4151156232374507, 2
agnostic: 0.4151156232374507, 2
meta: 0.4151156232374507, 2
theoretic: 0.4151156232374507, 2
off: 0.4151156232374507, 2
experiential: 0.4151156232374507, 2
intuitive: 0.4151156232374507, 2
plans: 0.4151156232374507, 2
nets: 0.4151156232374507, 2
distance: 0.4151156232374507, 2
eeg: 0.4151156232374507, 2
historical: 0.4151156232374507, 2
trees: 0.4151156232374507, 2
prospects: 0.4151156232374507, 2
convergence: 0.4151156232374507, 2
diversity: 0.4151156232374507, 2
pedagogical: 0.4151156232374507, 2
choice: 0.4151156232374507, 2
recommendation: 0.4151156232374507, 2
heterogeneous: 0.4151156232374507, 2
peg: 0.4151156232374507, 2
architectural: 0.4151156232374507, 2
matching: 0.4151156232374507, 2
uncertainty: 0.4151156232374507, 2
navigate: 0.4151156232374507, 2
call: 0.4151156232374507, 2
sofm: 0.4151156232374507, 2
facilitation: 0.4151156232374507, 2
steps: 0.4151156232374507, 2
prospective: 0.4151156232374507, 2
exploratory: 0.4151156232374507, 2
particle: 0.4151156232374507, 2
velocimetry: 0.4151156232374507, 2
leaf: 0.4151156232374507, 2
growth: 0.4151156232374507, 2
plant: 0.4151156232374507, 2
turing: 0.4151156232374507, 2
add: 0.4151156232374507, 2
analyzing: 0.4151156232374507, 2
descriptions: 0.4151156232374507, 2
ontological: 0.4151156232374507, 2
biomedical: 0.4151156232374507, 2
densities: 0.4151156232374507, 2
modified: 0.4151156232374507, 2
decisions: 0.4151156232374507, 2
determining: 0.4151156232374507, 2
coordinate: 0.4151156232374507, 2
adjectives: 0.4151156232374507, 2
nouns: 0.4151156232374507, 2
abstractions: 0.4151156232374507, 2
inferring: 0.4151156232374507, 2
flexible: 0.4151156232374507, 2
contemporary: 0.4151156232374507, 2
residual: 0.4151156232374507, 2
segments: 0.4151156232374507, 2
interdependent: 0.4151156232374507, 2
plasticity: 0.4151156232374507, 2
pairs: 0.4151156232374507, 2
taxonomy: 0.4151156232374507, 2
philosophy: 0.4151156232374507, 2
implementing: 0.4151156232374507, 2
around: 0.4151156232374507, 2
2006: 0.4151156232374507, 2
2012: 0.4151156232374507, 2
pattern: 0.4151156232374507, 2
2014: 0.4151156232374507, 2
reactive: 0.4151156232374507, 2
individual: 0.4151156232374507, 2
methodology: 0.4151156232374507, 2
50: 0.4151156232374507, 2
stream: 0.4151156232374507, 2
mappings: 0.4151156232374507, 2
cascaded: 0.4151156232374507, 2
description: 0.4151156232374507, 2
reconstruction: 0.4151156232374507, 2
channel: 0.4151156232374507, 2
ego: 0.4151156232374507, 2
siamese: 0.4151156232374507, 2
passive: 0.4151156232374507, 2
matrix: 0.4151156232374507, 2
product: 0.4151156232374507, 2
equations: 0.4151156232374507, 2
standards: 0.4151156232374507, 2
constraint: 0.4151156232374507, 2
verification: 0.4151156232374507, 2
interpretation: 0.4151156232374507, 2
move: 0.4151156232374507, 2
considered: 0.4151156232374507, 2
animate: 0.4151156232374507, 2
successes: 0.4151156232374507, 2
disentangling: 0.4151156232374507, 2
mathematical: 0.4151156232374507, 2
mixed: 0.4151156232374507, 2
macs: 0.4151156232374507, 2
representing: 0.4151156232374507, 2
changes: 0.4151156232374507, 2
bio: 0.4151156232374507, 2
dominant: 0.4151156232374507, 2
contact: 0.4151156232374507, 2
occluded: 0.4151156232374507, 2
staged: 0.4151156232374507, 2
motionese: 0.4151156232374507, 2
assimilation: 0.4151156232374507, 2
manipulators: 0.4151156232374507, 2
gaussian: 0.4151156232374507, 2
consummatory: 0.4151156232374507, 2
primitive: 0.4151156232374507, 2
similarities: 0.4151156232374507, 2
line: 0.4151156232374507, 2
abstraction: 0.4151156232374507, 2
web: 0.4151156232374507, 2
collision: 0.4151156232374507, 2
risk: 0.4151156232374507, 2
cycle: 0.4151156232374507, 2
denoising: 0.4151156232374507, 2
formulation: 0.4151156232374507, 2
manipulator: 0.4151156232374507, 2
attribute: 0.4151156232374507, 2
anthropomorphic: 0.4151156232374507, 2
lifelong: 0.4151156232374507, 2
logic: 0.4151156232374507, 2
maneuvering: 0.4151156232374507, 2
formalism: 0.4151156232374507, 2
flying: 0.4151156232374507, 2
approximate: 0.4151156232374507, 2
scalable: 0.4151156232374507, 2
tensor: 0.4151156232374507, 2
affect: 0.4151156232374507, 2
synergies: 0.4151156232374507, 2
dof: 0.4151156232374507, 2
comparative: 0.4151156232374507, 2
coupling: 0.4151156232374507, 2
philosophical: 0.4151156232374507, 2
assisted: 0.4151156232374507, 2
simulating: 0.4151156232374507, 2
total: 0.4151156232374507, 2
chairs: 0.4151156232374507, 2
warping: 0.4151156232374507, 2
pascal: 0.4151156232374507, 2
legacy: 0.4151156232374507, 2
obstacle: 0.4151156232374507, 2
avoidance: 0.4151156232374507, 2
kitti: 0.4151156232374507, 2
frustratingly: 0.4151156232374507, 2
helicopter: 0.4151156232374507, 2
correspondence: 0.4151156232374507, 2
supersizing: 0.4151156232374507, 2
hours: 0.4151156232374507, 2
morphing: 0.4151156232374507, 2
style: 0.4151156232374507, 2
doom: 0.4151156232374507, 2
type: 0.4151156232374507, 2
2: 0.4151156232374507, 2
overview: 0.4151156232374507, 2
completion: 0.4151156232374507, 2
addressing: 0.4151156232374507, 2
randomization: 0.4151156232374507, 2
grammars: 0.4151156232374507, 2
refinement: 0.4151156232374507, 2
generalizable: 0.4151156232374507, 2
vr: 0.4151156232374507, 2
goggles: 0.4151156232374507, 2
sim: 0.4151156232374507, 2
eye: 0.4151156232374507, 2
alignment: 0.4151156232374507, 2
let: 0.4151156232374507, 2
quantitative: 0.4151156232374507, 2
age: 0.4151156232374507, 2
organizing: 0.4151156232374507, 2
stimulus: 0.4151156232374507, 2
compatibility: 0.4151156232374507, 2
events: 0.4151156232374507, 2
playground: 0.4151156232374507, 2
wayfinding: 0.4151156232374507, 2
introduction: 0.4151156232374507, 2
environmentally: 0.4151156232374507, 2
intuition: 0.4151156232374507, 2
talent: 0.4151156232374507, 2
indirect: 0.4151156232374507, 2
semiotics: 0.4151156232374507, 2
behaviour: 0.4151156232374507, 2
ict: 0.4151156232374507, 2
detail: 0.4151156232374507, 2
reconciling: 0.4151156232374507, 2
sharing: 0.4151156232374507, 2
creating: 0.4151156232374507, 2
abduction: 0.4151156232374507, 2
rough: 0.4151156232374507, 2
granular: 0.4151156232374507, 2
16: 0.4151156232374507, 2
association: 0.4151156232374507, 2
construction: 0.4151156232374507, 2
behavioral: 0.4151156232374507, 2
significance: 0.4151156232374507, 2
activation: 0.4151156232374507, 2
emergency: 0.4151156232374507, 2
evacuation: 0.4151156232374507, 2
kind: 0.4151156232374507, 2
discovering: 0.4151156232374507, 2
instance: 0.4151156232374507, 2
contributions: 0.4151156232374507, 2
advice: 0.4151156232374507, 2
combining: 0.4151156232374507, 2
monitoring: 0.4151156232374507, 2
solution: 0.4151156232374507, 2
q: 0.4151156232374507, 2
industrial: 0.4151156232374507, 2
areas: 0.4151156232374507, 2
wild: 0.4151156232374507, 2
prosthetic: 0.4151156232374507, 2
moral: 0.4151156232374507, 2
distinct: 0.4151156232374507, 2
clustering: 0.4151156232374507, 2
reducing: 0.4151156232374507, 2
stereo: 0.4151156232374507, 2
quantum: 0.4151156232374507, 2
conservation: 0.4151156232374507, 2
discretize: 0.4151156232374507, 2
vq: 0.4151156232374507, 2
vae: 0.4151156232374507, 2
feasibility: 0.4151156232374507, 2
assessing: 0.4151156232374507, 2
possess: 0.4151156232374507, 2
profound: 0.4151156232374507, 2
richness: 0.4151156232374507, 2
combines: 0.4151156232374507, 2
involving: 0.4151156232374507, 2
variation: 0.4151156232374507, 2
identifies: 0.4151156232374507, 2
trajectories: 0.4151156232374507, 2
geometries: 0.4151156232374507, 2
targeted: 0.4151156232374507, 2
deliberate: 0.4151156232374507, 2
encounters: 0.4151156232374507, 2
potentially: 0.4151156232374507, 2
doing: 0.4151156232374507, 2
examine: 0.4151156232374507, 2
increasingly: 0.4151156232374507, 2
seen: 0.4151156232374507, 2
often: 0.4151156232374507, 2
modify: 0.4151156232374507, 2
growing: 0.4151156232374507, 2
fail: 0.4151156232374507, 2
lack: 0.4151156232374507, 2
unlike: 0.4151156232374507, 2
collected: 0.4151156232374507, 2
observe: 0.4151156232374507, 2
fine: 0.4151156232374507, 2
analyze: 0.4151156232374507, 2
biases: 0.4151156232374507, 2
suggests: 0.4151156232374507, 2
co: 0.4151156232374507, 2
teach: 0.4151156232374507, 2
impacts: 0.4151156232374507, 2
investigated: 0.4151156232374507, 2
identified: 0.4151156232374507, 2
deterministic: 0.4151156232374507, 2
remains: 0.4151156232374507, 2
learnt: 0.4151156232374507, 2
incorporate: 0.4151156232374507, 2
autoregressive: 0.4151156232374507, 2
pairing: 0.4151156232374507, 2
providing: 0.4151156232374507, 2
repertoire: 0.4151156232374507, 2
regime: 0.4151156232374507, 2
itself: 0.4151156232374507, 2
accomplish: 0.4151156232374507, 2
setting: 0.4151156232374507, 2
frequently: 0.4151156232374507, 2
topic: 0.4151156232374507, 2
greater: 0.4151156232374507, 2
difficulties: 0.4151156232374507, 2
localize: 0.4151156232374507, 2
assign: 0.4151156232374507, 2
probable: 0.4151156232374507, 2
multiclass: 0.4151156232374507, 2
deconvolutional: 0.4151156232374507, 2
resizing: 0.4151156232374507, 2
150ms: 0.4151156232374507, 2
constitutes: 0.4151156232374507, 2
namely: 0.4151156232374507, 2
fused: 0.4151156232374507, 2
aforementioned: 0.4151156232374507, 2
surpassing: 0.4151156232374507, 2
neuro: 0.4151156232374507, 2
biologically: 0.4151156232374507, 2
utilize: 0.4151156232374507, 2
popular: 0.4151156232374507, 2
regarded: 0.4151156232374507, 2
developments: 0.4151156232374507, 2
discussed: 0.4151156232374507, 2
summarize: 0.4151156232374507, 2
final: 0.4151156232374507, 2
direction: 0.4151156232374507, 2
definition: 0.4151156232374507, 2
include: 0.4151156232374507, 2
dominating: 0.4151156232374507, 2
healthcare: 0.4151156232374507, 2
equip: 0.4151156232374507, 2
sheds: 0.4151156232374507, 2
highlights: 0.4151156232374507, 2
extension: 0.4151156232374507, 2
originally: 0.4151156232374507, 2
representational: 0.4151156232374507, 2
heuristically: 0.4151156232374507, 2
determined: 0.4151156232374507, 2
emphasize: 0.4151156232374507, 2
argue: 0.4151156232374507, 2
react: 0.4151156232374507, 2
verify: 0.4151156232374507, 2
concrete: 0.4151156232374507, 2
implementation: 0.4151156232374507, 2
candidates: 0.4151156232374507, 2
successful: 0.4151156232374507, 2
highly: 0.4151156232374507, 2
contribution: 0.4151156232374507, 2
brought: 0.4151156232374507, 2
suffer: 0.4151156232374507, 2
induced: 0.4151156232374507, 2
done: 0.4151156232374507, 2
inferred: 0.4151156232374507, 2
highlight: 0.4151156232374507, 2
taken: 0.4151156232374507, 2
reason: 0.4151156232374507, 2
collect: 0.4151156232374507, 2
builds: 0.4151156232374507, 2
referred: 0.4151156232374507, 2
extensively: 0.4151156232374507, 2
assistants: 0.4151156232374507, 2
home: 0.4151156232374507, 2
acquire: 0.4151156232374507, 2
trainers: 0.4151156232374507, 2
crossmodal: 0.4151156232374507, 2
advises: 0.4151156232374507, 2
cleaning: 0.4151156232374507, 2
table: 0.4151156232374507, 2
serving: 0.4151156232374507, 2
completes: 0.4151156232374507, 2
although: 0.4151156232374507, 2
slowly: 0.4151156232374507, 2
fewer: 0.4151156232374507, 2
needed: 0.4151156232374507, 2
inconsistencies: 0.4151156232374507, 2
cause: 0.4151156232374507, 2
delay: 0.4151156232374507, 2
combine: 0.4151156232374507, 2
dimensional: 0.4151156232374507, 2
restricted: 0.4151156232374507, 2
lights: 0.4151156232374507, 2
probability: 0.4151156232374507, 2
thesis: 0.4151156232374507, 2
occlusion: 0.4151156232374507, 2
false: 0.4151156232374507, 2
exploited: 0.4151156232374507, 2
tend: 0.4151156232374507, 2
presence: 0.4151156232374507, 2
induces: 0.4151156232374507, 2
hierarchy: 0.4151156232374507, 2
seconds: 0.4151156232374507, 2
efficiency: 0.4151156232374507, 2
distractors: 0.4151156232374507, 2
encountered: 0.4151156232374507, 2
actor: 0.4151156232374507, 2
critic: 0.4151156232374507, 2
access: 0.4151156232374507, 2
experimentally: 0.4151156232374507, 2
interactively: 0.4151156232374507, 2
biasing: 0.4151156232374507, 2
biased: 0.4151156232374507, 2
component: 0.4151156232374507, 2
layer: 0.4151156232374507, 2
operation: 0.4151156232374507, 2
forms: 0.4151156232374507, 2
manipulated: 0.4151156232374507, 2
participants: 0.4151156232374507, 2
rewarded: 0.4151156232374507, 2
extinction: 0.4151156232374507, 2
subject: 0.4151156232374507, 2
despite: 0.4151156232374507, 2
bumps: 0.4151156232374507, 2
drop: 0.4151156232374507, 2
disturbances: 0.4151156232374507, 2
needs: 0.4151156232374507, 2
outperform: 0.4151156232374507, 2
severe: 0.4151156232374507, 2
correctly: 0.4151156232374507, 2
minimize: 0.4151156232374507, 2
evaluations: 0.4151156232374507, 2
starting: 0.4151156232374507, 2
easily: 0.4151156232374507, 2
shift: 0.4151156232374507, 2
looking: 0.4151156232374507, 2
amount: 0.4151156232374507, 2
attended: 0.4151156232374507, 2
subjects: 0.4151156232374507, 2
complete: 0.4151156232374507, 2
difficult: 0.4151156232374507, 2
effort: 0.4151156232374507, 2
begin: 0.4151156232374507, 2
guide: 0.4151156232374507, 2
fraction: 0.4151156232374507, 2
formulate: 0.4151156232374507, 2
velocities: 0.4151156232374507, 2
lead: 0.4151156232374507, 2
functionalities: 0.4151156232374507, 2
boxes: 0.4151156232374507, 2
supports: 0.4151156232374507, 2
9: 0.4151156232374507, 2
strongly: 0.4151156232374507, 2
missing: 0.4151156232374507, 2
extracted: 0.4151156232374507, 2
supported: 0.4151156232374507, 2
yield: 0.4151156232374507, 2
modules: 0.4151156232374507, 2
aleatoric: 0.4151156232374507, 2
reveals: 0.4151156232374507, 2
truly: 0.4151156232374507, 2
densely: 0.4151156232374507, 2
manner: 0.4151156232374507, 2
incomplete: 0.4151156232374507, 2
harder: 0.4151156232374507, 2
competing: 0.4151156232374507, 2
involve: 0.4151156232374507, 2
manual: 0.4151156232374507, 2
largely: 0.4151156232374507, 2
texture: 0.4151156232374507, 2
overall: 0.4151156232374507, 2
surpass: 0.4151156232374507, 2
regard: 0.4151156232374507, 2
displacements: 0.4151156232374507, 2
advance: 0.4151156232374507, 2
improvements: 0.4151156232374507, 2
caused: 0.4151156232374507, 2
original: 0.4151156232374507, 2
installation: 0.4151156232374507, 2
old: 0.4151156232374507, 2
phillipp: 0.4151156232374507, 2
amongst: 0.4151156232374507, 2
apas: 0.4151156232374507, 2
proximity: 0.4151156232374507, 2
sensor: 0.4151156232374507, 2
project: 0.4151156232374507, 2
report: 0.4151156232374507, 2
rules: 0.4151156232374507, 2
usually: 0.4151156232374507, 2
assume: 0.4151156232374507, 2
link: 0.4151156232374507, 2
though: 0.4151156232374507, 2
critical: 0.4151156232374507, 2
wheeled: 0.4151156232374507, 2
terrain: 0.4151156232374507, 2
prototype: 0.4151156232374507, 2
validate: 0.4151156232374507, 2
pan: 0.4151156232374507, 2
tilt: 0.4151156232374507, 2
telescopic: 0.4151156232374507, 2
provided: 0.4151156232374507, 2
according: 0.4151156232374507, 2
entire: 0.4151156232374507, 2
drive: 0.4151156232374507, 2
bootstrap: 0.4151156232374507, 2
execute: 0.4151156232374507, 2
83: 0.4151156232374507, 2
abundant: 0.4151156232374507, 2
quantitatively: 0.4151156232374507, 2
led: 0.4151156232374507, 2
shifts: 0.4151156232374507, 2
stages: 0.4151156232374507, 2
annotating: 0.4151156232374507, 2
already: 0.4151156232374507, 2
whether: 0.4151156232374507, 2
transferred: 0.4151156232374507, 2
cracking: 0.4151156232374507, 2
hammer: 0.4151156232374507, 2
consisting: 0.4151156232374507, 2
picks: 0.4151156232374507, 2
zones: 0.4151156232374507, 2
decomposes: 0.4151156232374507, 2
focuses: 0.4151156232374507, 2
ycb: 0.4151156232374507, 2
coherent: 0.4151156232374507, 2
contiguous: 0.4151156232374507, 2
subsequent: 0.4151156232374507, 2
labeled: 0.4151156232374507, 2
socially: 0.4151156232374507, 2
practical: 0.4151156232374507, 2
desirable: 0.4151156232374507, 2
tackles: 0.4151156232374507, 2
systematically: 0.4151156232374507, 2
least: 0.4151156232374507, 2
determines: 0.4151156232374507, 2
proxies: 0.4151156232374507, 2
pr2: 0.4151156232374507, 2
selecting: 0.4151156232374507, 2
plays: 0.4151156232374507, 2
showing: 0.4151156232374507, 2
measures: 0.4151156232374507, 2
postures: 0.4151156232374507, 2
bn: 0.4151156232374507, 2
extract: 0.4151156232374507, 2
occur: 0.4151156232374507, 2
da: 0.4151156232374507, 2
modality: 0.4151156232374507, 2
walls: 0.4151156232374507, 2
stabilized: 0.4151156232374507, 2
sparse: 0.4151156232374507, 2
capacity: 0.4151156232374507, 2
anticipate: 0.20755781161872536, 1
probing: 0.20755781161872536, 1
collective: 0.20755781161872536, 1
pomdp: 0.20755781161872536, 1
priming: 0.20755781161872536, 1
empathy: 0.20755781161872536, 1
visualizing: 0.20755781161872536, 1
weaknesses: 0.20755781161872536, 1
whatever: 0.20755781161872536, 1
disjuncture: 0.20755781161872536, 1
difference: 0.20755781161872536, 1
economy: 0.20755781161872536, 1
principal: 0.20755781161872536, 1
thin: 0.20755781161872536, 1
plate: 0.20755781161872536, 1
splines: 0.20755781161872536, 1
decomposition: 0.20755781161872536, 1
deformations: 0.20755781161872536, 1
hippocampus: 0.20755781161872536, 1
analogies: 0.20755781161872536, 1
associations: 0.20755781161872536, 1
stalking: 0.20755781161872536, 1
elusive: 0.20755781161872536, 1
constructive: 0.20755781161872536, 1
remembering: 0.20755781161872536, 1
interplay: 0.20755781161872536, 1
latest: 0.20755781161872536, 1
tc: 0.20755781161872536, 1
uncovering: 0.20755781161872536, 1
zsar: 0.20755781161872536, 1
teacher: 0.20755781161872536, 1
incremental: 0.20755781161872536, 1
re: 0.20755781161872536, 1
dexterous: 0.20755781161872536, 1
encodings: 0.20755781161872536, 1
fare: 0.20755781161872536, 1
atari: 0.20755781161872536, 1
bayes: 0.20755781161872536, 1
biped: 0.20755781161872536, 1
walking: 0.20755781161872536, 1
quadrupedal: 0.20755781161872536, 1
imagenet: 0.20755781161872536, 1
mastering: 0.20755781161872536, 1
pixelcnn: 0.20755781161872536, 1
decoders: 0.20755781161872536, 1
slow: 0.20755781161872536, 1
mpc: 0.20755781161872536, 1
bidirectional: 0.20755781161872536, 1
transformers: 0.20755781161872536, 1
parametric: 0.20755781161872536, 1
rewards: 0.20755781161872536, 1
maximizing: 0.20755781161872536, 1
hindsight: 0.20755781161872536, 1
accelerating: 0.20755781161872536, 1
skew: 0.20755781161872536, 1
universal: 0.20755781161872536, 1
approximators: 0.20755781161872536, 1
incentivizing: 0.20755781161872536, 1
mahalanobis: 0.20755781161872536, 1
classifiers: 0.20755781161872536, 1
electrodes: 0.20755781161872536, 1
nineteenth: 0.20755781161872536, 1
century: 0.20755781161872536, 1
delinquency: 0.20755781161872536, 1
temporality: 0.20755781161872536, 1
ralph: 0.20755781161872536, 1
liberal: 0.20755781161872536, 1
mobilities: 0.20755781161872536, 1
crowdsourcing: 0.20755781161872536, 1
modern: 0.20755781161872536, 1
induction: 0.20755781161872536, 1
explainable: 0.20755781161872536, 1
responsible: 0.20755781161872536, 1
databases: 0.20755781161872536, 1
autonomic: 0.20755781161872536, 1
putting: 0.20755781161872536, 1
again: 0.20755781161872536, 1
constructivist: 0.20755781161872536, 1
myth: 0.20755781161872536, 1
explanation: 0.20755781161872536, 1
geographic: 0.20755781161872536, 1
costs: 0.20755781161872536, 1
n: 0.20755781161872536, 1
fault: 0.20755781161872536, 1
multiscale: 0.20755781161872536, 1
benchmarks: 0.20755781161872536, 1
molecular: 0.20755781161872536, 1
persistence: 0.20755781161872536, 1
offloading: 0.20755781161872536, 1
dependency: 0.20755781161872536, 1
pay: 0.20755781161872536, 1
robustifying: 0.20755781161872536, 1
neurophenomenology: 0.20755781161872536, 1
sketching: 0.20755781161872536, 1
talk: 0.20755781161872536, 1
dialogue: 0.20755781161872536, 1
navigability: 0.20755781161872536, 1
purposive: 0.20755781161872536, 1
primate: 0.20755781161872536, 1
following: 0.20755781161872536, 1
prehensile: 0.20755781161872536, 1
pathways: 0.20755781161872536, 1
anatomy: 0.20755781161872536, 1
pet: 0.20755781161872536, 1
keypoints: 0.20755781161872536, 1
pick: 0.20755781161872536, 1
acquiring: 0.20755781161872536, 1
sensitive: 0.20755781161872536, 1
cardioactive: 0.20755781161872536, 1
drugs: 0.20755781161872536, 1
statistics: 0.20755781161872536, 1
factory: 0.20755781161872536, 1
scheduling: 0.20755781161872536, 1
packet: 0.20755781161872536, 1
switched: 0.20755781161872536, 1
center: 0.20755781161872536, 1
mission: 0.20755781161872536, 1
organisms: 0.20755781161872536, 1
nested: 0.20755781161872536, 1
organizational: 0.20755781161872536, 1
structuration: 0.20755781161872536, 1
characterising: 0.20755781161872536, 1
broken: 0.20755781161872536, 1
rgbd: 0.20755781161872536, 1
logical: 0.20755781161872536, 1
manipulative: 0.20755781161872536, 1
symbolic: 0.20755781161872536, 1
iros: 0.20755781161872536, 1
microsoft: 0.20755781161872536, 1
0: 0.20755781161872536, 1
extracting: 0.20755781161872536, 1
refining: 0.20755781161872536, 1
joining: 0.20755781161872536, 1
exemplified: 0.20755781161872536, 1
handwriting: 0.20755781161872536, 1
cue: 0.20755781161872536, 1
chair: 0.20755781161872536, 1
workspace: 0.20755781161872536, 1
scenegrok: 0.20755781161872536, 1
proper: 0.20755781161872536, 1
handovers: 0.20755781161872536, 1
inter: 0.20755781161872536, 1
usage: 0.20755781161872536, 1
foraging: 0.20755781161872536, 1
moped: 0.20755781161872536, 1
stair: 0.20755781161872536, 1
nonlinear: 0.20755781161872536, 1
emulation: 0.20755781161872536, 1
fingered: 0.20755781161872536, 1
foot: 0.20755781161872536, 1
validation: 0.20755781161872536, 1
loco: 0.20755781161872536, 1
pushability: 0.20755781161872536, 1
liftability: 0.20755781161872536, 1
refine: 0.20755781161872536, 1
atrous: 0.20755781161872536, 1
connected: 0.20755781161872536, 1
crfs: 0.20755781161872536, 1
nesting: 0.20755781161872536, 1
2019: 0.20755781161872536, 1
pyramid: 0.20755781161872536, 1
parsing: 0.20755781161872536, 1
2013: 0.20755781161872536, 1
icar: 0.20755781161872536, 1
segment: 0.20755781161872536, 1
disparity: 0.20755781161872536, 1
differentiation: 0.20755781161872536, 1
pytorch: 0.20755781161872536, 1
ex: 0.20755781161872536, 1
paucis: 0.20755781161872536, 1
binge: 0.20755781161872536, 1
sitcoms: 0.20755781161872536, 1
deleted: 0.20755781161872536, 1
2011: 0.20755781161872536, 1
2017: 0.20755781161872536, 1
lstm: 0.20755781161872536, 1
precipitation: 0.20755781161872536, 1
nowcasting: 0.20755781161872536, 1
primal: 0.20755781161872536, 1
difficulty: 0.20755781161872536, 1
feedforward: 0.20755781161872536, 1
comparisons: 0.20755781161872536, 1
ranking: 0.20755781161872536, 1
foreground: 0.20755781161872536, 1
quaternionic: 0.20755781161872536, 1
dance: 0.20755781161872536, 1
performances: 0.20755781161872536, 1
mocap: 0.20755781161872536, 1
forward: 0.20755781161872536, 1
flora: 0.20755781161872536, 1
alpine: 0.20755781161872536, 1
histograms: 0.20755781161872536, 1
joints: 0.20755781161872536, 1
identity: 0.20755781161872536, 1
interactional: 0.20755781161872536, 1
targeting: 0.20755781161872536, 1
deepgaze: 0.20755781161872536, 1
reading: 0.20755781161872536, 1
isar: 0.20755781161872536, 1
devil: 0.20755781161872536, 1
gans: 0.20755781161872536, 1
tell: 0.20755781161872536, 1
multiviews: 0.20755781161872536, 1
viewpoints: 0.20755781161872536, 1
imperative: 0.20755781161872536, 1
hotspots: 0.20755781161872536, 1
filters: 0.20755781161872536, 1
parallelizable: 0.20755781161872536, 1
rpn: 0.20755781161872536, 1
liver: 0.20755781161872536, 1
ct: 0.20755781161872536, 1
normalization: 0.20755781161872536, 1
translationally: 0.20755781161872536, 1
porous: 0.20755781161872536, 1
medium: 0.20755781161872536, 1
equation: 0.20755781161872536, 1
heat: 0.20755781161872536, 1
flows: 0.20755781161872536, 1
absolutely: 0.20755781161872536, 1
unstable: 0.20755781161872536, 1
compression: 0.20755781161872536, 1
polynomial: 0.20755781161872536, 1
expansion: 0.20755781161872536, 1
extra: 0.20755781161872536, 1
receptive: 0.20755781161872536, 1
codes: 0.20755781161872536, 1
revisiting: 0.20755781161872536, 1
watching: 0.20755781161872536, 1
streams: 0.20755781161872536, 1
29: 0.20755781161872536, 1
coherency: 0.20755781161872536, 1
senses: 0.20755781161872536, 1
gravitational: 0.20755781161872536, 1
laws: 0.20755781161872536, 1
decomposing: 0.20755781161872536, 1
equivariant: 0.20755781161872536, 1
surgical: 0.20755781161872536, 1
team: 0.20755781161872536, 1
members: 0.20755781161872536, 1
initiative: 0.20755781161872536, 1
automaton: 0.20755781161872536, 1
decoupling: 0.20755781161872536, 1
dependable: 0.20755781161872536, 1
causally: 0.20755781161872536, 1
cumulative: 0.20755781161872536, 1
explorative: 0.20755781161872536, 1
categorizing: 0.20755781161872536, 1
locations: 0.20755781161872536, 1
orienting: 0.20755781161872536, 1
forming: 0.20755781161872536, 1
separating: 0.20755781161872536, 1
containers: 0.20755781161872536, 1
noncontainers: 0.20755781161872536, 1
neurodynamical: 0.20755781161872536, 1
pile: 0.20755781161872536, 1
propagation: 0.20755781161872536, 1
slippage: 0.20755781161872536, 1
symbol: 0.20755781161872536, 1
extending: 0.20755781161872536, 1
contingency: 0.20755781161872536, 1
hallucinated: 0.20755781161872536, 1
responding: 0.20755781161872536, 1
projecting: 0.20755781161872536, 1
intentionally: 0.20755781161872536, 1
indexed: 0.20755781161872536, 1
biomimetic: 0.20755781161872536, 1
assembly: 0.20755781161872536, 1
rehearses: 0.20755781161872536, 1
internally: 0.20755781161872536, 1
increasing: 0.20755781161872536, 1
stable: 0.20755781161872536, 1
visuo: 0.20755781161872536, 1
fill: 0.20755781161872536, 1
containability: 0.20755781161872536, 1
localizing: 0.20755781161872536, 1
substitute: 0.20755781161872536, 1
wrench: 0.20755781161872536, 1
spaces: 0.20755781161872536, 1
anticipative: 0.20755781161872536, 1
situ: 0.20755781161872536, 1
equivalences: 0.20755781161872536, 1
versatile: 0.20755781161872536, 1
altruistic: 0.20755781161872536, 1
fusion: 0.20755781161872536, 1
defense: 0.20755781161872536, 1
ended: 0.20755781161872536, 1
pertinent: 0.20755781161872536, 1
landscapes: 0.20755781161872536, 1
rediscovering: 0.20755781161872536, 1
aggressive: 0.20755781161872536, 1
pixelmpc: 0.20755781161872536, 1
readiness: 0.20755781161872536, 1
foundation: 0.20755781161872536, 1
collaborative: 0.20755781161872536, 1
toddlers: 0.20755781161872536, 1
6: 0.20755781161872536, 1
example: 0.20755781161872536, 1
catchableness: 0.20755781161872536, 1
fly: 0.20755781161872536, 1
adoption: 0.20755781161872536, 1
earth: 0.20755781161872536, 1
mixup: 0.20755781161872536, 1
rainfall: 0.20755781161872536, 1
satellite: 0.20755781161872536, 1
sensors: 0.20755781161872536, 1
forests: 0.20755781161872536, 1
msg: 0.20755781161872536, 1
seviri: 0.20755781161872536, 1
cellular: 0.20755781161872536, 1
automata: 0.20755781161872536, 1
forest: 0.20755781161872536, 1
simulations: 0.20755781161872536, 1
frameworks: 0.20755781161872536, 1
critically: 0.20755781161872536, 1
theatre: 0.20755781161872536, 1
inquiry: 0.20755781161872536, 1
epilogue: 0.20755781161872536, 1
turn: 0.20755781161872536, 1
thought: 0.20755781161872536, 1
grouping: 0.20755781161872536, 1
metaphors: 0.20755781161872536, 1
judgments: 0.20755781161872536, 1
face: 0.20755781161872536, 1
pure: 0.20755781161872536, 1
modelers: 0.20755781161872536, 1
objectives: 0.20755781161872536, 1
graphics: 0.20755781161872536, 1
adapting: 0.20755781161872536, 1
trust: 0.20755781161872536, 1
regret: 0.20755781161872536, 1
elephants: 0.20755781161872536, 1
chess: 0.20755781161872536, 1
origin: 0.20755781161872536, 1
optics: 0.20755781161872536, 1
layered: 0.20755781161872536, 1
photographs: 0.20755781161872536, 1
interpolation: 0.20755781161872536, 1
mach: 0.20755781161872536, 1
maximum: 0.20755781161872536, 1
average: 0.20755781161872536, 1
height: 0.20755781161872536, 1
filter: 0.20755781161872536, 1
lessons: 0.20755781161872536, 1
babies: 0.20755781161872536, 1
produced: 0.20755781161872536, 1
stimulation: 0.20755781161872536, 1
force: 0.20755781161872536, 1
operational: 0.20755781161872536, 1
meets: 0.20755781161872536, 1
aerobatics: 0.20755781161872536, 1
apprenticeship: 0.20755781161872536, 1
movies: 0.20755781161872536, 1
formations: 0.20755781161872536, 1
nonholonomic: 0.20755781161872536, 1
arcade: 0.20755781161872536, 1
land: 0.20755781161872536, 1
aerobatic: 0.20755781161872536, 1
superpixels: 0.20755781161872536, 1
engine: 0.20755781161872536, 1
repository: 0.20755781161872536, 1
50k: 0.20755781161872536, 1
tries: 0.20755781161872536, 1
700: 0.20755781161872536, 1
ldi: 0.20755781161872536, 1
regenerative: 0.20755781161872536, 1
losses: 0.20755781161872536, 1
super: 0.20755781161872536, 1
resolution: 0.20755781161872536, 1
virtualworlds: 0.20755781161872536, 1
synthia: 0.20755781161872536, 1
empagliflozin: 0.20755781161872536, 1
diabetes: 0.20755781161872536, 1
clinical: 0.20755781161872536, 1
malmo: 0.20755781161872536, 1
experimentation: 0.20755781161872536, 1
transferrable: 0.20755781161872536, 1
inside: 0.20755781161872536, 1
benchmarking: 0.20755781161872536, 1
outdoor: 0.20755781161872536, 1
photorealistic: 0.20755781161872536, 1
sampling: 0.20755781161872536, 1
fidelity: 0.20755781161872536, 1
vehicles: 0.20755781161872536, 1
behaviours: 0.20755781161872536, 1
racing: 0.20755781161872536, 1
million: 0.20755781161872536, 1
proximal: 0.20755781161872536, 1
photographic: 0.20755781161872536, 1
worlds: 0.20755781161872536, 1
return: 0.20755781161872536, 1
autoencoders: 0.20755781161872536, 1
interpreting: 0.20755781161872536, 1
aggregation: 0.20755781161872536, 1
dilated: 0.20755781161872536, 1
convolutions: 0.20755781161872536, 1
newtonian: 0.20755781161872536, 1
unfolding: 0.20755781161872536, 1
texturing: 0.20755781161872536, 1
reconstructions: 0.20755781161872536, 1
configurable: 0.20755781161872536, 1
linear: 0.20755781161872536, 1
inequalities: 0.20755781161872536, 1
enactments: 0.20755781161872536, 1
motivational: 0.20755781161872536, 1
serious: 0.20755781161872536, 1
cantonese: 0.20755781161872536, 1
opera: 0.20755781161872536, 1
wearable: 0.20755781161872536, 1
holistic: 0.20755781161872536, 1
e: 0.20755781161872536, 1
commerce: 0.20755781161872536, 1
live: 0.20755781161872536, 1
streaming: 0.20755781161872536, 1
gift: 0.20755781161872536, 1
giving: 0.20755781161872536, 1
purchase: 0.20755781161872536, 1
slicer: 0.20755781161872536, 1
simultaneous: 0.20755781161872536, 1
weirdest: 0.20755781161872536, 1
practices: 0.20755781161872536, 1
videogames: 0.20755781161872536, 1
subtraction: 0.20755781161872536, 1
surveillance: 0.20755781161872536, 1
wealth: 0.20755781161872536, 1
production: 0.20755781161872536, 1
transforms: 0.20755781161872536, 1
markets: 0.20755781161872536, 1
freedom: 0.20755781161872536, 1
metaverse: 0.20755781161872536, 1
multidisciplinary: 0.20755781161872536, 1
agenda: 0.20755781161872536, 1
socializing: 0.20755781161872536, 1
pretend: 0.20755781161872536, 1
emotional: 0.20755781161872536, 1
disabled: 0.20755781161872536, 1
students: 0.20755781161872536, 1
deficits: 0.20755781161872536, 1
objectification: 0.20755781161872536, 1
auditory: 0.20755781161872536, 1
origins: 0.20755781161872536, 1
occupation: 0.20755781161872536, 1
parent: 0.20755781161872536, 1
child: 0.20755781161872536, 1
competence: 0.20755781161872536, 1
underpinnings: 0.20755781161872536, 1
librarianship: 0.20755781161872536, 1
encountering: 0.20755781161872536, 1
lived: 0.20755781161872536, 1
film: 0.20755781161872536, 1
explorations: 0.20755781161872536, 1
perceptional: 0.20755781161872536, 1
infancy: 0.20755781161872536, 1
schema: 0.20755781161872536, 1
centuries: 0.20755781161872536, 1
errors: 0.20755781161872536, 1
consciousness: 0.20755781161872536, 1
intentionality: 0.20755781161872536, 1
threshhold: 0.20755781161872536, 1
thinking: 0.20755781161872536, 1
merging: 0.20755781161872536, 1
40: 0.20755781161872536, 1
street: 0.20755781161872536, 1
means: 0.20755781161872536, 1
shaping: 0.20755781161872536, 1
dimensions: 0.20755781161872536, 1
passability: 0.20755781161872536, 1
symmetry: 0.20755781161872536, 1
specificity: 0.20755781161872536, 1
raison: 0.20755781161872536, 1
anchored: 0.20755781161872536, 1
participation: 0.20755781161872536, 1
autocatakinetic: 0.20755781161872536, 1
collide: 0.20755781161872536, 1
clarifying: 0.20755781161872536, 1
teleoperation: 0.20755781161872536, 1
constructing: 0.20755781161872536, 1
multiagent: 0.20755781161872536, 1
failings: 0.20755781161872536, 1
event: 0.20755781161872536, 1
kant: 0.20755781161872536, 1
essays: 0.20755781161872536, 1
playscape: 0.20755781161872536, 1
inertial: 0.20755781161872536, 1
schemas: 0.20755781161872536, 1
orientation: 0.20755781161872536, 1
buildings: 0.20755781161872536, 1
epistemology: 0.20755781161872536, 1
roger: 0.20755781161872536, 1
william: 0.20755781161872536, 1
radical: 0.20755781161872536, 1
empiricism: 0.20755781161872536, 1
texts: 0.20755781161872536, 1
cognizing: 0.20755781161872536, 1
airspace: 0.20755781161872536, 1
sustainable: 0.20755781161872536, 1
fitting: 0.20755781161872536, 1
pieces: 0.20755781161872536, 1
puzzle: 0.20755781161872536, 1
knowing: 0.20755781161872536, 1
combinatory: 0.20755781161872536, 1
grammar: 0.20755781161872536, 1
actors: 0.20755781161872536, 1
debate: 0.20755781161872536, 1
lens: 0.20755781161872536, 1
broader: 0.20755781161872536, 1
brightness: 0.20755781161872536, 1
sociable: 0.20755781161872536, 1
cscl: 0.20755781161872536, 1
pickup: 0.20755781161872536, 1
nonspecifying: 0.20755781161872536, 1
entail: 0.20755781161872536, 1
garden: 0.20755781161872536, 1
restorative: 0.20755781161872536, 1
hospitalized: 0.20755781161872536, 1
children: 0.20755781161872536, 1
commentary: 0.20755781161872536, 1
conole: 0.20755781161872536, 1
dyke: 0.20755781161872536, 1
geography: 0.20755781161872536, 1
services: 0.20755781161872536, 1
sociocultural: 0.20755781161872536, 1
integration: 0.20755781161872536, 1
drives: 0.20755781161872536, 1
aperture: 0.20755781161872536, 1
crossing: 0.20755781161872536, 1
musical: 0.20755781161872536, 1
opportunity: 0.20755781161872536, 1
evoking: 0.20755781161872536, 1
substitution: 0.20755781161872536, 1
conceptualizations: 0.20755781161872536, 1
reinventing: 0.20755781161872536, 1
invented: 0.20755781161872536, 1
06231: 0.20755781161872536, 1
abstracts: 0.20755781161872536, 1
traversibility: 0.20755781161872536, 1
culture: 0.20755781161872536, 1
hockey: 0.20755781161872536, 1
sticks: 0.20755781161872536, 1
touch: 0.20755781161872536, 1
petri: 0.20755781161872536, 1
fitness: 0.20755781161872536, 1
embedded: 0.20755781161872536, 1
pivotal: 0.20755781161872536, 1
chances: 0.20755781161872536, 1
12th: 0.20755781161872536, 1
rsfdgrc: 0.20755781161872536, 1
december: 0.20755781161872536, 1
word: 0.20755781161872536, 1
qualities: 0.20755781161872536, 1
going: 0.20755781161872536, 1
actualize: 0.20755781161872536, 1
parameters: 0.20755781161872536, 1
classroom: 0.20755781161872536, 1
taps: 0.20755781161872536, 1
awareness: 0.20755781161872536, 1
multilinguals: 0.20755781161872536, 1
versus: 0.20755781161872536, 1
bilinguals: 0.20755781161872536, 1
cognates: 0.20755781161872536, 1
materiality: 0.20755781161872536, 1
invite: 0.20755781161872536, 1
reconsidering: 0.20755781161872536, 1
music: 0.20755781161872536, 1
sidebar: 0.20755781161872536, 1
book: 0.20755781161872536, 1
gamers: 0.20755781161872536, 1
gis: 0.20755781161872536, 1
terrorism: 0.20755781161872536, 1
suggestive: 0.20755781161872536, 1
ecosystem: 0.20755781161872536, 1
esl: 0.20755781161872536, 1
conceptualizing: 0.20755781161872536, 1
recognizing: 0.20755781161872536, 1
anything: 0.20755781161872536, 1
explanations: 0.20755781161872536, 1
grip: 0.20755781161872536, 1
inner: 0.20755781161872536, 1
reconstructing: 0.20755781161872536, 1
keyword: 0.20755781161872536, 1
interpretations: 0.20755781161872536, 1
theorizing: 0.20755781161872536, 1
request: 0.20755781161872536, 1
refuse: 0.20755781161872536, 1
subtask: 0.20755781161872536, 1
prehension: 0.20755781161872536, 1
affording: 0.20755781161872536, 1
adopting: 0.20755781161872536, 1
heuristic: 0.20755781161872536, 1
weighted: 0.20755781161872536, 1
exercising: 0.20755781161872536, 1
cortical: 0.20755781161872536, 1
aiding: 0.20755781161872536, 1
oh: 0.20755781161872536, 1
places: 0.20755781161872536, 1
posting: 0.20755781161872536, 1
changing: 0.20755781161872536, 1
personalized: 0.20755781161872536, 1
pervasive: 0.20755781161872536, 1
incorporating: 0.20755781161872536, 1
innovative: 0.20755781161872536, 1
uav: 0.20755781161872536, 1
orientated: 0.20755781161872536, 1
harnessing: 0.20755781161872536, 1
fixtures: 0.20755781161872536, 1
waste: 0.20755781161872536, 1
recycling: 0.20755781161872536, 1
bin: 0.20755781161872536, 1
picking: 0.20755781161872536, 1
boundary: 0.20755781161872536, 1
preserving: 0.20755781161872536, 1
embeddings: 0.20755781161872536, 1
curing: 0.20755781161872536, 1
eco: 0.20755781161872536, 1
limits: 0.20755781161872536, 1
correction: 0.20755781161872536, 1
manifold: 0.20755781161872536, 1
upper: 0.20755781161872536, 1
limb: 0.20755781161872536, 1
locomotive: 0.20755781161872536, 1
brains: 0.20755781161872536, 1
affordable: 0.20755781161872536, 1
robo: 0.20755781161872536, 1
programmer: 0.20755781161872536, 1
headlight: 0.20755781161872536, 1
gmm: 0.20755781161872536, 1
illumination: 0.20755781161872536, 1
inequity: 0.20755781161872536, 1
neighborhood: 0.20755781161872536, 1
los: 0.20755781161872536, 1
angeles: 0.20755781161872536, 1
differentiating: 0.20755781161872536, 1
safflower: 0.20755781161872536, 1
germplasm: 0.20755781161872536, 1
price: 0.20755781161872536, 1
fairness: 0.20755781161872536, 1
food: 0.20755781161872536, 1
fsqca: 0.20755781161872536, 1
phenotype: 0.20755781161872536, 1
fish: 0.20755781161872536, 1
aquaculture: 0.20755781161872536, 1
foliage: 0.20755781161872536, 1
finder: 0.20755781161872536, 1
revolutionizing: 0.20755781161872536, 1
lane: 0.20755781161872536, 1
marking: 0.20755781161872536, 1
ease: 0.20755781161872536, 1
transformer: 0.20755781161872536, 1
compositional: 0.20755781161872536, 1
capability: 0.20755781161872536, 1
transitioning: 0.20755781161872536, 1
utilization: 0.20755781161872536, 1
depicting: 0.20755781161872536, 1
accurately: 0.20755781161872536, 1
enhance: 0.20755781161872536, 1
discerning: 0.20755781161872536, 1
implausible: 0.20755781161872536, 1
pragmatic: 0.20755781161872536, 1
stick: 0.20755781161872536, 1
manifolds: 0.20755781161872536, 1
correlate: 0.20755781161872536, 1
traversing: 0.20755781161872536, 1
backpropagation: 0.20755781161872536, 1
indicate: 0.20755781161872536, 1
accessing: 0.20755781161872536, 1
criteria: 0.20755781161872536, 1
generalist: 0.20755781161872536, 1
zero: 0.20755781161872536, 1
settings: 0.20755781161872536, 1
finetune: 0.20755781161872536, 1
accommodate: 0.20755781161872536, 1
accelerate: 0.20755781161872536, 1
kinds: 0.20755781161872536, 1
attempt: 0.20755781161872536, 1
thereby: 0.20755781161872536, 1
update: 0.20755781161872536, 1
rapidly: 0.20755781161872536, 1
maximisation: 0.20755781161872536, 1
optimise: 0.20755781161872536, 1
modifies: 0.20755781161872536, 1
width: 0.20755781161872536, 1
inconsistently: 0.20755781161872536, 1
demonstrating: 0.20755781161872536, 1
quantifying: 0.20755781161872536, 1
curate: 0.20755781161872536, 1
15: 0.20755781161872536, 1
annotate: 0.20755781161872536, 1
sentences: 0.20755781161872536, 1
reveal: 0.20755781161872536, 1
comes: 0.20755781161872536, 1
uncommon: 0.20755781161872536, 1
vlms: 0.20755781161872536, 1
necessarily: 0.20755781161872536, 1
contributes: 0.20755781161872536, 1
lm: 0.20755781161872536, 1
advancing: 0.20755781161872536, 1
studied: 0.20755781161872536, 1
sender: 0.20755781161872536, 1
receiver: 0.20755781161872536, 1
reference: 0.20755781161872536, 1
dedicated: 0.20755781161872536, 1
communicative: 0.20755781161872536, 1
protocol: 0.20755781161872536, 1
partitions: 0.20755781161872536, 1
partners: 0.20755781161872536, 1
aside: 0.20755781161872536, 1
accounting: 0.20755781161872536, 1
modulate: 0.20755781161872536, 1
intuitively: 0.20755781161872536, 1
studying: 0.20755781161872536, 1
frustration: 0.20755781161872536, 1
addition: 0.20755781161872536, 1
ml: 0.20755781161872536, 1
conducted: 0.20755781161872536, 1
loop: 0.20755781161872536, 1
creates: 0.20755781161872536, 1
nine: 0.20755781161872536, 1
iml: 0.20755781161872536, 1
compliance: 0.20755781161872536, 1
quantised: 0.20755781161872536, 1
differs: 0.20755781161872536, 1
vaes: 0.20755781161872536, 1
quantisation: 0.20755781161872536, 1
circumvent: 0.20755781161872536, 1
latents: 0.20755781161872536, 1
ignored: 0.20755781161872536, 1
paired: 0.20755781161872536, 1
speaker: 0.20755781161872536, 1
conversion: 0.20755781161872536, 1
appealing: 0.20755781161872536, 1
master: 0.20755781161872536, 1
repurpose: 0.20755781161872536, 1
once: 0.20755781161872536, 1
commanded: 0.20755781161872536, 1
proposals: 0.20755781161872536, 1
impossible: 0.20755781161872536, 1
prevents: 0.20755781161872536, 1
intensive: 0.20755781161872536, 1
reviews: 0.20755781161872536, 1
investigates: 0.20755781161872536, 1
hoping: 0.20755781161872536, 1
pursue: 0.20755781161872536, 1
acceleration: 0.20755781161872536, 1
classifies: 0.20755781161872536, 1
delves: 0.20755781161872536, 1
methodologies: 0.20755781161872536, 1
rationales: 0.20755781161872536, 1
representative: 0.20755781161872536, 1
reporting: 0.20755781161872536, 1
summarizes: 0.20755781161872536, 1
outlines: 0.20755781161872536, 1
discusses: 0.20755781161872536, 1
established: 0.20755781161872536, 1
physiologically: 0.20755781161872536, 1
fusing: 0.20755781161872536, 1
publicly: 0.20755781161872536, 1
aid: 0.20755781161872536, 1
papers: 0.20755781161872536, 1
draw: 0.20755781161872536, 1
connections: 0.20755781161872536, 1
technical: 0.20755781161872536, 1
remark: 0.20755781161872536, 1
modalities: 0.20755781161872536, 1
predictable: 0.20755781161872536, 1
hypothesis: 0.20755781161872536, 1
box: 0.20755781161872536, 1
processed: 0.20755781161872536, 1
crf: 0.20755781161872536, 1
detected: 0.20755781161872536, 1
noisy: 0.20755781161872536, 1
walk: 0.20755781161872536, 1
man: 0.20755781161872536, 1
had: 0.20755781161872536, 1
proven: 0.20755781161872536, 1
encompasses: 0.20755781161872536, 1
endeavors: 0.20755781161872536, 1
fostering: 0.20755781161872536, 1
facilitating: 0.20755781161872536, 1
daily: 0.20755781161872536, 1
lives: 0.20755781161872536, 1
enjoy: 0.20755781161872536, 1
technologies: 0.20755781161872536, 1
enough: 0.20755781161872536, 1
plenty: 0.20755781161872536, 1
researches: 0.20755781161872536, 1
guidelines: 0.20755781161872536, 1
valuable: 0.20755781161872536, 1
notions: 0.20755781161872536, 1
comprehensively: 0.20755781161872536, 1
conclude: 0.20755781161872536, 1
discussions: 0.20755781161872536, 1
theme: 0.20755781161872536, 1
worth: 0.20755781161872536, 1
constrain: 0.20755781161872536, 1
themselves: 0.20755781161872536, 1
explicitly: 0.20755781161872536, 1
scaled: 0.20755781161872536, 1
approximating: 0.20755781161872536, 1
appear: 0.20755781161872536, 1
surrounding: 0.20755781161872536, 1
norms: 0.20755781161872536, 1
imposed: 0.20755781161872536, 1
happen: 0.20755781161872536, 1
ade: 0.20755781161872536, 1
containing: 0.20755781161872536, 1
propagate: 0.20755781161872536, 1
detailed: 0.20755781161872536, 1
showcased: 0.20755781161872536, 1
ablation: 0.20755781161872536, 1
pointing: 0.20755781161872536, 1
fall: 0.20755781161872536, 1
modular: 0.20755781161872536, 1
advantages: 0.20755781161872536, 1
highway: 0.20755781161872536, 1
lacking: 0.20755781161872536, 1
stop: 0.20755781161872536, 1
directional: 0.20755781161872536, 1
68: 0.20755781161872536, 1
carla: 0.20755781161872536, 1
signs: 0.20755781161872536, 1
accidents: 0.20755781161872536, 1
generalised: 0.20755781161872536, 1
yields: 0.20755781161872536, 1
harvest: 0.20755781161872536, 1
relates: 0.20755781161872536, 1
reliable: 0.20755781161872536, 1
prototypical: 0.20755781161872536, 1
patches: 0.20755781161872536, 1
instances: 0.20755781161872536, 1
platforms: 0.20755781161872536, 1
facilitated: 0.20755781161872536, 1
attain: 0.20755781161872536, 1
holding: 0.20755781161872536, 1
handled: 0.20755781161872536, 1
positive: 0.20755781161872536, 1
informed: 0.20755781161872536, 1
clusters: 0.20755781161872536, 1
handles: 0.20755781161872536, 1
occlusions: 0.20755781161872536, 1
imposing: 0.20755781161872536, 1
generalizability: 0.20755781161872536, 1
approximately: 0.20755781161872536, 1
consist: 0.20755781161872536, 1
utilizing: 0.20755781161872536, 1
graphs: 0.20755781161872536, 1
whilst: 0.20755781161872536, 1
decoupled: 0.20755781161872536, 1
duration: 0.20755781161872536, 1
against: 0.20755781161872536, 1
closely: 0.20755781161872536, 1
poor: 0.20755781161872536, 1
visuals: 0.20755781161872536, 1
randomisation: 0.20755781161872536, 1
encourages: 0.20755781161872536, 1
increases: 0.20755781161872536, 1
negatively: 0.20755781161872536, 1
variations: 0.20755781161872536, 1
alleviate: 0.20755781161872536, 1
robustness: 0.20755781161872536, 1
increased: 0.20755781161872536, 1
april: 0.20755781161872536, 1
purely: 0.20755781161872536, 1
accelerated: 0.20755781161872536, 1
leading: 0.20755781161872536, 1
outside: 0.20755781161872536, 1
successively: 0.20755781161872536, 1
grows: 0.20755781161872536, 1
internal: 0.20755781161872536, 1
nodes: 0.20755781161872536, 1
check: 0.20755781161872536, 1
existence: 0.20755781161872536, 1
leaves: 0.20755781161872536, 1
receives: 0.20755781161872536, 1
alternately: 0.20755781161872536, 1
modifying: 0.20755781161872536, 1
updating: 0.20755781161872536, 1
lend: 0.20755781161872536, 1
applicability: 0.20755781161872536, 1
usefulness: 0.20755781161872536, 1
phenomenon: 0.20755781161872536, 1
persists: 0.20755781161872536, 1
removing: 0.20755781161872536, 1
included: 0.20755781161872536, 1
colors: 0.20755781161872536, 1
1: 0.20755781161872536, 1
week: 0.20755781161872536, 1
remained: 0.20755781161872536, 1
irrelevant: 0.20755781161872536, 1
except: 0.20755781161872536, 1
frequency: 0.20755781161872536, 1
exposure: 0.20755781161872536, 1
suggesting: 0.20755781161872536, 1
depend: 0.20755781161872536, 1
demands: 0.20755781161872536, 1
vulnerability: 0.20755781161872536, 1
indicates: 0.20755781161872536, 1
promise: 0.20755781161872536, 1
impressive: 0.20755781161872536, 1
vulnerable: 0.20755781161872536, 1
accidental: 0.20755781161872536, 1
distracted: 0.20755781161872536, 1
disturbance: 0.20755781161872536, 1
prevent: 0.20755781161872536, 1
augmenting: 0.20755781161872536, 1
demonstrations: 0.20755781161872536, 1
specified: 0.20755781161872536, 1
red: 0.20755781161872536, 1
bowl: 0.20755781161872536, 1
concentrate: 0.20755781161872536, 1
benign: 0.20755781161872536, 1
tfa: 0.20755781161872536, 1
consistently: 0.20755781161872536, 1
variant: 0.20755781161872536, 1
regularly: 0.20755781161872536, 1
recovers: 0.20755781161872536, 1
causing: 0.20755781161872536, 1
baseline: 0.20755781161872536, 1
almost: 0.20755781161872536, 1
never: 0.20755781161872536, 1
exhibiting: 0.20755781161872536, 1
reminiscent: 0.20755781161872536, 1
anchor: 0.20755781161872536, 1
bolt: 0.20755781161872536, 1
insertion: 0.20755781161872536, 1
holes: 0.20755781161872536, 1
automate: 0.20755781161872536, 1
setup: 0.20755781161872536, 1
gap: 0.20755781161872536, 1
misleading: 0.20755781161872536, 1
sap: 0.20755781161872536, 1
shorter: 0.20755781161872536, 1
assumption: 0.20755781161872536, 1
integrates: 0.20755781161872536, 1
cup: 0.20755781161872536, 1
away: 0.20755781161872536, 1
asynchronous: 0.20755781161872536, 1
accumulates: 0.20755781161872536, 1
foveal: 0.20755781161872536, 1
glimpses: 0.20755781161872536, 1
learner: 0.20755781161872536, 1
exert: 0.20755781161872536, 1
searching: 0.20755781161872536, 1
fovea: 0.20755781161872536, 1
accumulated: 0.20755781161872536, 1
decide: 0.20755781161872536, 1
attempted: 0.20755781161872536, 1
influences: 0.20755781161872536, 1
sequential: 0.20755781161872536, 1
engage: 0.20755781161872536, 1
modest: 0.20755781161872536, 1
counterparts: 0.20755781161872536, 1
applicable: 0.20755781161872536, 1
itpsilas: 0.20755781161872536, 1
proposes: 0.20755781161872536, 1
populate: 0.20755781161872536, 1
neutral: 0.20755781161872536, 1
assist: 0.20755781161872536, 1
dasiateachpsila: 0.20755781161872536, 1
invariances: 0.20755781161872536, 1
invaluable: 0.20755781161872536, 1
phenomena: 0.20755781161872536, 1
translation: 0.20755781161872536, 1
equivariance: 0.20755781161872536, 1
layers: 0.20755781161872536, 1
others: 0.20755781161872536, 1
molecules: 0.20755781161872536, 1
chemical: 0.20755781161872536, 1
compounds: 0.20755781161872536, 1
dealing: 0.20755781161872536, 1
equivariances: 0.20755781161872536, 1
broad: 0.20755781161872536, 1
spectrum: 0.20755781161872536, 1
resort: 0.20755781161872536, 1
huge: 0.20755781161872536, 1
impose: 0.20755781161872536, 1
principle: 0.20755781161872536, 1
indissoluble: 0.20755781161872536, 1
traits: 0.20755781161872536, 1
velocity: 0.20755781161872536, 1
expresses: 0.20755781161872536, 1
might: 0.20755781161872536, 1
mostly: 0.20755781161872536, 1
rarely: 0.20755781161872536, 1
accompanied: 0.20755781161872536, 1
certh: 0.20755781161872536, 1
14: 0.20755781161872536, 1
13: 0.20755781161872536, 1
seven: 0.20755781161872536, 1
probabilistically: 0.20755781161872536, 1
reasonable: 0.20755781161872536, 1
purposes: 0.20755781161872536, 1
anticipated: 0.20755781161872536, 1
approached: 0.20755781161872536, 1
localized: 0.20755781161872536, 1
tackle: 0.20755781161872536, 1
adopt: 0.20755781161872536, 1
ignoring: 0.20755781161872536, 1
concerning: 0.20755781161872536, 1
corpus: 0.20755781161872536, 1
predicts: 0.20755781161872536, 1
heatmap: 0.20755781161872536, 1
quantify: 0.20755781161872536, 1
adapt: 0.20755781161872536, 1
rcnn: 0.20755781161872536, 1
monte: 0.20755781161872536, 1
carlo: 0.20755781161872536, 1
variability: 0.20755781161872536, 1
adapts: 0.20755781161872536, 1
comparing: 0.20755781161872536, 1
masks: 0.20755781161872536, 1
grained: 0.20755781161872536, 1
contours: 0.20755781161872536, 1
appears: 0.20755781161872536, 1
freely: 0.20755781161872536, 1
cases: 0.20755781161872536, 1
transfers: 0.20755781161872536, 1
pspnet: 0.20755781161872536, 1
net: 0.20755781161872536, 1
frequent: 0.20755781161872536, 1
assigned: 0.20755781161872536, 1
infrequent: 0.20755781161872536, 1
operates: 0.20755781161872536, 1
easier: 0.20755781161872536, 1
implement: 0.20755781161872536, 1
linked: 0.20755781161872536, 1
succeeded: 0.20755781161872536, 1
construct: 0.20755781161872536, 1
correlates: 0.20755781161872536, 1
vectors: 0.20755781161872536, 1
sufficiently: 0.20755781161872536, 1
unrealistic: 0.20755781161872536, 1
sintel: 0.20755781161872536, 1
fluid: 0.20755781161872536, 1
aerospace: 0.20755781161872536, 1
piv: 0.20755781161872536, 1
handcrafted: 0.20755781161872536, 1
generalizing: 0.20755781161872536, 1
automated: 0.20755781161872536, 1
true: 0.20755781161872536, 1
gold: 0.20755781161872536, 1
pixels: 0.20755781161872536, 1
boundaries: 0.20755781161872536, 1
spatiotemporal: 0.20755781161872536, 1
select: 0.20755781161872536, 1
discarding: 0.20755781161872536, 1
troublesome: 0.20755781161872536, 1
times: 0.20755781161872536, 1
cast: 0.20755781161872536, 1
compete: 0.20755781161872536, 1
really: 0.20755781161872536, 1
schedule: 0.20755781161872536, 1
presenting: 0.20755781161872536, 1
stacked: 0.20755781161872536, 1
elaborate: 0.20755781161872536, 1
introducing: 0.20755781161872536, 1
subnetwork: 0.20755781161872536, 1
specializing: 0.20755781161872536, 1
marginally: 0.20755781161872536, 1
slower: 0.20755781161872536, 1
decreases: 0.20755781161872536, 1
par: 0.20755781161872536, 1
running: 0.20755781161872536, 1
variants: 0.20755781161872536, 1
140fps: 0.20755781161872536, 1
jol: 0.20755781161872536, 1
playful: 0.20755781161872536, 1
filled: 0.20755781161872536, 1
stage: 0.20755781161872536, 1
cranked: 0.20755781161872536, 1
conceptualized: 0.20755781161872536, 1
german: 0.20755781161872536, 1
astronomer: 0.20755781161872536, 1
inventor: 0.20755781161872536, 1
hahn: 0.20755781161872536, 1
wondrously: 0.20755781161872536, 1
intricate: 0.20755781161872536, 1
arithmetical: 0.20755781161872536, 1
initiating: 0.20755781161872536, 1
precision: 0.20755781161872536, 1
industry: 0.20755781161872536, 1
scrutinized: 0.20755781161872536, 1
equally: 0.20755781161872536, 1
wondrous: 0.20755781161872536, 1
axis: 0.20755781161872536, 1
bosch: 0.20755781161872536, 1
gmbh: 0.20755781161872536, 1
engineers: 0.20755781161872536, 1
specialising: 0.20755781161872536, 1
deceptively: 0.20755781161872536, 1
sheathed: 0.20755781161872536, 1
speeds: 0.20755781161872536, 1
laser: 0.20755781161872536, 1
measurement: 0.20755781161872536, 1
invisible: 0.20755781161872536, 1
before: 0.20755781161872536, 1
generally: 0.20755781161872536, 1
mean: 0.20755781161872536, 1
thomson: 0.20755781161872536, 1
viewer: 0.20755781161872536, 1
documentation: 0.20755781161872536, 1
entirety: 0.20755781161872536, 1
alongside: 0.20755781161872536, 1
visualizations: 0.20755781161872536, 1
punctuated: 0.20755781161872536, 1
textual: 0.20755781161872536, 1
excerpts: 0.20755781161872536, 1
drawn: 0.20755781161872536, 1
european: 0.20755781161872536, 1
parliament: 0.20755781161872536, 1
civil: 0.20755781161872536, 1
law: 0.20755781161872536, 1
consideration: 0.20755781161872536, 1
liabilities: 0.20755781161872536, 1
animals: 0.20755781161872536, 1
coined: 0.20755781161872536, 1
precise: 0.20755781161872536, 1
establish: 0.20755781161872536, 1
illustrative: 0.20755781161872536, 1
simpler: 0.20755781161872536, 1
consequence: 0.20755781161872536, 1
stored: 0.20755781161872536, 1
reused: 0.20755781161872536, 1
ensure: 0.20755781161872536, 1
nao: 0.20755781161872536, 1
speedups: 0.20755781161872536, 1
solves: 0.20755781161872536, 1
employ: 0.20755781161872536, 1
deal: 0.20755781161872536, 1
employed: 0.20755781161872536, 1
situations: 0.20755781161872536, 1
numbers: 0.20755781161872536, 1
manipulates: 0.20755781161872536, 1
spatiotemporally: 0.20755781161872536, 1
correlated: 0.20755781161872536, 1
actually: 0.20755781161872536, 1
requirement: 0.20755781161872536, 1
inability: 0.20755781161872536, 1
discern: 0.20755781161872536, 1
vegetation: 0.20755781161872536, 1
obstacles: 0.20755781161872536, 1
hampers: 0.20755781161872536, 1
obtains: 0.20755781161872536, 1
presented: 0.20755781161872536, 1
estimated: 0.20755781161872536, 1
acquainted: 0.20755781161872536, 1
raise: 0.20755781161872536, 1
prioritised: 0.20755781161872536, 1
morphological: 0.20755781161872536, 1
paradigms: 0.20755781161872536, 1
parse: 0.20755781161872536, 1
reflex: 0.20755781161872536, 1
indicators: 0.20755781161872536, 1
falling: 0.20755781161872536, 1
extremes: 0.20755781161872536, 1
complement: 0.20755781161872536, 1
stands: 0.20755781161872536, 1
sharp: 0.20755781161872536, 1
contrast: 0.20755781161872536, 1
inferential: 0.20755781161872536, 1
distinction: 0.20755781161872536, 1
trace: 0.20755781161872536, 1
newer: 0.20755781161872536, 1
formalizations: 0.20755781161872536, 1
opinions: 0.20755781161872536, 1
reviewed: 0.20755781161872536, 1
striking: 0.20755781161872536, 1
neglected: 0.20755781161872536, 1
continental: 0.20755781161872536, 1
philosophers: 0.20755781161872536, 1
links: 0.20755781161872536, 1
maximize: 0.20755781161872536, 1
pillars: 0.20755781161872536, 1
concise: 0.20755781161872536, 1
predicated: 0.20755781161872536, 1
surprising: 0.20755781161872536, 1
found: 0.20755781161872536, 1
meant: 0.20755781161872536, 1
diffuse: 0.20755781161872536, 1
virtue: 0.20755781161872536, 1
pursuit: 0.20755781161872536, 1
paths: 0.20755781161872536, 1
until: 0.20755781161872536, 1
existed: 0.20755781161872536, 1
defining: 0.20755781161872536, 1
rooted: 0.20755781161872536, 1
prominent: 0.20755781161872536, 1
import: 0.20755781161872536, 1
qualitatively: 0.20755781161872536, 1
questions: 0.20755781161872536, 1
believe: 0.20755781161872536, 1
inform: 0.20755781161872536, 1
prioritize: 0.20755781161872536, 1
comparatively: 0.20755781161872536, 1
resnet: 0.20755781161872536, 1
blend: 0.20755781161872536, 1
cost: 0.20755781161872536, 1
segmentations: 0.20755781161872536, 1
indeed: 0.20755781161872536, 1
reliably: 0.20755781161872536, 1
engineered: 0.20755781161872536, 1
devising: 0.20755781161872536, 1
manually: 0.20755781161872536, 1
enter: 0.20755781161872536, 1
promote: 0.20755781161872536, 1
synthetically: 0.20755781161872536, 1
suggest: 0.20755781161872536, 1
schedules: 0.20755781161872536, 1
coarse: 0.20755781161872536, 1
iit: 0.20755781161872536, 1
causality: 0.20755781161872536, 1
nut: 0.20755781161872536, 1
painting: 0.20755781161872536, 1
observes: 0.20755781161872536, 1
rational: 0.20755781161872536, 1
forces: 0.20755781161872536, 1
quantity: 0.20755781161872536, 1
viewed: 0.20755781161872536, 1
merely: 0.20755781161872536, 1
memorizing: 0.20755781161872536, 1
brings: 0.20755781161872536, 1
wearer: 0.20755781161872536, 1
fluidly: 0.20755781161872536, 1
persistent: 0.20755781161872536, 1
gain: 0.20755781161872536, 1
primary: 0.20755781161872536, 1
topological: 0.20755781161872536, 1
series: 0.20755781161872536, 1
visits: 0.20755781161872536, 1
consolidated: 0.20755781161872536, 1
epic: 0.20755781161872536, 1
kitchens: 0.20755781161872536, 1
rise: 0.20755781161872536, 1
enormous: 0.20755781161872536, 1
reasons: 0.20755781161872536, 1
regresses: 0.20755781161872536, 1
refines: 0.20755781161872536, 1
51: 0.20755781161872536, 1
graspability: 0.20755781161872536, 1
contains: 0.20755781161872536, 1
133k: 0.20755781161872536, 1
21: 0.20755781161872536, 1
28m: 0.20755781161872536, 1
33: 0.20755781161872536, 1
thorough: 0.20755781161872536, 1
robustly: 0.20755781161872536, 1
cluttered: 0.20755781161872536, 1
inspiration: 0.20755781161872536, 1
analyse: 0.20755781161872536, 1
proto: 0.20755781161872536, 1
units: 0.20755781161872536, 1
entity: 0.20755781161872536, 1
complementary: 0.20755781161872536, 1
classified: 0.20755781161872536, 1
manipulable: 0.20755781161872536, 1
categorize: 0.20755781161872536, 1
meka: 0.20755781161872536, 1
20: 0.20755781161872536, 1
continuity: 0.20755781161872536, 1
regularization: 0.20755781161872536, 1
encouraging: 0.20755781161872536, 1
fails: 0.20755781161872536, 1
dissimilarity: 0.20755781161872536, 1
hence: 0.20755781161872536, 1
here: 0.20755781161872536, 1
tuning: 0.20755781161872536, 1
derives: 0.20755781161872536, 1
specifications: 0.20755781161872536, 1
try: 0.20755781161872536, 1
categorisation: 0.20755781161872536, 1
assumed: 0.20755781161872536, 1
matter: 0.20755781161872536, 1
fruitful: 0.20755781161872536, 1
divided: 0.20755781161872536, 1
represents: 0.20755781161872536, 1
adjacent: 0.20755781161872536, 1
solved: 0.20755781161872536, 1
minimization: 0.20755781161872536, 1
briefly: 0.20755781161872536, 1
pioneering: 0.20755781161872536, 1
delve: 0.20755781161872536, 1
insightful: 0.20755781161872536, 1
cover: 0.20755781161872536, 1
seem: 0.20755781161872536, 1
applies: 0.20755781161872536, 1
proved: 0.20755781161872536, 1
considerably: 0.20755781161872536, 1
signature: 0.20755781161872536, 1
exact: 0.20755781161872536, 1
module: 0.20755781161872536, 1
filtering: 0.20755781161872536, 1
cells: 0.20755781161872536, 1
described: 0.20755781161872536, 1
hubel: 0.20755781161872536, 1
offers: 0.20755781161872536, 1
conjecture: 0.20755781161872536, 1
ventral: 0.20755781161872536, 1
cortex: 0.20755781161872536, 1
whom: 0.20755781161872536, 1
generalizes: 0.20755781161872536, 1
accounts: 0.20755781161872536, 1
instrument: 0.20755781161872536, 1
extend: 0.20755781161872536, 1
tackled: 0.20755781161872536, 1
published: 0.20755781161872536, 1
canonical: 0.20755781161872536, 1
happens: 0.20755781161872536, 1
subsequently: 0.20755781161872536, 1
associate: 0.20755781161872536, 1
permits: 0.20755781161872536, 1
specify: 0.20755781161872536, 1
translate: 0.20755781161872536, 1
additionally: 0.20755781161872536, 1
vary: 0.20755781161872536, 1
note: 0.20755781161872536, 1
combinations: 0.20755781161872536, 1
pull: 0.20755781161872536, 1
positioning: 0.20755781161872536, 1
endowing: 0.20755781161872536, 1
computationally: 0.20755781161872536, 1
dependencies: 0.20755781161872536, 1
inferences: 0.20755781161872536, 1
item: 0.20755781161872536, 1
passively: 0.20755781161872536, 1
acted: 0.20755781161872536, 1
scans: 0.20755781161872536, 1
become: 0.20755781161872536, 1
unreliable: 0.20755781161872536, 1
clutter: 0.20755781161872536, 1
positions: 0.20755781161872536, 1
unable: 0.20755781161872536, 1
impart: 0.20755781161872536, 1
prone: 0.20755781161872536, 1
max: 0.20755781161872536, 1
improves: 0.20755781161872536, 1
willow: 0.20755781161872536, 1
garage: 0.20755781161872536, 1
initialized: 0.20755781161872536, 1
enclose: 0.20755781161872536, 1
parameter: 0.20755781161872536, 1
exercises: 0.20755781161872536, 1
predictors: 0.20755781161872536, 1
cooperative: 0.20755781161872536, 1
realization: 0.20755781161872536, 1
sole: 0.20755781161872536, 1
blocks: 0.20755781161872536, 1
findings: 0.20755781161872536, 1
progression: 0.20755781161872536, 1
kinect: 0.20755781161872536, 1
fitted: 0.20755781161872536, 1
newly: 0.20755781161872536, 1
inspecting: 0.20755781161872536, 1
gas: 0.20755781161872536, 1
quickly: 0.20755781161872536, 1
commands: 0.20755781161872536, 1
abnormal: 0.20755781161872536, 1
variate: 0.20755781161872536, 1
kinematic: 0.20755781161872536, 1
distributions: 0.20755781161872536, 1
encode: 0.20755781161872536, 1
encodes: 0.20755781161872536, 1
tuple: 0.20755781161872536, 1
precondition: 0.20755781161872536, 1
postcondition: 0.20755781161872536, 1
measureable: 0.20755781161872536, 1
addressed: 0.20755781161872536, 1
arguably: 0.20755781161872536, 1
arms: 0.20755781161872536, 1
emerge: 0.20755781161872536, 1
formalize: 0.20755781161872536, 1
quantization: 0.20755781161872536, 1
melded: 0.20755781161872536, 1
channels: 0.20755781161872536, 1
activated: 0.20755781161872536, 1
modulated: 0.20755781161872536, 1
suggested: 0.20755781161872536, 1
instrumental: 0.20755781161872536, 1
correct: 0.20755781161872536, 1
selects: 0.20755781161872536, 1
list: 0.20755781161872536, 1
neurocomputational: 0.20755781161872536, 1
transformation: 0.20755781161872536, 1
volitional: 0.20755781161872536, 1
started: 0.20755781161872536, 1
square: 0.20755781161872536, 1
consequent: 0.20755781161872536, 1
tapping: 0.20755781161872536, 1
pulling: 0.20755781161872536, 1
choosing: 0.20755781161872536, 1
lssvm: 0.20755781161872536, 1
reliability: 0.20755781161872536, 1
values: 0.20755781161872536, 1
share: 0.20755781161872536, 1
much: 0.20755781161872536, 1
devised: 0.20755781161872536, 1
encapsulate: 0.20755781161872536, 1
held: 0.20755781161872536, 1
oms: 0.20755781161872536, 1
egi: 0.20755781161872536, 1
quite: 0.20755781161872536, 1
facilitates: 0.20755781161872536, 1
manifestations: 0.20755781161872536, 1
actively: 0.20755781161872536, 1
serves: 0.20755781161872536, 1
fuse: 0.20755781161872536, 1
retrieve: 0.20755781161872536, 1
accuracies: 0.20755781161872536, 1
fixing: 0.20755781161872536, 1
added: 0.20755781161872536, 1
scores: 0.20755781161872536, 1
wider: 0.20755781161872536, 1
detects: 0.20755781161872536, 1
maximally: 0.20755781161872536, 1
starts: 0.20755781161872536, 1
placed: 0.20755781161872536, 1
runs: 0.20755781161872536, 1
consistent: 0.20755781161872536, 1
albeit: 0.20755781161872536, 1
rule: 0.20755781161872536, 1
everything: 0.20755781161872536, 1
determine: 0.20755781161872536, 1
continual: 0.20755781161872536, 1
gripper: 0.20755781161872536, 1
infrared: 0.20755781161872536, 1
secondary: 0.20755781161872536, 1
marker: 0.20755781161872536, 1
gripped: 0.20755781161872536, 1
floor: 0.20755781161872536, 1
users: 0.20755781161872536, 1
replicate: 0.20755781161872536, 1
1219: 0.20755781161872536, 1
characterize: 0.20755781161872536, 1
whose: 0.20755781161872536, 1
develops: 0.20755781161872536, 1
insect: 0.20755781161872536, 1
antennal: 0.20755781161872536, 1
lobe: 0.20755781161872536, 1
mushroom: 0.20755781161872536, 1
perceives: 0.20755781161872536, 1
webcam: 0.20755781161872536, 1
canny: 0.20755781161872536, 1
border: 0.20755781161872536, 1
opencv: 0.20755781161872536, 1
massive: 0.20755781161872536, 1
preprocessed: 0.20755781161872536, 1
relayed: 0.20755781161872536, 1
isolated: 0.20755781161872536, 1
isolation: 0.20755781161872536, 1
strings: 0.20755781161872536, 1
elapse: 0.20755781161872536, 1
resources: 0.20755781161872536, 1
period: 0.20755781161872536, 1
finally: 0.20755781161872536, 1
occupancy: 0.20755781161872536, 1
neighboring: 0.20755781161872536, 1
refined: 0.20755781161872536, 1
highest: 0.20755781161872536, 1