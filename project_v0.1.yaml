keywords:
    positive:
        - affordance
        - imagin
        - encode
        - robot
        - embodied
        - agent
        - task
        - egocentric
        - emergent
        - ecological
        - ecology
        - self-supervised
        - percept
        - unsupervised
        - visual cortex
    negative:
        - large language model
        - language model
        - llm
        - supervised
        - label
        - discretize
    neutral:
        - visual
        - predict
        - centric
        - learn
        - transformer
        - unseen
        - embedding
        - latent
        - multimodal
        - behavior
        - mice
        - mouse
        - animal
        - biolog
        - primate
references:
    Visual affordance prediction for guiding robot exploration: &ref_20
        title: Visual affordance prediction for guiding robot exploration
        doi: 10.1109/icra48891.2023.10161288
        year: null
        publisherFlags: null
        authorNames:
            - H Bharadhwaj
            - A Gupta
        link: 'https://ieeexplore.ieee.org/abstract/document/10161288/'
        pdfLink: 'https://ieeexplore.ieee.org/iel7/10160211/10160212/10161288.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|abstract
        reasonsNotRelevant: []
        relevanceStages:
            - title
            - abstract
        possibleYear: '2023'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '15979805052095820058'
        multiArticleId: '15979805052095820058'
        citedByLink: 'https://scholar.google.com//scholar?cites=15979805052095820058&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: '2023 IEEE International, 2023'
        events:
            saw title: '2024-12-12T16:30:43.287Z'
        abstract: 'Motivated by the intuitive understanding humans have about the space of possible interactions, and the ease with which they can generalize this understanding to previously unseen scenes, we develop an approach for learning ‘visual affordances’. Given an input image of a scene, we infer a distribution over plausible future states that can be achieved via interactions with it. To allow predicting diverse plausible futures, we discretize the space of continuous images with a VQ-VAE and use a Transformer-based model to learn a conditional distribution in the latent embedding space. We show that these models can be trained using large-scale and diverse passive data, and that the learned models exhibit compositional generalization to diverse objects beyond the training distribution. We evaluate the quality and diversity of the generations, and demonstrate how the trained affordance model can be used for guiding exploration during visual goal-conditioned policy learning in robotic manipulation.'
        accordingTo:
            $manuallyEntered: {}
    'RAIL: Robot Affordance Imagination with Large Language Models': &ref_21
        title: 'RAIL: Robot Affordance Imagination with Large Language Models'
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - C Zhang
            - X Meng
            - D Qi
            - GS Chirikjian�
        link: 'https://arxiv.org/abs/2403.19369'
        pdfLink: 'https://arxiv.org/pdf/2403.19369'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: unclear|title
        reasonsNotRelevant: []
        relevanceStages: []
        possibleYear: '1936'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '8172269612940938567'
        multiArticleId: '8172269612940938567'
        citedByLink: 'https://scholar.google.com//scholar?cites=8172269612940938567&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'arXiv preprint arXiv:2403.19369, 2024'
        events:
            saw title: '2024-12-12T16:30:58.343Z'
        accordingTo:
            $manuallyEntered: {}
    Affordance-Based Goal Imagination for Embodied AI Agents: &ref_22
        title: Affordance-Based Goal Imagination for Embodied AI Agents
        doi: 10.1109/icdl61372.2024.10644764
        year: null
        publisherFlags: null
        authorNames:
            - V Aregbede
            - SS Abraham
            - A Persson
        link: 'https://ieeexplore.ieee.org/abstract/document/10644764/'
        pdfLink: 'https://ieeexplore.ieee.org/iel8/10644131/10644157/10644764.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|abstract
        reasonsNotRelevant:
            - abstract
        relevanceStages:
            - title
        possibleYear: '2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: null
        multiArticleId: null
        citedByLink: null
        publisherInfo: 'on Development and, 2024'
        events:
            saw title: '2024-12-12T16:31:01.901Z'
        abstract: 'Goal imagination in robotics is an emerging concept and involves the capability to automatically generate realistic goals, which, in turn, requires the assessment of the feasibility of transitioning from the current conditions of an initial scene to the desired goal state. Existing research has explored the utilization of diverse image-generative models to create images depicting potential goal states based on the current state and instructions. In this paper, we illustrate the limitations of current state-of-the-art image generative models in accurately assessing the feasibility of specific actions in particular situations. Consequently, we present how integrating large language models, which possess profound knowledge of real-world objects and affordances, can enhance the performance of image-generative models in discerning plausible from implausible actions and simulating the outcomes of actions in a given context. This will be a step towards achieving the pragmatic goal of imagination in robotics.'
        accordingTo:
            $manuallyEntered: {}
    Learning to anticipate egocentric actions by imagination: &ref_23
        title: Learning to anticipate egocentric actions by imagination
        doi: 10.1109/tip.2020.3040521
        year: null
        publisherFlags: null
        authorNames:
            - Y Wu
            - L Zhu
            - X Wang
            - Y Yang
        link: 'https://ieeexplore.ieee.org/abstract/document/9280353/'
        pdfLink: 'https://ieeexplore.ieee.org/iel7/83/9263394/09280353.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: unclear|title
        reasonsNotRelevant: []
        relevanceStages: []
        possibleYear: '2020'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '6012752103031775791'
        multiArticleId: '6012752103031775791'
        citedByLink: 'https://scholar.google.com//scholar?cites=6012752103031775791&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'IEEE Transactions on, 2020'
        events:
            saw title: '2024-12-12T21:30:37.470Z'
        accordingTo:
            $manuallyEntered: {}
    Imagine that! Leveraging emergent affordances for 3d tool synthesis: &ref_24
        title: Imagine that! Leveraging emergent affordances for 3d tool synthesis
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - Y Wu
            - S Kasewa
            - O Groth
            - S Salter
            - L Sun
        link: 'https://arxiv.org/abs/1909.13561'
        pdfLink: 'https://arxiv.org/pdf/1909.13561'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|abstract
        reasonsNotRelevant: []
        relevanceStages:
            - title
            - abstract
        possibleYear: '2019'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '1893778644382906900'
        multiArticleId: '1893778644382906900'
        citedByLink: 'https://scholar.google.com//scholar?cites=1893778644382906900&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'arXiv preprint arXiv, 2019'
        events:
            saw title: '2024-12-12T21:30:43.250Z'
        abstract: '     In this paper we explore the richness of information captured by the latent space of a vision-based generative model. The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool. While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand. Our results indicate that affordances-like the utility for reaching-are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way. '
        accordingTo:
            $manuallyEntered: {}
    What can i do here? learning new skills by imagining visual affordances: &ref_25
        title: What can i do here? learning new skills by imagining visual affordances
        doi: 10.1109/icra48506.2021.9561692
        year: null
        publisherFlags: null
        authorNames:
            - A Khazatsky
            - A Nair
            - D Jing
        link: 'https://ieeexplore.ieee.org/abstract/document/9561692/'
        pdfLink: 'https://ieeexplore.ieee.org/iel7/9560720/9560666/09561692.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: super-relevent|abstract
        reasonsNotRelevant: []
        relevanceStages:
            - title
            - abstract
        possibleYear: '2021'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '15430638141641676250'
        multiArticleId: '15430638141641676250'
        citedByLink: 'https://scholar.google.com//scholar?cites=15430638141641676250&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: '2021 IEEE International, 2021'
        events:
            saw title: '2024-12-12T21:40:37.271Z'
            saw abstract: '2024-12-13T00:50:28.321Z'
        abstract: 'A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always possible. When the robot encounters a new environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But crucially, previously learned behaviors and models should still be suitable to accelerate this relearning. In this paper, we aim to study how generative models of possible outcomes can allow a robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those outcomes. In effect, prior data is used to learn what kinds of outcomes may be possible, such that when the robot encounters an unfamiliar setting, it can sample potential outcomes from its model, attempt to reach them, and thereby update both its skills and its outcome model. We show that this approach can be used to train goal-conditioned policies that operate on raw image inputs, and can rapidly learn to manipulate new objects via our proposed affordance-directed exploration scheme.'
        pdfWasDownloaded: true
        accordingTo:
            $manuallyEntered: {}
    'Learning to act properly: Predicting and explaining affordances from images': &ref_26
        title: 'Learning to act properly: Predicting and explaining affordances from images'
        doi: 10.1109/cvpr.2018.00108
        year: null
        publisherFlags: null
        authorNames:
            - CY Chuang
            - J Li
            - A Torralba
        link: 'http://openaccess.thecvf.com/content_cvpr_2018/html/Chuang_Learning_to_Act_CVPR_2018_paper.html'
        pdfLink: 'https://openaccess.thecvf.com/content_cvpr_2018/papers/Chuang_Learning_to_Act_CVPR_2018_paper.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|abstract
        reasonsNotRelevant: []
        relevanceStages:
            - title
            - abstract
        possibleYear: '2018'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '9413040909214203914'
        multiArticleId: '9413040909214203914'
        citedByLink: 'https://scholar.google.com//scholar?cites=9413040909214203914&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Proceedings of the IEEE, 2018'
        events:
            saw title: '2024-12-12T21:41:04.424Z'
            saw abstract: '2024-12-13T00:59:40.912Z'
        abstract: ' We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent’s actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which containing annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.'
        pdfWasDownloaded: true
        accordingTo:
            $manuallyEntered: {}
    Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks: &ref_27
        title: Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - Y Wu
            - S Kasewa
            - O Groth
            - S Salter
            - L Sun
            - OP Jones…
        link: 'https://openreview.net/forum?id=BkeyOxrYwH'
        pdfLink: 'https://openreview.net/pdf?id=BkeyOxrYwH'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|title
        reasonsNotRelevant: []
        relevanceStages:
            - title
        possibleYear: '2019'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '2961549995427504858'
        multiArticleId: null
        citedByLink: 'https://scholar.google.com//scholar?cites=2961549995427504858&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: '2019'
        events:
            saw title: '2024-12-12T21:41:38.912Z'
        accordingTo:
            $manuallyEntered: {}
    'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text': &ref_28
        title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
        doi: 10.18653/v1/2024.conll-1.27
        year: null
        publisherFlags: null
        authorNames:
            - S Adak
            - D Agrawal
            - A Mukherjee
        link: 'https://aclanthology.org/2024.conll-1.27/'
        pdfLink: 'https://aclanthology.org/2024.conll-1.27.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|title
        reasonsNotRelevant: []
        relevanceStages:
            - title
        possibleYear: '2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: null
        multiArticleId: null
        citedByLink: null
        publisherInfo: 'Proceedings of the 28th, 2024'
        events:
            saw title: '2024-12-12T21:43:23.737Z'
        accordingTo:
            $manuallyEntered: {}
    Learning affordances in object-centric generative models: &ref_29
        title: Learning affordances in object-centric generative models
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - Y Wu
            - S Kasewa
            - O Groth
            - S Salter
            - L Sun…
        link: 'https://ora.ox.ac.uk/objects/uuid:003cbbd9-a3aa-42e7-8e2d-bcc6b22db89a'
        pdfLink: 'https://ora.ox.ac.uk/objects/uuid:003cbbd9-a3aa-42e7-8e2d-bcc6b22db89a/download_file?safe_filename=OOL_7.pdf&file_format=pdf&type_of_work=Conference+item'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|abstract
        reasonsNotRelevant: []
        relevanceStages:
            - title
            - abstract
        possibleYear: '2020'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '7478229689628350853'
        multiArticleId: '7478229689628350853'
        citedByLink: 'https://scholar.google.com//scholar?cites=7478229689628350853&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: '2020'
        events:
            saw title: '2024-12-12T21:43:33.262Z'
            saw abstract: '2024-12-13T00:54:58.419Z'
        abstract: 'Given visual observations of a reaching task together with a stick-like tool, we propose a novel approach that learns to exploit task-relevant object affordances by combining generative modelling with a task-based performance predictor. The embedding learned by the generative model captures the factors of variation in object geometry, e.g. length, width, and configuration. The performance predictor identifies sub-manifolds correlated with task success in a weakly supervised manner. Using a 3D simulation environment, we demonstrate that traversing the latent space in this task-driven way results in appropriate tool geometries for the task at hand. Our results suggest that affordances are encoded along smooth trajectories in the learned latent space. Given only high-level performance criteria (such as task success), accessing these emergent affordances via gradient descent enables the agent to manipulate learned object geometries in a targeted and deliberate way. '
        pdfWasDownloaded: true
        accordingTo:
            $manuallyEntered: {}
    Applications of machine learning to ecological modelling: &ref_10
        title: Applications of machine learning to ecological modelling
        doi: 10.1016/s0304-3800(01)00316-7
        year: null
        publisherFlags: null
        authorNames:
            - F Recknagel
        link: 'https://www.sciencedirect.com/science/article/pii/S0304380001003167'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2001'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '16336708018187229886'
        multiArticleId: '16336708018187229886'
        citedByLink: 'https://scholar.google.com//scholar?cites=16336708018187229886&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Ecological modelling, 2001'
        events:
            added: '2024-12-12T23:32:47.064Z'
            saw title: '2024-12-13T00:33:53.566Z'
        accordingTo:
            $manuallyEntered: {}
    'Machine learning in landscape ecological analysis: a review of recent approaches': &ref_11
        title: 'Machine learning in landscape ecological analysis: a review of recent approaches'
        doi: 10.1007/s10980-021-01366-9
        year: null
        publisherFlags: null
        authorNames:
            - MS Stupariu
            - SA Cushman
            - AI Pleşoianu
        link: 'https://link.springer.com/article/10.1007/s10980-021-01366-9'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2022'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '17693767866700412269'
        multiArticleId: '17693767866700412269'
        citedByLink: 'https://scholar.google.com//scholar?cites=17693767866700412269&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Ecology, 2022'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:00.529Z'
        accordingTo:
            $manuallyEntered: {}
    'Study becomes insight: ecological learning from machine learning': &ref_12
        title: 'Study becomes insight: ecological learning from machine learning'
        doi: 10.1111/2041-210x.13686
        year: null
        publisherFlags: null
        authorNames:
            - Q Yu
            - W Ji
            - L Prihodko
            - CW Ross
        link: 'https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13686'
        pdfLink: 'https://besjournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/2041-210X.13686'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: unclear|title
        reasonsNotRelevant: []
        relevanceStages: []
        possibleYear: '2021'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '2395320027051734214'
        multiArticleId: '2395320027051734214'
        citedByLink: 'https://scholar.google.com//scholar?cites=2395320027051734214&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Methods in Ecology, 2021'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:26.466Z'
        accordingTo:
            $manuallyEntered: {}
    Machine learning and deep learning—A review for ecologists: &ref_13
        title: Machine learning and deep learning—A review for ecologists
        doi: 10.1111/2041-210x.14061
        year: null
        publisherFlags: null
        authorNames:
            - M Pichler
            - F Hartig
        link: 'https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.14061'
        pdfLink: 'https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.14061'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2023'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '9552878792133591305'
        multiArticleId: '9552878792133591305'
        citedByLink: 'https://scholar.google.com//scholar?cites=9552878792133591305&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Methods in Ecology and Evolution, 2023'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:15.907Z'
        accordingTo:
            $manuallyEntered: {}
    A review of supervised machine learning algorithms and their applications to ecological data: &ref_14
        title: A review of supervised machine learning algorithms and their applications to ecological data
        doi: 10.1016/j.ecolmodel.2012.03.001
        year: null
        publisherFlags: null
        authorNames:
            - C Crisci
            - B Ghattas
            - G Perera
        link: 'https://www.sciencedirect.com/science/article/pii/S0304380012001081'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2012'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '17952662263087878820'
        multiArticleId: '17952662263087878820'
        citedByLink: 'https://scholar.google.com//scholar?cites=17952662263087878820&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Ecological Modelling, 2012'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:48.798Z'
        accordingTo:
            $manuallyEntered: {}
    'Application of machine-learning methods in forest ecology: recent progress and future challenges': &ref_15
        title: 'Application of machine-learning methods in forest ecology: recent progress and future challenges'
        doi: 10.1139/er-2018-0034
        year: null
        publisherFlags: null
        authorNames:
            - Z Liu
            - C Peng
            - T Work
            - JN Candau
        link: 'https://cdnsciencepub.com/doi/abs/10.1139/er-2018-0034'
        pdfLink: 'https://www.jstor.org/stable/pdf/90026557.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2018'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '4453085440788621880'
        multiArticleId: '4453085440788621880'
        citedByLink: 'https://scholar.google.com//scholar?cites=4453085440788621880&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Environmental, 2018'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:41.241Z'
        accordingTo:
            $manuallyEntered: {}
    Machine learning of poorly predictable ecological data: &ref_16
        title: Machine learning of poorly predictable ecological data
        doi: 10.1016/j.ecolmodel.2005.11.015
        year: null
        publisherFlags: null
        authorNames:
            - Y Shan
            - D Paull
            - RI McKay
        link: 'https://www.sciencedirect.com/science/article/pii/S0304380005005843'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2006'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '16457976426863564747'
        multiArticleId: '16457976426863564747'
        citedByLink: 'https://scholar.google.com//scholar?cites=16457976426863564747&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Ecological Modelling, 2006'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:51.950Z'
        accordingTo:
            $manuallyEntered: {}
    Deep learning as a tool for ecology and evolution: &ref_17
        title: Deep learning as a tool for ecology and evolution
        doi: 10.32942/osf.io/nt3as
        year: null
        publisherFlags: null
        authorNames:
            - ML Borowiec
            - RB Dikow
            - PB Frandsen
        link: 'https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13901'
        pdfLink: 'https://besjournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/2041-210X.13901'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2022'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '13767141339350237515'
        multiArticleId: '13767141339350237515'
        citedByLink: 'https://scholar.google.com//scholar?cites=13767141339350237515&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Methods in Ecology, 2022'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:18.070Z'
        accordingTo:
            $manuallyEntered: {}
    Towards machine learning of predictive models from ecological data: &ref_18
        title: Towards machine learning of predictive models from ecological data
        doi: 10.1007/978-3-319-23708-4_11
        year: null
        publisherFlags: null
        authorNames:
            - A Tamaddoni-Nezhad
            - D Bohan
            - A Raybould
        link: 'https://link.springer.com/chapter/10.1007/978-3-319-23708-4_11'
        pdfLink: 'https://openresearch.surrey.ac.uk/view/pdfCoverPage?instCode=44SUR_INST&filePid=13145139490002346&download=true'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2014'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '14916557987462187508'
        multiArticleId: '14916557987462187508'
        citedByLink: 'https://scholar.google.com//scholar?cites=14916557987462187508&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Conference, ILP 2014, 2015'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:33:44.390Z'
        accordingTo:
            $manuallyEntered: {}
    'Machine learning in marine ecology: an overview of techniques and applications': &ref_19
        title: 'Machine learning in marine ecology: an overview of techniques and applications'
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - P Rubbens
            - S Brodie
            - T Cordier
        link: 'https://academic.oup.com/icesjms/article-abstract/80/7/1829/7236451'
        pdfLink: 'https://academic.oup.com/icesjms/article-pdf/80/7/1829/51951447/fsad100.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        reasonsNotRelevant:
            - title
        relevanceStages: []
        possibleYear: '2023'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        discoveryMethod: null
        citationId: '6370710186406620474'
        multiArticleId: '6370710186406620474'
        citedByLink: 'https://scholar.google.com//scholar?cites=6370710186406620474&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'ICES Journal of, 2023'
        events:
            added: '2024-12-12T23:32:47.065Z'
            saw title: '2024-12-13T00:32:12.720Z'
        accordingTo:
            $manuallyEntered: {}
    Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation: &ref_0
        title: Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation
        doi: 10.1101/2021.06.16.448730
        year: null
        publisherFlags: null
        authorNames:
            - A Nayebi
            - NCL Kong
            - C Zhuang
        link: 'https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011506'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: super-relevent|abstract
        possibleYear: '2023'
        reasonsNotRelevant: []
        relevanceStages:
            - title
            - abstract
        isCitedBy: null
        discoveryMethod: null
        citationId: '2471520982509518470'
        multiArticleId: '2471520982509518470'
        citedByLink: 'https://scholar.google.com//scholar?cites=2471520982509518470&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'PLOS Computational, 2023'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:30:51.400Z'
        abstract: 'Studies of the mouse visual system have revealed a variety of visual brain areas that are thought to support a multitude of behavioral capacities, ranging from stimulus-reward associations, to goal-directed navigation, and object-centric discriminations. However, an overall understanding of the mouse’s visual cortex, and how it supports a range of behaviors, remains unknown. Here, we take a computational approach to help address these questions, providing a high-fidelity quantitative model of mouse visual cortex and identifying key structural and functional principles underlying that model’s success. Structurally, we find that a comparatively shallow network structure with a low-resolution input is optimal for modeling mouse visual cortex. Our main finding is functional—that models trained with task-agnostic, self-supervised objective functions based on the concept of contrastive embeddings are much better matches to mouse cortex, than models trained on supervised objectives or alternative self-supervised methods. This result is very much unlike in primates where prior work showed that the two were roughly equivalent, naturally leading us to ask the question of why these self-supervised objectives are better matches than supervised ones in mouse. To this end, we show that the self-supervised, contrastive objective builds a general-purpose visual representation that enables the system to achieve better transfer on out-of-distribution visual scene understanding and reward-based navigation tasks. Our results suggest that mouse visual cortex is a low-resolution, shallow network that makes best use of the mouse’s limited resources to create a light-weight, general-purpose visual system—in contrast to the deep, high-resolution, and more categorization-dominated visual system of primates.'
        accordingTo:
            $manuallyEntered: {}
    'Decoding the brain: From neural representations to mechanistic models': &ref_1
        title: 'Decoding the brain: From neural representations to mechanistic models'
        doi: 10.1016/j.cell.2024.08.051
        year: null
        publisherFlags: null
        authorNames:
            - MW Mathis
            - AP Rotondo
            - EF Chang
            - AS Tolias
        link: 'https://www.cell.com/cell/fulltext/S0092-8674(24)00980-2'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: unclear|title
        possibleYear: '2024'
        reasonsNotRelevant: []
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: '11232172302713992781'
        multiArticleId: '11232172302713992781'
        citedByLink: 'https://scholar.google.com//scholar?cites=11232172302713992781&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Cell, 2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:32:20.239Z'
        accordingTo:
            $manuallyEntered: {}
    Unsupervised learning of mid-level visual representations: &ref_2
        title: Unsupervised learning of mid-level visual representations
        doi: 10.1016/j.conb.2023.102834
        year: null
        publisherFlags: null
        authorNames:
            - G Matteucci
            - E Piasini
            - D Zoccolan
        link: 'https://www.sciencedirect.com/science/article/pii/S0959438823001599'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        possibleYear: '2024'
        reasonsNotRelevant:
            - title
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: '15740752807531591599'
        multiArticleId: '15740752807531591599'
        citedByLink: 'https://scholar.google.com//scholar?cites=15740752807531591599&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Current opinion in neurobiology, 2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:31:08.609Z'
        accordingTo:
            $manuallyEntered: {}
    Multimodal deep learning model unveils behavioral dynamics of V1 activity in freely moving mice: &ref_3
        title: Multimodal deep learning model unveils behavioral dynamics of V1 activity in freely moving mice
        doi: 10.1101/2023.05.30.542912
        year: null
        publisherFlags: null
        authorNames:
            - A Xu
            - Y Hou
            - C Niell
            - M Beyeler
        link: 'https://proceedings.neurips.cc/paper_files/paper/2023/hash/31a19921acd38cdf7a8c86ec032cef2d-Abstract-Conference.html'
        pdfLink: 'https://proceedings.neurips.cc/paper_files/paper/2023/file/31a19921acd38cdf7a8c86ec032cef2d-Paper-Conference.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        possibleYear: '2024'
        reasonsNotRelevant:
            - title
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: '10466266600843421970'
        multiArticleId: '10466266600843421970'
        citedByLink: 'https://scholar.google.com//scholar?cites=10466266600843421970&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Advances in Neural, 2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:32:42.659Z'
        accordingTo:
            $manuallyEntered: {}
    Integrating statistical and machine learning approaches for neural classification: &ref_4
        title: Integrating statistical and machine learning approaches for neural classification
        doi: 10.1109/access.2022.3221436
        year: null
        publisherFlags: null
        authorNames:
            - M Sarmashghi
            - SP Jadhav
            - UT Eden
        link: 'https://ieeexplore.ieee.org/abstract/document/9945938/'
        pdfLink: 'https://ieeexplore.ieee.org/iel7/6287639/9668973/09945938.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        possibleYear: '2022'
        reasonsNotRelevant:
            - title
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: '8883119780015359249'
        multiArticleId: '8883119780015359249'
        citedByLink: 'https://scholar.google.com//scholar?cites=8883119780015359249&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'IEEE Access, 2022'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:33:37.080Z'
        accordingTo:
            $manuallyEntered: {}
    Top-down perceptual inference shaping the activity of early visual cortex: &ref_5
        title: Top-down perceptual inference shaping the activity of early visual cortex
        doi: 10.1101/2023.11.29.569262
        year: null
        publisherFlags: null
        authorNames:
            - F Csikor
            - B Meszna
            - G Orbn
        link: 'https://www.biorxiv.org/content/10.1101/2023.11.29.569262.abstract'
        pdfLink: 'https://www.biorxiv.org/content/biorxiv/early/2023/12/02/2023.11.29.569262.full.pdf'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: relevent|title
        possibleYear: '2023'
        reasonsNotRelevant: []
        relevanceStages:
            - title
        isCitedBy: null
        discoveryMethod: null
        citationId: '17366242440903237122'
        multiArticleId: '17366242440903237122'
        citedByLink: 'https://scholar.google.com//scholar?cites=17366242440903237122&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'BioRxiv, 2023'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:31:25.585Z'
        accordingTo:
            $manuallyEntered: {}
    Population encoding of stimulus features along the visual hierarchy: &ref_6
        title: Population encoding of stimulus features along the visual hierarchy
        doi: 10.1073/pnas.2317773121
        year: null
        publisherFlags: null
        authorNames:
            - L Dyballa
            - AM Rudzite
            - MS Hoseini
        link: 'https://www.pnas.org/doi/abs/10.1073/pnas.2317773121'
        pdfLink: 'https://www.pnas.org/doi/pdf/10.1073/pnas.2317773121'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        possibleYear: '2024'
        reasonsNotRelevant:
            - title
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: '10526467924589966000'
        multiArticleId: '10526467924589966000'
        citedByLink: 'https://scholar.google.com//scholar?cites=10526467924589966000&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'Proceedings of the, 2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:32:51.107Z'
        accordingTo:
            $manuallyEntered: {}
    'Integrative, Embodied Agents to Reverse-Engineer Natural Intelligence': &ref_7
        title: 'Integrative, Embodied Agents to Reverse-Engineer Natural Intelligence'
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - A Nayebi
        link: 'https://kilthub.cmu.edu/ndownloader/files/49110250'
        pdfLink: 'https://kilthub.cmu.edu/ndownloader/files/49110250'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: unclear|title
        possibleYear: '2024'
        reasonsNotRelevant: []
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: null
        multiArticleId: '15381445104070421971'
        citedByLink: null
        publisherInfo: ''
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:13:04.397Z'
        accordingTo:
            $manuallyEntered: {}
    Heterogeneous orientation tuning in the primary visual cortex of mice diverges from Gabor-like receptive fields in primates: &ref_8
        title: Heterogeneous orientation tuning in the primary visual cortex of mice diverges from Gabor-like receptive fields in primates
        doi: 10.1016/j.celrep.2024.114639
        year: null
        publisherFlags: null
        authorNames:
            - J Fu
            - PA Pierzchlewicz
            - KF Willeke
            - M Bashiri
        link: 'https://www.cell.com/cell-reports/fulltext/S2211-1247(24)00989-6'
        pdfLink: null
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: irrelevent|title
        possibleYear: '2024'
        reasonsNotRelevant:
            - title
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: null
        multiArticleId: '12707078753044341556'
        citedByLink: null
        publisherInfo: 'Cell reports, 2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:32:06.511Z'
        accordingTo:
            $manuallyEntered: {}
    Self-supervised learning facilitates neural representation structures that can be unsupervisedly aligned to human behaviors: &ref_9
        title: Self-supervised learning facilitates neural representation structures that can be unsupervisedly aligned to human behaviors
        doi: null
        year: null
        publisherFlags: null
        authorNames:
            - S Takahashi
            - M Sasaki
            - K Takeda
        link: 'https://openreview.net/forum?id=MaFIvgBceX'
        pdfLink: 'https://openreview.net/pdf?id=MaFIvgBceX'
        cites: {}
        citedBy: {}
        sources: {}
        resumeStatus: unclear|title
        possibleYear: '2024'
        reasonsNotRelevant: []
        relevanceStages: []
        isCitedBy: null
        discoveryMethod: null
        citationId: '7385240804493898907'
        multiArticleId: '7385240804493898907'
        citedByLink: 'https://scholar.google.com//scholar?cites=7385240804493898907&as_sdt=5,44&sciodt=0,44&hl=en&oe=ASCII'
        publisherInfo: 'ICLR 2024 Workshop on, 2024'
        notesConsideredRelevent: null
        notesCustomKeywords: []
        notesComment: null
        notesWasRelatedTo: []
        notesIsCitedByTitles: []
        notesCites: []
        events:
            added: '2024-12-12T23:45:35.827Z'
            saw title: '2024-12-13T00:30:37.491Z'
        accordingTo:
            $manuallyEntered: {}
discoveryAttempts:
    -
        query: ecologically-general representation machine learning
        date: 12/12/2024
        dateTime: 1734047135516
        searchEngine: scholar
        referenceLinks:
            -
                hadBeenSeenBefore: false
                title: Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation
                link: *ref_0
            -
                hadBeenSeenBefore: false
                title: 'Decoding the brain: From neural representations to mechanistic models'
                link: *ref_1
            -
                hadBeenSeenBefore: false
                title: Unsupervised learning of mid-level visual representations
                link: *ref_2
            -
                hadBeenSeenBefore: false
                title: Multimodal deep learning model unveils behavioral dynamics of V1 activity in freely moving mice
                link: *ref_3
            -
                hadBeenSeenBefore: false
                title: Integrating statistical and machine learning approaches for neural classification
                link: *ref_4
            -
                hadBeenSeenBefore: false
                title: Top-down perceptual inference shaping the activity of early visual cortex
                link: *ref_5
            -
                hadBeenSeenBefore: false
                title: Population encoding of stimulus features along the visual hierarchy
                link: *ref_6
            -
                hadBeenSeenBefore: false
                title: 'Integrative, Embodied Agents to Reverse-Engineer Natural Intelligence'
                link: *ref_7
            -
                hadBeenSeenBefore: false
                title: Heterogeneous orientation tuning in the primary visual cortex of mice diverges from Gabor-like receptive fields in primates
                link: *ref_8
            -
                hadBeenSeenBefore: false
                title: Self-supervised learning facilitates neural representation structures that can be unsupervisedly aligned to human behaviors
                link: *ref_9
    -
        query: ecological representation machine learning
        date: 12/12/2024
        dateTime: 1734046366799
        searchEngine: scholar
        referenceLinks:
            -
                hadBeenSeenBefore: false
                title: Applications of machine learning to ecological modelling
                link: *ref_10
            -
                hadBeenSeenBefore: false
                title: 'Machine learning in landscape ecological analysis: a review of recent approaches'
                link: *ref_11
            -
                hadBeenSeenBefore: false
                title: 'Study becomes insight: ecological learning from machine learning'
                link: *ref_12
            -
                hadBeenSeenBefore: false
                title: Machine learning and deep learning—A review for ecologists
                link: *ref_13
            -
                hadBeenSeenBefore: false
                title: A review of supervised machine learning algorithms and their applications to ecological data
                link: *ref_14
            -
                hadBeenSeenBefore: false
                title: 'Application of machine-learning methods in forest ecology: recent progress and future challenges'
                link: *ref_15
            -
                hadBeenSeenBefore: false
                title: Machine learning of poorly predictable ecological data
                link: *ref_16
            -
                hadBeenSeenBefore: false
                title: Deep learning as a tool for ecology and evolution
                link: *ref_17
            -
                hadBeenSeenBefore: false
                title: Towards machine learning of predictive models from ecological data
                link: *ref_18
            -
                hadBeenSeenBefore: false
                title: 'Machine learning in marine ecology: an overview of techniques and applications'
                link: *ref_19
    -
        query: affordance based encoder imagination prediction
        date: 12/12/2024
        dateTime: 1734020081395
        searchEngine: scholar
        referenceLinks:
            -
                hadBeenSeenBefore: false
                title: Visual affordance prediction for guiding robot exploration
                link: *ref_20
            -
                hadBeenSeenBefore: false
                title: 'RAIL: Robot Affordance Imagination with Large Language Models'
                link: *ref_21
            -
                hadBeenSeenBefore: false
                title: Affordance-Based Goal Imagination for Embodied AI Agents
                link: *ref_22
            -
                hadBeenSeenBefore: false
                title: Learning to anticipate egocentric actions by imagination
                link: *ref_23
            -
                hadBeenSeenBefore: false
                title: Imagine that! Leveraging emergent affordances for 3d tool synthesis
                link: *ref_24
            -
                hadBeenSeenBefore: false
                title: What can i do here? learning new skills by imagining visual affordances
                link: *ref_25
            -
                hadBeenSeenBefore: false
                title: 'Learning to act properly: Predicting and explaining affordances from images'
                link: *ref_26
            -
                hadBeenSeenBefore: false
                title: Imagine that! leveraging emergent affordances for tool synthesis in reaching tasks
                link: *ref_27
            -
                hadBeenSeenBefore: false
                title: 'Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text'
                link: *ref_28
            -
                hadBeenSeenBefore: false
                title: Learning affordances in object-centric generative models
                link: *ref_29
